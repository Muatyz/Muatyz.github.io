<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>FOR FOUNDATIONAL DISCOVERIES AND INVENTIONS THAT ENABLE MACHINE LEARNING WITH ARTIFICIAL NEURAL NETWORKS | æ— å¤„æƒ¹å°˜åŸƒ</title>
<meta name="keywords" content="">
<meta name="description" content="2024 å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/reference/2024-nobel-prize-in-physics/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/reference/2024-nobel-prize-in-physics/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">





<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="FOR FOUNDATIONAL DISCOVERIES AND INVENTIONS THAT ENABLE MACHINE LEARNING WITH ARTIFICIAL NEURAL NETWORKS" />
<meta property="og:description" content="2024 å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/reference/2024-nobel-prize-in-physics/" />
<meta property="og:image" content="https://s2.loli.net/2025/10/08/jsH1leAN6qnIV4J.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-05T00:18:23+08:00" />
<meta property="article:modified_time" content="2022-05-05T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/10/08/jsH1leAN6qnIV4J.png" />
<meta name="twitter:title" content="FOR FOUNDATIONAL DISCOVERIES AND INVENTIONS THAT ENABLE MACHINE LEARNING WITH ARTIFICIAL NEURAL NETWORKS"/>
<meta name="twitter:description" content="2024 å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ“• é˜…è¯»",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "ğŸ“• æ–‡çŒ®",
          "item": "https://Muatyz.github.io/posts/read/reference/"
        }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "FOR FOUNDATIONAL DISCOVERIES AND INVENTIONS THAT ENABLE MACHINE LEARNING WITH ARTIFICIAL NEURAL NETWORKS",
      "item": "https://Muatyz.github.io/posts/read/reference/2024-nobel-prize-in-physics/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "FOR FOUNDATIONAL DISCOVERIES AND INVENTIONS THAT ENABLE MACHINE LEARNING WITH ARTIFICIAL NEURAL NETWORKS",
  "name": "FOR FOUNDATIONAL DISCOVERIES AND INVENTIONS THAT ENABLE MACHINE LEARNING WITH ARTIFICIAL NEURAL NETWORKS",
  "description": "2024 å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–",
  "keywords": [
    ""
  ],
  "articleBody": " The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Physics 2024 jointly to John J. Hopfield and Geoffrey Hinton â€œfor foundational discoveries and inventions that enable machine learning with artificial neural networksâ€.\nç‘å…¸çš‡å®¶ç§‘å­¦é™¢å†³å®šå°† 2024 å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–è”åˆæˆäºˆçº¦ç¿°-éœæ™®è²å°”å¾·ï¼ˆJohn J. Hopfieldï¼‰å’Œæ°å¼—é‡Œ-è¾›é¡¿ï¼ˆGeoffrey Hintonï¼‰ï¼Œâ€œä»¥è¡¨å½°ä»–ä»¬åˆ©ç”¨äººå·¥ç¥ç»ç½‘ç»œå®ç°æœºå™¨å­¦ä¹ çš„å¥ åŸºæ€§å‘ç°å’Œå‘æ˜â€ã€‚\nIntroduction With its roots in the 1940s, machine learning based on artificial neural networks (ANNs) has developed over the past three decades into a versatile and powerful tool, with both everyday and advanced scientific applications. With ANNs the boundaries of physics are extended to host phenomena of life as well as computation.\nèµ·æºäº 20 ä¸–çºª 40 å¹´ä»£çš„åŸºäº äººå·¥ç¥ç»ç½‘ç»œ(ANN) çš„æœºå™¨å­¦ä¹ ï¼Œåœ¨è¿‡å»çš„ä¸‰åå¹´ä¸­å‘å±•æˆä¸ºä¸€ç§å¤šåŠŸèƒ½ä¸”å¼ºå¤§çš„å·¥å…·ï¼Œæ—¢æœ‰æ—¥å¸¸åº”ç”¨ï¼Œä¹Ÿæœ‰å…ˆè¿›çš„ç§‘å­¦åº”ç”¨ã€‚é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œï¼Œç‰©ç†å­¦çš„è¾¹ç•Œå¾—ä»¥æ‰©å±•ï¼Œä»¥å®¹çº³ç”Ÿå‘½ç°è±¡å’Œè®¡ç®—ç°è±¡ã€‚\nInspired by biological neurons in the brain, ANNs are large collections of â€œneuronsâ€, or nodes, connected by â€œsynapsesâ€, or weighted couplings, which are trained to perform certain tasks rather than asked to execute a predetermined set of instructions. Their basic structure has close similarities with spin models in statistical physics applied to magnetism or alloy theory. This yearâ€™s Nobel Prize in Physics recognizes research exploiting this connection to make breakthrough methodological advances in the field of ANN.\nå—å¤§è„‘ä¸­ç”Ÿç‰©ç¥ç»å…ƒçš„å¯å‘ï¼Œäººå·¥ç¥ç»ç½‘ç»œæ˜¯ç”±å¤§é‡â€œç¥ç»å…ƒâ€æˆ–èŠ‚ç‚¹ç»„æˆçš„é›†åˆï¼Œè¿™äº›èŠ‚ç‚¹é€šè¿‡â€œçªè§¦â€æˆ–åŠ æƒè€¦åˆè¿æ¥åœ¨ä¸€èµ·ï¼Œç»è¿‡è®­ç»ƒä»¥æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œè€Œä¸æ˜¯æ‰§è¡Œé¢„å®šçš„æŒ‡ä»¤é›†ã€‚å®ƒä»¬çš„åŸºæœ¬ç»“æ„ä¸åº”ç”¨äºç£å­¦æˆ–åˆé‡‘ç†è®ºçš„ç»Ÿè®¡ç‰©ç†ä¸­çš„è‡ªæ—‹æ¨¡å‹æœ‰ç€å¯†åˆ‡çš„ç›¸ä¼¼æ€§ã€‚ä»Šå¹´çš„è¯ºè´å°”ç‰©ç†å­¦å¥–è¡¨å½°äº†åˆ©ç”¨è¿™ç§è”ç³»åœ¨äººå·¥ç¥ç»ç½‘ç»œé¢†åŸŸå–å¾—çªç ´æ€§æ–¹æ³•è¿›å±•çš„ç ”ç©¶ã€‚\nHistorical background The first electronic-based computers appeared in the 1940s, and were invented for military and scientific purposes. They were intended to carry out computations that were cumbersome and time-consuming for humans. In the 1950s, the opposite need emerged, namely to get computers to do what humans and other mammals are good at â€“ pattern recognition.\n20 ä¸–çºª 40 å¹´ä»£å‡ºç°äº†ç¬¬ä¸€æ‰¹åŸºäºç”µå­çš„è®¡ç®—æœºï¼Œæœ€åˆæ˜¯ä¸ºå†›äº‹å’Œç§‘å­¦ç›®çš„è€Œå‘æ˜çš„ã€‚å®ƒä»¬æ—¨åœ¨æ‰§è¡Œå¯¹äººç±»æ¥è¯´ç¹çä¸”è€—æ—¶çš„è®¡ç®—ã€‚20 ä¸–çºª 50 å¹´ä»£ï¼Œå‡ºç°äº†ç›¸åçš„éœ€æ±‚ï¼Œå³è®©è®¡ç®—æœºèƒ½å¤Ÿå®Œæˆé‚£äº›äººç±»å’Œå…¶ä»–å“ºä¹³åŠ¨ç‰©æ“…é•¿çš„ä»»åŠ¡â€”â€”æ¨¡å¼è¯†åˆ«ã€‚\nThis artificial intelligence-oriented objective was first approached by mathematicians and computer scientists, who developed programs based on logical rules. This approach was pursued until the 1980s, but the computational resources that were required for the exact classifications, for example, of images became prohibitive.\nè¿™ä¸€é¢å‘äººå·¥æ™ºèƒ½çš„ç›®æ ‡æœ€åˆç”±æ•°å­¦å®¶å’Œè®¡ç®—æœºç§‘å­¦å®¶æå‡ºï¼Œä»–ä»¬å¼€å‘äº†åŸºäºé€»è¾‘è§„åˆ™çš„ç¨‹åºã€‚è¿™ç§æ–¹æ³•ä¸€ç›´æŒç»­åˆ° 20 ä¸–çºª 80 å¹´ä»£ï¼Œä½†æ‰€éœ€çš„è®¡ç®—èµ„æºå˜å¾—è¿‡äºåºå¤§ï¼Œä¾‹å¦‚å¯¹å›¾åƒè¿›è¡Œç²¾ç¡®åˆ†ç±»ã€‚\nIn parallel, efforts had been initiated to find out how biological systems solve the pattern recognition problem. As early as 1943, Warren McCulloch and Walter Pitts, a neuroscientist and a logician, respectively, had proposed a model for how the neurons in the brain cooperate. In their model, a neuron formed a weighted sum of binary incoming signals from other neurons, which determined a binary outgoing signal. Their work became a launch pad for later research into both biological and artificial neural networks.\nä¸æ­¤åŒæ—¶ï¼Œäººä»¬å¼€å§‹åŠªåŠ›ç ”ç©¶ç”Ÿç‰©ç³»ç»Ÿå¦‚ä½•è§£å†³æ¨¡å¼è¯†åˆ«é—®é¢˜ã€‚æ—©åœ¨ 1943 å¹´ï¼Œç¥ç»ç§‘å­¦å®¶ Warren McCulloch å’Œé€»è¾‘å­¦å®¶ Walter Pitts å°±æå‡ºäº†ä¸€ä¸ªå…³äºå¤§è„‘ä¸­ç¥ç»å…ƒå¦‚ä½•åä½œçš„æ¨¡å‹ã€‚åœ¨ä»–ä»¬çš„æ¨¡å‹ä¸­ï¼Œç¥ç»å…ƒå¯¹æ¥è‡ªå…¶ä»–ç¥ç»å…ƒçš„äºŒè¿›åˆ¶è¾“å…¥ä¿¡å·è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œä»è€Œå†³å®šè¾“å‡ºçš„äºŒè¿›åˆ¶ä¿¡å·ã€‚ä»–ä»¬çš„å·¥ä½œæˆä¸ºåç»­ç”Ÿç‰©å’Œäººå·¥ç¥ç»ç½‘ç»œç ”ç©¶çš„èµ·ç‚¹ã€‚\nAnother influential early contribution came from the psychologist Donald Hebb. In 1949, Hebb proposed a mechanism for learning and memories, where the simultaneous and repeated activation of two neurons leads to an increased strength of the synapse between them.\nå¦ä¸€é¡¹æœ‰å½±å“åŠ›çš„æ—©æœŸè´¡çŒ®æ¥è‡ªå¿ƒç†å­¦å®¶ Donald Hebbã€‚1949 å¹´ï¼ŒHebb æå‡ºäº†ä¸€ç§å­¦ä¹ å’Œè®°å¿†æœºåˆ¶ï¼Œå³ä¸¤ä¸ªç¥ç»å…ƒçš„åŒæ—¶å’Œåå¤æ¿€æ´»ä¼šå¯¼è‡´å®ƒä»¬ä¹‹é—´çªè§¦å¼ºåº¦çš„å¢åŠ ã€‚\nIn the ANN area, two architectures for systems of interconnected nodes were explored, â€œrecurrentâ€ and â€œfeedforwardâ€ networks, where the former allows for feedback interactions (Figures 1 and 2). A feedforward network has input and output layers and may also contain additional layers of hidden nodes sandwiched in-between.\nåœ¨äººå·¥ç¥ç»ç½‘ç»œé¢†åŸŸï¼Œæ¢ç´¢äº†ä¸¤ç§äº’è¿èŠ‚ç‚¹ç³»ç»Ÿçš„æ¶æ„ï¼šâ€œé€’å½’â€å’Œâ€œå‰é¦ˆâ€ç½‘ç»œï¼Œå‰è€…å…è®¸åé¦ˆäº¤äº’ï¼ˆå›¾ 1 å’Œå›¾ 2ï¼‰ã€‚å‰é¦ˆç½‘ç»œå…·æœ‰è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ï¼Œå¹¶ä¸”å¯èƒ½åŒ…å«å¤¹åœ¨ä¸­é—´çš„éšè—èŠ‚ç‚¹çš„é™„åŠ å±‚ã€‚\nIn 1957, Frank Rosenblatt proposed a feedforward network for image interpretation, which was also implemented in computer hardware. It had three layers of nodes, with adjustable weights only between the middle and output layers. Those weights were determined in a systematic fashion.\n1957 å¹´ï¼ŒFrank Rosenblatt æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒè§£é‡Šçš„å‰é¦ˆç½‘ç»œï¼Œå¹¶åœ¨è®¡ç®—æœºç¡¬ä»¶ä¸­å®ç°ã€‚å®ƒæœ‰ä¸‰å±‚èŠ‚ç‚¹ï¼Œåªæœ‰ä¸­é—´å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´çš„æƒé‡æ˜¯å¯è°ƒçš„ã€‚è¿™äº›æƒé‡æ˜¯ä»¥ç³»ç»ŸåŒ–çš„æ–¹å¼ç¡®å®šçš„ã€‚\nRosenblattâ€™s system attracted considerable attention, but it had limitations when it came to nonlinear problems. A simple example is the â€œone or the other but not bothâ€ (XOR) problem. These limitations were pointed out in an influential book by Marvin Minsky and Seymour Papert in 1969, which led to a hiatus funding-wise for ANN research.\nRosenblatt çš„ç³»ç»Ÿå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä½†åœ¨å¤„ç†éçº¿æ€§é—®é¢˜æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¸€ä¸ªç®€å•çš„ä¾‹å­æ˜¯â€œè¦ä¹ˆè¿™ä¸ªï¼Œè¦ä¹ˆé‚£ä¸ªï¼Œä½†ä¸èƒ½åŒæ—¶æ˜¯ä¸¤ä¸ªâ€ï¼ˆXORï¼‰é—®é¢˜ã€‚è¿™äº›å±€é™æ€§åœ¨ Marvin Minsky å’Œ Seymour Papert äº 1969 å¹´å‡ºç‰ˆçš„ä¸€æœ¬æœ‰å½±å“åŠ›çš„ä¹¦ä¸­è¢«æŒ‡å‡ºï¼Œè¿™å¯¼è‡´äº†äººå·¥ç¥ç»ç½‘ç»œç ”ç©¶èµ„é‡‘çš„ä¸­æ–­ã€‚\nA parallel development took inspiration from magnetic systems, which were to create models for recurrent neural networks and investigate their collective properties.\nç”±ç£æ€§ç³»ç»Ÿçš„å¹³è¡Œå‘å±•å—åˆ°å¯å‘ï¼Œåˆ›å»ºäº†é€’å½’ç¥ç»ç½‘ç»œçš„æ¨¡å‹å¹¶ç ”ç©¶å…¶é›†ä½“å±æ€§ã€‚\nRecurrent networks of $N$ binary nodes $s_i$ (0 or 1), with connection weights $w_{ij}$. (Left) The Hopfield model. (Centre) Boltzmann machine. The nodes are divided into two groups, visible (open circles) and hidden (grey) nodes. The network is trained to approximate the probability distribution of a given set of visible patterns. Once trained, the network can be used to generate new instances from the learned distribution. (Right) Restricted Boltzmann Machine (RBM). Same as the Boltzmann machine, but without any couplings within the visible layer or between hidden nodes. This variant can be used for layer-by-layer pre-training of deep networks.\nå›¾ 1. $N$ ä¸ªäºŒè¿›åˆ¶èŠ‚ç‚¹ $s_i$ï¼ˆ0 æˆ– 1ï¼‰çš„é€’å½’ç½‘ç»œï¼Œè¿æ¥æƒé‡ä¸º $w_{ij}$ã€‚ï¼ˆå·¦ï¼‰Hopfield æ¨¡å‹ã€‚ï¼ˆä¸­ï¼‰ç»å°”å…¹æ›¼æœºã€‚èŠ‚ç‚¹åˆ†ä¸ºä¸¤ç»„ï¼Œå¯è§èŠ‚ç‚¹ï¼ˆç©ºå¿ƒåœ†ï¼‰å’Œéšè—èŠ‚ç‚¹ï¼ˆç°è‰²ï¼‰ã€‚ç½‘ç»œç»è¿‡è®­ç»ƒä»¥è¿‘ä¼¼ç»™å®šå¯è§æ¨¡å¼é›†çš„æ¦‚ç‡åˆ†å¸ƒã€‚è®­ç»ƒå®Œæˆåï¼Œç½‘ç»œå¯ä»¥ç”¨æ¥ä»å­¦ä¹ åˆ°çš„åˆ†å¸ƒä¸­ç”Ÿæˆæ–°å®ä¾‹ã€‚ï¼ˆå³ï¼‰å—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰ã€‚ä¸ç»å°”å…¹æ›¼æœºç›¸åŒï¼Œä½†åœ¨å¯è§å±‚å†…æˆ–éšè—èŠ‚ç‚¹ä¹‹é—´æ²¡æœ‰ä»»ä½•è€¦åˆã€‚è¿™ç§å˜ä½“å¯ç”¨äºæ·±åº¦ç½‘ç»œçš„é€å±‚é¢„è®­ç»ƒã€‚\nThe 1980s The 1980s saw major breakthroughs in the areas of both recurrent and feedforward neural networks, which led to a rapid expansion of the ANN field.\n20 ä¸–çºª 80 å¹´ä»£ï¼Œé€’å½’ç¥ç»ç½‘ç»œå’Œå‰é¦ˆç¥ç»ç½‘ç»œé¢†åŸŸéƒ½å–å¾—äº†é‡å¤§çªç ´ï¼Œå¯¼è‡´äººå·¥ç¥ç»ç½‘ç»œé¢†åŸŸçš„å¿«é€Ÿæ‰©å±•ã€‚\nJohn Hopfield, a theoretical physicist, is a towering figure in biological physics. His seminal work in the 1970s examined electron transfer between biomolecules and error correction in biochemical reactions (kinetic proofreading).\nç†è®ºç‰©ç†å­¦å®¶ John Hopfield æ˜¯ç”Ÿç‰©ç‰©ç†å­¦é¢†åŸŸçš„æ°å‡ºäººç‰©ã€‚ä»–åœ¨ 20 ä¸–çºª 70 å¹´ä»£çš„å¼€åˆ›æ€§å·¥ä½œç ”ç©¶äº†ç”Ÿç‰©åˆ†å­ä¹‹é—´çš„ç”µå­è½¬ç§»å’Œç”ŸåŒ–ååº”ä¸­çš„é”™è¯¯æ ¡æ­£ï¼ˆåŠ¨åŠ›å­¦æ ¡å¯¹ï¼‰ã€‚\nIn 1982, Hopfield published a dynamical model for an associative memory based on a simple recurrent neural network. Collective phenomena frequently occur in physical systems, such as domains in magnetic systems and vortices in fluid flow. Hopfield asked whether emergent collective phenomena in large collections of neurons could give rise to â€œcomputationalâ€ abilities.\n1982 å¹´ï¼ŒHopfield åŸºäºä¸€ä¸ªç®€å•çš„é€’å½’ç¥ç»ç½‘ç»œå‘è¡¨äº†ä¸€ä¸ªç”¨äºè”æƒ³è®°å¿†çš„åŠ¨æ€æ¨¡å‹ã€‚é›†ä½“ç°è±¡åœ¨ç‰©ç†ç³»ç»Ÿä¸­ç»å¸¸å‘ç”Ÿï¼Œä¾‹å¦‚ç£æ€§ç³»ç»Ÿä¸­çš„ç•´å’Œæµä½“æµåŠ¨ä¸­çš„æ¶¡æ—‹ã€‚Hopfield è¯¢é—®å¤§é‡ç¥ç»å…ƒä¸­å‡ºç°çš„é›†ä½“ç°è±¡æ˜¯å¦èƒ½å¤Ÿäº§ç”Ÿâ€œè®¡ç®—â€èƒ½åŠ›ã€‚\nNoting that collective properties in many physical systems are robust to changes in model details, he addressed this question using a neural network with $N$ binary nodes $s_i$ (0 or 1). The dynamics were asynchronous with threshold updates of individual nodes at random times. The new value of a node $s_i$ was determined by a weighted sum over all other nodes,\n$$ h_{i} = \\sum_{j\\neq i}w_{ij}s_{j}, $$\nand was set to $s_i=1$ if $h_i\u003e0$, and $s_i=0$ otherwise (with the threshold set to zero). The couplings $w_{ij}$ were assumed symmetric and to reflect pairwise correlations between the nodes in stored memories, which is referred to as the Hebb rule. The symmetry of the weights guarantees stable dynamics. Stationary states were identified as memories, distributed over the $N$ nodes in a nonlocal storage. Furthermore, the network was assigned an energy $E$ given by\n$$ E = -\\sum_{i",
  "wordCount" : "8717",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/10/08/jsH1leAN6qnIV4J.png","datePublished": "2022-05-05T00:18:23+08:00",
  "dateModified": "2022-05-05T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/reference/2024-nobel-prize-in-physics/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "æ— å¤„æƒ¹å°˜åŸƒ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿— (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿—</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/">ğŸ“• é˜…è¯»</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/">ğŸ“• æ–‡çŒ®</a></div>
            <h1 class="post-title">
                FOR FOUNDATIONAL DISCOVERIES AND INVENTIONS THAT ENABLE MACHINE LEARNING WITH ARTIFICIAL NEURAL NETWORKS
            </h1>
            <div class="post-description">
                2024 å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2022-05-05
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>8717å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>18åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/10/08/jsH1leAN6qnIV4J.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#historical-background" aria-label="Historical background">Historical background</a></li>
                <li>
                    <a href="#the-1980s" aria-label="The 1980s">The 1980s</a></li>
                <li>
                    <a href="#toward-deep-learning" aria-label="Toward deep learning">Toward deep learning</a></li>
                <li>
                    <a href="#anns-as-powerful-tools-in-physics-and-other-scientific-disciplines" aria-label="ANNs as powerful tools in physics and other scientific disciplines">ANNs as powerful tools in physics and other scientific disciplines</a></li>
                <li>
                    <a href="#anns-in-everyday-life" aria-label="ANNs in everyday life">ANNs in everyday life</a></li>
                <li>
                    <a href="#concluding-remarks" aria-label="Concluding remarks">Concluding remarks</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><blockquote>
<p>The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Physics 2024 jointly to John J. Hopfield and Geoffrey Hinton â€œfor foundational discoveries and inventions that enable machine learning with artificial neural networksâ€.</p>
</blockquote>
<p>ç‘å…¸çš‡å®¶ç§‘å­¦é™¢å†³å®šå°† 2024 å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–è”åˆæˆäºˆçº¦ç¿°-éœæ™®è²å°”å¾·ï¼ˆJohn J. Hopfieldï¼‰å’Œæ°å¼—é‡Œ-è¾›é¡¿ï¼ˆGeoffrey Hintonï¼‰ï¼Œ&ldquo;ä»¥è¡¨å½°ä»–ä»¬åˆ©ç”¨äººå·¥ç¥ç»ç½‘ç»œå®ç°æœºå™¨å­¦ä¹ çš„å¥ åŸºæ€§å‘ç°å’Œå‘æ˜&rdquo;ã€‚</p>
<h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<blockquote>
<p>With its roots in the 1940s, machine learning based on <strong>artificial neural networks (ANNs)</strong> has developed over the past three decades into a versatile and powerful tool, with both everyday and advanced scientific applications. With ANNs the boundaries of physics are extended to host phenomena of life as well as computation.</p>
</blockquote>
<p>èµ·æºäº 20 ä¸–çºª 40 å¹´ä»£çš„åŸºäº <strong>äººå·¥ç¥ç»ç½‘ç»œ(ANN)</strong> çš„æœºå™¨å­¦ä¹ ï¼Œåœ¨è¿‡å»çš„ä¸‰åå¹´ä¸­å‘å±•æˆä¸ºä¸€ç§å¤šåŠŸèƒ½ä¸”å¼ºå¤§çš„å·¥å…·ï¼Œæ—¢æœ‰æ—¥å¸¸åº”ç”¨ï¼Œä¹Ÿæœ‰å…ˆè¿›çš„ç§‘å­¦åº”ç”¨ã€‚é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œï¼Œç‰©ç†å­¦çš„è¾¹ç•Œå¾—ä»¥æ‰©å±•ï¼Œä»¥å®¹çº³ç”Ÿå‘½ç°è±¡å’Œè®¡ç®—ç°è±¡ã€‚</p>
<blockquote>
<p>Inspired by biological neurons in the brain, ANNs are large collections of â€œneuronsâ€, or nodes, connected by â€œsynapsesâ€, or weighted couplings, which are trained to perform certain tasks rather than asked to execute a predetermined set of instructions. Their basic structure has close similarities with spin models in statistical physics applied to magnetism or alloy theory. This yearâ€™s Nobel Prize in Physics recognizes research exploiting this connection to make breakthrough methodological advances in the field of ANN.</p>
</blockquote>
<p>å—å¤§è„‘ä¸­ç”Ÿç‰©ç¥ç»å…ƒçš„å¯å‘ï¼Œäººå·¥ç¥ç»ç½‘ç»œæ˜¯ç”±å¤§é‡â€œç¥ç»å…ƒâ€æˆ–èŠ‚ç‚¹ç»„æˆçš„é›†åˆï¼Œè¿™äº›èŠ‚ç‚¹é€šè¿‡â€œçªè§¦â€æˆ–åŠ æƒè€¦åˆè¿æ¥åœ¨ä¸€èµ·ï¼Œç»è¿‡è®­ç»ƒä»¥æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œè€Œä¸æ˜¯æ‰§è¡Œé¢„å®šçš„æŒ‡ä»¤é›†ã€‚å®ƒä»¬çš„åŸºæœ¬ç»“æ„ä¸åº”ç”¨äºç£å­¦æˆ–åˆé‡‘ç†è®ºçš„ç»Ÿè®¡ç‰©ç†ä¸­çš„è‡ªæ—‹æ¨¡å‹æœ‰ç€å¯†åˆ‡çš„ç›¸ä¼¼æ€§ã€‚ä»Šå¹´çš„è¯ºè´å°”ç‰©ç†å­¦å¥–è¡¨å½°äº†åˆ©ç”¨è¿™ç§è”ç³»åœ¨äººå·¥ç¥ç»ç½‘ç»œé¢†åŸŸå–å¾—çªç ´æ€§æ–¹æ³•è¿›å±•çš„ç ”ç©¶ã€‚</p>
<h1 id="historical-background">Historical background<a hidden class="anchor" aria-hidden="true" href="#historical-background">#</a></h1>
<blockquote>
<p>The first electronic-based computers appeared in the 1940s, and were invented for military and scientific purposes. They were intended to carry out computations that were cumbersome and time-consuming for humans. In the 1950s, the opposite need emerged, namely to get computers to do what humans and other mammals are good at â€“ pattern recognition.</p>
</blockquote>
<p>20 ä¸–çºª 40 å¹´ä»£å‡ºç°äº†ç¬¬ä¸€æ‰¹åŸºäºç”µå­çš„è®¡ç®—æœºï¼Œæœ€åˆæ˜¯ä¸ºå†›äº‹å’Œç§‘å­¦ç›®çš„è€Œå‘æ˜çš„ã€‚å®ƒä»¬æ—¨åœ¨æ‰§è¡Œå¯¹äººç±»æ¥è¯´ç¹çä¸”è€—æ—¶çš„è®¡ç®—ã€‚20 ä¸–çºª 50 å¹´ä»£ï¼Œå‡ºç°äº†ç›¸åçš„éœ€æ±‚ï¼Œå³è®©è®¡ç®—æœºèƒ½å¤Ÿå®Œæˆé‚£äº›äººç±»å’Œå…¶ä»–å“ºä¹³åŠ¨ç‰©æ“…é•¿çš„ä»»åŠ¡â€”â€”æ¨¡å¼è¯†åˆ«ã€‚</p>
<blockquote>
<p>This artificial intelligence-oriented objective was first approached by mathematicians and computer scientists, who developed programs based on logical rules. This approach was pursued until the 1980s, but the computational resources that were required for the exact classifications, for example, of images became prohibitive.</p>
</blockquote>
<p>è¿™ä¸€é¢å‘äººå·¥æ™ºèƒ½çš„ç›®æ ‡æœ€åˆç”±æ•°å­¦å®¶å’Œè®¡ç®—æœºç§‘å­¦å®¶æå‡ºï¼Œä»–ä»¬å¼€å‘äº†åŸºäºé€»è¾‘è§„åˆ™çš„ç¨‹åºã€‚è¿™ç§æ–¹æ³•ä¸€ç›´æŒç»­åˆ° 20 ä¸–çºª 80 å¹´ä»£ï¼Œä½†æ‰€éœ€çš„è®¡ç®—èµ„æºå˜å¾—è¿‡äºåºå¤§ï¼Œä¾‹å¦‚å¯¹å›¾åƒè¿›è¡Œç²¾ç¡®åˆ†ç±»ã€‚</p>
<blockquote>
<p>In parallel, efforts had been initiated to find out how biological systems solve the pattern recognition problem. As early as 1943, Warren McCulloch and Walter Pitts, a neuroscientist and a logician, respectively, had proposed a model for how the neurons in the brain cooperate. In their model, a neuron formed a weighted sum of binary incoming signals from other neurons, which determined a binary outgoing signal. Their work became a launch pad for later research into both biological and artificial neural networks.</p>
</blockquote>
<p>ä¸æ­¤åŒæ—¶ï¼Œäººä»¬å¼€å§‹åŠªåŠ›ç ”ç©¶ç”Ÿç‰©ç³»ç»Ÿå¦‚ä½•è§£å†³æ¨¡å¼è¯†åˆ«é—®é¢˜ã€‚æ—©åœ¨ 1943 å¹´ï¼Œç¥ç»ç§‘å­¦å®¶ Warren McCulloch å’Œé€»è¾‘å­¦å®¶ Walter Pitts å°±æå‡ºäº†ä¸€ä¸ªå…³äºå¤§è„‘ä¸­ç¥ç»å…ƒå¦‚ä½•åä½œçš„æ¨¡å‹ã€‚åœ¨ä»–ä»¬çš„æ¨¡å‹ä¸­ï¼Œç¥ç»å…ƒå¯¹æ¥è‡ªå…¶ä»–ç¥ç»å…ƒçš„äºŒè¿›åˆ¶è¾“å…¥ä¿¡å·è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œä»è€Œå†³å®šè¾“å‡ºçš„äºŒè¿›åˆ¶ä¿¡å·ã€‚ä»–ä»¬çš„å·¥ä½œæˆä¸ºåç»­ç”Ÿç‰©å’Œäººå·¥ç¥ç»ç½‘ç»œç ”ç©¶çš„èµ·ç‚¹ã€‚</p>
<blockquote>
<p>Another influential early contribution came from the psychologist Donald Hebb. In 1949, Hebb proposed a mechanism for learning and memories, where the simultaneous and repeated activation of two neurons leads to an increased strength of the synapse between them.</p>
</blockquote>
<p>å¦ä¸€é¡¹æœ‰å½±å“åŠ›çš„æ—©æœŸè´¡çŒ®æ¥è‡ªå¿ƒç†å­¦å®¶ Donald Hebbã€‚1949 å¹´ï¼ŒHebb æå‡ºäº†ä¸€ç§å­¦ä¹ å’Œè®°å¿†æœºåˆ¶ï¼Œå³ä¸¤ä¸ªç¥ç»å…ƒçš„åŒæ—¶å’Œåå¤æ¿€æ´»ä¼šå¯¼è‡´å®ƒä»¬ä¹‹é—´çªè§¦å¼ºåº¦çš„å¢åŠ ã€‚</p>
<blockquote>
<p>In the ANN area, two architectures for systems of interconnected nodes were explored, â€œrecurrentâ€ and â€œfeedforwardâ€ networks, where the former allows for feedback interactions (Figures 1 and 2). A feedforward network has input and output layers and may also contain additional layers of hidden nodes sandwiched in-between.</p>
</blockquote>
<p>åœ¨äººå·¥ç¥ç»ç½‘ç»œé¢†åŸŸï¼Œæ¢ç´¢äº†ä¸¤ç§äº’è¿èŠ‚ç‚¹ç³»ç»Ÿçš„æ¶æ„ï¼šâ€œé€’å½’â€å’Œâ€œå‰é¦ˆâ€ç½‘ç»œï¼Œå‰è€…å…è®¸åé¦ˆäº¤äº’ï¼ˆå›¾ 1 å’Œå›¾ 2ï¼‰ã€‚å‰é¦ˆç½‘ç»œå…·æœ‰è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ï¼Œå¹¶ä¸”å¯èƒ½åŒ…å«å¤¹åœ¨ä¸­é—´çš„éšè—èŠ‚ç‚¹çš„é™„åŠ å±‚ã€‚</p>
<blockquote>
<p>In 1957, Frank Rosenblatt proposed a feedforward network for image interpretation, which was also implemented in computer hardware. It had three layers of nodes, with adjustable weights only between the middle and output layers. Those weights were determined in a systematic fashion.</p>
</blockquote>
<p>1957 å¹´ï¼ŒFrank Rosenblatt æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒè§£é‡Šçš„å‰é¦ˆç½‘ç»œï¼Œå¹¶åœ¨è®¡ç®—æœºç¡¬ä»¶ä¸­å®ç°ã€‚å®ƒæœ‰ä¸‰å±‚èŠ‚ç‚¹ï¼Œåªæœ‰ä¸­é—´å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´çš„æƒé‡æ˜¯å¯è°ƒçš„ã€‚è¿™äº›æƒé‡æ˜¯ä»¥ç³»ç»ŸåŒ–çš„æ–¹å¼ç¡®å®šçš„ã€‚</p>
<blockquote>
<p>Rosenblattâ€™s system attracted considerable attention, but it had limitations when it came to nonlinear problems. A simple example is the â€œone or the other but not bothâ€ (XOR) problem. These limitations were pointed out in an influential book by Marvin Minsky and Seymour Papert in 1969, which led to a hiatus funding-wise for ANN research.</p>
</blockquote>
<p>Rosenblatt çš„ç³»ç»Ÿå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä½†åœ¨å¤„ç†éçº¿æ€§é—®é¢˜æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¸€ä¸ªç®€å•çš„ä¾‹å­æ˜¯â€œè¦ä¹ˆè¿™ä¸ªï¼Œè¦ä¹ˆé‚£ä¸ªï¼Œä½†ä¸èƒ½åŒæ—¶æ˜¯ä¸¤ä¸ªâ€ï¼ˆXORï¼‰é—®é¢˜ã€‚è¿™äº›å±€é™æ€§åœ¨ Marvin Minsky å’Œ Seymour Papert äº 1969 å¹´å‡ºç‰ˆçš„ä¸€æœ¬æœ‰å½±å“åŠ›çš„ä¹¦ä¸­è¢«æŒ‡å‡ºï¼Œè¿™å¯¼è‡´äº†äººå·¥ç¥ç»ç½‘ç»œç ”ç©¶èµ„é‡‘çš„ä¸­æ–­ã€‚</p>
<blockquote>
<p>A parallel development took inspiration from magnetic systems, which were to create models for recurrent neural networks and investigate their collective properties.</p>
</blockquote>
<p>ç”±ç£æ€§ç³»ç»Ÿçš„å¹³è¡Œå‘å±•å—åˆ°å¯å‘ï¼Œåˆ›å»ºäº†é€’å½’ç¥ç»ç½‘ç»œçš„æ¨¡å‹å¹¶ç ”ç©¶å…¶é›†ä½“å±æ€§ã€‚</p>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/10/08/aEFOm7wrWqpNeGb.png" alt=""  /></p>
<blockquote>
<p>Recurrent networks of $N$ binary nodes $s_i$ (0 or 1), with connection weights $w_{ij}$. (Left) The Hopfield model. (Centre) Boltzmann machine. The nodes are divided into two groups, visible (open circles) and hidden (grey) nodes. The network is trained to approximate the probability distribution of a given set of visible patterns. Once trained, the network can be used to generate new instances from the learned distribution. (Right) Restricted Boltzmann Machine (RBM). Same as the Boltzmann machine, but without any couplings within the visible layer or between hidden nodes. This variant can be used for layer-by-layer pre-training of deep networks.</p>
</blockquote>
<p>å›¾ 1. $N$ ä¸ªäºŒè¿›åˆ¶èŠ‚ç‚¹ $s_i$ï¼ˆ0 æˆ– 1ï¼‰çš„é€’å½’ç½‘ç»œï¼Œè¿æ¥æƒé‡ä¸º $w_{ij}$ã€‚ï¼ˆå·¦ï¼‰Hopfield æ¨¡å‹ã€‚ï¼ˆä¸­ï¼‰ç»å°”å…¹æ›¼æœºã€‚èŠ‚ç‚¹åˆ†ä¸ºä¸¤ç»„ï¼Œå¯è§èŠ‚ç‚¹ï¼ˆç©ºå¿ƒåœ†ï¼‰å’Œéšè—èŠ‚ç‚¹ï¼ˆç°è‰²ï¼‰ã€‚ç½‘ç»œç»è¿‡è®­ç»ƒä»¥è¿‘ä¼¼ç»™å®šå¯è§æ¨¡å¼é›†çš„æ¦‚ç‡åˆ†å¸ƒã€‚è®­ç»ƒå®Œæˆåï¼Œç½‘ç»œå¯ä»¥ç”¨æ¥ä»å­¦ä¹ åˆ°çš„åˆ†å¸ƒä¸­ç”Ÿæˆæ–°å®ä¾‹ã€‚ï¼ˆå³ï¼‰å—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰ã€‚ä¸ç»å°”å…¹æ›¼æœºç›¸åŒï¼Œä½†åœ¨å¯è§å±‚å†…æˆ–éšè—èŠ‚ç‚¹ä¹‹é—´æ²¡æœ‰ä»»ä½•è€¦åˆã€‚è¿™ç§å˜ä½“å¯ç”¨äºæ·±åº¦ç½‘ç»œçš„é€å±‚é¢„è®­ç»ƒã€‚</p>
</blockquote>
<h1 id="the-1980s">The 1980s<a hidden class="anchor" aria-hidden="true" href="#the-1980s">#</a></h1>
<blockquote>
<p>The 1980s saw major breakthroughs in the areas of both recurrent and feedforward neural networks, which led to a rapid expansion of the ANN field.</p>
</blockquote>
<p>20 ä¸–çºª 80 å¹´ä»£ï¼Œé€’å½’ç¥ç»ç½‘ç»œå’Œå‰é¦ˆç¥ç»ç½‘ç»œé¢†åŸŸéƒ½å–å¾—äº†é‡å¤§çªç ´ï¼Œå¯¼è‡´äººå·¥ç¥ç»ç½‘ç»œé¢†åŸŸçš„å¿«é€Ÿæ‰©å±•ã€‚</p>
<blockquote>
<p>John Hopfield, a theoretical physicist, is a towering figure in biological physics. His seminal work in the 1970s examined electron transfer between biomolecules and error correction in biochemical reactions (kinetic proofreading).</p>
</blockquote>
<p>ç†è®ºç‰©ç†å­¦å®¶ John Hopfield æ˜¯ç”Ÿç‰©ç‰©ç†å­¦é¢†åŸŸçš„æ°å‡ºäººç‰©ã€‚ä»–åœ¨ 20 ä¸–çºª 70 å¹´ä»£çš„å¼€åˆ›æ€§å·¥ä½œç ”ç©¶äº†ç”Ÿç‰©åˆ†å­ä¹‹é—´çš„ç”µå­è½¬ç§»å’Œç”ŸåŒ–ååº”ä¸­çš„é”™è¯¯æ ¡æ­£ï¼ˆåŠ¨åŠ›å­¦æ ¡å¯¹ï¼‰ã€‚</p>
<blockquote>
<p>In 1982, Hopfield published a dynamical model for an associative memory based on a simple recurrent neural network. Collective phenomena frequently occur in physical systems, such as domains in magnetic systems and vortices in fluid flow. Hopfield asked whether emergent collective phenomena in large collections of neurons could give rise to â€œcomputationalâ€ abilities.</p>
</blockquote>
<p>1982 å¹´ï¼ŒHopfield åŸºäºä¸€ä¸ªç®€å•çš„é€’å½’ç¥ç»ç½‘ç»œå‘è¡¨äº†ä¸€ä¸ªç”¨äºè”æƒ³è®°å¿†çš„åŠ¨æ€æ¨¡å‹ã€‚é›†ä½“ç°è±¡åœ¨ç‰©ç†ç³»ç»Ÿä¸­ç»å¸¸å‘ç”Ÿï¼Œä¾‹å¦‚ç£æ€§ç³»ç»Ÿä¸­çš„ç•´å’Œæµä½“æµåŠ¨ä¸­çš„æ¶¡æ—‹ã€‚Hopfield è¯¢é—®å¤§é‡ç¥ç»å…ƒä¸­å‡ºç°çš„é›†ä½“ç°è±¡æ˜¯å¦èƒ½å¤Ÿäº§ç”Ÿâ€œè®¡ç®—â€èƒ½åŠ›ã€‚</p>
<blockquote>
<p>Noting that collective properties in many physical systems are robust to changes in model details, he addressed this question using a neural network with $N$ binary nodes $s_i$ (0 or 1). The dynamics were asynchronous with threshold updates of individual nodes at random times. The new value of a node $s_i$ was determined by a weighted sum over all other nodes,</p>
<p>$$
h_{i} = \sum_{j\neq i}w_{ij}s_{j},
$$</p>
<p>and was set to $s_i=1$ if $h_i&gt;0$, and $s_i=0$ otherwise (with the threshold set to zero). The couplings $w_{ij}$  were assumed symmetric and to reflect pairwise correlations between the nodes in stored memories, which is referred to as the Hebb rule. The symmetry of the weights guarantees stable dynamics. Stationary states were identified as memories, distributed over the $N$ nodes in a nonlocal storage. Furthermore, the network was assigned an energy $E$ given by</p>
<p>$$
E = -\sum_{i&lt;j}w_{ij}s_{i}s_{j}
$$</p>
<p>which is a monotonically decreasing function under the dynamics of the network. Notable is that the connection between the world of physics, as defined in the 1980s, and ANNs was obvious already from these two equations. The first equation can be used to represent the Weiss molecular field (after the French physicist Pierre Weiss) that describes how atomic magnetic moments align in a solid, and the latter is often used to evaluate the energy of a magnetic configuration, e.g. a ferromagnet. Hopfield was naturally well aware of how these equations were used to describe magnetic materials.</p>
</blockquote>
<p>ä»–æ³¨æ„åˆ°è®¸å¤šç‰©ç†ç³»ç»Ÿçš„é›†ä½“å±æ€§å¯¹æ¨¡å‹ç»†èŠ‚çš„å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œå› æ­¤åˆ©ç”¨ä¸€ä¸ªå…·æœ‰ $N$ äºŒè¿›åˆ¶èŠ‚ç‚¹ $s_i$ï¼ˆ0 æˆ– 1ï¼‰çš„ç¥ç»ç½‘ç»œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åŠ¨æ€å˜åŒ–æ˜¯å¼‚æ­¥çš„ï¼Œå•ä¸ªèŠ‚ç‚¹çš„é˜ˆå€¼æ›´æ–°æ—¶é—´æ˜¯éšæœºçš„ã€‚èŠ‚ç‚¹ $s_i$ çš„æ–°å€¼ç”±æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹çš„åŠ æƒå’Œå†³å®š</p>
<p>$$
h_{i} = \sum_{j\neq i}w_{ij}s_{j},
$$</p>
<p>å¹¶ä¸”å¦‚æœ $h_i&gt;0$ åˆ™è®¾ç½®ä¸º $s_i=1$ï¼Œå¦åˆ™è®¾ç½®ä¸º $s_i=0$ï¼ˆé˜ˆå€¼è®¾ä¸ºé›¶ï¼‰ã€‚è€¦åˆ $w_{ij}$ è¢«å‡å®šä¸ºå¯¹ç§°çš„ï¼Œå¹¶åæ˜ å­˜å‚¨è®°å¿†ä¸­èŠ‚ç‚¹ä¹‹é—´çš„æˆå¯¹ç›¸å…³æ€§ï¼Œè¿™è¢«ç§°ä¸º Hebb è§„åˆ™ã€‚æƒé‡çš„å¯¹ç§°æ€§ä¿è¯äº†åŠ¨æ€çš„ç¨³å®šæ€§ã€‚å¹³ç¨³çŠ¶æ€è¢«è¯†åˆ«ä¸ºè®°å¿†ï¼Œåˆ†å¸ƒåœ¨ $N$ ä¸ªèŠ‚ç‚¹ä¸Šè¿›è¡Œéå±€éƒ¨å­˜å‚¨ã€‚æ­¤å¤–ï¼Œç½‘ç»œè¢«èµ‹äºˆä¸€ä¸ªèƒ½é‡ $E$ï¼Œå…¶å®šä¹‰ä¸º</p>
<p>$$
E = -\sum_{i&lt;j}w_{ij}s_{i}s_{j}
$$</p>
<p>å®ƒæ˜¯åœ¨ç½‘ç»œåŠ¨æ€ä¸‹å•è°ƒé€’å‡çš„å‡½æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ20 ä¸–çºª 80 å¹´ä»£å®šä¹‰çš„ç‰©ç†ä¸–ç•Œä¸äººå·¥ç¥ç»ç½‘ç»œä¹‹é—´çš„è”ç³»å·²ç»ä»è¿™ä¸¤ä¸ªæ–¹ç¨‹ä¸­æ˜¾è€Œæ˜“è§ã€‚ç¬¬ä¸€ä¸ªæ–¹ç¨‹å¯ç”¨äºè¡¨ç¤º Weiss åˆ†å­åœºï¼ˆä»¥æ³•å›½ç‰©ç†å­¦å®¶ Pierre Weiss å‘½åï¼‰ï¼Œè¯¥åœºæè¿°äº†å›ºä½“ä¸­åŸå­ç£çŸ©çš„æ’åˆ—æ–¹å¼ï¼Œåè€…é€šå¸¸ç”¨äºè¯„ä¼°ç£æ€§é…ç½®çš„èƒ½é‡ï¼Œä¾‹å¦‚é“ç£ä½“ã€‚Hopfield è‡ªç„¶éå¸¸æ¸…æ¥šè¿™äº›æ–¹ç¨‹å¦‚ä½•ç”¨äºæè¿°ç£æ€§ææ–™ã€‚</p>
<blockquote>
<p>Metaphorically, the dynamics drive the system with $N$ nodes to the valleys of an $N$-dimensional energy landscape, in which the stationary states are located. The stationary states represent memories learned by the Hebb rule. Initially, the number of memories that could be stored in Hopfieldâ€™s dynamical model was limited. Methods to alleviate this problem were developed in later work.</p>
</blockquote>
<p>éšå–»åœ°è¯´ï¼Œå…·æœ‰ $N$ ä¸ªèŠ‚ç‚¹çš„ç³»ç»Ÿçš„åŠ¨æ€é©±åŠ¨å…¶è¿›å…¥ $N$ ç»´èƒ½é‡æ™¯è§‚çš„è°·åº•ï¼Œå¹³ç¨³çŠ¶æ€ä½äºå…¶ä¸­ã€‚å¹³ç¨³çŠ¶æ€ä»£è¡¨é€šè¿‡ Hebb è§„åˆ™å­¦ä¹ çš„è®°å¿†ã€‚æœ€åˆï¼ŒHopfield åŠ¨æ€æ¨¡å‹ä¸­å¯ä»¥å­˜å‚¨çš„è®°å¿†æ•°é‡æ˜¯æœ‰é™çš„ã€‚åæ¥å·¥ä½œä¸­å¼€å‘äº†ç¼“è§£è¿™ä¸€é—®é¢˜çš„æ–¹æ³•ã€‚</p>
<blockquote>
<p>Hopfield used his model as an associative memory or as a method for error correction or pattern completion. A system initialized with an incorrect pattern, perhaps a misspelled word, is attracted to the nearest local energy minimum in his model, whereby a correction occurs. The model gained additional traction when it became clear that basic properties, such as the storage capacity, could be understood analytically, by using methods from spin glass theory.</p>
</blockquote>
<p>Hopfield å°†ä»–çš„æ¨¡å‹ç”¨ä½œè”æƒ³è®°å¿†æˆ–é”™è¯¯æ ¡æ­£æˆ–æ¨¡å¼å®Œæˆçš„æ–¹æ³•ã€‚åœ¨ä»–çš„æ¨¡å‹ä¸­ï¼Œç”¨ä¸æ­£ç¡®çš„æ¨¡å¼ï¼ˆä¾‹å¦‚æ‹¼å†™é”™è¯¯çš„å•è¯ï¼‰åˆå§‹åŒ–çš„ç³»ç»Ÿä¼šè¢«å¸å¼•åˆ°æœ€è¿‘çš„å±€éƒ¨èƒ½é‡æœ€å°å€¼ï¼Œä»è€Œè¿›è¡Œçº æ­£ã€‚å½“äººä»¬æ¸…æ¥šåœ°è®¤è¯†åˆ°å¯ä»¥ä½¿ç”¨è‡ªæ—‹ç»ç’ƒç†è®ºçš„æ–¹æ³•æ¥åˆ†æç†è§£å­˜å‚¨å®¹é‡ç­‰åŸºæœ¬å±æ€§æ—¶ï¼Œè¯¥æ¨¡å‹è·å¾—äº†æ›´å¤šçš„å…³æ³¨ã€‚</p>
<blockquote>
<p>A legitimate question at the time was whether the properties of this model are an artifact of its crude binary structure. Hopfield answered this question by creating an analog version of the model, with continuous-time dynamics given by the equations of motion for an electronic circuit. His analysis of the analog model demonstrated that the binary nodes could be replaced by analog ones without losing the emergent collective properties of the original model. The stationary states of the analog model corresponded to mean-field solutions of the binary system at an effective adjustable temperature, and approached the stationary states of the binary model at low temperature.</p>
</blockquote>
<p>å½“æ—¶ä¸€ä¸ªåˆç†çš„é—®é¢˜æ˜¯ï¼Œè¿™ä¸ªæ¨¡å‹çš„å±æ€§æ˜¯å¦æ˜¯å…¶ç²—ç³™äºŒè¿›åˆ¶ç»“æ„çš„äº§ç‰©ã€‚Hopfield é€šè¿‡åˆ›å»ºè¯¥æ¨¡å‹çš„æ¨¡æ‹Ÿç‰ˆæœ¬æ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œå…¶è¿ç»­æ—¶é—´åŠ¨æ€ç”±ç”µå­ç”µè·¯çš„è¿åŠ¨æ–¹ç¨‹ç»™å‡ºã€‚ä»–å¯¹æ¨¡æ‹Ÿæ¨¡å‹çš„åˆ†æè¡¨æ˜ï¼ŒäºŒè¿›åˆ¶èŠ‚ç‚¹å¯ä»¥è¢«æ¨¡æ‹ŸèŠ‚ç‚¹æ›¿æ¢ï¼Œè€Œä¸ä¼šå¤±å»åŸå§‹æ¨¡å‹çš„é›†ä½“ç°è±¡ã€‚æ¨¡æ‹Ÿæ¨¡å‹çš„å¹³ç¨³çŠ¶æ€å¯¹åº”äºäºŒè¿›åˆ¶ç³»ç»Ÿåœ¨æœ‰æ•ˆå¯è°ƒæ¸©åº¦ä¸‹çš„å¹³å‡åœºè§£ï¼Œå¹¶åœ¨ä½æ¸©ä¸‹æ¥è¿‘äºŒè¿›åˆ¶æ¨¡å‹çš„å¹³ç¨³çŠ¶æ€ã€‚</p>
<blockquote>
<p>The close correspondence between the analog and binary models was subsequently used by Hopfield and David Tank to develop a method for solving difficult discrete optimization problems based on the continuous-time dynamics of the analog model. Here, the optimization problem to be solved, including constraints, is encoded in the interaction parameters (weights) of the network. They chose to use the dynamics of the analog model in order to have a â€œsofterâ€ energy landscape and thereby facilitate the search. The above-mentioned effective temperature of the analog system was gradually decreased, as in global optimization with simulated annealing. Optimization occurs through integration of the equations of motion of an electronic circuit, during which the nodes evolve without instructions from a central unit. This approach constitutes a pioneering example of using a dynamical system to seek solutions to difficult discrete optimization problems. A more recent example is quantum annealing.</p>
</blockquote>
<p>æ¨¡æ‹Ÿå’ŒäºŒè¿›åˆ¶æ¨¡å‹ä¹‹é—´çš„å¯†åˆ‡å¯¹åº”å…³ç³»éšåè¢« Hopfield å’Œ David Tank ç”¨æ¥å¼€å‘ä¸€ç§åŸºäºæ¨¡æ‹Ÿæ¨¡å‹çš„è¿ç»­æ—¶é—´åŠ¨æ€æ¥è§£å†³å›°éš¾çš„ç¦»æ•£ä¼˜åŒ–é—®é¢˜çš„æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œè¦è§£å†³çš„ä¼˜åŒ–é—®é¢˜ï¼ŒåŒ…æ‹¬çº¦æŸæ¡ä»¶ï¼Œéƒ½è¢«ç¼–ç åœ¨ç½‘ç»œçš„ç›¸äº’ä½œç”¨å‚æ•°ï¼ˆæƒé‡ï¼‰ä¸­ã€‚ä»–ä»¬é€‰æ‹©ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹çš„åŠ¨æ€ï¼Œä»¥ä¾¿æ‹¥æœ‰ä¸€ä¸ªâ€œæ›´æŸ”å’Œâ€çš„èƒ½é‡æ™¯è§‚ï¼Œä»è€Œä¿ƒè¿›æœç´¢ã€‚ä¸Šè¿°æ¨¡æ‹Ÿç³»ç»Ÿçš„æœ‰æ•ˆæ¸©åº¦é€æ¸é™ä½ï¼Œå°±åƒä½¿ç”¨æ¨¡æ‹Ÿé€€ç«è¿›è¡Œå…¨å±€ä¼˜åŒ–ä¸€æ ·ã€‚é€šè¿‡é›†æˆç”µå­ç”µè·¯çš„è¿åŠ¨æ–¹ç¨‹æ¥è¿›è¡Œä¼˜åŒ–ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼ŒèŠ‚ç‚¹åœ¨æ²¡æœ‰ä¸­å¤®å•å…ƒæŒ‡ä»¤çš„æƒ…å†µä¸‹æ¼”åŒ–ã€‚è¿™ç§æ–¹æ³•æ„æˆäº†ä½¿ç”¨åŠ¨æ€ç³»ç»Ÿæ¥å¯»æ‰¾å›°éš¾ç¦»æ•£ä¼˜åŒ–é—®é¢˜è§£å†³æ–¹æ¡ˆçš„å¼€åˆ›æ€§ä¾‹å­ã€‚ä¸€ä¸ªæ›´è¿‘çš„ä¾‹å­æ˜¯é‡å­é€€ç«ã€‚</p>
<blockquote>
<p>By creating and exploring the above physics-based dynamical models â€“ not only the milestone associative memory model but also those that followed â€“ Hopfield made a foundational contribution to our understanding of the computational abilities of neural networks.</p>
</blockquote>
<p>é€šè¿‡åˆ›å»ºå’Œæ¢ç´¢ä¸Šè¿°åŸºäºç‰©ç†çš„åŠ¨æ€æ¨¡å‹â€”â€”ä¸ä»…æ˜¯é‡Œç¨‹ç¢‘å¼çš„è”æƒ³è®°å¿†æ¨¡å‹ï¼Œè¿˜æœ‰åç»­çš„æ¨¡å‹â€”â€”Hopfield å¯¹æˆ‘ä»¬ç†è§£ç¥ç»ç½‘ç»œçš„è®¡ç®—èƒ½åŠ›åšå‡ºäº†å¥ åŸºæ€§çš„è´¡çŒ®ã€‚</p>
<blockquote>
<p>In 1983â€“1985 Geoffrey Hinton, together with Terrence Sejnowski and other coworkers, developed a stochastic extension of Hopfieldâ€™s model from 1982, called the Boltzmann machine. Here, each state $\mathbb{s}=(s_1,\cdots,s_N)$ of the network is assigned a probability given by the Boltzmann distribution</p>
<p>$$
P(\mathbb{s}) \propto e^{-E/T} \quad E = -\sum_{i&lt;j}w_{ij}s_{i}s_{j} - \sum_{i}\theta_{i}s_{i}
$$</p>
<p>where $T$ is a fictive temperature and $\theta_{i}$ is a bias, or local field.</p>
</blockquote>
<p>1983-1985 å¹´ï¼ŒGeoffrey Hinton ä¸ Terrence Sejnowski å’Œå…¶ä»–åŒäº‹ä¸€èµ·ï¼Œå¼€å‘äº† Hopfield 1982 å¹´æ¨¡å‹çš„éšæœºæ‰©å±•ï¼Œç§°ä¸ºç»å°”å…¹æ›¼æœºã€‚åœ¨è¿™é‡Œï¼Œç½‘ç»œçš„æ¯ä¸ªçŠ¶æ€ $\mathbb{s}=(s_1,\cdots,s_N)$ éƒ½è¢«èµ‹äºˆä¸€ä¸ªç”±ç»å°”å…¹æ›¼åˆ†å¸ƒç»™å‡ºçš„æ¦‚ç‡</p>
<p>$$
P(\mathbb{s}) \propto e^{-E/T} \quad E = -\sum_{i&lt;j}w_{ij}s_{i}s_{j} - \sum_{i}\theta_{i}s_{i}
$$</p>
<p>å…¶ä¸­ $T$ æ˜¯ä¸€ä¸ªè™šæ‹Ÿæ¸©åº¦ï¼Œ$\theta_{i}$ æ˜¯åç½®æˆ–å±€éƒ¨åœºã€‚</p>
<blockquote>
<p>The Boltzmann machine is a generative model. Unlike the Hopfield model, it focuses on statistical distributions of patterns rather than individual patterns. It contains visible nodes that correspond to the patterns to be learned as well as additional hidden nodes, where the latter are included to enable modelling of more general probability distributions.</p>
</blockquote>
<p>ç»å°”å…¹æ›¼æœºæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ã€‚ä¸ Hopfield æ¨¡å‹ä¸åŒï¼Œå®ƒå…³æ³¨æ¨¡å¼çš„ç»Ÿè®¡åˆ†å¸ƒè€Œä¸æ˜¯å•ä¸ªæ¨¡å¼ã€‚å®ƒåŒ…å«ä¸è¦å­¦ä¹ çš„æ¨¡å¼å¯¹åº”çš„å¯è§èŠ‚ç‚¹ä»¥åŠé¢å¤–çš„éšè—èŠ‚ç‚¹ï¼Œåè€…è¢«åŒ…å«åœ¨å†…ä»¥å®ç°å¯¹æ›´ä¸€èˆ¬æ¦‚ç‡åˆ†å¸ƒçš„å»ºæ¨¡ã€‚</p>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/10/08/2s8IP5yom6n34hS.png" alt=""  /></p>
<blockquote>
<p>Feedward network with two layers of hidden nodes between the input and
output layers.</p>
</blockquote>
<p>å›¾ 2. è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´æœ‰ä¸¤å±‚éšè—èŠ‚ç‚¹çš„å‰é¦ˆç½‘ç»œã€‚</p>
</blockquote>
<blockquote>
<p>The weight and bias parameters of the network, which define the energy $E$, are determined so that the statistical distribution of visible patterns generated by the model deviates minimally from the statistical distribution of a given set of training patterns. Hinton and his colleagues developed a formally elegant gradient-based learning algorithm for the parameter determination; however, each step of the algorithm involves time-consuming equilibrium simulations for two different ensembles.</p>
</blockquote>
<p>ç½‘ç»œçš„æƒé‡å’Œåç½®å‚æ•°å®šä¹‰äº†èƒ½é‡ $E$ï¼Œå…¶ç¡®å®šæ–¹å¼æ˜¯ä½¿æ¨¡å‹ç”Ÿæˆçš„å¯è§æ¨¡å¼çš„ç»Ÿè®¡åˆ†å¸ƒä¸ç»™å®šè®­ç»ƒæ¨¡å¼é›†çš„ç»Ÿè®¡åˆ†å¸ƒä¹‹é—´çš„åå·®æœ€å°ã€‚Hinton å’Œä»–çš„åŒäº‹ä»¬ä¸ºå‚æ•°ç¡®å®šå¼€å‘äº†ä¸€ç§å½¢å¼ä¸Šä¼˜é›…çš„åŸºäºæ¢¯åº¦çš„å­¦ä¹ ç®—æ³•ï¼›ç„¶è€Œï¼Œè¯¥ç®—æ³•çš„æ¯ä¸€æ­¥éƒ½æ¶‰åŠä¸¤ä¸ªä¸åŒé›†åˆçš„è€—æ—¶å¹³è¡¡æ¨¡æ‹Ÿã€‚</p>
<blockquote>
<p>While theoretically interesting, in practice, the Boltzmann machine was initially of limited use. However, a slimmed-down version of it with fewer weights, called the restricted Boltzmann machine, developed into a versatile tool (see next section).</p>
</blockquote>
<p>è™½ç„¶åœ¨ç†è®ºä¸Šå¾ˆæœ‰è¶£ï¼Œä½†åœ¨å®è·µä¸­ï¼Œç»å°”å…¹æ›¼æœºæœ€åˆçš„ç”¨é€”æœ‰é™ã€‚ç„¶è€Œï¼Œå…¶ç®€åŒ–ç‰ˆæœ¬ï¼Œç§°ä¸ºå—é™ç»å°”å…¹æ›¼æœºï¼Œå…·æœ‰æ›´å°‘çš„æƒé‡ï¼Œå‘å±•æˆä¸ºä¸€ç§å¤šåŠŸèƒ½å·¥å…·ï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰ã€‚</p>
<blockquote>
<p>Both the Hopfield model and the Boltzmann machine are recurrent neural networks. The 1980s also saw important progress on feedforward networks. A key advance was the demonstration by David Rumelhart, Hinton and Ronald Williams in 1986 of how architectures with one or more hidden layers could be trained for classification using an algorithm known as backpropagation. Here, the objective is to minimize the mean square deviation, $D$, between output from the network and training data, by gradient descent. This requires computing the partial derivatives of $D$ with respect to all weights in the network. Rumelhart, Hinton and Williams reinvented a scheme for this, which had previously been applied to related problems by others. Additionally, and more importantly, they demonstrated that networks with a hidden layer could be trained by this method to perform tasks known to be unsolvable without such a layer. Furthermore, they elucidated the function of hidden nodes.</p>
</blockquote>
<p>Hopfield æ¨¡å‹å’Œç»å°”å…¹æ›¼æœºéƒ½æ˜¯é€’å½’ç¥ç»ç½‘ç»œã€‚20 ä¸–çºª 80 å¹´ä»£è¿˜åœ¨å‰é¦ˆç½‘ç»œæ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ã€‚ä¸€ä¸ªå…³é”®çš„è¿›å±•æ˜¯ David Rumelhartã€Hinton å’Œ Ronald Williams åœ¨ 1986 å¹´å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ç§°ä¸ºåå‘ä¼ æ’­çš„ç®—æ³•è®­ç»ƒå…·æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚çš„æ¶æ„è¿›è¡Œåˆ†ç±»ã€‚åœ¨è¿™é‡Œï¼Œç›®æ ‡æ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™æœ€å°åŒ–ç½‘ç»œè¾“å‡ºä¸è®­ç»ƒæ•°æ®ä¹‹é—´çš„å‡æ–¹åå·® $D$ã€‚è¿™éœ€è¦è®¡ç®— $D$ å…³äºç½‘ç»œä¸­æ‰€æœ‰æƒé‡çš„åå¯¼æ•°ã€‚Rumelhartã€Hinton å’Œ Williams é‡æ–°å‘æ˜äº†ä¸€ä¸ªæ–¹æ¡ˆï¼Œä¹‹å‰å…¶ä»–äººå·²ç»å°†å…¶åº”ç”¨äºç›¸å…³é—®é¢˜ã€‚æ­¤å¤–ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œä»–ä»¬è¯æ˜äº†å¯ä»¥é€šè¿‡è¿™ç§æ–¹æ³•è®­ç»ƒå…·æœ‰éšè—å±‚çš„ç½‘ç»œæ¥æ‰§è¡Œå·²çŸ¥æ— æ³•åœ¨æ²¡æœ‰è¯¥å±‚çš„æƒ…å†µä¸‹è§£å†³çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œä»–ä»¬é˜æ˜äº†éšè—èŠ‚ç‚¹çš„åŠŸèƒ½ã€‚</p>
<h1 id="toward-deep-learning">Toward deep learning<a hidden class="anchor" aria-hidden="true" href="#toward-deep-learning">#</a></h1>
<blockquote>
<p>The methodological breakthroughs in the 1980s were soon followed by successful applications, including pattern recognition in images, languages and clinical data. An important method was multilayered convolutional neural networks (CNN) trained by backpropagation, as advanced by Yann LeCun and Yoshua Bengio. The CNN architecture had its roots in the neocognitron method created by Kunihiko Fukushima, who in turn was inspired by work of David Hubel and Torsten Wiesel, Nobel Prize Laureates in Physiology or Medicine in 1981. The CNN approach developed by LeCun and coworkers became used by several American banks for classifying handwritten digits on checks from the mid-1990s. Another successful example from this period is the long short-term memory method created by Sepp Hochreiter and JÃ¼rgen Schmidhuber. This is a recurrent network for processing sequential data, as in speech and language, and can be mapped to a multilayered network by unfolding in time.</p>
</blockquote>
<p>20 ä¸–çºª 80 å¹´ä»£çš„æ–¹æ³•è®ºçªç ´å¾ˆå¿«å°±è¢«æˆåŠŸåº”ç”¨ï¼ŒåŒ…æ‹¬å›¾åƒã€è¯­è¨€å’Œä¸´åºŠæ•°æ®çš„æ¨¡å¼è¯†åˆ«ã€‚ä¸€ç§é‡è¦çš„æ–¹æ³•æ˜¯ç”± Yann LeCun å’Œ Yoshua Bengio æå‡ºçš„é€šè¿‡åå‘ä¼ æ’­è®­ç»ƒçš„å¤šå±‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚CNN æ¶æ„æºäºç¦å²›é‚¦å½¦ï¼ˆKunihiko Fukushimaï¼‰åˆ›å»ºçš„æ–°è®¤çŸ¥ç¥ç»ç½‘ç»œï¼ˆneocognitronï¼‰æ–¹æ³•ï¼Œè€Œç¦å²›é‚¦å½¦çš„çµæ„Ÿåˆ™æ¥è‡ª 1981 å¹´è¯ºè´å°”ç”Ÿç†å­¦æˆ–åŒ»å­¦å¥–å¾—ä¸»å¤§å«-èƒ¡è´å°”ï¼ˆDavid Hubelï¼‰å’Œæ‰˜å°”æ–¯æ»•-ç»´å¡å°”ï¼ˆTorsten Wieselï¼‰çš„ç ”ç©¶ã€‚ä» 20 ä¸–çºª 90 å¹´ä»£ä¸­æœŸå¼€å§‹ï¼ŒLeCun åŠå…¶åŒäº‹å¼€å‘çš„ CNN æ–¹æ³•è¢«å‡ å®¶ç¾å›½é“¶è¡Œç”¨äºå¯¹æ”¯ç¥¨ä¸Šçš„æ‰‹å†™æ•°å­—è¿›è¡Œåˆ†ç±»ã€‚è¿™ä¸€æ—¶æœŸçš„å¦ä¸€ä¸ªæˆåŠŸæ¡ˆä¾‹æ˜¯ Sepp Hochreiter å’Œ JÃ¼rgen Schmidhuber åˆ›é€ çš„é•¿çŸ­æœŸè®°å¿†æ–¹æ³•ã€‚è¿™æ˜¯ä¸€ç§ç”¨äºå¤„ç†è¿ç»­æ•°æ®ï¼ˆå¦‚è¯­éŸ³å’Œè¯­è¨€ï¼‰çš„é€’å½’ç½‘ç»œï¼Œå¯é€šè¿‡æ—¶é—´å±•å¼€æ˜ å°„ä¸ºå¤šå±‚ç½‘ç»œã€‚</p>
<blockquote>
<p>While certain multilayered architectures led to successful applications in the 1990s, it remained a challenge to train deep multilayered networks with many connections between consecutive layers. To many researchers in the field, training dense multilayered networks seemed out of reach. The situation changed in the 2000s. A leading figure in this breakthrough was Hinton, and an important tool was the restricted Boltzmann machine (RBM).</p>
</blockquote>
<p>è™½ç„¶æŸäº›å¤šå±‚æ¶æ„åœ¨ 20 ä¸–çºª 90 å¹´ä»£å–å¾—äº†æˆåŠŸçš„åº”ç”¨ï¼Œä½†è®­ç»ƒå…·æœ‰è®¸å¤šè¿æ¥çš„æ·±å±‚å¤šå±‚ç½‘ç»œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¯¹äºè¯¥é¢†åŸŸçš„è®¸å¤šç ”ç©¶äººå‘˜æ¥è¯´ï¼Œè®­ç»ƒå¯†é›†çš„å¤šå±‚ç½‘ç»œä¼¼ä¹é¥ä¸å¯åŠã€‚æƒ…å†µåœ¨ 21 ä¸–çºªåˆå‘ç”Ÿäº†å˜åŒ–ã€‚è¿™ä¸€çªç ´çš„é¢†å¯¼äººç‰©æ˜¯ Hintonï¼Œè€Œä¸€ä¸ªé‡è¦çš„å·¥å…·æ˜¯å—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰ã€‚</p>
<blockquote>
<p>An RBM network has weights only between visible and hidden nodes, and no weights connect two nodes of the same type. For RBMs, Hinton created an efficient approximate learning algorithm, called contrastive divergence, which was much faster than that for the full Boltzmann machine. With Simon Osindero and Yee-Whye Teh, he then developed a pretraining procedure for multilayer networks, in which the layers are trained one by one using an RBM. An early application of this approach was an autoencoder network for dimensional reduction. After pre-training, it became possible to perform a global parameter finetuning using the backpropagation algorithm. The pre-training with RBMs picked up structures in data, such as corners in images, without using labelled training data. Having found these structures, labelling those by backpropagation turned out to be a relatively simple task.</p>
</blockquote>
<p>RBM ç½‘ç»œåªæœ‰å¯è§èŠ‚ç‚¹å’Œéšè—èŠ‚ç‚¹ä¹‹é—´çš„æƒé‡ï¼Œæ²¡æœ‰æƒé‡è¿æ¥åŒä¸€ç±»å‹çš„ä¸¤ä¸ªèŠ‚ç‚¹ã€‚å¯¹äº RBMï¼ŒHinton åˆ›å»ºäº†ä¸€ç§é«˜æ•ˆçš„è¿‘ä¼¼å­¦ä¹ ç®—æ³•ï¼Œç§°ä¸ºå¯¹æ¯”æ•£åº¦ï¼Œå…¶é€Ÿåº¦è¿œå¿«äºå®Œæ•´ç»å°”å…¹æ›¼æœºçš„å­¦ä¹ ç®—æ³•ã€‚éšåï¼Œä»–ä¸ Simon Osindero å’Œ Yee-Whye Teh ä¸€èµ·å¼€å‘äº†ä¸€ç§å¤šå±‚ç½‘ç»œçš„é¢„è®­ç»ƒç¨‹åºï¼Œå…¶ä¸­å„å±‚ä½¿ç”¨ RBM é€ä¸€è®­ç»ƒã€‚è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªæ—©æœŸåº”ç”¨æ˜¯ç”¨äºé™ç»´çš„è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œã€‚ç»è¿‡é¢„è®­ç»ƒåï¼Œå¯ä»¥ä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œå…¨å±€å‚æ•°å¾®è°ƒã€‚ä½¿ç”¨ RBM è¿›è¡Œçš„é¢„è®­ç»ƒæ•æ‰åˆ°äº†æ•°æ®ä¸­çš„ç»“æ„ï¼Œä¾‹å¦‚å›¾åƒä¸­çš„è§’è½ï¼Œè€Œæ— éœ€ä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®ã€‚åœ¨æ‰¾åˆ°è¿™äº›ç»“æ„åï¼Œé€šè¿‡åå‘ä¼ æ’­å¯¹å…¶è¿›è¡Œæ ‡è®°è¢«è¯æ˜æ˜¯ä¸€ä¸ªç›¸å¯¹ç®€å•çš„ä»»åŠ¡ã€‚</p>
<blockquote>
<p>By linking layers pre-trained in this way, Hinton was able to successfully implement examples of deep and dense networks, a milestone toward what is now known as deep learning. Later on, it became possible to replace RBM-based pre-training by other methods to achieve the same performance of deep and dense ANNs.</p>
</blockquote>
<p>é€šè¿‡ä»¥è¿™ç§æ–¹å¼é“¾æ¥é¢„è®­ç»ƒçš„å±‚ï¼ŒHinton æˆåŠŸåœ°å®ç°äº†æ·±åº¦å’Œå¯†é›†ç½‘ç»œçš„ç¤ºä¾‹ï¼Œè¿™æ˜¯è¿ˆå‘ç°åœ¨æ‰€è°“çš„æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªé‡Œç¨‹ç¢‘ã€‚åæ¥ï¼Œå¯ä»¥ç”¨å…¶ä»–æ–¹æ³•æ›¿ä»£åŸºäº RBM çš„é¢„è®­ç»ƒï¼Œä»¥å®ç°æ·±åº¦å’Œå¯†é›†äººå·¥ç¥ç»ç½‘ç»œçš„ç›¸åŒæ€§èƒ½ã€‚</p>
<h1 id="anns-as-powerful-tools-in-physics-and-other-scientific-disciplines">ANNs as powerful tools in physics and other scientific disciplines<a hidden class="anchor" aria-hidden="true" href="#anns-as-powerful-tools-in-physics-and-other-scientific-disciplines">#</a></h1>
<blockquote>
<p>Much of the above discussion is focused on how physics has been a driving force underlying inventions and development of ANNs. Conversely, ANNs are increasingly playing an important role as a powerful tool for modelling and analysis in almost all of physics.</p>
</blockquote>
<p>ä¸Šè¿°è®¨è®ºçš„å¤§éƒ¨åˆ†å†…å®¹éƒ½é›†ä¸­åœ¨ç‰©ç†å­¦å¦‚ä½•æˆä¸ºæ¨åŠ¨äººå·¥ç¥ç»ç½‘ç»œå‘æ˜å’Œå‘å±•çš„é©±åŠ¨åŠ›ã€‚ç›¸åï¼Œäººå·¥ç¥ç»ç½‘ç»œä½œä¸ºä¸€ç§å¼ºå¤§çš„å»ºæ¨¡å’Œåˆ†æå·¥å…·ï¼Œåœ¨å‡ ä¹æ‰€æœ‰ç‰©ç†å­¦é¢†åŸŸä¸­å‘æŒ¥ç€è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ã€‚</p>
<blockquote>
<p>In some applications, ANNs are employed as a function approximator; i.e. the ANNs are used to provide a â€œcopycatâ€ for the physics model in question. This can significantly reduce the computational resources required, thereby allowing larger systems to be probed at higher resolution. Significant advances have been achieved in this way, e.g. for quantum-mechanical many-body problems. Here, deep learning architectures are trained to reproduce energies of phases of materials, as well as the shape and strength of interatomic forces, with an accuracy comparable to ab initio quantum-mechanical models. With these ANN trained atomic models, considerably faster determination of phase stabilities and the dynamics of new materials can be made. Examples showing the success of these methods involve the prediction of new photovoltaic materials.</p>
</blockquote>
<p>åœ¨æŸäº›åº”ç”¨ä¸­ï¼Œäººå·¥ç¥ç»ç½‘ç»œè¢«ç”¨ä½œå‡½æ•°é€¼è¿‘å™¨ï¼›å³ï¼Œäººå·¥ç¥ç»ç½‘ç»œè¢«ç”¨æ¥ä¸ºæ‰€è®¨è®ºçš„ç‰©ç†æ¨¡å‹æä¾›ä¸€ä¸ªâ€œæ¨¡ä»¿è€…â€ã€‚è¿™å¯ä»¥æ˜¾è‘—å‡å°‘æ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œä»è€Œå…è®¸ä»¥æ›´é«˜çš„åˆ†è¾¨ç‡æ¢æµ‹æ›´å¤§çš„ç³»ç»Ÿã€‚é€šè¿‡è¿™ç§æ–¹å¼å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä¾‹å¦‚å¯¹äºé‡å­åŠ›å­¦å¤šä½“é—®é¢˜ã€‚åœ¨è¿™é‡Œï¼Œæ·±åº¦å­¦ä¹ æ¶æ„ç»è¿‡è®­ç»ƒä»¥å†ç°ææ–™ç›¸çš„èƒ½é‡ï¼Œä»¥åŠåŸå­é—´åŠ›çš„å½¢çŠ¶å’Œå¼ºåº¦ï¼Œå…¶ç²¾åº¦å¯ä¸ä»å¤´ç®—é‡å­åŠ›å­¦æ¨¡å‹ç›¸åª²ç¾ã€‚é€šè¿‡è¿™äº›ç»è¿‡äººå·¥ç¥ç»ç½‘ç»œè®­ç»ƒçš„åŸå­æ¨¡å‹ï¼Œå¯ä»¥æ›´å¿«åœ°ç¡®å®šæ–°ææ–™çš„ç›¸ç¨³å®šæ€§å’ŒåŠ¨åŠ›å­¦ã€‚è¿™äº›æ–¹æ³•æˆåŠŸçš„ä¾‹å­åŒ…æ‹¬æ–°å‹å…‰ä¼ææ–™çš„é¢„æµ‹ã€‚</p>
<blockquote>
<p>With these models, it is also possible to study phase transitions as well as the thermodynamical properties of water. Similarly, the development of ANN representations has made it possible to reach higher resolutions in explicit physics-based climate models without resorting to additional computing power.</p>
</blockquote>
<p>é€šè¿‡è¿™äº›æ¨¡å‹ï¼Œè¿˜å¯ä»¥ç ”ç©¶ç›¸å˜ä»¥åŠæ°´çš„çƒ­åŠ›å­¦æ€§è´¨ã€‚ç±»ä¼¼åœ°ï¼Œäººå·¥ç¥ç»ç½‘ç»œè¡¨ç¤ºçš„å‘å±•ä½¿å¾—åœ¨ä¸è¯‰è¯¸é¢å¤–è®¡ç®—èƒ½åŠ›çš„æƒ…å†µä¸‹ï¼Œåœ¨æ˜¾å¼åŸºäºç‰©ç†çš„æ°”å€™æ¨¡å‹ä¸­è¾¾åˆ°æ›´é«˜çš„åˆ†è¾¨ç‡æˆä¸ºå¯èƒ½ã€‚</p>
<blockquote>
<p>During the 1990s, ANNs became a standard data analysis tool within particle physics experiments of ever-increasing complexity. Highly sought-after fundamental particles, such as the Higgs boson, only exist for a fraction of a second after being created in high-energy collisions (e.g. $\sim 10^{-22}$ s for the Higgs boson). Their presence needs to be inferred from tracking information and energy deposits in large electronic detectors. Often the anticipated detector signature is very rare and could be mimicked by more common background processes. To identify particle decays and increase the efficiency of analyses, ANNs were trained to pick out specific patterns in the large volumes of detector data being generated at a high rate.</p>
</blockquote>
<p>åœ¨ 20 ä¸–çºª 90 å¹´ä»£ï¼Œäººå·¥ç¥ç»ç½‘ç»œæˆä¸ºç²’å­ç‰©ç†å®éªŒä¸­ä¸€ç§æ ‡å‡†çš„æ•°æ®åˆ†æå·¥å…·ï¼Œè¿™äº›å®éªŒçš„å¤æ‚æ€§ä¸æ–­å¢åŠ ã€‚å¤‡å—è¿½æ§çš„åŸºæœ¬ç²’å­ï¼Œå¦‚å¸Œæ ¼æ–¯ç»è‰²å­ï¼Œåœ¨é«˜èƒ½ç¢°æ’ä¸­äº§ç”Ÿååªå­˜åœ¨ä¸€å°æ®µæ—¶é—´ï¼ˆä¾‹å¦‚ï¼Œå¸Œæ ¼æ–¯ç»è‰²å­çš„å¯¿å‘½çº¦ä¸º $10^{-22}$ ç§’ï¼‰ã€‚å®ƒä»¬çš„å­˜åœ¨éœ€è¦é€šè¿‡å¤§å‹ç”µå­æ¢æµ‹å™¨ä¸­çš„è·Ÿè¸ªä¿¡æ¯å’Œèƒ½é‡æ²‰ç§¯æ¥æ¨æ–­ã€‚é€šå¸¸ï¼Œé¢„æœŸçš„æ¢æµ‹å™¨ä¿¡å·éå¸¸ç½•è§ï¼Œå¹¶ä¸”å¯èƒ½è¢«æ›´å¸¸è§çš„èƒŒæ™¯è¿‡ç¨‹æ‰€æ¨¡ä»¿ã€‚ä¸ºäº†è¯†åˆ«ç²’å­è¡°å˜å¹¶æé«˜åˆ†ææ•ˆç‡ï¼Œäººå·¥ç¥ç»ç½‘ç»œè¢«è®­ç»ƒä»¥ä»é«˜é€Ÿç”Ÿæˆçš„å¤§é‡æ¢æµ‹å™¨æ•°æ®ä¸­æŒ‘é€‰å‡ºç‰¹å®šæ¨¡å¼ã€‚</p>
<blockquote>
<p>ANNs improved the sensitivity of searches for the Higgs boson at the CERN Large ElectronPositron (LEP) collider during the 1990s, and were used in the analysis of data that led to its discovery at the CERN Large Hadron Collider in 2012. ANNs were also used in studies of the top quark at Fermilab.</p>
</blockquote>
<p>äººå·¥ç¥ç»ç½‘ç»œæé«˜äº† 20 ä¸–çºª 90 å¹´ä»£ CERN å¤§å‹ç”µå­æ­£ç”µå­å¯¹æ’æœºï¼ˆLEPï¼‰ä¸Šå¯¹å¸Œæ ¼æ–¯ç»è‰²å­çš„æœç´¢çµæ•åº¦ï¼Œå¹¶è¢«ç”¨äºåˆ†æå¯¼è‡´å…¶åœ¨ 2012 å¹´ CERN å¤§å‹å¼ºå­å¯¹æ’æœºå‘ç°çš„æ•°æ®ã€‚äººå·¥ç¥ç»ç½‘ç»œè¿˜è¢«ç”¨äºè´¹ç±³å®éªŒå®¤å¯¹é¡¶å¤¸å…‹çš„ç ”ç©¶ã€‚</p>
<blockquote>
<p>In astrophysics and astronomy, ANNs have also become a standard data analysis tool. A recent example is an ANN-driven analysis of data from the IceCube neutrino detector at the South Pole, which resulted in a neutrino image of the Milky Way. Exoplanet transits have been identified by the Kepler Mission using ANNs. The Event Horizon Telescope image of the black hole at the centre of the Milky Way used ANNs for data processing.</p>
</blockquote>
<p>åœ¨å¤©ä½“ç‰©ç†å­¦å’Œå¤©æ–‡å­¦ä¸­ï¼Œäººå·¥ç¥ç»ç½‘ç»œä¹Ÿå·²æˆä¸ºä¸€ç§æ ‡å‡†çš„æ•°æ®åˆ†æå·¥å…·ã€‚ä¸€ä¸ªæœ€è¿‘çš„ä¾‹å­æ˜¯å¯¹å—æå†°ç«‹æ–¹ä¸­å¾®å­æ¢æµ‹å™¨æ•°æ®çš„äººå·¥ç¥ç»ç½‘ç»œé©±åŠ¨åˆ†æï¼Œå¾—å‡ºäº†é“¶æ²³ç³»çš„ä¸­å¾®å­å›¾åƒã€‚å¼€æ™®å‹’ä»»åŠ¡ä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œè¯†åˆ«äº†ç³»å¤–è¡Œæ˜Ÿå‡Œæ—¥ç°è±¡ã€‚äº‹ä»¶è§†ç•Œæœ›è¿œé•œå¯¹é“¶æ²³ç³»ä¸­å¿ƒé»‘æ´çš„æˆåƒä½¿ç”¨äº†äººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œæ•°æ®å¤„ç†ã€‚</p>
<blockquote>
<p>So far, the most spectacular scientific breakthrough using deep learning ANN methods is the AlphaFold tool for prediction of three-dimensional protein structures, given their amino acid sequences. In modelling of industrial physics and chemistry applications, ANNs also play an increasingly important role.</p>
</blockquote>
<p>åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ äººå·¥ç¥ç»ç½‘ç»œæ–¹æ³•å–å¾—çš„æœ€å£®è§‚çš„ç§‘å­¦çªç ´æ˜¯ AlphaFold å·¥å…·ï¼Œå®ƒå¯ä»¥æ ¹æ®æ°¨åŸºé…¸åºåˆ—é¢„æµ‹ä¸‰ç»´è›‹ç™½è´¨ç»“æ„ã€‚åœ¨å·¥ä¸šç‰©ç†å’ŒåŒ–å­¦åº”ç”¨çš„å»ºæ¨¡ä¸­ï¼Œäººå·¥ç¥ç»ç½‘ç»œä¹Ÿå‘æŒ¥ç€è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ã€‚</p>
<h1 id="anns-in-everyday-life">ANNs in everyday life<a hidden class="anchor" aria-hidden="true" href="#anns-in-everyday-life">#</a></h1>
<blockquote>
<p>The list of applications used in everyday life that are based on ANNs is long. These networks are behind almost everything we do with computers, such as image recognition, language generation, and more.</p>
</blockquote>
<p>åŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„æ—¥å¸¸ç”Ÿæ´»åº”ç”¨æ¸…å•å¾ˆé•¿ã€‚è¿™äº›ç½‘ç»œæ”¯æŒæˆ‘ä»¬ä½¿ç”¨è®¡ç®—æœºè¿›è¡Œçš„å‡ ä¹æ‰€æœ‰æ“ä½œï¼Œä¾‹å¦‚å›¾åƒè¯†åˆ«ã€è¯­è¨€ç”Ÿæˆç­‰ã€‚</p>
<blockquote>
<p>Decision support within health care is also a well-established application for ANNs. For example, a recent prospective randomized study of mammographic screening images showed a clear benefit of using machine learning in improving detection of breast cancer. Another recent example is motion correction for magnetic resonance imaging (MRI) scans.</p>
</blockquote>
<p>åŒ»ç–—ä¿å¥é¢†åŸŸçš„å†³ç­–æ”¯æŒä¹Ÿæ˜¯äººå·¥æ™ºèƒ½ç½‘ç»œçš„ä¸€ä¸ªæˆç†Ÿåº”ç”¨ã€‚ä¾‹å¦‚ï¼Œæœ€è¿‘ä¸€é¡¹å…³äºä¹³æˆ¿ X çº¿ç…§ç›¸ç­›æŸ¥å›¾åƒçš„å‰ç»æ€§éšæœºç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æœºå™¨å­¦ä¹ åœ¨æé«˜ä¹³è…ºç™Œæ£€æµ‹ç‡æ–¹é¢æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚æœ€è¿‘çš„å¦ä¸€ä¸ªä¾‹å­æ˜¯ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æçš„è¿åŠ¨æ ¡æ­£ã€‚</p>
<h1 id="concluding-remarks">Concluding remarks<a hidden class="anchor" aria-hidden="true" href="#concluding-remarks">#</a></h1>
<blockquote>
<p>The pioneering methods and concepts developed by Hopfield and Hinton have been instrumental in shaping the field of ANNs. In addition, Hinton played a leading role in the efforts to extend the methods to deep and dense ANNs.</p>
</blockquote>
<p>Hopfield å’Œ Hinton å¼€å‘çš„å¼€åˆ›æ€§æ–¹æ³•å’Œæ¦‚å¿µåœ¨å¡‘é€ äººå·¥ç¥ç»ç½‘ç»œé¢†åŸŸæ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚æ­¤å¤–ï¼ŒHinton åœ¨å°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°æ·±åº¦å’Œå¯†é›†äººå·¥ç¥ç»ç½‘ç»œçš„åŠªåŠ›ä¸­å‘æŒ¥äº†é¢†å¯¼ä½œç”¨ã€‚</p>
<blockquote>
<p>With their breakthroughs, that stand on the foundations of physical science, they have showed a completely new way for us to use computers to aid and to guide us to tackle many of the challenges our society face. Simply put, thanks to their work Humanity now has a new item in its toolbox, which we can choose to use for good purposes. Machine learning based on ANNs is currently revolutionizing science, engineering and daily life. The field is already on its way to enable breakthroughs toward building a sustainable society, e.g. by helping to identify new functional materials. How deep learning by ANNs will be used in the future depends on how we humans choose to use these incredibly potent tools, already present in many aspects of our lives.</p>
</blockquote>
<p>é€šè¿‡ä»–ä»¬çš„çªç ´ï¼Œå»ºç«‹åœ¨ç‰©ç†ç§‘å­¦åŸºç¡€ä¹‹ä¸Šï¼Œä»–ä»¬ä¸ºæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§å…¨æ–°çš„æ–¹å¼æ¥ä½¿ç”¨è®¡ç®—æœºæ¥å¸®åŠ©å’ŒæŒ‡å¯¼æˆ‘ä»¬åº”å¯¹ç¤¾ä¼šé¢ä¸´çš„è®¸å¤šæŒ‘æˆ˜ã€‚ç®€å•åœ°è¯´ï¼Œæ„Ÿè°¢ä»–ä»¬çš„å·¥ä½œï¼Œäººç±»ç°åœ¨åœ¨å…¶å·¥å…·ç®±ä¸­æœ‰äº†ä¸€ä»¶æ–°ç‰©å“ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©å°†å…¶ç”¨äºè‰¯å¥½çš„ç›®çš„ã€‚åŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ æ­£åœ¨å½»åº•æ”¹å˜ç§‘å­¦ã€å·¥ç¨‹å’Œæ—¥å¸¸ç”Ÿæ´»ã€‚è¯¥é¢†åŸŸå·²ç»åœ¨æœç€å®ç°çªç ´è¿ˆè¿›ï¼Œä»¥å»ºç«‹ä¸€ä¸ªå¯æŒç»­çš„ç¤¾ä¼šï¼Œä¾‹å¦‚é€šè¿‡å¸®åŠ©è¯†åˆ«æ–°çš„åŠŸèƒ½ææ–™ã€‚æœªæ¥æ·±åº¦å­¦ä¹ å°†å¦‚ä½•è¢«ä½¿ç”¨ï¼Œå–å†³äºæˆ‘ä»¬äººç±»å¦‚ä½•é€‰æ‹©ä½¿ç”¨è¿™äº›å·²ç»å­˜åœ¨äºæˆ‘ä»¬ç”Ÿæ´»è®¸å¤šæ–¹é¢çš„æå…¶å¼ºå¤§çš„å·¥å…·ã€‚</p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/phy/hcmp/tbg-fig-python/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>ä½¿ç”¨Pythonç»˜åˆ¶è½¬è§’çŸ³å¢¨çƒ¯</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/phy/calphy/itensor-mannual/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>ITensoråŸºç¡€è¯­æ³•è¯´æ˜</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/fz6m/Private-web@1.2/js/custom/click.min.js"></script>

</body>





<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  </head>


<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
