<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Attractor and integrator networks in the brain | æ— å¤„æƒ¹å°˜åŸƒ</title>
<meta name="keywords" content="">
<meta name="description" content="å¤§è„‘ä¸­çš„å¸å¼•å­ä¸ç§¯åˆ†ç½‘ç»œ">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/reference/attractor-and-integrator-networks-in-the-brain/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/reference/attractor-and-integrator-networks-in-the-brain/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>







<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="Attractor and integrator networks in the brain" />
<meta property="og:description" content="å¤§è„‘ä¸­çš„å¸å¼•å­ä¸ç§¯åˆ†ç½‘ç»œ" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/reference/attractor-and-integrator-networks-in-the-brain/" />
<meta property="og:image" content="https://s2.loli.net/2025/11/23/gm4Vu15dJcpnh9M.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-23T00:18:23+08:00" />
<meta property="article:modified_time" content="2025-11-23T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/11/23/gm4Vu15dJcpnh9M.png" />
<meta name="twitter:title" content="Attractor and integrator networks in the brain"/>
<meta name="twitter:description" content="å¤§è„‘ä¸­çš„å¸å¼•å­ä¸ç§¯åˆ†ç½‘ç»œ"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ“• é˜…è¯»",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "ğŸ“• æ–‡çŒ®",
          "item": "https://Muatyz.github.io/posts/read/reference/"
        }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Attractor and integrator networks in the brain",
      "item": "https://Muatyz.github.io/posts/read/reference/attractor-and-integrator-networks-in-the-brain/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Attractor and integrator networks in the brain",
  "name": "Attractor and integrator networks in the brain",
  "description": "å¤§è„‘ä¸­çš„å¸å¼•å­ä¸ç§¯åˆ†ç½‘ç»œ",
  "keywords": [
    ""
  ],
  "articleBody": "Abstract In this Review, we describe the singular success of attractor neural network models in describing how the brain maintains persistent activity states for working memory, corrects errors and integrates noisy cues. We consider the mechanisms by which simple and forgetful units can organize to collectively generate dynamics on the long timescales required for such computations.\nWe discuss the myriad potential uses of attractor dynamics for computation in the brain, and showcase notable examples of brain systems in which inherently low-dimensional continuous-attractor dynamics have been concretely and rigorously identified. Thus, it is now possible to conclusively state that the brain constructs and uses such systems for computation.\nFinally, we highlight recent theoretical advances in understanding how the fundamental trade-offs between robustness and capacity and between structure and flexibility can be overcome by reusing and recombining the same set of modular attractors for multiple functions, so they together produce representations that are structurally constrained and robust but exhibit high capacity and are flexible.\nåœ¨è¿™ç¯‡ç»¼è¿°ä¸­, æˆ‘ä»¬æè¿°äº†å¸å¼•å­ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨æè¿°å¤§è„‘å¦‚ä½•ç»´æŒå·¥ä½œè®°å¿†çš„æŒç»­æ´»åŠ¨çŠ¶æ€ã€çº æ­£é”™è¯¯å’Œç§¯åˆ†å™ªå£°çº¿ç´¢æ–¹é¢çš„ç‹¬ç‰¹æˆåŠŸ. æˆ‘ä»¬è€ƒè™‘äº†ç®€å•ä¸”æ˜“å¿˜çš„å•å…ƒå¦‚ä½•ç»„ç»‡èµ·æ¥, å…±åŒç”Ÿæˆæ‰€éœ€çš„é•¿æ—¶é—´å°ºåº¦åŠ¨åŠ›å­¦æœºåˆ¶, ä»¥è¿›è¡Œæ­¤ç±»è®¡ç®—.\næˆ‘ä»¬è®¨è®ºäº†å¸å¼•å­åŠ¨åŠ›å­¦åœ¨å¤§è„‘è®¡ç®—ä¸­çš„æ— æ•°æ½œåœ¨ç”¨é€”, å¹¶å±•ç¤ºäº†åœ¨å…¶ä¸­å›ºæœ‰çš„ä½ç»´è¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦å·²è¢«å…·ä½“ä¸”ä¸¥æ ¼è¯†åˆ«çš„æ˜¾è‘—å¤§è„‘ç³»ç»Ÿç¤ºä¾‹. å› æ­¤, ç°åœ¨å¯ä»¥æ˜ç¡®åœ°è¯´, å¤§è„‘æ„å»ºå¹¶ä½¿ç”¨è¿™æ ·çš„ç³»ç»Ÿè¿›è¡Œè®¡ç®—.\næœ€å, æˆ‘ä»¬å¼ºè°ƒäº†æœ€è¿‘åœ¨ç†è§£ç¨³å¥æ€§ä¸å®¹é‡ä¹‹é—´ä»¥åŠç»“æ„ä¸çµæ´»æ€§ä¹‹é—´çš„åŸºæœ¬æƒè¡¡æ–¹é¢çš„ç†è®ºè¿›å±•, è¿™äº›è¿›å±•å¯ä»¥é€šè¿‡é‡å¤ä½¿ç”¨å’Œé‡æ–°ç»„åˆåŒä¸€ç»„æ¨¡å—åŒ–å¸å¼•å­æ¥å…‹æœ, ä»è€Œå…±åŒäº§ç”Ÿç»“æ„å—é™ä¸”ç¨³å¥ä½†å…·æœ‰é«˜å®¹é‡ä¸”çµæ´»çš„è¡¨ç¤º.\nIntroduction One of biologyâ€™s grand challenges is to explain how order and complex function spring from inanimate physical systems composed of much simpler parts. The brain creates order in its representations of the world and performs complex functions through the collective interactions of simpler elements.\nIn this Review, we describe and evaluate the hypothesis that attractor dynamics in widespread regions of the CNS have a key role in constructing some of these representations, generating long timescales to support integration and memory functions and endowing all these functions with robustness.\nWe review the specific predictions of attractor-based models and the now extensive body of work testing these predictions. Thus, we illustrate that the theory and validation of computation with attractor dynamics in the brain is one of the biggest success stories in systems neuroscience.\nç”Ÿç‰©å­¦çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜æ˜¯è§£é‡Šæ— ç”Ÿå‘½çš„ç‰©ç†ç³»ç»Ÿå¦‚ä½•ä»æ›´ç®€å•çš„éƒ¨åˆ†ä¸­äº§ç”Ÿç§©åºå’Œå¤æ‚åŠŸèƒ½. å¤§è„‘é€šè¿‡æ›´ç®€å•å…ƒç´ çš„é›†ä½“ç›¸äº’ä½œç”¨, åœ¨å…¶å¯¹ä¸–ç•Œçš„è¡¨å¾ä¸­åˆ›é€ ç§©åºå¹¶æ‰§è¡Œå¤æ‚åŠŸèƒ½.\nåœ¨è¿™ç¯‡ç»¼è¿°ä¸­, æˆ‘ä»¬æè¿°å¹¶è¯„ä¼°äº†è¿™æ ·ä¸€ä¸ªå‡è®¾: ä¸­æ¢ç¥ç»ç³»ç»Ÿ (CNS) å¹¿æ³›åŒºåŸŸä¸­çš„å¸å¼•å­åŠ¨åŠ›å­¦åœ¨æ„å»ºè¿™äº›è¡¨å¾ã€ç”Ÿæˆæ”¯æŒæ•´åˆå’Œè®°å¿†åŠŸèƒ½çš„é•¿æ—¶é—´å°ºåº¦ä»¥åŠèµ‹äºˆæ‰€æœ‰è¿™äº›åŠŸèƒ½ä»¥ç¨³å¥æ€§æ–¹é¢èµ·ç€å…³é”®ä½œç”¨.\næˆ‘ä»¬å›é¡¾äº†åŸºäºå¸å¼•å­çš„æ¨¡å‹çš„å…·ä½“é¢„æµ‹ä»¥åŠç°åœ¨å¹¿æ³›çš„å·¥ä½œæ¥æµ‹è¯•è¿™äº›é¢„æµ‹. å› æ­¤, æˆ‘ä»¬è¯´æ˜äº†åœ¨å¤§è„‘ä¸­ä½¿ç”¨å¸å¼•å­åŠ¨åŠ›å­¦è¿›è¡Œè®¡ç®—çš„ç†è®ºå’ŒéªŒè¯æ˜¯ç³»ç»Ÿç¥ç»ç§‘å­¦ä¸­æœ€å¤§çš„æˆåŠŸæ•…äº‹ä¹‹ä¸€.\nSome of the first formal circuit-level models of brain function focused on the problem of associative memory and how neural circuits might generate spatially distributed, stable patterns of activity that could function as such a memory.\nHopfield networks, with multiple stable states constructed by inscribing input patterns into connection weights, were proposed more than four decades ago.\nNetwork models possessing a continuous set of stable states that could be used to represent continuous variables were also first proposed in the same period. Subsequently, many canonical brain circuits for motor control, sensory amplification and memory, motion integration, evidence integration, decision-making and spatial navigation have been modelled using the same general principle â€” that a set of states can be stabilized through collective positive feedback.\nä¸€äº›æœ€æ—©çš„æ­£å¼å›è·¯çº§å¤§è„‘åŠŸèƒ½æ¨¡å‹é›†ä¸­åœ¨ è”æƒ³è®°å¿† çš„é—®é¢˜ä¸Š, ä»¥åŠç¥ç»å›è·¯å¦‚ä½•äº§ç”Ÿç©ºé—´åˆ†å¸ƒçš„ç¨³å®šæ´»åŠ¨æ¨¡å¼, è¿™äº›æ¨¡å¼å¯ä»¥ä½œä¸ºè¿™æ ·çš„è®°å¿†.\nHopfield ç½‘ç»œé€šè¿‡å°†è¾“å…¥æ¨¡å¼ é“­åˆ» åˆ°è¿æ¥æƒé‡ä¸­, æ„å»ºäº†å¤šä¸ªç¨³å®šçŠ¶æ€, è¿™ä¸€æ¦‚å¿µåœ¨å››åå¤šå¹´å‰å°±è¢«æå‡ºäº†.\nå…·æœ‰ä¸€ç»„è¿ç»­ç¨³å®šçŠ¶æ€çš„ç½‘ç»œæ¨¡å‹ä¹Ÿé¦–æ¬¡åœ¨åŒä¸€æ—¶æœŸè¢«æå‡º, è¿™äº›çŠ¶æ€å¯ç”¨äºè¡¨ç¤ºè¿ç»­å˜é‡. éšå, è®¸å¤šç”¨äºè¿åŠ¨æ§åˆ¶ã€æ„Ÿè§‰æ”¾å¤§å’Œè®°å¿†ã€è¿åŠ¨ç§¯åˆ†ã€è¯æ®ç§¯åˆ†ã€å†³ç­–å’Œç©ºé—´å¯¼èˆªçš„å…¸å‹å¤§è„‘å›è·¯éƒ½ä½¿ç”¨äº†ç›¸åŒçš„ä¸€èˆ¬åŸç†è¿›è¡Œå»ºæ¨¡â€”â€”å³é€šè¿‡é›†ç¾¤æ­£åé¦ˆå¯ä»¥ç¨³å®šä¸€ç»„çŠ¶æ€.\nBecause these are circuit-level models, but were typically inspired by experimental characterization of neurons recorded singly or a few at a time, the patterns of connectivity and the cell-activity correlations in the models automatically became novel and relatively specific predictions about the population dynamics and architecture of such circuits.\nAs we discuss below, the combination of these prediction-rich (yet conceptually simple) models, modern experimental breakthroughs in the acquisition of cellular-resolution population activity data and novel and rigorous analyses of such data on the basis of the model predictions has provided much evidence that the brain constructs and exploits attractor networks for performing several essential computations.\nç”±äºè¿™äº›æ˜¯å›è·¯çº§æ¨¡å‹, ä½†é€šå¸¸æ˜¯å—å•ç‹¬æˆ–å°‘é‡è®°å½•çš„ç¥ç»å…ƒçš„å®éªŒè¡¨å¾å¯å‘, å› æ­¤æ¨¡å‹ä¸­çš„è¿æ¥æ¨¡å¼å’Œç»†èƒæ´»åŠ¨ç›¸å…³æ€§è‡ªç„¶è€Œç„¶æˆä¸ºå…³äºæ­¤ç±»å›è·¯çš„ ç¾¤ä½“åŠ¨åŠ›å­¦ å’Œæ¶æ„çš„æ–°é¢–ä¸”ç›¸å¯¹å…·ä½“çš„é¢„æµ‹.\næ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·, è¿™äº›å¯Œæœ‰é¢„æµ‹åŠ› (ä½†æ¦‚å¿µä¸Šç®€å•) æ¨¡å‹çš„ç»“åˆã€åœ¨è·å–ç»†èƒåˆ†è¾¨ç‡ç¾¤ä½“æ´»åŠ¨æ•°æ®æ–¹é¢çš„ç°ä»£å®éªŒçªç ´ä»¥åŠåŸºäºæ¨¡å‹é¢„æµ‹å¯¹è¿™äº›æ•°æ®è¿›è¡Œçš„æ–°é¢–ä¸”ä¸¥æ ¼çš„åˆ†æ, æä¾›äº†å¤§é‡è¯æ®è¡¨æ˜å¤§è„‘æ„å»ºå¹¶åˆ©ç”¨å¸å¼•å­ç½‘ç»œæ¥æ‰§è¡Œå‡ é¡¹åŸºæœ¬è®¡ç®—.\nWe begin by defining attractors, and then describe proposed mechanisms for the construction of attractor network models in neuroscience. We provide an overview of why attractor networks can be important for computation in the brain and highlight criteria for determining whether a system has non-trivial attractor dynamics.\nWe also discuss examples of brain circuits with non-trivial attractor dynamics. We end with a summary of new directions in our understanding of how these simple circuits could contribute to flexible computation through reuse in multiple contexts.\næˆ‘ä»¬é¦–å…ˆå®šä¹‰å¸å¼•å­, ç„¶åæè¿°ç¥ç»ç§‘å­¦ä¸­å¸å¼•å­ç½‘ç»œæ¨¡å‹æ„å»ºçš„å‡å®šæœºåˆ¶. æˆ‘ä»¬æ¦‚è¿°äº†ä¸ºä»€ä¹ˆå¸å¼•å­ç½‘ç»œå¯¹äºå¤§è„‘ä¸­çš„è®¡ç®—å¯èƒ½å¾ˆé‡è¦, å¹¶å¼ºè°ƒäº†ç¡®å®šç³»ç»Ÿæ˜¯å¦å…·æœ‰éå¹³å‡¡å¸å¼•å­åŠ¨åŠ›å­¦çš„æ ‡å‡†.\næˆ‘ä»¬è¿˜è®¨è®ºäº†å…·æœ‰éå¹³å‡¡å¸å¼•å­åŠ¨åŠ›å­¦çš„å¤§è„‘å›è·¯ç¤ºä¾‹. æœ€å, æˆ‘ä»¬æ€»ç»“äº†æˆ‘ä»¬å¯¹è¿™äº›ç®€å•å›è·¯å¦‚ä½•é€šè¿‡åœ¨å¤šç§ç¯å¢ƒä¸­é‡å¤ä½¿ç”¨æ¥ä¿ƒè¿›çµæ´»è®¡ç®—çš„æ–°ç†è§£æ–¹å‘.\nWhat are attractors? To define an attractor, we first define a dynamical system and its states.\nA dynamical system is a set of variables together with all the rules that determine their changes in value with the passage of time.\nThe value of these variables at any given instant is called the state of the system at that moment. The state is a point (vector) in the state space of the dynamical system.\nAn attractor is the minimal set of states in a state space, to which all nearby states eventually flow with time. One simple example of an attractor is a stable fixed point: all neighbouring states flow to it.\nTransferring these crisp mathematical definitions to the context of the brain involves challenges and simplifications that revolve around identifying a sufficiently self-contained system and the variables necessary to determine its dynamics.\nä¸ºäº†å®šä¹‰å¸å¼•å­, æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªåŠ¨åŠ›ç³»ç»ŸåŠå…¶çŠ¶æ€.\nåŠ¨åŠ›ç³»ç»Ÿæ˜¯ä¸€ç»„å˜é‡ä»¥åŠæ‰€æœ‰å†³å®šå®ƒä»¬éšæ—¶é—´å˜åŒ–çš„è§„åˆ™.\nè¿™äº›å˜é‡åœ¨ä»»ä½•ç»™å®šæ—¶åˆ»çš„å€¼ç§°ä¸ºè¯¥æ—¶åˆ»ç³»ç»Ÿçš„çŠ¶æ€. çŠ¶æ€æ˜¯åŠ¨åŠ›ç³»ç»ŸçŠ¶æ€ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ (çŸ¢é‡).\nå¸å¼•å­æ˜¯çŠ¶æ€ç©ºé—´ä¸­æœ€å°çš„çŠ¶æ€é›†, æ‰€æœ‰é™„è¿‘çŠ¶æ€æœ€ç»ˆéƒ½ä¼šéšç€æ—¶é—´æµå‘è¯¥çŠ¶æ€é›†. å¸å¼•å­çš„ä¸€ä¸ªç®€å•ä¾‹å­æ˜¯ç¨³å®šçš„ä¸åŠ¨ç‚¹: æ‰€æœ‰é‚»è¿‘çŠ¶æ€éƒ½æµå‘å®ƒ.\nå°†è¿™äº›æ¸…æ™°çš„æ•°å­¦å®šä¹‰è½¬ç§»åˆ°å¤§è„‘çš„èƒŒæ™¯ä¸­æ¶‰åŠæŒ‘æˆ˜å’Œç®€åŒ–, è¿™äº›æŒ‘æˆ˜å’Œç®€åŒ–å›´ç»•è¯†åˆ«ä¸€ä¸ª è¶³å¤Ÿè‡ªåŒ…å«çš„ç³»ç»Ÿ ä»¥åŠç¡®å®šå…¶åŠ¨åŠ›å­¦æ‰€éœ€çš„å˜é‡å±•å¼€.\nDefining the state of a neural system Inherent in the definition of a dynamical system is the assumption that there are no external dynamical inputs to the system (or, equivalently, that the system definition includes all such external variables).\nåœ¨åŠ¨åŠ›ç³»ç»Ÿçš„å®šä¹‰ä¸­, å›ºæœ‰åœ°å‡è®¾ç³»ç»Ÿæ²¡æœ‰å¤–éƒ¨åŠ¨åŠ›è¾“å…¥ (æˆ–è€…ç­‰æ•ˆåœ°, ç³»ç»Ÿå®šä¹‰åŒ…æ‹¬æ‰€æœ‰æ­¤ç±»å¤–éƒ¨å˜é‡).\nThe first simplification in characterizing the dynamics of a neural circuit is to assume that, at least on the timescale of interest, the system evolves in an autonomous way.\nGiven that subcircuits in the brain are interconnected with others, and that the brain itself interacts with the world, it is impossible to isolate these circuits completely into autonomous systems.\nHowever, we may define a notion of â€˜effectively autonomousâ€™ dynamics, whereby inputs do not vary over time and are untuned, in the sense that they do not provide differential drive to subsets of the putative set of attractor states.\nè¡¨å¾ç¥ç»å›è·¯åŠ¨åŠ›å­¦çš„ç¬¬ä¸€ä¸ªç®€åŒ–æ˜¯ å‡è®¾è‡³å°‘åœ¨æ„Ÿå…´è¶£çš„æ—¶é—´å°ºåº¦ä¸Š, ç³»ç»Ÿä»¥ è‡ªä¸» çš„æ–¹å¼æ¼”åŒ–.\né‰´äºå¤§è„‘ä¸­çš„å­å›è·¯ä¸å…¶ä»–å›è·¯ç›¸äº’è¿æ¥, å¹¶ä¸”å¤§è„‘æœ¬èº«ä¸ä¸–ç•Œäº’åŠ¨, å› æ­¤ä¸å¯èƒ½å°†è¿™äº›å›è·¯å®Œå…¨éš”ç¦»æˆè‡ªä¸»ç³»ç»Ÿ.\nç„¶è€Œ, æˆ‘ä»¬å¯ä»¥å®šä¹‰ â€œç­‰æ•ˆè‡ªä¸»â€ åŠ¨åŠ›å­¦çš„æ¦‚å¿µ, å³è¾“å…¥ä¸éšæ—¶é—´å˜åŒ–ä¸”æœªå—è°ƒåˆ¶, ä»è€Œä¸ä¼šä¸ºå‡å®šçš„å¸å¼•å­çŠ¶æ€é›†çš„å­é›†æä¾›å·®å¼‚é©±åŠ¨.\nThe second simplification is in defining the states of the system.\nThe changes in state of a circuit in the brain over time may depend on the detailed pattern of all the spikes in all neurons, the levels of associated ions, neurotransmitters and modulators, and even the states of the ion channels.\nThe weights and connections between neurons may be considered as parameters (rather than variables) on short timescales, but are themselves variables if considering a longer timescale.\nOne widely used simplification in describing a neural circuit on the timescale of seconds is to use just the spiking outputs of the neurons in the circuit as the states, often further simplified as time-varying spike rates.\nIf such a description is sufficient to predict the state changes of the system at the relevant timescales, it can be viewed as a reasonable dynamical system model of the circuit.\nAlthough spike or spike-rate descriptions ignore subcellular and molecular variables to make the grossly simplifying assumption that the relevant circuit dynamics are governed by spikes, the state space of a vertebrate microcircuit described in this way is nevertheless very high-dimensional, comprising the number of neurons in the circuit, which can be in the order of $10^2-10^7$ cells.\nAs we discuss below, such simplified models can nevertheless yield rich and accurate predictions about neural circuits.\nç¬¬äºŒä¸ªç®€åŒ–æ˜¯åœ¨å®šä¹‰ç³»ç»ŸçŠ¶æ€æ—¶.\nå¤§è„‘ä¸­å›è·¯éšæ—¶é—´å˜åŒ–çš„çŠ¶æ€å˜åŒ–å¯èƒ½å–å†³äºæ‰€æœ‰ç¥ç»å…ƒä¸­æ‰€æœ‰è„‰å†²çš„è¯¦ç»†æ¨¡å¼ã€ç›¸å…³ç¦»å­ã€ç¥ç»é€’è´¨ å’Œ è°ƒèŠ‚å‰‚ çš„æ°´å¹³, ç”šè‡³ç¦»å­é€šé“çš„çŠ¶æ€.\nåœ¨çŸ­æ—¶é—´å°ºåº¦ä¸Š, ç¥ç»å…ƒä¹‹é—´çš„æƒé‡å’Œè¿æ¥å¯ä»¥è¢«è§†ä¸ºå‚æ•° (è€Œä¸æ˜¯å˜é‡) , ä½†å¦‚æœè€ƒè™‘æ›´é•¿çš„æ—¶é—´å°ºåº¦, å®ƒä»¬æœ¬èº«å°±æ˜¯å˜é‡.\nåœ¨æè¿°ç§’çº§æ—¶é—´å°ºåº¦ä¸Šçš„ç¥ç»å›è·¯æ—¶, ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„ç®€åŒ–æ˜¯ä»…ä½¿ç”¨å›è·¯ä¸­ç¥ç»å…ƒçš„ è„‰å†²è¾“å‡º ä½œä¸ºçŠ¶æ€, é€šå¸¸è¿›ä¸€æ­¥ç®€åŒ–ä¸ºå«æ—¶è„‰å†²ç‡.\nå¦‚æœè¿™æ ·çš„æè¿°è¶³ä»¥é¢„æµ‹ç›¸å…³æ—¶é—´å°ºåº¦ä¸Šç³»ç»Ÿçš„çŠ¶æ€å˜åŒ–, åˆ™å¯ä»¥å°†å…¶è§†ä¸ºå›è·¯çš„åˆç†åŠ¨åŠ›ç³»ç»Ÿæ¨¡å‹.\nå°½ç®¡è„‰å†²æˆ–è„‰å†²ç‡è¡¨è¿°å¿½ç•¥äº†äºšç»†èƒå’Œåˆ†å­å˜é‡, ä»¥åšå‡ºç›¸å…³å›è·¯åŠ¨åŠ›å­¦ç”±è„‰å†²æ§åˆ¶çš„ç²—ç•¥ç®€åŒ–å‡è®¾, ä½†ä»¥è¿™ç§æ–¹å¼æè¿°çš„ è„Šæ¤åŠ¨ç‰© å¾®å›è·¯çš„çŠ¶æ€ç©ºé—´ä»ç„¶æ˜¯éå¸¸é«˜ç»´çš„, åŒ…æ‹¬å›è·¯ä¸­çš„ç¥ç»å…ƒæ•°é‡, å¯èƒ½åœ¨ $10^2-10^7$ ä¸ªç»†èƒçš„èŒƒå›´å†….\næ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·, è¿™æ ·çš„ç®€åŒ–æ¨¡å‹ä»ç„¶å¯ä»¥äº§ç”Ÿå…³äºç¥ç»å›è·¯ä¸°å¯Œä¸”å‡†ç¡®çš„é¢„æµ‹.\nAttractors exist in various flavours: an attractor may consist of a single state, a set of discrete states, a set of states that effectively behave as a continuous set or many such near-continuous sets (Fig. 1).\nIf a set of attractor states traces out a shape in state space that is approximately continuous and locally Euclidean, it is known as an attractor manifold.\nNonlinear continuous-attractor manifolds can be curved and topologically complex (for example, resembling rings, tori and so on; Fig. 1c,d, rightmost column). States on an attractor may be stationary, or might flow along the attractor to trace out trajectories that are periodic (known as limit cycles; Fig. 1f, rightmost column) or chaotic (that is, with dynamics that are inherently unpredictable owing to high sensitivity to small changes in the state).\nå¸å¼•å­å­˜åœ¨å„ç§å½¢å¼: å¸å¼•å­å¯ä»¥ç”±å•ä¸ªçŠ¶æ€ã€ä¸€ç»„ç¦»æ•£çŠ¶æ€ã€ä¸€ç»„æœ‰æ•ˆåœ°è¡¨ç°ä¸ºè¿ç»­é›†çš„çŠ¶æ€æˆ–è®¸å¤šè¿™æ ·çš„è¿‘è¿ç»­é›†ç»„æˆ (å›¾ 1).\nå¦‚æœä¸€ç»„å¸å¼•å­çŠ¶æ€åœ¨çŠ¶æ€ç©ºé—´ä¸­æç»˜å‡ºä¸€ä¸ªè¿‘ä¼¼è¿ç»­ä¸”å±€éƒ¨æ¬§å¼å‡ ä½•çš„å½¢çŠ¶, åˆ™ç§°å…¶ä¸º å¸å¼•å­æµå½¢.\néçº¿æ€§è¿ç»­å¸å¼•å­æµå½¢å¯ä»¥æ˜¯å¼¯æ›²ä¸”æ‹“æ‰‘å¤æ‚çš„ (ä¾‹å¦‚, ç±»ä¼¼äºç¯ã€ç¯é¢ç­‰; å›¾ 1cã€d, æœ€å³åˆ—). å¸å¼•å­ä¸Šçš„çŠ¶æ€å¯ä»¥æ˜¯é™æ­¢çš„, æˆ–è€…å¯èƒ½æ²¿ç€å¸å¼•å­æµåŠ¨ä»¥æç»˜å‡ºå‘¨æœŸæ€§è½¨è¿¹ (ç§°ä¸º æé™ç¯; å›¾ 1f, æœ€å³åˆ—) æˆ– æ··æ²Œ (å³, ç”±äºå¯¹çŠ¶æ€å¾®å°å˜åŒ–çš„é«˜åº¦æ•æ„Ÿæ€§è€Œå…·æœ‰å›ºæœ‰ä¸å¯é¢„æµ‹æ€§çš„åŠ¨åŠ›å­¦).\nFig. 1 | Mechanisms of attractor formation.\nLeft columns: open grey circles represent neurons, and connections between them are excitatory (black lines ending in bars) or inhibitory (black lines ending in circles). For layout of neurons and connections, connectivity matrices are shown as the inset, with black to white colours indicating strongly inhibitory to excitatory interactions, respectively.\nMiddle columns: examples of stable population activity patterns.\nRight columns: state-space views of population states and dynamics. Red circles with shades of blue rings indicate the activity states shown in middle column; grey lines denote transient dynamic trajectories and red denotes attracting states.\nå›¾ 1 | å¸å¼•å­å½¢æˆæœºåˆ¶.\nå·¦åˆ—: å¼€æ”¾çš„ç°è‰²åœ†åœˆä»£è¡¨ç¥ç»å…ƒ, è¿æ¥å®ƒä»¬çš„æ˜¯ å…´å¥‹æ€§ (ä»¥æ£’ç»“å°¾çš„é»‘çº¿) æˆ– æŠ‘åˆ¶æ€§ (ä»¥åœ†åœˆç»“å°¾çš„é»‘çº¿). å¯¹äºç¥ç»å…ƒå’Œè¿æ¥çš„å¸ƒå±€, æ’å›¾ä¸­æ˜¾ç¤ºäº†è¿æ¥çŸ©é˜µ, é»‘è‰²åˆ°ç™½è‰²è¡¨ç¤ºå¼ºæŠ‘åˆ¶åˆ°(å¼º)å…´å¥‹æ€§ç›¸äº’ä½œç”¨.\nä¸­é—´åˆ—: ç¨³å®šçš„ç¾¤ä½“æ´»åŠ¨æ¨¡å¼ç¤ºä¾‹.\nå³åˆ—: ç¾¤ä½“çŠ¶æ€å’ŒåŠ¨åŠ›å­¦çš„çŠ¶æ€ç©ºé—´è§†å›¾. å¸¦æœ‰è“è‰²ç¯é˜´å½±çš„çº¢è‰²åœ†åœˆè¡¨ç¤ºä¸­é—´åˆ—ä¸­æ˜¾ç¤ºçš„æ´»åŠ¨çŠ¶æ€; ç°è‰²çº¿è¡¨ç¤ºç¬æ€åŠ¨æ€è½¨è¿¹, çº¢è‰²è¡¨ç¤ºå¸å¼•çŠ¶æ€.\na, A network with dense symmetric connections determined by associative Hebbian learning on a set of input patterns (middle) stores them as stable attractor states. This defines a Hopfield network.\naã€ç”±ä¸€ç»„è¾“å…¥æ¨¡å¼ä¸Šçš„è”æƒ³ Hebbian å­¦ä¹ ç¡®å®šçš„å¯†é›†å¯¹ç§°è¿æ¥ç½‘ç»œ (ä¸­é—´) å°†å®ƒä»¬å­˜å‚¨ä¸ºç¨³å®šçš„å¸å¼•å­çŠ¶æ€. è¿™å®šä¹‰äº† Hopfield ç½‘ç»œ.\nb, Disjoint groups of neurons that interact through within-group excitation and across-group inhibition lead to group winner-takes-all (WTA) dynamics. Stable states are any patterns with one winning group. The state-space plot collapses all activities of neurons in group $g_{i}$ along the axis $r_{gi}$.\nbã€é€šè¿‡ ç»„å†…å…´å¥‹ å’Œ ç»„é—´æŠ‘åˆ¶ ç›¸äº’ä½œç”¨çš„ä¸ç›¸äº¤ç¥ç»å…ƒç»„å¯¼è‡´ç»„èµ¢å®¶é€šåƒ (WTA) åŠ¨åŠ›å­¦. ç¨³å®šçŠ¶æ€æ˜¯å…·æœ‰ä¸€ä¸ªè·èƒœç»„çš„ä»»ä½•æ¨¡å¼. çŠ¶æ€ç©ºé—´å›¾æ²¿è½´ $r_{g_{i}}$ æŠ˜å äº†ç»„ $g_{i}$ ä¸­ç¥ç»å…ƒçš„æ‰€æœ‰æ´»åŠ¨.\nc, Neurons arranged in a ring with global inhibition and either local excitation or a lack of local inhibition, combined with uniform excitatory input to all neurons, produce localized activity bumps (middle) as the stable states.\nBumps may be centred anywhere on the neural ring, defining a near-continuum of attractor states that form a ring in state space (right).\ncã€åœ¨ç¯ä¸Šæ’åˆ—çš„ç¥ç»å…ƒå…·æœ‰ å…¨å±€æŠ‘åˆ¶ å’Œ å±€éƒ¨å…´å¥‹ æˆ– å±€éƒ¨æŠ‘åˆ¶ç¼ºä¹, ç»“åˆå¯¹æ‰€æœ‰ç¥ç»å…ƒçš„å‡åŒ€å…´å¥‹è¾“å…¥, äº§ç”Ÿå±€éƒ¨æ´»åŠ¨å³°å€¼ (ä¸­é—´) ä½œä¸ºç¨³å®šçŠ¶æ€.\nå³°å€¼å¯ä»¥ä½äºç¥ç»ç¯ä¸Šçš„ä»»ä½•ä½ç½®, åœ¨çŠ¶æ€ç©ºé—´ä¸­å®šä¹‰äº†ä¸€ä¸ªè¿‘è¿ç»­çš„å¸å¼•å­çŠ¶æ€ç¯ (å³).\nd, Neurons arranged on a two-dimensional neural sheet, interacting through local inhibition and either centre excitation or a lack of inhibition near the centre with uniform excitatory input to all neurons, result in a pattern of multiple periodically spaced activity bumps (middle).\nAny two-dimensional phase shift of the periodic pattern up to the lattice periodicity results in distinct but equivalent stable states, and then the states repeat; thus, the result is a torus of stable states.\ndã€åœ¨äºŒç»´ç¥ç»ç‰‡ä¸Šæ’åˆ—çš„ç¥ç»å…ƒé€šè¿‡ å±€éƒ¨æŠ‘åˆ¶ ç›¸äº’ä½œç”¨, å¹¶ä¸”åœ¨ä¸­å¿ƒé™„è¿‘å…·æœ‰ ä¸­å¿ƒå…´å¥‹ æˆ– æŠ‘åˆ¶ç¼ºä¹, å¯¹æ‰€æœ‰ç¥ç»å…ƒè¿›è¡Œå‡åŒ€å…´å¥‹è¾“å…¥, å¯¼è‡´å¤šä¸ªå‘¨æœŸæ€§é—´éš”æ´»åŠ¨å³°å€¼çš„æ¨¡å¼ (ä¸­é—´).\nå‘¨æœŸæ€§æ¨¡å¼çš„ä»»ä½•(æ™¶æ ¼å‘¨æœŸæ€§çš„)äºŒç»´ç›¸ç§»éƒ½ä¼šå¯¼è‡´ä¸åŒä½†ç­‰æ•ˆçš„ç¨³å®šçŠ¶æ€, ç„¶åçŠ¶æ€é‡å¤; å› æ­¤, ç»“æœæ˜¯ä¸€ä¸ªç¨³å®šçŠ¶æ€ç¯é¢.\nåœ†ç¯å’Œåœ†ç¯çš„ç›´ç§¯å½¢æˆç¯é¢(torus), å³ $S^{1}\\otimes S^{1} = T^{2}$\nç”¨åˆ°çš„è¿æ¥æ–¹å¼è¢«æ¯”å–»ä¸º Mexican-hat. e, Two neuron groups with in-group excitation and across-group inhibition, precisely tuned interaction strengths and quasi-linear neural input-output responses can counteract activity decay in the network and produce persistent activity over a continuum of activity levels in the two populations, defining ramp-like neural tuning and a line of attractor states.\neã€ä¸¤ä¸ªå…·æœ‰ ç»„å†…å…´å¥‹ å’Œ ç»„é—´æŠ‘åˆ¶ã€ç²¾ç¡®è°ƒåˆ¶ç›¸äº’ä½œç”¨å¼ºåº¦å’Œå‡†çº¿æ€§ç¥ç»è¾“å…¥-è¾“å‡ºå“åº”çš„ç¥ç»å…ƒç»„å¯ä»¥æŠµæ¶ˆç½‘ç»œä¸­çš„æ´»åŠ¨è¡°å‡, å¹¶åœ¨ä¸¤ä¸ªç¾¤ä½“ä¸­äº§ç”Ÿè¿ç»­çš„æ´»åŠ¨æ°´å¹³ä¸Šçš„æŒç»­æ´»åŠ¨, å®šä¹‰äº†é˜¶æ¢¯çŠ¶ç¥ç»è°ƒåˆ¶å’Œä¸€ç³»åˆ—å¸å¼•å­çŠ¶æ€.\nf, Neurons arranged on a ring with asymmetric connections drive a flow of neural activity in a particular direction.\nThe network forms localized activity bumps that sequentially move around the ring in that direction (middle). The state space contains a limit-cycle attractor (right).\nfã€åœ¨ç¯ä¸Šæ’åˆ—çš„ç¥ç»å…ƒå…·æœ‰ä¸å¯¹ç§°è¿æ¥, é©±åŠ¨ç¥ç»æ´»åŠ¨æœç‰¹å®šæ–¹å‘æµåŠ¨.\nç½‘ç»œå½¢æˆå±€éƒ¨æ´»åŠ¨å³°å€¼, ä¾æ¬¡å›´ç»•è¯¥ç¯ç§»åŠ¨ (ä¸­é—´). çŠ¶æ€ç©ºé—´åŒ…å«ä¸€ä¸ª æé™ç¯å¸å¼•å­ (å³).\ng, The copy-and-offset mechanism for constructing integrators, illustrated for the ring (left) and grid (right) attractor circuits. Each network copy receives velocity inputs tuned to the corresponding shift direction.\ngã€æ„å»ºç§¯åˆ†å™¨çš„ å¤åˆ¶å’Œåç§»æœºåˆ¶, è¯´æ˜äº†ç¯ (å·¦) å’Œç½‘æ ¼ (å³) å¸å¼•å­å›è·¯. æ¯ä¸ªç½‘ç»œå‰¯æœ¬æ¥æ”¶è°ƒåˆ¶äºç›¸åº”åç§»æ–¹å‘çš„é€Ÿåº¦è¾“å…¥.\nVarious combinations of such attractors, of different dimensions, geometries and topologies, may coexist in different regions of the state space of a single dynamical system.\nTypically, the set of attractors in a dynamical system comprises a small subset of the state space, and attractor manifolds are usually much lower-dimensional than the state space.\nIn cases in which a system has multiple attractor states, the initial condition determines the attractor state to which the system flows.\nå„ç§ä¸åŒç»´åº¦ã€å‡ ä½•å½¢çŠ¶å’Œæ‹“æ‰‘ç»“æ„çš„å¸å¼•å­ç»„åˆå¯èƒ½å…±å­˜äºå•ä¸€åŠ¨åŠ›ç³»ç»ŸçŠ¶æ€ç©ºé—´çš„ä¸åŒåŒºåŸŸä¸­.\né€šå¸¸, åŠ¨åŠ›ç³»ç»Ÿä¸­çš„å¸å¼•å­é›†åˆæ„æˆçŠ¶æ€ç©ºé—´çš„ä¸€ä¸ªå°å­é›†, å¹¶ä¸”å¸å¼•å­æµå½¢é€šå¸¸æ¯”çŠ¶æ€ç©ºé—´ç»´æ•°ä½å¾—å¤š.\nè‹¥ç³»ç»Ÿå…·æœ‰å¤šä¸ªå¸å¼•å­çŠ¶æ€, åˆå§‹æ¡ä»¶å†³å®šäº†ç³»ç»Ÿæµå‘çš„å¸å¼•å­çŠ¶æ€.\nAttractors in the presence of noise Any real physical system unavoidably behaves non-deterministically from the perspective of a model of the system.\nThis is because one cannot observe and describe all variables, and all uncharacterized variables together with true stochastic sources of variation (such as synaptic signalling noise from stochastic vesicle release; fluctuations in ion concentrations during processes such as spike initiation and calcium signalling; or fluctuations in small copy numbers of proteins) serve as effective sources of noise in the model.\nNoise can disrupt states so they do not strictly localize to the attractor described in a noise-free version of the model, and can drive the system to escape from an attractor over time.\nHowever, the general idea of attractor states remains, in that, if the system is initialized near such a state, it tends to flow towards it and subsequently remains localized around it, for extended periods.\nä»ç³»ç»Ÿæ¨¡å‹çš„è§’åº¦æ¥çœ‹, ä»»ä½•çœŸå®çš„ç‰©ç†ç³»ç»Ÿä¸å¯é¿å…åœ°è¡¨ç°å‡ºä¸ç¡®å®šæ€§.\nè¿™æ˜¯å› ä¸ºæ— æ³•è§‚å¯Ÿå’Œæè¿°æ‰€æœ‰å˜é‡, æ‰€æœ‰æœªè¡¨å¾çš„å˜é‡ä»¥åŠçœŸæ­£çš„éšæœºå˜åŒ–æºå…±åŒä½œä¸ºæ¨¡å‹ä¸­çš„ç­‰æ•ˆå™ªå£°æº (ä¾‹å¦‚æ¥è‡ªéšæœºå›Šæ³¡é‡Šæ”¾çš„ çªè§¦ä¿¡å·å™ªå£°; åœ¨å°–å³°å¯åŠ¨å’Œé’™ä¿¡å·ç­‰è¿‡ç¨‹ä¸­ç¦»å­æµ“åº¦çš„æ³¢åŠ¨; æˆ–è›‹ç™½è´¨çš„å°æ‹·è´æ•°æ³¢åŠ¨).\nå™ªå£°å¯èƒ½ä¼šå¹²æ‰°çŠ¶æ€, å› æ­¤(çŠ¶æ€)ä¸ä¼šä¸¥æ ¼ å®šåŸŸ åˆ°æ— å™ªå£°æ¨¡å‹ä¸­æè¿°çš„å¸å¼•å­, ä¸”éšç€æ—¶é—´çš„æ¨ç§», å™ªå£°å¯èƒ½é©±åŠ¨ç³»ç»Ÿé€ƒç¦»å¸å¼•å­.\nç„¶è€Œ, å¹¿ä¹‰çš„å¸å¼•å­çŠ¶æ€æ¦‚å¿µä»ç„¶å­˜åœ¨, å³å¦‚æœç³»ç»Ÿåˆæ€åœ¨è¿™æ ·çš„çŠ¶æ€é™„è¿‘, å®ƒå€¾å‘äºæµå‘å®ƒå¹¶éšåé•¿æ—¶é—´ä¿æŒåœ¨å…¶å‘¨å›´.\nBecause attractor states are where systems tend to localize (when not externally driven), they should be observable in the autonomous dynamics of real systems.\nThis basic property is the basis for the most fundamental and robust tests of attractor dynamics in neural systems, as we discuss below.\nIn a nutshell, the central signatures of attractors in real systems (discussed in more detail in later sections of this Review) can be summarized as: the localization of the states of a system to a lower-dimensional subset; the flow of the states towards the subset after perturbation; and the long-time and (effectively) autonomous stability of states in that subset.\nç”±äºå¸å¼•å­çŠ¶æ€æ˜¯ç³»ç»Ÿ (æ²¡æœ‰å¤–éƒ¨é©±åŠ¨æ—¶) å€¾å‘äºå®šåŸŸçš„åœ°æ–¹, å› æ­¤å®ƒä»¬åº”è¯¥å¯ä»¥åœ¨çœŸå®ç³»ç»Ÿçš„è‡ªä¸»åŠ¨åŠ›å­¦ä¸­è§‚å¯Ÿåˆ°.\nè¿™ä¸€åŸºæœ¬å±æ€§æ˜¯ç¥ç»ç³»ç»Ÿä¸­å¸å¼•å­åŠ¨åŠ›å­¦æœ€åŸºæœ¬å’Œæœ€ç¨³å¥æµ‹è¯•çš„åŸºç¡€, æ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·.\nç®€è€Œè¨€ä¹‹, çœŸå®ç³»ç»Ÿä¸­å¸å¼•å­çš„ä¸­å¿ƒç‰¹å¾ (åœ¨æœ¬ç»¼è¿°çš„åç»­éƒ¨åˆ†ä¸­å°†æ›´è¯¦ç»†åœ°è®¨è®º) å¯ä»¥æ€»ç»“ä¸º: ç³»ç»ŸçŠ¶æ€å®šåŸŸè‡³ä½ç»´å­é›†; å¾®æ‰°åçŠ¶æ€æµå‘è¯¥å­é›†; ä»¥åŠè¯¥å­é›†ä¸­çŠ¶æ€çš„é•¿æœŸå’Œ (æœ‰æ•ˆåœ°) è‡ªä¸»ç¨³å®šæ€§.\nConstruction and mechanisms The general principle underlying the formation of non-trivial attractor states in neural circuits is strong recurrent positive feedback. Positive feedback fights activity decay to stabilize certain states, and has been posited to be the basis for the stabilization of memory traces and persistent activity in the brain.\nWhich states become stabilized into attractors depends on how the network sculpts the positive feedback, which, according to the synaptic hypothesis, is determined by synaptic weights.\nåœ¨ç¥ç»å›è·¯ä¸­å½¢æˆéå¹³å‡¡å¸å¼•å­çŠ¶æ€çš„åŸºæœ¬åŸç†æ˜¯ å¼ºçƒˆçš„é€’å½’æ­£åé¦ˆ. æ­£åé¦ˆæŠµæŠ—æ¿€æ´»è¡°å‡ä»¥ç¨³å®šæŸäº›çŠ¶æ€, è¿™è¢«å‡å®šä¸ºå¤§è„‘ä¸­è®°å¿†ç—•è¿¹å’ŒæŒç»­æ¿€æ´»ç¨³å®šåŒ–çš„åŸºç¡€.\næ ¹æ®çªè§¦å‡è¯´, â€œä½•ç§çŠ¶æ€è¢«ç¨³å®šä¸ºå¸å¼•å­â€ å–å†³äº â€œç½‘ç»œå¦‚ä½•å¡‘é€ æ­£åé¦ˆâ€, éœ€è¦è¢«çªè§¦æƒé‡å†³å®š.\nIn general, characterizing the relationship between structure and function in a large collection of interacting elements is extremely difficult.\nFor example, a large collection of simple polar three-atom molecules of hydrogen and oxygen give rise to the emergent phenomena we associate with water â€” such as liquidness, wetness and freezing into a solid â€” that cannot be predicted through intuition or by drawing box and arrow diagrams.\nNevertheless, the transitions and properties of emergent states can be described relatively simply, with very few key parameters and variables.\nä¸€èˆ¬æ¥è¯´, è¡¨å¾å¤§é‡ç›¸äº’ä½œç”¨å…ƒç´ ä¹‹é—´çš„ç»“æ„ä¸åŠŸèƒ½å…³ç³»æ˜¯éå¸¸å›°éš¾çš„.\nä¾‹å¦‚, å¤§é‡ç”±æ°¢å’Œæ°§ç»„æˆçš„ç®€å•ææ€§ä¸‰åŸå­åˆ†å­äº§ç”Ÿäº†æˆ‘ä»¬ä¸æ°´ç›¸å…³çš„ æ¶Œç°ç°è±¡â€”â€”æ¯”å¦‚æ¶²æ€ã€æ¹¿æ¶¦å’Œå†»ç»“æˆå›ºä½“â€”â€”è¿™äº›ç°è±¡æ— æ³•é€šè¿‡ç›´è§‰æˆ–ç»˜åˆ¶æ¡†å›¾å’Œç®­å¤´å›¾æ¥é¢„æµ‹.\nç„¶è€Œ, æ¶Œç°çŠ¶æ€ çš„è½¬å˜å’Œå±æ€§å¯ä»¥ç›¸å¯¹ç®€å•åœ°æè¿°, åªéœ€å¾ˆå°‘çš„å…³é”®å‚æ•°å’Œå˜é‡.\nOne way to characterize the relationship between synaptic weights and attractor dynamics is to ask what attractor states a given set of weights produces (the â€˜forwardâ€™ problem).\nWith a given set of weights, one can simulate a circuit and explore the resulting dynamics to find attractors of the system. A more powerful method, the Lyapunov function approach, holds for symmetric weight matrices ($W_{ij} = W_{ji}$) and ratebased neural dynamics.\nFor this class of models, a generalized energy function (the Lyapunov function), which is a function of the weights and neural activation function, analytically specifies the networkâ€™s dynamics.\nStable and unstable attractor states are the energy minima and maxima of the derived landscape, respectively, and the networkâ€™s state flows downhill towards the attractors (Fig. 2e) in the way a ball rolls down a gravitational potential.\nè¡¨å¾çªè§¦æƒé‡ä¸å¸å¼•å­åŠ¨åŠ›å­¦ä¹‹é—´å…³ç³»çš„ä¸€ç§æ–¹æ³•æ˜¯è¯¢é—®ç»™å®šæƒé‡é›†äº§ç”Ÿäº†å“ªäº›å¸å¼•å­çŠ¶æ€ ( â€œæ­£å‘â€ é—®é¢˜).\né€šè¿‡ç»™å®šä¸€ç»„æƒé‡, å¯ä»¥æ¨¡æ‹Ÿå›è·¯å¹¶æ¢ç´¢ç”±æ­¤äº§ç”Ÿçš„åŠ¨åŠ›å­¦ä»¥æ‰¾åˆ°ç³»ç»Ÿçš„å¸å¼•å­. æ›´å¼ºå¤§çš„æ–¹æ³•æ˜¯ Lyapunov å‡½æ•°æ–¹æ³•, å®ƒé€‚ç”¨äºå¯¹ç§°æƒé‡çŸ©é˜µ ($W_{ij} = W_{ji}$) å’ŒåŸºäºé€Ÿç‡çš„ç¥ç»åŠ¨åŠ›å­¦.\nå¯¹äºè¿™ä¸€ç±»æ¨¡å‹, å¹¿ä¹‰èƒ½é‡å‡½æ•° (Lyapunov å‡½æ•°) , å®ƒæ˜¯æƒé‡å’Œç¥ç»æ¿€æ´»å‡½æ•°çš„å‡½æ•°, è§£æåœ°æŒ‡å®šäº†ç½‘ç»œçš„åŠ¨åŠ›å­¦.\nç¨³å®šå’Œä¸ç¨³å®šçš„å¸å¼•å­çŠ¶æ€åˆ†åˆ«æ˜¯å¯¼å‡ºæ™¯è§‚çš„èƒ½é‡æå°å€¼å’Œæå¤§å€¼, ç½‘ç»œçš„çŠ¶æ€æ²¿ç€å¸å¼•å­å‘ä¸‹æµåŠ¨ (å›¾ 2e) , å°±åƒçƒæ²¿ç€å¼•åŠ›åŠ¿æ»šåŠ¨ä¸€æ ·.\nFig. 2 | The utility of low-dimensional attractor networks.\nå›¾ 2 | ä½ç»´å¸å¼•å­ç½‘ç»œçš„å®ç”¨æ€§.\na, Persistent and stable states generated by attractor networks (red) can be used to represent and remember external variables (blue) by constructing an appropriate mapping between them (vertical lines).\naã€ å¸å¼•å­ç½‘ç»œ (çº¢è‰²) äº§ç”Ÿçš„æŒç»­ä¸”ç¨³å®šçš„çŠ¶æ€å¯ä»¥é€šè¿‡åœ¨å®ƒä»¬ä¹‹é—´æ„å»ºé€‚å½“çš„æ˜ å°„ (å‚ç›´çº¿) æ¥è¡¨ç¤ºå’Œè®°å¿†å¤–éƒ¨å˜é‡ (è“è‰²).\nb, Attractor networks can correct errors by mapping noisy states to the nearest attractor state.\n$N$-dimensional noise drawn from the unit sphere centred on a one-dimensional attractor has a projection strength of only $1/N$ along the attractor: in this counter-intuitive high-dimensional geometry, a ball is more similar to a pancake, with the attractor orthogonal to the large dimensions.\nbã€å¸å¼•å­ç½‘ç»œå¯ä»¥é€šè¿‡å°† å—å™ªçŠ¶æ€ æ˜ å°„åˆ°æœ€è¿‘çš„å¸å¼•å­çŠ¶æ€æ¥çº æ­£é”™è¯¯.\nä»ä»¥ä¸€ç»´å¸å¼•å­ä¸ºä¸­å¿ƒçš„å•ä½çƒä¸­ç»˜åˆ¶çš„ $N$ ç»´å™ªå£°åœ¨å¸å¼•å­ä¸Šçš„æŠ•å½±å¼ºåº¦ä»…ä¸º $1/N$: åœ¨è¿™ç§è¿åç›´è§‰çš„é«˜ç»´å‡ ä½•ä¸­, çƒæ›´ç±»ä¼¼äºç…é¥¼, å¸å¼•å­ä¸å¤§ç»´åº¦æ­£äº¤.\nc, Flow to the nearest (continuous or discrete) attractor can perform a nearest-neighbour computation and, thus, perform classification. For example, the two attractors may represent â€˜catâ€™ and â€˜dogâ€™ perceptual manifolds, and the blue dot a specific input data point.\ncã€æµå‘æœ€è¿‘çš„ (è¿ç»­æˆ–ç¦»æ•£) å¸å¼•å­å¯ä»¥æ‰§è¡Œ æœ€è¿‘é‚»è®¡ç®—, ä»è€Œæ‰§è¡Œåˆ†ç±». ä¾‹å¦‚, ä¸¤ä¸ªå¸å¼•å­å¯èƒ½ä»£è¡¨ â€œçŒ«â€ å’Œ â€œç‹—â€ çš„æ„ŸçŸ¥æµå½¢, è“ç‚¹ä»£è¡¨ç‰¹å®šçš„è¾“å…¥æ•°æ®ç‚¹.\nd, Left: continuous attractors can become integrators if velocities or movements in the external space are inputs to the network and induce proportional shifts in the internal attractor state. The current state on the attractor is then the integral of past velocity inputs relative to the starting state.\nRight: if the input to an integrating attractor consists of temporally varying evidence pulses (bottom, evidence about one option in dark blue and evidence about the opposing option in light blue), these will move the state on the attractor (top) so the systemâ€™s current state reflects the integral of the total evidence.\ndã€å·¦: å¦‚æœå¤–éƒ¨ç©ºé—´ä¸­çš„é€Ÿåº¦æˆ–è¿åŠ¨æ˜¯ç½‘ç»œçš„è¾“å…¥å¹¶å¼•èµ·å†…éƒ¨å¸å¼•å­çŠ¶æ€çš„æˆæ¯”ä¾‹åç§», åˆ™è¿ç»­å¸å¼•å­å¯ä»¥æˆä¸ºç§¯åˆ†å™¨. é‚£ä¹ˆ, å¸å¼•å­ä¸Šçš„å½“å‰çŠ¶æ€æ˜¯ç›¸å¯¹äºèµ·å§‹çŠ¶æ€çš„è¿‡å»é€Ÿåº¦è¾“å…¥çš„ç§¯åˆ†.\nå³: å¦‚æœç§¯åˆ†å¸å¼•å­çš„è¾“å…¥ç”±æ—¶é—´å˜åŒ–çš„ è¯æ®è„‰å†² ç»„æˆ (åº•éƒ¨, æ·±è“è‰²è¡¨ç¤ºä¸€ä¸ªé€‰é¡¹çš„è¯æ®, æµ…è“è‰²è¡¨ç¤ºç›¸åé€‰é¡¹çš„è¯æ®) , è¿™äº›å°†ç§»åŠ¨å¸å¼•å­ä¸Šçš„çŠ¶æ€ (é¡¶éƒ¨) , å› æ­¤ç³»ç»Ÿçš„å½“å‰çŠ¶æ€åæ˜ äº†æ‰€æœ‰è¯æ®çš„ç§¯åˆ†.\ne, The energy ($E$) landscape of a combined integration and decision-making network: inputs push the state left or right, and as the system integrates, the network state also moves towards one of two discrete attractors (left and right; white arrows, two sample trajectories). Arrival in the basin of one of the discrete attractors is a decision point.\neã€ç§¯åˆ†å’Œå†³ç­–ç½‘ç»œçš„èƒ½é‡ ($E$) æ™¯è§‚: è¾“å…¥å‘å·¦æˆ–å‘å³æ¨åŠ¨çŠ¶æ€, å¹¶ä¸”éšç€ç³»ç»Ÿç§¯åˆ†, ç½‘ç»œçŠ¶æ€ä¹Ÿç§»åŠ¨åˆ°ä¸¤ä¸ªç¦»æ•£å¸å¼•å­ä¹‹ä¸€ (å·¦å’Œå³; ç™½è‰²ç®­å¤´, ä¸¤ä¸ªæ ·æœ¬è½¨è¿¹). åˆ°è¾¾å…¶ä¸­ä¸€ä¸ªç¦»æ•£å¸å¼•å­çš„ç›†åœ°æ˜¯ä¸€ä¸ªå†³ç­–ç‚¹.\nf, An integrator can be quickly re-purposed to represent multiple different and new external variables simply by yoking its velocity shift mechanism to different external velocities cues through feedforward learning.\nThis mechanism also supports zero-shot learning and inference: given an initial state and an input velocity trajectory, it will generate a self-consistent representation for the current state even if the trajectory is different and new each time.\nfã€ç§¯åˆ†å™¨å¯ä»¥é€šè¿‡å°†å…¶é€Ÿåº¦åç§»æœºåˆ¶ä¸ä¸åŒçš„å¤–éƒ¨é€Ÿåº¦çº¿ç´¢é€šè¿‡å‰é¦ˆå­¦ä¹ è”ç³»èµ·æ¥, å¿«é€Ÿé‡æ–°ç”¨äºè¡¨ç¤ºå¤šä¸ªä¸åŒä¸”æ–°çš„å¤–éƒ¨å˜é‡.\nè¿™ç§æœºåˆ¶è¿˜æ”¯æŒ é›¶æ ·æœ¬å­¦ä¹  å’Œæ¨ç†: ç»™å®šåˆå§‹çŠ¶æ€å’Œè¾“å…¥é€Ÿåº¦è½¨è¿¹, å³ä½¿æ¯æ¬¡è½¨è¿¹ä¸åŒä¸”å…¨æ–°, å®ƒä¹Ÿä¼šç”Ÿæˆå½“å‰çŠ¶æ€çš„è‡ªæ´½è¡¨ç¤º.\ng, A set of (continuous or discrete) attractor subnetworks (red boxes at bottom) can interact bidirectionally with a shared network to form a high-capacity attractor network.\ngã€ä¸€ç»„ (è¿ç»­æˆ–ç¦»æ•£) å¸å¼•å­å­ç½‘ç»œ (åº•éƒ¨çš„çº¢è‰²æ¡†) å¯ä»¥ä¸å…±äº«ç½‘ç»œåŒå‘äº¤äº’ä»¥å½¢æˆé«˜å®¹é‡å¸å¼•å­ç½‘ç»œ.\nh, Mixed modular representations can enable representation of inputs of different dimensions, by reusing the same attractors of fixed dimension each. Velocities ($v_{i}$) from external spaces of potentially different dimension are selected by a set of selection signals ($s_{i}$). The selected velocity (green) is routed through random projections to a set of $M$ modular integrator networks of dimension $K$ each. This kind of mixed modular circuit can interchangeably represent various input spaces of dimension $D \\leq MK$ while smoothly trading off resolution for dimension.\nhã€æ··åˆæ¨¡å—åŒ–è¡¨è±¡ å¯ä»¥é€šè¿‡é‡å¤ä½¿ç”¨æ¯ä¸ªå›ºå®šç»´åº¦çš„ç›¸åŒå¸å¼•å­æ¥è¡¨ç¤ºä¸åŒç»´åº¦çš„è¾“å…¥. ä¸€ç»„é€‰æ‹©ä¿¡å· ($s_{i}$) é€‰æ‹©æ¥è‡ªæ½œåœ¨ä¸åŒç»´åº¦çš„å¤–éƒ¨ç©ºé—´çš„é€Ÿåº¦ ($v_{i}$). é€‰æ‹©çš„é€Ÿåº¦ (ç»¿è‰²) é€šè¿‡éšæœºæŠ•å½±è·¯ç”±åˆ°ä¸€ç»„æ¯ä¸ªç»´åº¦ä¸º $K$ çš„ $M$ ä¸ªæ¨¡å—åŒ–ç§¯åˆ†å™¨ç½‘ç»œ. è¿™ç§æ··åˆæ¨¡å—åŒ–å›è·¯å¯ä»¥äº’æ¢åœ°è¡¨ç¤ºå„ç§ç»´åº¦ä¸º $D \\leq MK$ çš„è¾“å…¥ç©ºé—´, åŒæ—¶å¹³æ»‘åœ°æƒè¡¡åˆ†è¾¨ç‡ä¸ç»´åº¦.\nAnother way to characterize the relationship between attractors and network structure is to consider the â€˜inverseâ€™ problem: given a set of attractors, what network structure could generate it?\nNeuroscientists want to solve the inverse problem to make predictions about underlying mechanisms and, because neural activations are more readily observed than synaptic weights, the inverse problem is more frequently encountered than the forward problem.\nBy contrast, evolution, the brain and artificially intelligent systems must solve the inverse problem to be able to perform computations that require a given type of attractor dynamics (discussed below). Theoretical neuroscience has discovered some solutions to the inverse problem for different types of attractors, as we describe below.\nå¦ä¸€ç§è¡¨å¾å¸å¼•å­ä¸ç½‘ç»œç»“æ„ä¹‹é—´å…³ç³»çš„æ–¹æ³•æ˜¯è€ƒè™‘ â€œé€†â€ é—®é¢˜: ç»™å®šä¸€ç»„å¸å¼•å­, ä»€ä¹ˆç½‘ç»œç»“æ„å¯ä»¥ç”Ÿæˆå®ƒï¼Ÿ\nç¥ç»ç§‘å­¦å®¶å¸Œæœ›è§£å†³é€†é—®é¢˜ä»¥å¯¹æ½œåœ¨æœºåˆ¶è¿›è¡Œé¢„æµ‹, å¹¶ä¸”ç”±äºç¥ç»æ¿€æ´»æ¯”çªè§¦æƒé‡æ›´å®¹æ˜“è§‚å¯Ÿåˆ°, å› æ­¤é€†é—®é¢˜æ¯”æ­£å‘é—®é¢˜æ›´å¸¸è§.\nç›¸æ¯”ä¹‹ä¸‹, è¿›åŒ–ã€å¤§è„‘å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿå¿…é¡»è§£å†³é€†é—®é¢˜, ä»¥ä¾¿èƒ½å¤Ÿæ‰§è¡Œéœ€è¦ç‰¹å®šç±»å‹å¸å¼•å­åŠ¨åŠ›å­¦çš„è®¡ç®— (ä¸‹é¢è®¨è®º). æ­£å¦‚æˆ‘ä»¬ä¸‹é¢æè¿°çš„é‚£æ ·, ç†è®ºç¥ç»ç§‘å­¦å·²ç»å‘ç°äº†ä¸€äº›ä¸åŒç±»å‹å¸å¼•å­çš„é€†é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ.\nDiscrete attractors A well-known prescription for creating a set of discrete attractors at user-defined points is given by the Hopfield model5 (Fig. 1a).\nInput patterns of neural activation are inscribed into the network weights through a Hebbian-like learning rule, such that co-active neurons are connected by excitatory interactions and inhibit all the rest. Thus, these patterns stabilize themselves and become attractor states. If a sufficiently small number of patterns are learned, they can be retrieved from partial or corrupted versions of the stored states, and thus the network can be said to store content-addressable memories.\nMore generally, the attractors of simple rate-based networks with arbitrary symmetric weight matrices and without communication delays consist entirely of fixed points.\nSome non-symmetric networks can also support point attractors, but not generically, and they can require additional mechanisms such as homeostatic plasticity.\nä¸€ç§ä¼—æ‰€å‘¨çŸ¥çš„åœ¨è‡ªå®šä¹‰çš„ç‚¹åˆ›å»ºä¸€ç»„ç¦»æ•£å¸å¼•å­çš„å¤„ç†æ–¹æ³•æ˜¯ Hopfield æ¨¡å‹ (å›¾ 1a).\né€šè¿‡ç±»ä¼¼ Hebbian çš„å­¦ä¹ è§„åˆ™å°†ç¥ç»æ¿€æ´»çš„è¾“å…¥æ¨¡å¼ é“­åˆ» åˆ°ç½‘ç»œæƒé‡ä¸­, ä½¿å¾—å…±åŒæ¿€æ´»çš„ç¥ç»å…ƒé€šè¿‡å…´å¥‹æ€§ç›¸äº’ä½œç”¨è¿æ¥å¹¶æŠ‘åˆ¶å…¶ä½™éƒ¨åˆ†. å› æ­¤, è¿™äº›æ¨¡å¼ç¨³å®šè‡ªèº«å¹¶æˆä¸ºå¸å¼•å­çŠ¶æ€. å¦‚æœå­¦ä¹ çš„æ¨¡å¼æ•°é‡è¶³å¤Ÿå°‘, åˆ™å¯ä»¥ä»å­˜å‚¨çŠ¶æ€çš„éƒ¨åˆ†æˆ–æŸåç‰ˆæœ¬ä¸­æ£€ç´¢å®ƒä»¬, å› æ­¤å¯ä»¥è¯´ç½‘ç»œå­˜å‚¨äº† å†…å®¹å¯å¯»å€è®°å¿†.\næ›´ä¸€èˆ¬åœ°è¯´, å…·æœ‰ä»»æ„å¯¹ç§°æƒé‡çŸ©é˜µä¸”æ²¡æœ‰é€šä¿¡å»¶è¿Ÿçš„ç®€å•åŸºäºé€Ÿç‡çš„ç½‘ç»œçš„å¸å¼•å­å®Œå…¨ç”± ä¸åŠ¨ç‚¹ ç»„æˆ.\nä¸€äº›éå¯¹ç§°ç½‘ç»œä¹Ÿå¯ä»¥æ”¯æŒç‚¹å¸å¼•å­, ä½†ä¸æ˜¯é€šç”¨çš„, å¹¶ä¸”å®ƒä»¬å¯èƒ½éœ€è¦è¯¸å¦‚ç¨³æ€å¯å¡‘æ€§ä¹‹ç±»çš„é™„åŠ æœºåˆ¶.\nAttractor states in Hopfield-like networks typically have highly overlapping neural memberships, even when they are well separated in the state space (Fig. 1a, middle column). Thus, there is not a clear notion of distinct â€˜cell assembliesâ€™. In a special case of Hopfield networks, neurons are partitioned into largely disjointed groups with self-excitation within groups and inhibition between groups.\nIn these winner-take-all (WTA) networks, the attractor states consist of largely non-overlapping active cell groups, which might then be called â€˜assembliesâ€™ (Fig. 1b).\nHopfield ç±»ç½‘ç»œä¸­çš„å¸å¼•å­çŠ¶æ€é€šå¸¸å…·æœ‰é«˜åº¦é‡å çš„ç¥ç»å…ƒæˆå‘˜, å³ä½¿å®ƒä»¬åœ¨çŠ¶æ€ç©ºé—´ä¸­åˆ†ç¦»è‰¯å¥½ (å›¾ 1a, ä¸­é—´åˆ—). å› æ­¤, æ²¡æœ‰æ˜ç¡®çš„ â€œç»†èƒé›†ç¾¤â€ æ¦‚å¿µ. åœ¨ Hopfield ç½‘ç»œçš„ä¸€ä¸ªç‰¹æ®Šæƒ…å†µä¸‹, ç¥ç»å…ƒè¢«åˆ’åˆ†ä¸ºå¤§è‡´ä¸ç›¸äº¤çš„ç»„, ç»„å†…å…·æœ‰è‡ªæˆ‘å…´å¥‹, ç»„é—´å…·æœ‰æŠ‘åˆ¶.\nåœ¨è¿™äº› èµ¢å®¶é€šåƒ (WTA) ç½‘ç»œä¸­, å¸å¼•å­çŠ¶æ€ç”±å¤§è‡´ä¸é‡å çš„æ´»è·ƒç»†èƒç¾¤ç»„æˆ, ç„¶åå¯ä»¥ç§°ä¹‹ä¸º â€œé›†ç¾¤â€ (å›¾ 1b).\nContinuous attractors How can one construct networks with a continuum of stationary attractor states?\nWeight matrices with a particular symmetry (across the diagonal) give rise to discrete attractors, as we have seen.\nIf the weights instead exhibit a continuous symmetry â€” for example, if the weight profiles are invariant across neurons (they look the same at each neuron, thus the symmetry is translational) â€” then the set of formed attractors will be related by the same symmetry and could thus form a continuous set.\nå¦‚ä½•æ„å»ºå…·æœ‰è¿ç»­é™æ­¢å¸å¼•å­çŠ¶æ€çš„ç½‘ç»œï¼Ÿ\næˆ‘ä»¬å·²ç»çœ‹åˆ°, å…·æœ‰ç‰¹å®šå¯¹è§’çº¿å¯¹ç§°æ€§çš„æƒé‡çŸ©é˜µä¼šäº§ç”Ÿç¦»æ•£å¸å¼•å­.\nå¦‚æœæƒé‡è¡¨ç°å‡ºè¿ç»­å¯¹ç§°æ€§â€”â€”ä¾‹å¦‚, å¦‚æœæƒé‡é…ç½®åœ¨ç¥ç»å…ƒä¹‹é—´æ˜¯ä¸å˜çš„ (å®ƒä»¬åœ¨æ¯ä¸ªç¥ç»å…ƒå¤„çœ‹èµ·æ¥ç›¸åŒ, å› æ­¤å¯¹ç§°æ€§æ˜¯å¹³ç§»çš„) â€”â€”é‚£ä¹ˆå½¢æˆçš„å¸å¼•å­é›†å°†ä¸ç›¸åŒçš„å¯¹ç§°æ€§ç›¸å…³è”, å› æ­¤å¯ä»¥å½¢æˆè¿ç»­é›†.\nThe general principle for the formation of stationary continuous attractors is pattern formation. Simple and spatially local competitive interactions across the neural sheet lead to the emergence of spatially structured activity patterns that are stable states: neurons with excitatory coupling between them become co-active and suppress the rest of their neighbours through inhibition in what is known as a linear Turing instability.\nå½¢æˆé™æ­¢è¿ç»­å¸å¼•å­çš„åŸºæœ¬åŸç†æ˜¯ æ¨¡å¼å½¢æˆ. ç¥ç»ç‰‡ä¸Šç®€å•ä¸”ç©ºé—´å±€åŸŸçš„ç«äº‰æ€§ç›¸äº’ä½œç”¨å¯¼è‡´ç©ºé—´ç»“æ„åŒ–æ´»åŠ¨æ¨¡å¼çš„å‡ºç°, è¿™äº›æ¨¡å¼æ˜¯ç¨³å®šçŠ¶æ€: é€šè¿‡å…´å¥‹æ€§è€¦åˆçš„ç¥ç»å…ƒå˜å¾—å…±åŒæ´»è·ƒ, å¹¶é€šè¿‡æŠ‘åˆ¶æŠ‘åˆ¶å…¶ä½™è¿‘é‚», è¿™è¢«ç§°ä¸ºçº¿æ€§ Turing ä¸ç¨³å®šæ€§.\nThree conditions are generally sufficient (although not strictly necessary) to provide a solution to the inverse problem for forming stationary continuous attractors (Box 1).\nFirst, the system must include nonlinear neurons with saturating responses or inhibition-dominated recurrent interactions and a uniform excitatory drive to keep network activity bounded.\nSecond, the system must involve sufficiently strong recurrent weights with competitive dynamics in the form of local excitation or disinhibition, with broader inhibition, to drive spontaneous pattern formation through the Turing instability; these patterns become the attractor states.\nLast, the system requires some continuous symmetry in the weights (a continuous weight symmetry is one where as some variable is varied continuously, the weights remain invariant), such as translational or rotational invariance (Fig. 1c,d), to ensure a continuum of attractor states.\né€šå¸¸æœ‰ä¸‰ä¸ªæ¡ä»¶è¶³ä»¥ (å°½ç®¡ä¸æ˜¯ä¸¥æ ¼å¿…è¦çš„) ä¸º â€œå½¢æˆé™æ­¢è¿ç»­å¸å¼•å­â€ æä¾›é€†é—®é¢˜çš„è§£ (æ¡† 1).\né¦–å…ˆ, ç³»ç»Ÿå¿…é¡»åŒ…æ‹¬å…·æœ‰é¥±å’Œå“åº”çš„éçº¿æ€§ç¥ç»å…ƒ, æˆ–ä»¥æŠ‘åˆ¶ä¸ºä¸»å¯¼çš„é€’å½’ç›¸äº’ä½œç”¨ä»¥åŠå‡åŒ€çš„å…´å¥‹é©±åŠ¨ä»¥ä¿æŒç½‘ç»œæ´»åŠ¨æœ‰ç•Œ.\nå…¶æ¬¡, ç³»ç»Ÿå¿…é¡»æ¶‰åŠè¶³å¤Ÿå¼ºçš„ é€’å½’æƒé‡, å…·æœ‰å±€éƒ¨å…´å¥‹æˆ–å»æŠ‘åˆ¶å½¢å¼çš„ç«äº‰åŠ¨åŠ›å­¦, ä»¥åŠæ›´å¹¿æ³›çš„æŠ‘åˆ¶, ä»¥é€šè¿‡ Turing ä¸ç¨³å®šæ€§é©±åŠ¨è‡ªå‘æ¨¡å¼å½¢æˆ; è¿™äº›æ¨¡å¼æˆä¸ºå¸å¼•å­çŠ¶æ€.\næœ€å, ç³»ç»Ÿéœ€è¦æƒé‡ä¸­çš„æŸç§è¿ç»­å¯¹ç§°æ€§ (è¿ç»­æƒé‡å¯¹ç§°æ€§æ˜¯æŒ‡éšç€æŸä¸ªå˜é‡çš„è¿ç»­å˜åŒ–, æƒé‡ä¿æŒä¸å˜) , ä¾‹å¦‚å¹³ç§»æˆ–æ—‹è½¬ä¸å˜æ€§ (å›¾ 1cã€d) , ä»¥ç¡®ä¿å¸å¼•å­çŠ¶æ€çš„è¿ç»­æ€§.\nBOX 1\nAttractor dynamics, anatomical topography and weight symmetries\nAnatomical topography, in which functionally similar neurons are near one another, is neither a necessary nor a sufficient condition for the existence of an attractor, because any low-dimensional attractor network is mathematically unchanged if all weights are preserved but neuron locations are scrambled. However, if the network is merely a spatially scrambled version of the idealized model, then the symmetries of the weight matrix can be revealed after an appropriate reordering of the neurons. An advantage of anatomical topography from a biological perspective is that it can reduce the complexity of development, in that wiring decisions can be guided by spatial proximity rather than depending entirely on activity or other target cell-signalling mechanisms. For example, the locally competitive interactions of grid and head-direction circuit models could be largely constructed through local arborization. Anatomical topography also reduces overall wiring length in the mature circuit. However, a circuit with three-dimensional dynamics or higher that are represented in an unfactorizable form cannot be embedded topographically in a two-dimensional cell layout, limiting the feasibility of topographic layouts for circuits that represent higher-dimensional unfactorizable manifolds.\nåŠŸèƒ½ç›¸ä¼¼çš„ç¥ç»å…ƒå½¼æ­¤é è¿‘çš„è§£å‰–æ‹“æ‰‘æ—¢ä¸æ˜¯å¸å¼•å­å­˜åœ¨çš„å¿…è¦æ¡ä»¶, ä¹Ÿä¸æ˜¯å……åˆ†æ¡ä»¶, å› ä¸ºå¦‚æœä¿ç•™æ‰€æœ‰æƒé‡ä½†ç¥ç»å…ƒä½ç½®è¢«æ‰“ä¹±, ä»»ä½•ä½ç»´å¸å¼•å­ç½‘ç»œåœ¨æ•°å­¦ä¸Šéƒ½æ˜¯ä¸å˜çš„. ç„¶è€Œ, å¦‚æœç½‘ç»œä»…ä»…æ˜¯ç†æƒ³åŒ–æ¨¡å‹çš„ç©ºé—´æ··ä¹±ç‰ˆæœ¬, é‚£ä¹ˆåœ¨é€‚å½“é‡æ–°æ’åºç¥ç»å…ƒå, å¯ä»¥æ­ç¤ºæƒé‡çŸ©é˜µçš„å¯¹ç§°æ€§. ä»ç”Ÿç‰©å­¦è§’åº¦æ¥çœ‹, è§£å‰–æ‹“æ‰‘çš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯å®ƒå¯ä»¥å‡å°‘å‘å±•çš„å¤æ‚æ€§, å› ä¸ºå¸ƒçº¿å†³ç­–å¯ä»¥é€šè¿‡ç©ºé—´æ¥è¿‘æ€§æ¥æŒ‡å¯¼, è€Œä¸å®Œå…¨ä¾èµ–äºæ´»åŠ¨æˆ–å…¶ä»–ç›®æ ‡ç»†èƒä¿¡å·æœºåˆ¶. ä¾‹å¦‚, ç½‘æ ¼å’Œå¤´éƒ¨æ–¹å‘å›è·¯æ¨¡å‹çš„å±€éƒ¨ç«äº‰æ€§ç›¸äº’ä½œç”¨å¯ä»¥ä¸»è¦é€šè¿‡å±€éƒ¨æ ‘çªå½¢æˆæ¥æ„å»º. æˆç†Ÿå›è·¯ä¸­çš„è§£å‰–æ‹“æ‰‘è¿˜å‡å°‘äº†æ•´ä½“å¸ƒçº¿é•¿åº¦. ç„¶è€Œ, å…·æœ‰ä¸‰ç»´åŠ¨åŠ›å­¦æˆ–æ›´é«˜ç»´åº¦åŠ¨åŠ›å­¦ä¸”ä»¥ä¸å¯åˆ†è§£å½¢å¼è¡¨ç¤ºçš„å›è·¯æ— æ³•åœ¨äºŒç»´ç»†èƒå¸ƒå±€ä¸­è¿›è¡Œæ‹“æ‰‘åµŒå…¥, è¿™é™åˆ¶äº†è¡¨ç¤ºé«˜ç»´ä¸å¯åˆ†è§£æµå½¢çš„å›è·¯çš„æ‹“æ‰‘å¸ƒå±€çš„å¯è¡Œæ€§.\nIn addition, the posited weight symmetries in simple models of attractors need not exist in a biological instance of the circuit with the same dynamics: unscrambling or reordering neurons may not be sufficient to reveal the symmetries. Consider, for example, a scenario in which low-dimensional attractor dynamics are generated by a recurrent network of $N$ neurons, but are only needed downstream in a set of $M \u003c N$ neurons. In this situation, the weight symmetries needed for continuous-attractor dynamics can be spread across both the recurrent and readout networks, such that the weights of the recurrent network alone will not reflect the relevant symmetries. Unveiling the symmetry in the circuit weights will require combining the readout weights with the recurrent ones.\næ­¤å¤–, ç®€å•å¸å¼•å­æ¨¡å‹ä¸­å‡å®šçš„æƒé‡å¯¹ç§°æ€§ä¸ä¸€å®šå­˜åœ¨äºå…·æœ‰ç›¸åŒåŠ¨åŠ›å­¦çš„å›è·¯çš„ç”Ÿç‰©å®ä¾‹ä¸­: æ‰“ä¹±æˆ–é‡æ–°æ’åºç¥ç»å…ƒå¯èƒ½ä¸è¶³ä»¥æ­ç¤ºå¯¹ç§°æ€§. ä¾‹å¦‚, è€ƒè™‘è¿™æ ·ä¸€ç§æƒ…å†µ: ä½ç»´å¸å¼•å­åŠ¨åŠ›å­¦ç”± $N$ ä¸ªç¥ç»å…ƒçš„é€’å½’ç½‘ç»œç”Ÿæˆ, ä½†ä»…åœ¨ä¸‹æ¸¸çš„ä¸€ç»„ $M \u003c N$ ä¸ªç¥ç»å…ƒä¸­éœ€è¦. åœ¨è¿™ç§æƒ…å†µä¸‹, è¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦æ‰€éœ€çš„æƒé‡å¯¹ç§°æ€§å¯ä»¥åˆ†å¸ƒåœ¨é€’å½’å’Œè¯»å‡ºç½‘ç»œä¸­, å› æ­¤ä»…é€’å½’ç½‘ç»œçš„æƒé‡å°†ä¸ä¼šåæ˜ ç›¸å…³çš„å¯¹ç§°æ€§. æ­ç¤ºå›è·¯æƒé‡ä¸­çš„å¯¹ç§°æ€§å°†éœ€è¦å°†è¯»å‡ºæƒé‡ä¸é€’å½’æƒé‡ç»“åˆèµ·æ¥.\nThese considerations give rise to a hypothesis for circuits with continuous attractors of dimension $\\leq 2$: evolutionarily conserved circuits that do not require extensive early experience should be topographically organized. We might thus predict that the circuit that originates head-direction signals in mammals should be topographically organized. By contrast, if low-dimensional dynamics only emerge on the basis of activity-dependent plasticity with repetitive training, we may not expect the circuit to be topographically organized (or even localized to a single brain region).\nè¿™äº›è€ƒè™‘ä¸ºç»´åº¦ $\\leq 2$ çš„è¿ç»­å¸å¼•å­å›è·¯æå‡ºäº†ä¸€ä¸ªå‡è®¾: ä¸éœ€è¦å¹¿æ³›æ—©æœŸç»éªŒçš„è¿›åŒ–ä¿å®ˆå›è·¯åº”è¯¥æ˜¯æ‹“æ‰‘ç»„ç»‡çš„. å› æ­¤, æˆ‘ä»¬å¯ä»¥é¢„æµ‹èµ·æºäºå“ºä¹³åŠ¨ç‰©å¤´éƒ¨æ–¹å‘ä¿¡å·çš„å›è·¯åº”è¯¥æ˜¯æ‹“æ‰‘ç»„ç»‡çš„. ç›¸æ¯”ä¹‹ä¸‹, å¦‚æœä½ç»´åŠ¨åŠ›å­¦ä»…åŸºäºå…·æœ‰é‡å¤è®­ç»ƒçš„æ´»åŠ¨ä¾èµ–æ€§å¯å¡‘æ€§å‡ºç°, æˆ‘ä»¬å¯èƒ½ä¸æœŸæœ›è¯¥å›è·¯æ˜¯æ‹“æ‰‘ç»„ç»‡çš„ (ç”šè‡³ä¸å±€é™äºå•ä¸ªå¤§è„‘åŒºåŸŸ).\nRemarkably, despite these caveats, and in a beautiful example of the predictive power of simple theories in neuroscience, empirical evidence from the anatomy of the zebrafish oculomotor integrator and the fly head-direction circuit in the past few years shows that nature has used precisely the hypothesized constructions proposed in simple circuit models to build some integrator networks.\nå€¼å¾—æ³¨æ„çš„æ˜¯, å°½ç®¡å­˜åœ¨è¿™äº›è­¦å‘Š, å¹¶ä¸”åœ¨ç¥ç»ç§‘å­¦ä¸­ç®€å•ç†è®ºé¢„æµ‹èƒ½åŠ›çš„ä¸€ä¸ªç¾ä¸½ä¾‹å­ä¸­, è¿‡å»å‡ å¹´æ¥è‡ªæ–‘é©¬é±¼çœ¼åŠ¨ç§¯åˆ†å™¨å’Œæœè‡å¤´éƒ¨æ–¹å‘å›è·¯è§£å‰–å­¦çš„å®è¯è¯æ®è¡¨æ˜, è‡ªç„¶ç•Œå·²ç»ä½¿ç”¨äº†ç®€å•å›è·¯æ¨¡å‹ä¸­æå‡ºçš„å‡è®¾æ„é€ æ¥æ„å»ºä¸€äº›ç§¯åˆ†å™¨ç½‘ç»œ.\nA special set of networks generate continuous-attractor dynamics without pattern formation: those with linear, planar or hyperplanar attractors that are generated by neurons with linear or near-linear response functions.\nIn circuits of linear neurons, the feedback within the network is a linear function of activity ($Wr$, where $W$ is the weight matrix and $r$ are the neural activities), as is the activity decay (given by $âˆ’r$). Such networks can stabilize non-zero activity states simply by tuning positive feedback to cancel the decay. The matrix $W$ can direct feedback in state space; if feedback is directed largely along one dimension, the network can support a line attractor (Fig. 1e). If it is directed equally along two or more dimensions, it can support a plane or hyperplane attractor. To create long-lived attractors requires that the network feedback magnitude is finely tuned to precisely cancel the decay, in contrast to pattern-forming continuous-attractor systems where the weight shapes (but not magnitudes) are tuned to maintain continuous symmetry across neurons.\nä¸€ç»„ç‰¹æ®Šçš„ç½‘ç»œåœ¨æ²¡æœ‰æ¨¡å¼å½¢æˆçš„æƒ…å†µä¸‹äº§ç”Ÿè¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦: é‚£äº›ç”±å…·æœ‰çº¿æ€§æˆ–è¿‘çº¿æ€§å“åº”å‡½æ•°çš„ç¥ç»å…ƒç”Ÿæˆçš„çº¿æ€§ã€å¹³é¢æˆ–è¶…å¹³é¢å¸å¼•å­.\nåœ¨çº¿æ€§ç¥ç»å…ƒå›è·¯ä¸­, ç½‘ç»œå†…çš„åé¦ˆæ˜¯æ´»åŠ¨çš„çº¿æ€§å‡½æ•° ($Wr$, å…¶ä¸­ $W$ æ˜¯æƒé‡çŸ©é˜µ, $r$ æ˜¯ç¥ç»æ´»åŠ¨) , æ´»åŠ¨è¡°å‡ä¹Ÿæ˜¯å¦‚æ­¤ (ç”± $âˆ’r$ ç»™å‡º). é€šè¿‡è°ƒèŠ‚æ­£åé¦ˆä»¥æŠµæ¶ˆè¡°å‡, è¿™æ ·çš„ç½‘ç»œå¯ä»¥ç¨³å®šéé›¶æ´»åŠ¨çŠ¶æ€. çŸ©é˜µ $W$ å¯ä»¥åœ¨çŠ¶æ€ç©ºé—´ä¸­å¼•å¯¼åé¦ˆ; å¦‚æœåé¦ˆä¸»è¦æ²¿ä¸€ä¸ªç»´åº¦å¼•å¯¼, ç½‘ç»œå¯ä»¥æ”¯æŒçº¿æ€§å¸å¼•å­ (å›¾ 1e). å¦‚æœå®ƒåœ¨ä¸¤ä¸ªæˆ–æ›´å¤šç»´åº¦ä¸Šå‡åŒ€å¼•å¯¼, å®ƒå¯ä»¥æ”¯æŒå¹³é¢æˆ–è¶…å¹³é¢å¸å¼•å­. è¦åˆ›å»ºé•¿å¯¿å‘½å¸å¼•å­, è¦æ±‚ç½‘ç»œåé¦ˆå¹…åº¦è¢«ç²¾ç»†è°ƒåˆ¶ä»¥ç²¾ç¡®æŠµæ¶ˆè¡°å‡, è¿™ä¸æ¨¡å¼å½¢æˆçš„è¿ç»­å¸å¼•å­ç³»ç»Ÿå½¢æˆå¯¹æ¯”, åœ¨è¿™äº›ç³»ç»Ÿä¸­, æƒé‡å½¢çŠ¶ (è€Œä¸æ˜¯å¹…åº¦) è¢«è°ƒåˆ¶ä»¥ä¿æŒç¥ç»å…ƒä¹‹é—´çš„è¿ç»­å¯¹ç§°æ€§.\nç³»ç»Ÿçš„çº¿æ€§åŠ¨åŠ›å­¦:\n$$ \\frac{\\mathrm{d}r}{\\mathrm{d}t} = Wr - r $$\n$W$ çš„ä¸º 1 çš„ç‰¹å¾å€¼æ•°é‡ç¡®å®šäº†å¸å¼•å­çš„ç»´æ•°. æ¯”å¦‚çº¿å¸å¼•å­(1 ä¸ª), å¹³é¢å¸å¼•å­(2 ä¸ª)â€¦\n$\u003e1$ ä»£è¡¨æ¿€æ´»å‘æ•£, $\u003c1$ ä»£è¡¨æ¿€æ´»è¡°å‡.\nNon-stationary continuous attractors Large non-symmetric networks with nonlinear neurons and strong connectivity generically exhibit limit-cycle attractors or chaotic dynamics.\nJust as point attractors emerge generically in large networks with strong symmetric weights and bounded state spaces, chaotic attractors emerge generically in large recurrent networks with strong asymmetric weights.\nAdequate asymmetries are easily achieved if excitatory and inhibitory synapses emerge from distinct sets of neurons, as biologically necessitated by Daleâ€™s law.\nå«æœ‰éçº¿æ€§ç¥ç»å…ƒå’Œå¼ºè¿æ¥æ€§çš„éå¯¹ç§°å¤§å‹ç½‘ç»œé€šå¸¸è¡¨ç°å‡º æé™ç¯å¸å¼•å­ æˆ– æ··æ²ŒåŠ¨åŠ›å­¦.\næ­£å¦‚åœ¨å…·æœ‰å¼ºå¯¹ç§°æƒé‡å’Œæœ‰ç•ŒçŠ¶æ€ç©ºé—´çš„å¤§å‹ç½‘ç»œä¸­ç‚¹å¸å¼•å­æ™®éå‡ºç°ä¸€æ ·, åœ¨å…·æœ‰å¼ºéå¯¹ç§°æƒé‡çš„å¤§å‹é€’å½’ç½‘ç»œä¸­ æ··æ²Œå¸å¼•å­ ä¹Ÿæ™®éå‡ºç°.\nå¦‚æœå…´å¥‹æ€§å’ŒæŠ‘åˆ¶æ€§çªè§¦æ¥è‡ªä¸åŒçš„ç¥ç»å…ƒç»„ (æ­£å¦‚ Dale å®šå¾‹åœ¨ç”Ÿç‰©å­¦ä¸Šæ‰€å¿…éœ€çš„é‚£æ ·) , åˆ™å¯ä»¥è½»æ¾å®ç°è¶³å¤Ÿçš„éå¯¹ç§°æ€§.\nDespite the complexity of chaotic dynamics, chaotic attractors are also highly structured in that they typically exist in a relatively low number of dimensions compared with the number of neurons in the network.\nNon-symmetric networks that are dominated by inhibition exhibit a single attractor at zero activity, although the flow towards the attractor in response to perturbations can involve large transients in neural activation that temporarily move the state further away from the attractor.\nå°½ç®¡æ··æ²ŒåŠ¨åŠ›å­¦å¤æ‚, ä½†æ··æ²Œå¸å¼•å­ä¹Ÿå…·æœ‰é«˜åº¦ç»“æ„åŒ–çš„ç‰¹å¾, å› ä¸ºå®ƒä»¬é€šå¸¸å­˜åœ¨äºç›¸æ¯” ç½‘ç»œä¸­ç¥ç»å…ƒçš„æ•°é‡ ä½å¾—å¤šçš„ç»´æ•°ä¸­.\nä»¥æŠ‘åˆ¶ä¸ºä¸»å¯¼çš„éå¯¹ç§°ç½‘ç»œåœ¨é›¶æ¿€æ´»ä¸‹å±•ç°å‡ºå•ä¸€å¸å¼•å­, å°½ç®¡å“åº”æ‰°åŠ¨æ—¶, æœå‘å¸å¼•å­çš„æµåŠ¨å¯èƒ½æ¶‰åŠç¥ç»æ¿€æ´»ä¸­çš„å¤§ç¬æ€, è¿™äº›ç¬æ€ä¼šæš‚æ—¶ä½¿çŠ¶æ€è¿œç¦»å¸å¼•å­.\nAttractors for neural computation A system could theoretically be perfectly tuned such that every point in state space is a neutrally stable attractor, and thus the system has maximally high-dimensional attractor dynamics.\nHowever, because the robustness of attractor networks is related to the low-dimensionality of the attractor states (as discussed below), the system would lose most of its interesting computational properties: error correction or noise tolerance, nearest-neighbour computation, pattern completion and content-addressable memory. It could perform integration, but with no robustness to noise.\nAs such, networks with low-dimensional attractor dynamics exhibit myriad properties that can be vital for computation in the brain These include robust representation, memory, sequence generation, integration, and robust classification and decision-making ideas that have been extensively explored in the literature.\nIn a later section, we describe how, although attractor dynamics may be rigid and invariant as needed for the roles listed above, recent theoretical and experimental findings are beginning to reveal how these rigid constructions may also be exploited to perform flexible computation through reuse and recombination across tasks.\nä¸€ä¸ªç³»ç»Ÿç†è®ºä¸Šå¯ä»¥è¢«å®Œç¾ è°ƒåˆ¶, ä½¿å¾—çŠ¶æ€ç©ºé—´ä¸­çš„æ¯ä¸ªç‚¹éƒ½æ˜¯ä¸€ä¸ªä¸­æ€§ç¨³å®šçš„å¸å¼•å­, å› æ­¤ç³»ç»Ÿå…·æœ‰æœ€å¤§ç»´åº¦çš„å¸å¼•å­åŠ¨åŠ›å­¦.\nç„¶è€Œ, ç”±äºå¸å¼•å­ç½‘ç»œçš„ç¨³å¥æ€§ä¸å¸å¼•å­çŠ¶æ€çš„ä½ç»´æ€§æœ‰å…³ (å¦‚ä¸‹æ‰€è¿°) , ç³»ç»Ÿå°†å¤±å»å…¶å¤§éƒ¨åˆ†æœ‰è¶£çš„è®¡ç®—å±æ€§: é”™è¯¯çº æ­£æˆ–å™ªå£°å®¹å¿ã€æœ€è¿‘é‚»è®¡ç®—ã€æ¨¡å¼å®Œæˆå’Œå†…å®¹å¯å¯»å€è®°å¿†. å®ƒå¯ä»¥æ‰§è¡Œç§¯åˆ†, ä½†å¯¹å™ªå£°æ²¡æœ‰é²æ£’æ€§.\nå› æ­¤, å…·æœ‰ä½ç»´å¸å¼•å­åŠ¨åŠ›å­¦çš„ç½‘ç»œè¡¨ç°å‡ºæ— æ•°å¯¹äºå¤§è„‘è®¡ç®—è‡³å…³é‡è¦çš„å±æ€§. è¿™äº›åŒ…æ‹¬ç¨³å¥çš„è¡¨ç¤ºã€è®°å¿†ã€åºåˆ—ç”Ÿæˆã€ç§¯åˆ†, ä»¥åŠåœ¨æ–‡çŒ®ä¸­å·²è¢«å¹¿æ³›æ¢ç´¢çš„ç¨³å¥åˆ†ç±»å’Œå†³ç­–æ€æƒ³.\nåœ¨åé¢çš„ç« èŠ‚ä¸­, æˆ‘ä»¬æè¿°äº†å°½ç®¡å¸å¼•å­åŠ¨åŠ›å­¦å¯èƒ½æ˜¯åˆšæ€§çš„ä¸”ä¸å˜çš„, ä»¥æ»¡è¶³ä¸Šè¿°éœ€è¦, ä½†æœ€è¿‘çš„ç†è®ºå’Œå®éªŒå‘ç°å¼€å§‹æ­ç¤ºå¦‚ä½•åˆ©ç”¨è¿™äº› åˆšæ€§ç»“æ„ é€šè¿‡è·¨ä»»åŠ¡çš„å¤ç”¨å’Œé‡ç»„æ¥æ‰§è¡Œçµæ´»è®¡ç®—.\nRepresentation and memory A representation of a set of inputs means the assignment of inputs to representational states (not necessarily on a one-to-one basis), with the ability to reproducibly retrieve those states (â€˜labelsâ€™) when cued.\nAttractor networks provide a stable internal set of states that can be used for reproducible representation of discrete or analogue variables, by mapping states in the world to the attractor states. One way to achieve this mapping is through a feedforward learning process that associates each external state with an internal attractor state (Fig. 2a).\nå¯¹ä¸€ç»„è¾“å…¥çš„è¡¨ç¤ºæ„å‘³ç€å°†è¾“å…¥åˆ†é…ç»™è¡¨ç¤ºçŠ¶æ€ (ä¸ä¸€å®šæ˜¯ä¸€å¯¹ä¸€çš„åŸºç¡€) , å¹¶ä¸”åœ¨æç¤ºæ—¶èƒ½å¤Ÿå¯é‡å¤åœ°æ£€ç´¢è¿™äº›çŠ¶æ€ ( â€œæ ‡ç­¾â€ ).\nå¸å¼•å­ç½‘ç»œæä¾›äº†ä¸€ç»„ç¨³å®šçš„å†…éƒ¨çŠ¶æ€, å¯ç”¨äºé€šè¿‡å°†ä¸–ç•Œä¸­çš„çŠ¶æ€æ˜ å°„åˆ°å¸å¼•å­çŠ¶æ€æ¥å¯é‡å¤åœ°è¡¨ç¤ºç¦»æ•£æˆ–æ¨¡æ‹Ÿå˜é‡. å®ç°è¿™ç§æ˜ å°„çš„ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡ å‰é¦ˆå­¦ä¹ è¿‡ç¨‹, å°†æ¯ä¸ªå¤–éƒ¨çŠ¶æ€ä¸å†…éƒ¨å¸å¼•å­çŠ¶æ€ç›¸å…³è” (å›¾ 2a).\nAn attractor network can exhibit two kinds of memory.\nThe first is in the structure of the weights, which specify the set of all attractors. If these weights are specified through an input-driven learning process, this is a form of long-term memory about the inputs.\nThe second kind of memory is the ability to maintain persistent activity in a stationary attractor state: if a system with multiple stationary attractor states is initialized in one of them, it will tend to remain at or near the same state for some time. In other words, the activation levels of the neurons contributing to that state persist while the system remains in the state. This persistent activity response is thus a form of short-term memory of the input that initialized the circuit. If these persistent memory states can be activated without an explicit address, using just the content (or partial content) of the memory, they are content-addressable.\nå¸å¼•å­ç½‘ç»œå¯ä»¥è¡¨ç°å‡ºä¸¤ç§è®°å¿†.\nç¬¬ä¸€ç§æ˜¯å­˜å‚¨äºæƒé‡çš„ç»“æ„, å®ƒç¡®å®šäº†æ‰€æœ‰å¸å¼•å­çŠ¶æ€çš„é›†åˆ. å¦‚æœè¿™äº›æƒé‡æ˜¯é€šè¿‡è¾“å…¥é©±åŠ¨çš„å­¦ä¹ è¿‡ç¨‹ç¡®å®šçš„, è¿™æ˜¯ä¸€ç§å…³äºè¾“å…¥çš„é•¿æœŸè®°å¿†å½¢å¼.\nç¬¬äºŒç§è®°å¿†æ¥è‡ªç½‘ç»œåœ¨ä¸€é™æ­¢å¸å¼•å­çŠ¶æ€ä¸­ç»´æŒæŒç»­æ´»åŠ¨çš„èƒ½åŠ›: å¦‚æœä¸€ç³»ç»Ÿå…·æœ‰è‹¥å¹²é™æ­¢å¸å¼•å­çŠ¶æ€, å¹¶ä¸”è®¾ç½®åˆæ€ä¸ºå…¶ä¸­ä¸€ä¸ªå¸å¼•å­çŠ¶æ€, å®ƒå°†å€¾å‘äºåœ¨ä¸€æ®µæ—¶é—´å†…ä¿æŒåœ¨è¯¥çŠ¶æ€æˆ–é™„è¿‘. æ¢å¥è¯è¯´, è´¡çŒ®äºè¯¥çŠ¶æ€çš„ç¥ç»å…ƒçš„æ¿€æ´»æ°´å¹³åœ¨ç³»ç»Ÿä¿æŒåœ¨è¯¥çŠ¶æ€æ—¶æŒç»­å­˜åœ¨. å› æ­¤, è¿™ç§æŒç»­çš„æ´»åŠ¨å“åº”æ˜¯ä¸€ç§å¯¹åˆå§‹åŒ–å›è·¯çš„è¾“å…¥çš„çŸ­æœŸè®°å¿†å½¢å¼. å¦‚æœè¿™äº›æŒç»­çš„è®°å¿†çŠ¶æ€å¯ä»¥åœ¨æ²¡æœ‰ æ˜¾å¼åœ°å€ çš„æƒ…å†µä¸‹è¢«æ¿€æ´», ä»…ä½¿ç”¨è®°å¿†çš„å†…å®¹ (æˆ–éƒ¨åˆ†å†…å®¹) , åˆ™å®ƒä»¬æ˜¯å†…å®¹å¯å¯»å€çš„.\nThe short-term memory function of attractors depends on the prior formation of stable states through long-term plasticity.\nFor instance, in Hopfield-like networks, states cannot persist if they were not first trained to be attractor states. Even models of short-term memory that are based on presynaptic facilitation, rather than persistent activity, rely implicitly on prior long-term associative plasticity to construct recurrently stabilized neural ensembles that can be reinstated by random inputs. (Additionally, these models are not activity-silent in the delay period, in the sense that they would require ongoing activity to refresh the facilitation state over longer delays and to generate robustness against random background activity that would facilitate different synapses.)\nIn other words, these presynaptic facilitation models cannot explain short-term memory for entirely novel inputs; however, combinations of attractors could enable more flexible short-term memory, as we discuss later.\nå¸å¼•å­çš„çŸ­æœŸè®°å¿†åŠŸèƒ½ä¾èµ–äºé€šè¿‡é•¿æœŸå¯å¡‘æ€§å½¢æˆç¨³å®šçŠ¶æ€çš„å…ˆéªŒ.\nä¾‹å¦‚, åœ¨ Hopfield ç±»ç½‘ç»œä¸­, å¦‚æœçŠ¶æ€æ²¡æœ‰é¦–å…ˆè¢«è®­ç»ƒä¸ºå¸å¼•å­çŠ¶æ€, åˆ™å®ƒä»¬æ— æ³•æŒç»­å­˜åœ¨. å³ä½¿æ˜¯åŸºäº çªè§¦å‰æ˜“åŒ– è€Œä¸æ˜¯æŒç»­æ´»åŠ¨çš„çŸ­æœŸè®°å¿†æ¨¡å‹, ä¹Ÿéšå¼åœ°ä¾èµ–äºå…ˆå‰çš„é•¿æœŸè”æƒ³å¯å¡‘æ€§æ¥æ„å»ºå¯ä»¥é€šè¿‡éšæœºè¾“å…¥é‡æ–°å»ºç«‹çš„é€’å½’ç¨³å®šç¥ç»å…ƒé›†åˆ. (æ­¤å¤–, è¿™äº›æ¨¡å‹åœ¨å»¶è¿ŸæœŸé—´å¹¶éæ´»åŠ¨é™é»˜, å› ä¸ºå®ƒä»¬éœ€è¦æŒç»­çš„æ´»åŠ¨æ¥åˆ·æ–°ä¿ƒè¿›çŠ¶æ€ä»¥åº”å¯¹æ›´é•¿çš„å»¶è¿Ÿ, å¹¶ç”Ÿæˆå¯¹éšæœºèƒŒæ™¯æ´»åŠ¨çš„é²æ£’æ€§, è¿™äº›æ´»åŠ¨ä¼šä¿ƒè¿›ä¸åŒçš„çªè§¦. )\næ¢å¥è¯è¯´, è¿™äº›çªè§¦å‰ä¿ƒè¿›æ¨¡å‹æ— æ³•è§£é‡Šå¯¹å®Œå…¨æ–°é¢–è¾“å…¥çš„çŸ­æœŸè®°å¿†; ç„¶è€Œ, æ­£å¦‚æˆ‘ä»¬ç¨åè®¨è®ºçš„é‚£æ ·, å¸å¼•å­çš„ç»„åˆå¯ä»¥å®ç°æ›´çµæ´»çš„çŸ­æœŸè®°å¿†.\nDe-noising representations and memories If representational states are attractors, then the representations are robust in the sense that they perform de-noising: if the input cues or initial conditions reflect noisy or corrupted versions of an attractor state, the dynamics drive the state to a point on the representational attractor (Fig. 2b, inset).\nWhen attractors form a continuous manifold of dimension $K\\ll N$, where $N$ is the number of neurons in the circuit, all noise in $N-K$ dimensions is erased. A noise ball of unit radius in $N$ dimensions (corresponding to random independent noise per neuron) has a projection of size only $\\sim\\sqrt{K/N}\\ll 1$ along $K$ dimensions.\nIf $K$ is low-dimensional, as is often the case, and $N$ ranges from $10^2$ to $10^7$ as estimated before for common microcircuits, this constitutes a massive reduction in the sensitivity of the state to internal or input noise (Fig. 2b). Thus, most noise is rendered impotent by attractor dynamics.\nå¦‚æœè¡¨è±¡çŠ¶æ€æ˜¯å¸å¼•å­, é‚£ä¹ˆè¡¨è±¡åœ¨æŸç§æ„ä¹‰ä¸Šæ˜¯ç¨³å¥çš„, å› ä¸ºå®ƒä»¬æ‰§è¡Œå»å™ª: å¦‚æœè¾“å…¥æç¤ºæˆ–åˆå§‹æ¡ä»¶åæ˜ äº†åŠ å™ªæˆ–æŸåçš„å¸å¼•å­çŠ¶æ€, åŠ¨åŠ›å­¦ä¼šå°†çŠ¶æ€é©±åŠ¨åˆ°è¡¨è±¡çš„å¸å¼•å­ (å›¾ 2b, æ’å›¾).\nå½“å¸å¼•å­å½¢æˆç»´åº¦ä¸º $K\\ll N$ çš„è¿ç»­æµå½¢æ—¶, å…¶ä¸­ $N$ æ˜¯å›è·¯ä¸­ç¥ç»å…ƒçš„æ•°é‡, $N-K$ ç»´ä¸­çš„æ‰€æœ‰å™ªå£°éƒ½è¢«æŠ¹å». åœ¨ $N$ ç»´ä¸­çš„å•ä½åŠå¾„å™ªå£°çƒ (å¯¹åº”äºæ¯ä¸ªç¥ç»å…ƒçš„éšæœºç‹¬ç«‹å™ªå£°) åœ¨ $K$ ç»´ä¸Šçš„æŠ•å½±å¤§å°ä»…ä¸º $\\sim\\sqrt{K/N}\\ll 1$.\nå¦‚æœ $K$ æ˜¯ä½ç»´æ•°, æ­£å¦‚é€šå¸¸æƒ…å†µä¸€æ ·, å¹¶ä¸” $N$ èŒƒå›´ä»ä¹‹å‰ä¼°è®¡çš„å¸¸è§å¾®å›è·¯çš„ $10^{2}$ åˆ° $10^{7}$, è¿™æ„æˆäº†å¯¹çŠ¶æ€å¯¹å†…éƒ¨æˆ–è¾“å…¥å™ªå£°æ•æ„Ÿæ€§çš„å·¨å¤§é™ä½ (å›¾ 2b). å› æ­¤, å¤§å¤šæ•°å™ªå£°é€šè¿‡å¸å¼•å­åŠ¨åŠ›å­¦å˜å¾—æ— æ•ˆ.\nDe-noising owing to attractor dynamics is especially important for memory maintenance as, otherwise, noise-induced deviations would accumulate and grow over time.\nDiscrete attractors continually erase all noise by mapping perturbed states back to the point attractor, resulting in zero drift. With continuous attractors as memory states, all noise orthogonal to the manifold is corrected; thus, there is a net reduction of the effects of noise by the factor $\\sim\\sqrt{K/N}\\ll 1$ (refs.45,55).\nHowever, all states on the attractor manifold are neutrally stable, so the state can drift along the attractor. As such, components of noise along the $K$ attractor dimensions are not internally corrected and cause an accumulating drift away from the initial state, with variance proportional to $KT/N$, where $T$ is the elapsed time. Thus, through the $1/N$ decrease in variance, even continuous memory states can be well stabilized in sufficiently large attractor networks.\nå¸å¼•å­åŠ¨åŠ›å­¦å¼•èµ·çš„å»å™ªå¯¹äºè®°å¿†ç»´æŒå°¤ä¸ºé‡è¦, å¦åˆ™, å™ªå£°å¼•èµ·çš„åå·®ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œç§¯ç´¯å’Œå¢é•¿.\nç¦»æ•£å¸å¼•å­é€šè¿‡å°†æ‰°åŠ¨çŠ¶æ€æ˜ å°„å›ç‚¹å¸å¼•å­æ¥ä¸æ–­æŠ¹å»æ‰€æœ‰å™ªå£°, äº§ç”Ÿé›¶æ¼‚ç§». å¯¹äºä½œä¸ºè®°å¿†çŠ¶æ€çš„è¿ç»­å¸å¼•å­, æ‰€æœ‰æ­£äº¤äºæµå½¢çš„å™ªå£°éƒ½è¢«çº æ­£; å› æ­¤, å™ªå£°æ•ˆåº”å‡€å‡å°‘äº† $\\sim\\sqrt{K/N}\\ll 1$ çš„å› å­ (å‚è€ƒæ–‡çŒ® 45ã€55).\nç„¶è€Œ, å¸å¼•å­æµå½¢ä¸Šçš„æ‰€æœ‰çŠ¶æ€éƒ½æ˜¯ä¸­æ€§ç¨³å®šçš„, å› æ­¤çŠ¶æ€å¯ä»¥æ²¿ç€å¸å¼•å­æ¼‚ç§». å› æ­¤, æ²¿ç€ $K$ å¸å¼•å­ç»´åº¦çš„å™ªå£°åˆ†é‡ä¸ä¼šè¢«å†…éƒ¨çº æ­£, å¹¶å¯¼è‡´è¿œç¦»åˆå§‹çŠ¶æ€çš„ç´¯ç§¯æ¼‚ç§», å…¶æ–¹å·®ä¸ $KT/N$ æˆæ­£æ¯”, å…¶ä¸­ $T$ æ˜¯ç»è¿‡çš„æ—¶é—´. å› æ­¤, é€šè¿‡ $1/N$ æ–¹å·®çš„é™ä½, å³ä½¿æ˜¯è¿ç»­çš„è®°å¿†çŠ¶æ€ä¹Ÿå¯ä»¥åœ¨è¶³å¤Ÿå¤§çš„å¸å¼•å­ç½‘ç»œä¸­å¾—åˆ°å¾ˆå¥½çš„ç¨³å®š.\nAlthough content-addressable long-term memory and error reduction can be instantiated through feedforward computations involving only a few steps in place of attractor dynamics, recurrent attractor dynamics are indispensable for the generation of persistent activity states (and thus for short-term memory through persistent activity) and integration, as we discuss below.\nå°½ç®¡å†…å®¹å¯å¯»å€çš„é•¿æœŸè®°å¿†å’Œé”™è¯¯å‡å°‘å¯ä»¥é€šè¿‡æ¶‰åŠä»…å‡ ä¸ªæ­¥éª¤çš„å‰é¦ˆè®¡ç®—æ¥å®ç°, è€Œä¸æ˜¯å¸å¼•å­åŠ¨åŠ›å­¦, ä½†æ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·, é€’å½’å¸å¼•å­åŠ¨åŠ›å­¦å¯¹äºç”ŸæˆæŒç»­æ´»åŠ¨çŠ¶æ€ (å› æ­¤é€šè¿‡æŒç»­æ´»åŠ¨è¿›è¡ŒçŸ­æœŸè®°å¿†) å’Œç§¯åˆ†æ˜¯ä¸å¯æˆ–ç¼ºçš„.\nRobust classification When there are finitely many separated attractors (each a discrete attractor or a continuous manifold), states that are not initially on one of the attractors will flow to one of the attractors. An input to the network can then be classified according to the attractor to which the network state flows after initialization by the input. We can now identify inputs based on the attractors they flow to, a mechanism of classification. If the dynamics of the network further correctly assign corrupted versions of an input to the same attractor state as the uncorrupted input, this constitutes robust classification.\nIn other words, the dynamical basins of attraction of the network must align with the Voronoi regions of the attractor states (that is, corrupted inputs that are closest in distance to one of the uncorrupted inputs should flow to that inputâ€™s attractor through the dynamics and not another). This is approximately the case for attractor networks operating well below capacity, but typically deteriorates when attractor networks are pushed towards their capacity.\nå½“å­˜åœ¨æœ‰é™æ•°é‡çš„åˆ†ç¦»å¸å¼•å­ (æ¯ä¸ªéƒ½æ˜¯ç¦»æ•£å¸å¼•å­æˆ–è¿ç»­æµå½¢) æ—¶, æœ€åˆä¸åœ¨å…¶ä¸­ä¸€ä¸ªå¸å¼•å­ä¸Šçš„çŠ¶æ€å°†æµå‘å…¶ä¸­ä¸€ä¸ªå¸å¼•å­. ç„¶å, å¯ä»¥æ ¹æ®ç½‘ç»œçŠ¶æ€åœ¨è¾“å…¥åˆå§‹åŒ–åæµå‘çš„å¸å¼•å­å¯¹ç½‘ç»œçš„è¾“å…¥è¿›è¡Œåˆ†ç±». æˆ‘ä»¬ç°åœ¨å¯ä»¥æ ¹æ®å®ƒä»¬æµå‘çš„å¸å¼•å­æ¥è¯†åˆ«è¾“å…¥, è¿™æ˜¯ä¸€ç§åˆ†ç±»æœºåˆ¶. å¦‚æœç½‘ç»œçš„åŠ¨åŠ›å­¦è¿›ä¸€æ­¥å°†è¾“å…¥çš„æŸåç‰ˆæœ¬æ­£ç¡®åœ°åˆ†é…ç»™ä¸æœªæŸåè¾“å…¥ç›¸åŒçš„å¸å¼•å­çŠ¶æ€, è¿™å°±æ„æˆäº†ç¨³å¥åˆ†ç±».\næ¢å¥è¯è¯´, ç½‘ç»œçš„åŠ¨åŠ›è°·åœ°å¿…é¡»ä¸å¸å¼•å­çŠ¶æ€çš„ Voronoi åŒºåŸŸå¯¹é½ (å³, ä¸æœªæŸåè¾“å…¥è·ç¦»æœ€è¿‘çš„æŸåè¾“å…¥åº”è¯¥é€šè¿‡åŠ¨åŠ›å­¦æµå‘è¯¥è¾“å…¥çš„å¸å¼•å­, è€Œä¸æ˜¯å¦ä¸€ä¸ª). å¯¹äºåœ¨å®¹é‡ä»¥ä¸‹è‰¯å¥½è¿è¡Œçš„å¸å¼•å­ç½‘ç»œæ¥è¯´, è¿™å¤§è‡´æ˜¯æ­£ç¡®çš„, ä½†å½“å¸å¼•å­ç½‘ç»œè¢«æ¨å‘å…¶å®¹é‡æ—¶é€šå¸¸ä¼šæ¶åŒ–.\nIntegration Single neurons integrate their inputs, but usually can only do this over the timescales associated with their membrane capacitances, typically 10-100 ms. Continuous-attractor dynamics can enable neural circuits to integrate over much longer timescales (in the order of about 1-100 s).\nå•ä¸ªç¥ç»å…ƒç§¯åˆ†å®ƒä»¬çš„è¾“å…¥, ä½†é€šå¸¸åªèƒ½åœ¨ä¸å…¶è†œç”µå®¹ç›¸å…³çš„æ—¶é—´å°ºåº¦ä¸Šè¿›è¡Œç§¯åˆ†, é€šå¸¸ä¸º 10-100 æ¯«ç§’. è¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦å¯ä»¥ä½¿ç¥ç»å›è·¯åœ¨æ›´é•¿çš„æ—¶é—´å°ºåº¦ä¸Šè¿›è¡Œç§¯åˆ† (å¤§çº¦ 1-100 ç§’).\nA pattern-forming continuous-attractor network requires an additional mechanism to gain the functionality of an integrator: a way to shift the internal state along the attractor in response to an input that encodes changes in the external variable (Fig. 2d, left).\nConceptually, the simplest way to build a shift mechanism is by a copy-and-offset construction: construct multiple copies or subpopulations of the attractor network, each with slightly offset (asymmetric) weights in the sense that active neurons centre their excitation or point of maximal disinhibition slightly offset from themselves on the neural sheet (for example, see that the network in Fig. 1g is a slightly asymmetric version of the network in Fig. 1c). The states in each such network will then form a limit-cycle attractor, with patterns of activity flowing in the direction of the asymmetry in each copy.\nIf opposing copies are coupled together, the pattern is stabilized through a push-pull balance. A velocity input whose components project differentially to the copies will break the push-pull balance, driving the pattern along the flow direction of the more active copy (Fig. 1g).\nThus, the total direction and magnitude of the shift of the pattern, corresponding to movement along the attractor manifold, represents the time integral of the velocity input to the network. This common principle unifies the mechanisms across diverse integrator models.\nå½¢æˆæ¨¡å¼çš„è¿ç»­å¸å¼•å­ç½‘ç»œéœ€è¦ä¸€ä¸ªé¢å¤–çš„æœºåˆ¶æ¥ä½¿è·å¾—ç§¯åˆ†å™¨çš„åŠŸèƒ½: ä¸€ç§ç¼–ç  å¤–éƒ¨å˜é‡ å˜åŒ–é‡çš„è¾“å…¥, æ²¿å¸å¼•å­å¹³ç§»å†…éƒ¨çŠ¶æ€çš„æ–¹æ³• (å›¾ 2d, å·¦).\nä»æ¦‚å¿µä¸Šè®², æ„å»ºå¹³ç§»æœºåˆ¶çš„æœ€ç®€å•æ–¹æ³•æ˜¯é€šè¿‡ å¤åˆ¶å’Œåç§»ç»“æ„: æ„å»ºå¤šä¸ªå¸å¼•å­ç½‘ç»œçš„å¤‡ä»½æˆ–å­ç¾¤ä½“, æ¯ä¸ªå¤‡ä»½å…·æœ‰ç•¥å¾®åç§» (éå¯¹ç§°) æƒé‡, è¿™æ„å‘³ç€æ´»è·ƒç¥ç»å…ƒå°†å…¶å…´å¥‹æˆ–æœ€å¤§å»æŠ‘åˆ¶ç‚¹ç¨å¾®åç¦»ç¥ç»ç‰‡ä¸Šçš„è‡ªèº«ä¸­å¿ƒ (ä¾‹å¦‚, å‚è§å›¾ 1g ä¸­çš„ç½‘ç»œæ˜¯å›¾ 1c ä¸­ç½‘ç»œçš„ç•¥å¾®éå¯¹ç§°ç‰ˆæœ¬). ç„¶å, æ¯ä¸ªè¿™æ ·çš„ç½‘ç»œä¸­çš„çŠ¶æ€å°†å½¢æˆ æé™ç¯å¸å¼•å­, æ¿€æ´»æ¨¡å¼æ²¿ç€æ¯ä¸ªå‰¯æœ¬ä¸­éå¯¹ç§°æ€§çš„æ–¹å‘æµåŠ¨.\nå¦‚æœå°†ç›¸åçš„å‰¯æœ¬è€¦åˆåœ¨ä¸€èµ·, é€šè¿‡æ¨-æ‹‰å¹³è¡¡ç¨³å®šæ¨¡å¼. å…¶åˆ†é‡å·®å¼‚æ€§æŠ•å½±åˆ°å‰¯æœ¬çš„é€Ÿåº¦è¾“å…¥å°†æ‰“ç ´æ¨æ‹‰å¹³è¡¡, æ²¿ç€æ›´æ´»è·ƒå‰¯æœ¬çš„æµåŠ¨æ–¹å‘é©±åŠ¨æ¨¡å¼ (å›¾ 1g).\nå› æ­¤, æ¨¡å¼çš„æ€»æ–¹å‘å’Œå¹…åº¦åç§», å¯¹åº”äºæ²¿å¸å¼•å­æµå½¢çš„è¿åŠ¨, è¡¨ç¤ºç½‘ç»œå¯¹é€Ÿåº¦è¾“å…¥çš„æ—¶é—´ç§¯åˆ†. è¿™ä¸€å…±åŒåŸç†ç»Ÿä¸€äº†å„ç§ç§¯åˆ†å™¨æ¨¡å‹ä¸­çš„æœºåˆ¶.\næ•°å­¦ä¸Š\n$$ x(t) = x(t_{0}) + \\int_{t_{0}}^{t}\\dot{x}(\\tau)\\mathrm{d}\\tau $$\n$x$: bump ä½ç½®; $\\dot{x}$: bump ç§»åŠ¨é€Ÿåº¦.\nDecision-making If, instead of a velocity signal, the input to an integrator network consisted of temporally varying positive and negative evidence in support of each of two options (Fig. 2d, right) (or in the case of multiple options, evidence vectors instead of velocity vectors), the network would integrate those inputs and thus perform evidence accumulation.\nå¦‚æœ, ç§¯åˆ†å™¨ç½‘ç»œçš„è¾“å…¥ä¸æ˜¯é€Ÿåº¦ä¿¡å·, è€Œæ˜¯æ”¯æŒä¸¤ä¸ªé€‰é¡¹ä¸­æ¯ä¸ªé€‰é¡¹çš„æ—¶é—´å˜åŒ–çš„æ­£è´Ÿè¯æ® (å›¾ 2d, å³) (æˆ–è€…åœ¨å¤šé€‰æƒ…å†µä¸‹, æ˜¯è¯æ®å‘é‡è€Œä¸æ˜¯é€Ÿåº¦å‘é‡) , åˆ™ç½‘ç»œå°†ç§¯åˆ†è¿™äº›è¾“å…¥, ä»è€Œæ‰§è¡Œ è¯æ®ç§¯ç´¯.\nè®¾è¯æ®è¾“å…¥ä¸º $e(t) = e_{A}(t) - e_{B}(t)$, åˆ™è¯æ®ç§¯ç´¯ä¸º\n$$ s(t) = s(0) + \\int_{0}^{t}e(\\tau)\\mathrm{d}\\tau,\\quad \\text{or }\\mathbf{s}(t) = \\mathbf{s}(0) + \\int_{0}^{t}\\mathbf{e}(\\tau)\\mathrm{d}\\tau $$\nDecision-making can be viewed as a selection process applied to an integrator that is based on a readout that detects when the integrator state has accumulated enough evidence and moved past a decision threshold.\nThe selection process can be external to the integrator, in the form of a readout circuit that detects such threshold crossings and outputs the decision.\nAlternatively, the selection process can be built into the dynamics of the integrator itself, in the form of a more complex attractor landscape, in which the states move along a continuous attractor but, at some point, the continuous attractor gives way to a pair of discrete attractors, towards which the states flow (Fig. 2e).\nNeural WTA models implement such a hybrid analogue-discrete computation. The parameters of WTA networks determine the balance between integration dynamics and competitive dynamics, and thus how well the network integrates later evidence: when the network is tuned to be a perfect integrator, its response to inputs is gradual, and small amounts of evidence cause (reversible) flow along the continuous-attractor manifold. In cases in which competition dominates, the response to evidence is a fast flow towards one of the discrete attractors; beyond a point, the flow is nearly irreversible, leading to rapid decision-making and the discounting of later evidence.\nå¯ä»¥å°†å†³ç­–è§†ä¸ºåº”ç”¨äºç§¯åˆ†å™¨çš„é€‰æ‹©è¿‡ç¨‹, åŸºäºæ£€æµ‹ç§¯åˆ†å™¨çŠ¶æ€ä½•æ—¶ç§¯ç´¯è¶³å¤Ÿè¯æ®å¹¶è¶…è¿‡å†³ç­–é˜ˆå€¼çš„è¯»å‡º.\né€‰æ‹©è¿‡ç¨‹å¯ä»¥æ˜¯ç§¯åˆ†å™¨å¤–éƒ¨çš„, ä»¥è¯»å‡ºå›è·¯çš„å½¢å¼æ£€æµ‹æ­¤ç±»é˜ˆå€¼äº¤å‰å¹¶è¾“å‡ºå†³ç­–.\næˆ–è€…, é€‰æ‹©è¿‡ç¨‹å¯ä»¥å†…ç½®äºç§¯åˆ†å™¨æœ¬èº«çš„åŠ¨åŠ›å­¦ä¸­, ä»¥æ›´å¤æ‚çš„å¸å¼•å­æ™¯è§‚çš„å½¢å¼, å…¶ä¸­çŠ¶æ€æ²¿ç€è¿ç»­å¸å¼•å­ç§»åŠ¨, ä½†åœ¨æŸäº›ç‚¹ä¸Š, è¿ç»­å¸å¼•å­è®©ä½äºä¸€å¯¹ç¦»æ•£å¸å¼•å­, çŠ¶æ€æœå‘è¿™äº›å¸å¼•å­æµåŠ¨ (å›¾ 2e).\nç¥ç» WTA æ¨¡å‹å®ç°äº†è¿™ç§ æ··åˆæ¨¡æ‹Ÿ-ç¦»æ•£è®¡ç®—. WTA ç½‘ç»œçš„å‚æ•°å†³å®šäº†ç§¯åˆ†åŠ¨åŠ›å­¦å’Œç«äº‰åŠ¨åŠ›å­¦ä¹‹é—´çš„å¹³è¡¡, ä»è€Œå†³å®šäº†ç½‘ç»œæ•´åˆåç»­è¯æ®çš„èƒ½åŠ›: å½“ç½‘ç»œè¢«è°ƒåˆ¶ä¸ºå®Œç¾ç§¯åˆ†å™¨æ—¶, å…¶å¯¹è¾“å…¥çš„å“åº”æ˜¯æ¸è¿›çš„, å°‘é‡è¯æ®ä¼šå¯¼è‡´æ²¿è¿ç»­å¸å¼•å­æµå½¢çš„ (å¯é€†) æµåŠ¨. åœ¨ç«äº‰å ä¸»å¯¼åœ°ä½çš„æƒ…å†µä¸‹, å¯¹è¯æ®çš„å“åº”æ˜¯å¿«é€Ÿæµå‘å…¶ä¸­ä¸€ä¸ªç¦»æ•£å¸å¼•å­; è¶…è¿‡æŸä¸€ç‚¹å, æµåŠ¨å‡ ä¹æ˜¯ä¸å¯é€†çš„, å¯¼è‡´å¿«é€Ÿå†³ç­–å’Œå¯¹åç»­è¯æ®çš„æŠ˜æ‰£.\nNeural WTA networks can leverage specific neural non-linearities to accurately and rapidly (in $\\sim \\log{(N)}$ time) make the best decision among $N$ alternatives, even if the presented data are noisy (fluctuating over time around their means) and even if the number of options varies over orders of magnitude.\nç¥ç» WTA ç½‘ç»œå¯ä»¥åˆ©ç”¨ç‰¹å®šçš„ç¥ç»éçº¿æ€§, ä»¥å‡†ç¡®ä¸”å¿«é€Ÿ (åœ¨ $\\sim \\log{(N)}$ æ—¶é—´å†…) åœ¨ $N$ ä¸ªå¤‡é€‰æ–¹æ¡ˆä¸­åšå‡ºæœ€ä½³å†³ç­–, å³ä½¿æ‰€å‘ˆç°çš„æ•°æ®æ˜¯å˜ˆæ‚çš„ (å›´ç»•å…¶å‡å€¼éšæ—¶é—´æ³¢åŠ¨) , å³ä½¿é€‰é¡¹æ•°é‡å˜åŒ–äº†å‡ ä¸ªæ•°é‡çº§.\nSequence generation Attractor dynamics can be important for stabilizing another longtimescale behaviour: the generation of sequences. Robust sequences can be constructed as low-dimensional limit-cycle attractors, in which high-dimensional perturbations are corrected while along the attractor, there is a systematic, periodic or quasiperiodic flow of states.\nThe attractor property that affords ongoing de-noising is important for preventing spatial dispersion and temporal dissipation of the activity packet during sequence generation.\nå¸å¼•å­åŠ¨åŠ›å­¦å¯¹äºç¨³å®šå¦ä¸€ç§é•¿æœŸè¡Œä¸ºä¹Ÿå¾ˆé‡è¦: åºåˆ—ç”Ÿæˆ. ç¨³å¥çš„åºåˆ—å¯ä»¥æ„å»ºä¸ºä½ç»´æé™ç¯å¸å¼•å­, åœ¨è¿™ç§å¸å¼•å­ä¸­, é«˜ç»´æ‰°åŠ¨è¢«çº æ­£; æ²¿ç€å¸å¼•å­(æµå½¢), çŠ¶æ€å­˜åœ¨ç³»ç»Ÿçš„ä¸”(å‡†)å‘¨æœŸæ€§çš„æµåŠ¨.\nä¿æŒå»å™ªçš„å¸å¼•å­å±æ€§å¯¹äºé˜²æ­¢åºåˆ—ç”Ÿæˆè¿‡ç¨‹ä¸­æ´»åŠ¨åŒ…çš„ ç©ºé—´é¢‘æ•£ å’Œ æ—¶é—´è€—æ•£ éå¸¸é‡è¦.\nSimilar to the case for stationary attractor manifolds, the small components of noise along the limit-cycle attractors are not correctable and lead to a gradual accumulation of drift, which for sequence generation is manifest as timing variability: the standard deviation in the time of reaching the $T$th state in the sequence is predicted to grow as $\\sqrt{T}$ for unbiased random drift along the attractor.\nä¸é™æ­¢å¸å¼•å­æµå½¢çš„æƒ…å†µç±»ä¼¼, å™ªå£°æ²¿æé™ç¯å¸å¼•å­çš„å°åˆ†é‡æ˜¯ä¸å¯çº æ­£çš„, å¹¶å¯¼è‡´é€æ¸ç§¯ç´¯çš„æ¼‚ç§», å¯¹äºåºåˆ—ç”Ÿæˆæ¥è¯´, è¿™è¡¨ç°ä¸º æ—¶é—´å˜åŒ–æ€§: é¢„è®¡è¾¾åˆ°åºåˆ—ä¸­ç¬¬ $T$ ä¸ªçŠ¶æ€çš„æ—¶é—´çš„ æ ‡å‡†åå·® å°†éšç€æ²¿å¸å¼•å­çš„æ— åç½®éšæœºæ¼‚ç§»è€Œå¢é•¿ä¸º $\\sqrt{T}$.\nEvidence of attractors in the brain Criteria for attractor dynamics The fundamental predictions of attractor models centre on the statespace dynamics of the circuit, as initially explicitly discussed and tested in refs.9,15,77,78.\nFirst, a systemâ€™s states should be found localized at or around a low-dimensional set of states that correspond to the attractors in the state space.\nSecond, a systemâ€™s state should flow quickly back to the low-dimensional state after perturbation.\nThird, the set of attractor states â€” quantified either by direct characterization of the full state space or by the relationships between cells â€” should be invariant, persisting over time and after removal of tuned input, across conditions, across behavioural states and even when there are induced variations in the mapping from internal states to external inputs.\nFourth, integrator networks should further exhibit the property of isometry, whereby lengths of coding space along a dimension are allocated to equal displacements along a dimension of the external variable.\nAdditional predictions of attractor dynamics models, that are not as fundamental in the sense that they are not theoretically necessary or sufficient but are nevertheless of high importance because they are highly supportive of the mechanisms of attractor dynamics, are anatomical and structural correlates: the existence of low-dimensional physical structures and directly visible symmetries in connectivity between cells.\nå¸å¼•å­æ¨¡å‹çš„åŸºæœ¬é¢„æµ‹é›†ä¸­åœ¨å›è·¯çš„çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ä¸Š, æœ€åˆåœ¨å‚è€ƒæ–‡çŒ® 9ã€15ã€77ã€78 ä¸­æ˜ç¡®è®¨è®ºå’Œæµ‹è¯•.\né¦–å…ˆ, ç³»ç»Ÿçš„çŠ¶æ€åº”å®šä½åœ¨ä¸çŠ¶æ€ç©ºé—´ä¸­çš„å¸å¼•å­å¯¹åº”çš„ä½ç»´çŠ¶æ€é›†ä¸Šæˆ–é™„è¿‘.\nå…¶æ¬¡, ç³»ç»Ÿçš„çŠ¶æ€åœ¨æ‰°åŠ¨ååº”è¿…é€Ÿæµå›ä½ç»´çŠ¶æ€.\nç¬¬ä¸‰, å¸å¼•å­çŠ¶æ€é›†â€”â€”é€šè¿‡ å¯¹å®Œæ•´çŠ¶æ€ç©ºé—´çš„ç›´æ¥è¡¨å¾ æˆ–é€šè¿‡ ç»†èƒä¹‹é—´çš„å…³ç³» è¿›è¡Œé‡åŒ–â€”â€”åº”æ˜¯ä¸å˜çš„, åœ¨æ—¶é—´ä¸ŠæŒç»­å­˜åœ¨, å¹¶ä¸”åœ¨å»é™¤è°ƒåˆ¶è¾“å…¥å, åœ¨ä¸åŒæ¡ä»¶ä¸‹ã€ä¸åŒè¡Œä¸ºçŠ¶æ€ä¸‹, ç”šè‡³åœ¨å†…éƒ¨çŠ¶æ€åˆ°å¤–éƒ¨è¾“å…¥çš„æ˜ å°„å‘ç”Ÿè¯±å¯¼å˜åŒ–æ—¶ä¹Ÿæ˜¯å¦‚æ­¤.\nç¬¬å››, ç§¯åˆ†å™¨ç½‘ç»œè¿˜åº”è¡¨ç°å‡º ç­‰è·æ€§ çš„ç‰¹æ€§, å³ç¼–ç ç©ºé—´æ²¿æŸä¸€ç»´åº¦çš„é•¿åº¦åˆ†é…ç»™å¤–éƒ¨å˜é‡æŸä¸€ç»´åº¦ä¸Šçš„ç›¸ç­‰ä½ç§».\næ¯”å¦‚, ç½‘ç»œçŠ¶æ€æ²¿ç¯ç§»åŠ¨ 10 åº¦å¯¹åº”çœŸå®å¤´æœå‘ç§»åŠ¨ 10 åº¦. ç½‘ç»œæ´»åŠ¨ç©ºé—´ä¸ç‰©ç†ç©ºé—´å­˜åœ¨ç­‰æ¯”ä¾‹æ˜ å°„.\næ­¤å¤–ï¼Œå¸å¼•å­åŠ¨åŠ›å­¦æ¨¡å‹è¿˜æœ‰ä¸€äº›é™„åŠ é¢„æµ‹â€”â€”è¿™äº›é¢„æµ‹åœ¨ç†è®ºä¸Šå¹¶éå¿…è¦æˆ–å……åˆ†ï¼Œä½†ä»ç„¶éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä»¬ä¸ºå¸å¼•å­åŠ¨åŠ›æœºåˆ¶æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒâ€”â€”å³è§£å‰–å’Œç»“æ„æ–¹é¢çš„å¯¹åº”ç‰¹å¾ï¼šåŒ…æ‹¬ä½ç»´ç‰©ç†ç»“æ„çš„å­˜åœ¨ï¼Œä»¥åŠç»†èƒä¹‹é—´è¿æ¥ä¸­å¯ç›´æ¥è§‚å¯Ÿåˆ°çš„å¯¹ç§°æ€§ã€‚\nAs we have seen, attractor networks dynamics need not be used by the brain in an autonomous setting: inputs that drive attractor networks can be an important part of their function, for instance in integration and evidence accumulation.\nNevertheless, because attractor systems are characterized by their internally generated or autonomous dynamics, putative attractor networks are best tested in conditions that minimize external cues that are time-varying or tuned to provide localized inputs along the putative attractor â€” that is, in an effectively autonomous setting.\næ­£å¦‚æˆ‘ä»¬å·²ç»çœ‹åˆ°çš„ï¼Œå¸å¼•å­ç½‘ç»œçš„åŠ¨åŠ›å­¦å¹¶ä¸å¿…é¡»åœ¨ å®Œå…¨è‡ªä¸» çš„æƒ…å†µä¸‹è¢«å¤§è„‘ä½¿ç”¨ï¼šé©±åŠ¨å¸å¼•å­ç½‘ç»œçš„å¤–éƒ¨è¾“å…¥ä¹Ÿæ˜¯å…¶åŠŸèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¾‹å¦‚åœ¨ç§¯åˆ†å’Œ è¯æ®ç´¯ç§¯ ä¸­ã€‚\nå¤§è„‘ä½¿ç”¨å¸å¼•å­æ—¶, æœ‰æ—¶ä»¤å…¶è‡ªä¸»ç»´æŒ, æœ‰æ—¶ä»¤å…¶æ¥æ”¶å¤–éƒ¨è¾“å…¥ä»è€Œç§¯åˆ†.\nç„¶è€Œï¼Œç”±äºå¸å¼•å­ç³»ç»Ÿçš„ä¸€ä¸ªå®šä¹‰ç‰¹å¾æ˜¯å®ƒä»¬ç”±å†…éƒ¨äº§ç”Ÿçš„ã€å³ è‡ªä¸»åŠ¨åŠ›å­¦ï¼Œå› æ­¤ï¼Œåœ¨æµ‹è¯•å€™é€‰çš„å¸å¼•å­ç½‘ç»œæ—¶ï¼Œæœ€å¥½æ˜¯åœ¨è¿™æ ·çš„æ¡ä»¶: ä½¿ç”¨é‚£äº›èƒ½å¤Ÿæœ€å°åŒ–å«æ—¶çš„å¤–éƒ¨æç¤ºï¼Œæˆ–è€…æœ€å°åŒ–é‚£äº›è°ƒåˆ¶ä¸ºæ²¿ç€æ‰€è€ƒå¯Ÿçš„å¸å¼•å­ç»´åº¦æä¾›å±€éƒ¨åŒ–è¾“å…¥çš„æç¤ºâ€”â€”ä¹Ÿå°±æ˜¯ï¼Œåœ¨ä¸€ä¸ªâ€œæœ‰æ•ˆè‡ªä¸»â€çš„ç¯å¢ƒä¸‹è¿›è¡Œæµ‹è¯•ã€‚\nInnovations in recording methods that have made it possible to record multiple neurons simultaneously in animals performing naturalistic behaviours have enabled crucial tests of these state-space predictions of attractor models described above. The newest methods provide activity data from thousands of neurons in a circuit, enabling characterization of the low-dimensional state-space dynamics of whole circuits.\nè®°å½•æ–¹æ³•çš„åˆ›æ–°ä½¿å¾—åœ¨åŠ¨ç‰©è¿›è¡Œè‡ªç„¶è¡Œä¸ºæ—¶èƒ½å¤ŸåŒæ—¶è®°å½•å¤šä¸ªç¥ç»å…ƒæ´»åŠ¨ï¼Œè¿™ä¸ºéªŒè¯ä¸Šè¿°å¸å¼•å­æ¨¡å‹å¯¹çŠ¶æ€ç©ºé—´çš„é¢„æµ‹æä¾›äº†å…³é”®æµ‹è¯•æ‰‹æ®µã€‚æœ€æ–°æŠ€æœ¯å¯è·å–ç¥ç»å›è·¯ä¸­æ•°åƒä¸ªç¥ç»å…ƒçš„æ´»åŠ¨æ•°æ®ï¼Œä»è€Œèƒ½å¤Ÿè¡¨å¾æ•´ä¸ªç¥ç»å›è·¯çš„ä½ç»´çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ç‰¹æ€§ã€‚\nWhen the attractor manifolds have three or fewer dimensions, one can directly visualize them by projecting or embedding the highdimensional state spaces into dimension $\\leq 3$. This can be done using methods such as principle components analysis, multidimensional scaling, tensor factorization or other linear methods for projection; or Isomap, locally linear embedding, $t$-distributed stochastic neighbour embedding, variational autoencoders, latent factor analysis via dynamical systems and nonlinear tensor factorization, among others, for nonlinear embedding.\nThese methods can also be useful when manifolds have dimension $\\geq 3$ but are topologically simple. For topologically non-trivial structures (such as rings and tori), especially those of dimension $\\geq 3$, topological data analysis methods become important.\nå½“å¸å¼•å­æµå½¢çš„ç»´åº¦ä¸º 3 æˆ–æ›´å°‘æ—¶, å¯ä»¥é€šè¿‡å°†é«˜ç»´çŠ¶æ€ç©ºé—´æŠ•å½±æˆ– åµŒå…¥ åˆ°ç»´åº¦ $\\leq 3$ æ¥ç›´æ¥å¯è§†åŒ–å®ƒä»¬. è¿™å¯ä»¥ä½¿ç”¨ä¸»æˆåˆ†åˆ†æã€å¤šç»´å°ºåº¦åˆ†æã€å¼ é‡åˆ†è§£æˆ–å…¶ä»–çº¿æ€§æŠ•å½±æ–¹æ³•æ¥å®Œæˆ; æˆ–è€…ä½¿ç”¨ Isomapã€å±€éƒ¨çº¿æ€§åµŒå…¥ã€$t$ åˆ†å¸ƒéšæœºé‚»åŸŸåµŒå…¥ã€å˜åˆ†è‡ªç¼–ç å™¨ã€é€šè¿‡åŠ¨åŠ›ç³»ç»Ÿçš„æ½œåœ¨å› å­åˆ†æå’Œéçº¿æ€§å¼ é‡åˆ†è§£ç­‰è¿›è¡Œéçº¿æ€§åµŒå…¥.\nè¿™äº›æ–¹æ³•åœ¨æµå½¢çš„ç»´åº¦ $\\geq 3$ ä½†æ‹“æ‰‘ç»“æ„ç®€å•æ—¶ä¹Ÿå¾ˆæœ‰ç”¨. å¯¹äºæ‹“æ‰‘éå¹³å‡¡ç»“æ„ (å¦‚ç¯å’Œç¯é¢) , å°¤å…¶æ˜¯é‚£äº›ç»´åº¦ $\\geq 3$ çš„ç»“æ„, æ‹“æ‰‘æ•°æ®åˆ†ææ–¹æ³•å˜å¾—é‡è¦.\nTesting the first, second and third predictions of attractor models described above requires examination of the state-space structure of the population, rather than the more conventional characterization of relationships (tuning curves) between cell activity and input or output variables.\nThe most direct way to examine state-space structure is to record enough cells simultaneously that it is possible to characterize the full state-space manifold. However, the existence, stability and invariance of low-dimensional state-space structures (the first three predictions) can be inferred indirectly from smaller samples of simultaneously recorded cells, for example by characterizing invariant structure in pairwise cell-cell relationships, as has been successfully done in several studies.\næµ‹è¯•ä¸Šè¿°å¸å¼•å­æ¨¡å‹çš„ç¬¬ä¸€ä¸ªã€ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªé¢„æµ‹éœ€è¦æ£€æŸ¥é›†ç¾¤çš„çŠ¶æ€ç©ºé—´ç»“æ„, è€Œä¸æ˜¯æ›´ä¼ ç»Ÿçš„è¡¨å¾ç»†èƒæ´»åŠ¨ä¸è¾“å…¥æˆ–è¾“å‡ºå˜é‡ä¹‹é—´å…³ç³» (è°ƒåˆ¶æ›²çº¿).\næ£€æŸ¥çŠ¶æ€ç©ºé—´ç»“æ„çš„æœ€ç›´æ¥æ–¹æ³•æ˜¯åŒæ—¶è®°å½•è¶³å¤Ÿå¤šçš„ç»†èƒ, ä»¥ä¾¿èƒ½å¤Ÿè¡¨å¾å®Œæ•´çš„çŠ¶æ€ç©ºé—´æµå½¢. ç„¶è€Œ, ä½ç»´çŠ¶æ€ç©ºé—´ç»“æ„çš„å­˜åœ¨ã€ç¨³å®šæ€§å’Œä¸å˜æ€§ (å‰ä¸‰ä¸ªé¢„æµ‹) å¯ä»¥ä»åŒæ­¥è®°å½•çš„è¾ƒå°æ ·æœ¬ä¸­é—´æ¥æ¨æ–­å‡ºæ¥, ä¾‹å¦‚é€šè¿‡è¡¨å¾æˆå¯¹ç»†èƒ-ç»†èƒå…³ç³»ä¸­çš„ä¸å˜ç»“æ„, æ­£å¦‚å‡ é¡¹ç ”ç©¶ä¸­æˆåŠŸå®Œæˆçš„é‚£æ ·.\nThe existence and stability of low-dimensional state-space structures are necessary but not sufficient for identification of recurrent attractor dynamics in a target network.\nFirst, if the behaviours, circuit fluctuations and inputs to the network are themselves low-dimensional, then any observed low-dimensionality of the circuit states may be ascribed to those inputs and reveals little about intrinsic constraints imposed by the circuit.\nSecond, even if inputs and behaviours are highdimensional, a low-dimensional feedforward projection into the target network would generate low-dimensional states, and high-dimensional perturbations to the circuit would not persist.\nThe essential, defining prediction of attractor dynamics is that of invariance: because the states are internally generated and stabilized by strong recurrent connectivity, the population states and cell-cell relationships should be invariant when probed across time and across various input conditions, including when tuned input is removed and across waking and sleep. In simple terms, the stable low-dimensional states should be invariant across a broad range of conditions.\nä½ç»´çŠ¶æ€ç©ºé—´ç»“æ„çš„å­˜åœ¨å’Œç¨³å®šæ€§æ˜¯è¯†åˆ«ç›®æ ‡ç½‘ç»œä¸­é€’å½’å¸å¼•å­åŠ¨åŠ›å­¦çš„å¿…è¦ä½†ä¸å……åˆ†æ¡ä»¶.\né¦–å…ˆ, å¦‚æœç½‘ç»œçš„è¡Œä¸ºã€å›è·¯æ³¢åŠ¨å’Œè¾“å…¥æœ¬èº«æ˜¯ä½ç»´çš„, é‚£ä¹ˆå›è·¯çŠ¶æ€çš„ä»»ä½•è§‚å¯Ÿåˆ°çš„ä½ç»´æ€§éƒ½å¯ä»¥å½’å› äºè¿™äº›è¾“å…¥, å¹¶ä¸”å‡ ä¹æ²¡æœ‰æ­ç¤ºå›è·¯æ–½åŠ çš„å†…åœ¨çº¦æŸ.\nå…¶æ¬¡, å³ä½¿è¾“å…¥å’Œè¡Œä¸ºæ˜¯é«˜ç»´çš„, è¿›å…¥ç›®æ ‡ç½‘ç»œçš„ä½ç»´å‰é¦ˆæŠ•å½±ä¹Ÿä¼šç”Ÿæˆä½ç»´çŠ¶æ€, å¹¶ä¸”å¯¹å›è·¯çš„é«˜ç»´æ‰°åŠ¨ä¸ä¼šæŒç»­å­˜åœ¨.\nå¸å¼•å­åŠ¨åŠ›å­¦çš„åŸºæœ¬å®šä¹‰é¢„æµ‹æ˜¯ä¸å˜æ€§: ç”±äºçŠ¶æ€æ˜¯ç”±å¼ºé€’å½’è¿æ¥å†…éƒ¨ç”Ÿæˆå’Œç¨³å®šçš„, å› æ­¤å½“è·¨æ—¶é—´å’Œå„ç§è¾“å…¥æ¡ä»¶è¿›è¡Œæ¢æµ‹æ—¶, ç¾¤ä½“çŠ¶æ€å’Œç»†èƒ-ç»†èƒå…³ç³»åº”ä¿æŒä¸å˜, åŒ…æ‹¬åœ¨å»é™¤è°ƒåˆ¶è¾“å…¥ä»¥åŠåœ¨æ¸…é†’å’Œç¡çœ æœŸé—´. ç®€å•æ¥è¯´, ç¨³å®šçš„ä½ç»´çŠ¶æ€åº”åœ¨å¹¿æ³›æ¡ä»¶ä¸‹ä¿æŒä¸å˜.\nNext is the question of circuit localization: does a circuit exhibiting the key signatures of attractor dynamics give rise to these dynamics, or are they a readout of some other region?\nLocalization need not be a primary goal of establishing attractor dynamics: an important problem is to simply characterize whether the brain solves certain problems through attractor dynamics, regardless of which local circuits create these dynamics.\nNevertheless, the persistence of activity states in attractors can lend a helping hand to localization efforts. If a region gives rise to or is upstream (but not downstream) of the attractor dynamics, perturbations that alter its state along the set of attractors should persist after the perturbing drive is removed.\nä¸‹ä¸€ä¸ªé—®é¢˜æ˜¯ å›è·¯å®šä½: è¡¨ç°å‡ºå¸å¼•å­åŠ¨åŠ›å­¦å…³é”®ç‰¹å¾çš„å›è·¯, æ˜¯(è‡ªè¡Œ)äº§ç”Ÿäº†è¿™äº›åŠ¨åŠ›å­¦, è¿˜æ˜¯å®ƒä»¬æ˜¯åœ¨è¯»å–æŸä¸ªå…¶ä»–è„‘åŒº(çš„å¸å¼•å­çŠ¶æ€)ï¼Ÿ\nè¿™å¯¹äºç†è§£å¤§è„‘åŠŸèƒ½çš„åˆ†å·¥éå¸¸é‡è¦.\nå®šä½ä¸å¿…æ˜¯å»ºç«‹å¸å¼•å­åŠ¨åŠ›å­¦çš„é¦–è¦ç›®æ ‡: ä¸€ä¸ªé‡è¦çš„é—®é¢˜ä»…ä»…æ˜¯å¼„æ¸…å¤§è„‘æ˜¯å¦é€šè¿‡å¸å¼•å­åŠ¨åŠ›å­¦æ¥è§£å†³æŸäº›é—®é¢˜ï¼Œè€Œä¸ç®¡ç©¶ç«Ÿæ˜¯å“ªä¸€ä¸ªå±€éƒ¨ç¥ç»å›è·¯ç”Ÿæˆäº†è¿™äº›åŠ¨åŠ›å­¦.\nç„¶è€Œï¼Œå¸å¼•å­ä¸­æ´»åŠ¨çŠ¶æ€çš„æŒç»­æ€§å¯ä»¥å¸®åŠ©è¿›è¡Œå›è·¯å®šä½ã€‚å¦‚æœä¸€ä¸ªè„‘åŒºæœ¬èº«ç”Ÿæˆ (æˆ–å¤„äºå¸å¼•å­åŠ¨åŠ›å­¦çš„ä¸Šæ¸¸ï¼Œè€Œä¸æ˜¯ä¸‹æ¸¸) ï¼Œé‚£ä¹ˆå¯¹å…¶æ–½åŠ èƒ½å¤Ÿæ”¹å˜å…¶åœ¨å¸å¼•å­é›†åˆä¸Šä½ç½®çš„æ‰°åŠ¨åï¼Œåœ¨æ‰°åŠ¨è¾“å…¥ç§»é™¤ä¹‹åï¼Œè¿™äº›çŠ¶æ€å˜åŒ–ä»åº”æŒç»­å­˜åœ¨ã€‚\nAs we describe next, theoretically motivated analyses of population activity data have firmly established that low-dimensional attractor dynamics are ubiquitous in the brain, across levels in the brainâ€™s hierarchy and across species.\næ­£å¦‚æˆ‘ä»¬æ¥ä¸‹æ¥æè¿°çš„é‚£æ ·, å¯¹ç¾¤ä½“æ´»åŠ¨æ•°æ®çš„ç†è®ºåŠ¨æœºåˆ†æå·²ç»ç‰¢å›ºåœ°ç¡®ç«‹äº†ä½ç»´å¸å¼•å­åŠ¨åŠ›å­¦åœ¨å¤§è„‘ä¸­æ— å¤„ä¸åœ¨, è·¨è¶Šå¤§è„‘å±‚æ¬¡ç»“æ„çš„å„ä¸ªå±‚æ¬¡å’Œç‰©ç§.\nDiscrete attractors Up and down states. The simplest example of non-trivial discrete attractor dynamics (that is, beyond a single point attractor) is bistability.\nBistable dynamics are a feature of cortical activity in the form of up and down states, in which the subthreshold membrane potential of neurons switches between a hyperpolarized state and a relatively depolarized one, with long persistence (in the order of hundreds of milliseconds to seconds) per state (Fig. 3a).\nThe two states are relatively invariant over time, as seen in the relatively sharply peaked histograms (Fig. 3a), and despite presumed internal noise in the system the peaks are well separated, suggesting relatively rapid corrective dynamics towards the two states.\nThere is little evidence of a strong contribution from cellular bistability in supporting these states, suggesting that it is a network-driven phenomenon involving self-excitation and global inhibition. Transitions are believed to be driven through adaptation (from up to down) and by stochastic as well as external coordinating events (from down to up).\nAlthough these states and switches can occur in the cortex without input from the thalamus and striatum, they tend to be synchronous across the cortex and striatum. Thus, the origin of up and down states may be highly distributed.\néå¹³å‡¡ç¦»æ•£å¸å¼•å­åŠ¨åŠ›å­¦ (å³è¶…å‡ºå•ç‚¹å¸å¼•å­) çš„æœ€ç®€å•ä¾‹å­æ˜¯ åŒç¨³æ€.\nåŒç¨³æ€åŠ¨åŠ›å­¦æ˜¯ çš®å±‚æ´»åŠ¨ çš„ä¸€ä¸ªç‰¹å¾, è¡¨ç°ä¸ºä¸Šå‡å’Œä¸‹é™çŠ¶æ€, å…¶ä¸­ç¥ç»å…ƒçš„äºšé˜ˆå€¼è†œç”µä½åœ¨ è¶…æåŒ– çŠ¶æ€å’Œ ç›¸å¯¹å»æåŒ– çŠ¶æ€ä¹‹é—´åˆ‡æ¢, æ¯ä¸ªçŠ¶æ€æŒç»­æ—¶é—´è¾ƒé•¿ (å¤§çº¦æ•°ç™¾æ¯«ç§’åˆ°æ•°ç§’) (å›¾ 3a).\ndown: è¶…æåŒ–, æ¥è¿‘é™æ¯çŠ¶æ€; up: å»æåŒ–, æ¥è¿‘æ”¾ç”µé˜ˆå€¼, ç½‘ç»œæ´»åŠ¨å¼º.\nè¿™ä¸¤ä¸ªçŠ¶æ€åœ¨æ—¶é—´ä¸Šç›¸å¯¹ç¨³å®šï¼Œä»æ´»åŠ¨åˆ†å¸ƒç›´æ–¹å›¾ä¸­çš„å°–é”å³°å¯ä»¥çœ‹å‡ºæ¥ (å›¾3a) ã€‚å°½ç®¡ç³»ç»Ÿå†…éƒ¨å­˜åœ¨å™ªå£°ï¼Œè¿™ä¸¤ä¸ªå³°ä»ç„¶æ˜æ˜¾åˆ†ç¦»ï¼Œè¿™è¡¨æ˜ç³»ç»Ÿå…·æœ‰æœå‘è¿™ä¸¤ä¸ªçŠ¶æ€çš„å¿«é€Ÿçº æ­£åŠ¨åŠ›å­¦ã€‚\nç›®å‰å‡ ä¹æ²¡æœ‰è¯æ®è¡¨æ˜è¿™äº›çŠ¶æ€æ˜¯ç”±(å•ä¸ª)ç»†èƒå±‚é¢çš„åŒç¨³æ€äº§ç”Ÿçš„ï¼Œç›¸åï¼Œå®ƒä»¬ä¼¼ä¹æ˜¯ä¸€ä¸ªåŒ…å« è‡ªæ¿€æ´» å’Œ å…¨å±€æŠ‘åˆ¶ çš„ç½‘ç»œé©±åŠ¨ç°è±¡ã€‚çŠ¶æ€è½¬æ¢ä¸€èˆ¬è®¤ä¸ºæ˜¯é€šè¿‡é€‚åº”æœºåˆ¶ (ä»ä¸Šåˆ°ä¸‹) ä»¥åŠéšæœºäº‹ä»¶å’Œå¤–éƒ¨åè°ƒä¿¡å· (ä»ä¸‹åˆ°ä¸Š) è§¦å‘çš„ã€‚\nè™½ç„¶ä¸Š/ä¸‹çŠ¶æ€åœ¨æ²¡æœ‰æ¥è‡ªä¸˜è„‘æˆ–çº¹çŠ¶ä½“çš„è¾“å…¥æ—¶ä¹Ÿä¼šåœ¨çš®å±‚ä¸­å‘ç”Ÿï¼Œä½†å®ƒä»¬åœ¨çš®å±‚å’Œçº¹çŠ¶ä½“ä¹‹é—´å¾€å¾€æ˜¯åŒæ­¥çš„ã€‚å› æ­¤ï¼Œä¸Š/ä¸‹çŠ¶æ€çš„èµ·æºå¯èƒ½æ˜¯é«˜åº¦åˆ†å¸ƒå¼çš„.\nPerceptual bistability Visual and auditory percepts including binocular rivalry, the Necker cube and some auditory illusions offer clear examples of bistability in neural processing, suggesting the operation of a dynamical system with two attractors.\nIn these illusions, the brain (at the level of perceptual reports) selects one possible interpretation of an ambiguous input, often switching between possibilities. Although the phenomenon has long been known and studied, no localized bistable attractor circuit has been identified as the basis of perceptual bistability.\nIndeed, some percepts may involve top-down activation and modulation of activity across many brain areas, suggesting once again a widely distributed circuit for bistability.\nè§†è§‰å’Œå¬è§‰çŸ¥è§‰, åŒ…æ‹¬ åŒçœ¼ç«äº‰ã€Necker ç«‹æ–¹ä½“å’Œä¸€äº› å¹»å¬é”™è§‰, æä¾›äº†ç¥ç»å¤„ç†ä¸­çš„åŒç¨³æ€çš„æ¸…æ™°ä¾‹å­, è¡¨æ˜å…·æœ‰ä¸¤ä¸ªå¸å¼•å­çš„åŠ¨åŠ›ç³»ç»Ÿçš„è¿è¡Œ.\nåŒçœ¼ç«äº‰: ä¸¤åªçœ¼è¾“å…¥ä¸åŒå›¾åƒ, ä¸€æ¬¡åªèƒ½æ„è¯†åˆ°å…¶ä¸­ä¸€å¹…\nNecker ç«‹æ–¹ä½“: ä¸€ç§äºŒç»´å›¾åƒ, å¯è¢«çŸ¥è§‰ä¸ºä¸¤ç§ä¸åŒçš„ä¸‰ç»´ç«‹æ–¹ä½“è§†è§’\nåœ¨è¿™äº›é”™è§‰ä¸­, å¤§è„‘ (åœ¨æ„ŸçŸ¥æŠ¥å‘Šçš„å±‚é¢ä¸Š) é€‰æ‹©å¯¹æ¨¡ç³Šè¾“å…¥çš„ä¸€ç§å¯èƒ½è§£é‡Š, é€šå¸¸åœ¨å¯èƒ½æ€§ä¹‹é—´åˆ‡æ¢. å°½ç®¡è¿™ä¸€ç°è±¡æ—©å·²ä¸ºäººæ‰€çŸ¥å¹¶è¢«ç ”ç©¶, ä½†å°šæœªç¡®å®šä»»ä½•å±€éƒ¨åŒç¨³æ€å¸å¼•å­å›è·¯ä½œä¸ºæ„ŸçŸ¥åŒç¨³æ€çš„åŸºç¡€.\näº‹å®ä¸Š, ä¸€äº›çŸ¥è§‰å¯èƒ½æ¶‰åŠå¯¹è·¨è„‘åŒºçš„è‡ªä¸Šè€Œä¸‹çš„æ¿€æ´»å’Œæ´»åŠ¨è°ƒåˆ¶, å†æ¬¡æš—ç¤ºåŒç¨³æ€å¯èƒ½ç”±ä¸€ä¸ª é«˜åº¦åˆ†å¸ƒå¼ çš„å›è·¯äº§ç”Ÿ.\nBistability in a premotor area Recent studies identify and localize discrete attractor dynamics in a mouse premotor area, the anterior lateral motor cortex (ALM).\nIn a cued two-alternative delayed response task, ALM neurons exhibit persistent activity over a 1-s delay period.\nDuring the post-cue delay period, activity evolves towards one of two states that guide the response (Fig. 3b), fulfilling the first prediction of attractor dynamics. The delay-period terminal states are similar for cues from different sensory modalities, partially meeting the prediction of invariance.\nALM perturbations during the delay are either erased (corrected) by the circuit (Fig. 3b, top) or drive a jump to the opposite state (Fig. 3b, bottom), which results in the animal making the wrong action, suggesting bistable switching dynamics similar to the mechanism shown in either Fig. 1b or Fig. 2e.\næœ€è¿‘çš„ç ”ç©¶åœ¨å°é¼ çš„å‰è¿åŠ¨åŒºåŸŸâ€”â€”å‰å¤–ä¾§è¿åŠ¨çš®å±‚ (ALM) ä¸­è¯†åˆ«å‡ºå¹¶å®šä½äº†ç¦»æ•£å¸å¼•å­åŠ¨åŠ›å­¦ã€‚\nåœ¨ä¸€ä¸ªç”±æç¤ºå¼•å¯¼çš„äºŒé€‰ä¸€å»¶è¿Ÿååº”ä»»åŠ¡ä¸­ï¼ŒALM ç¥ç»å…ƒåœ¨çº¦ 1 ç§’çš„å»¶è¿ŸæœŸå†…è¡¨ç°å‡ºæŒç»­æ´»åŠ¨ã€‚\nåœ¨æç¤ºä¹‹åçš„å»¶è¿Ÿé˜¶æ®µï¼Œæ´»åŠ¨ä¼šå‘ä¸¤ä¸ªçŠ¶æ€ä¹‹ä¸€æ¼”åŒ–ï¼Œè€Œè¿™ä¸¤ä¸ªçŠ¶æ€å°†å¼•å¯¼åŠ¨ç‰©çš„è¡Œä¸ºååº” (å›¾ 3b) ï¼Œæ»¡è¶³äº†å¸å¼•å­åŠ¨åŠ›å­¦çš„ç¬¬ä¸€ä¸ªé¢„æµ‹ã€‚\nå¼ºæŒç»­æ´»åŠ¨; é€æ¸åˆ†åŒ–ä¸ºå·¦/å³å¸å¼•å­; çŠ¶æ€ä¸€æ—¦æ¥è¿‘å¸å¼•å­å°±è¢«å¸å¼•ç¨³å®š.\nå¯¹äºæ¥è‡ªä¸åŒæ„Ÿå®˜æ¨¡æ€çš„çº¿ç´¢ï¼Œå»¶è¿ŸæœŸçš„ç»ˆæœ«çŠ¶æ€å½¼æ­¤ç›¸ä¼¼ï¼Œä»è€Œéƒ¨åˆ†æ»¡è¶³äº†å¸å¼•å­åŠ¨åŠ›å­¦å…³äºâ€œä¸å˜æ€§â€çš„é¢„æµ‹ã€‚\nåœ¨å»¶è¿ŸæœŸé—´å¯¹ ALM è¿›è¡Œæ‰°åŠ¨æ—¶ï¼Œå…¶å½±å“è¦ä¹ˆè¢«å›è·¯æŠ¹é™¤ (çº æ­£) (å›¾ 3b ä¸Š) ï¼Œè¦ä¹ˆå°†çŠ¶æ€æ¨å‘å¦ä¸€ä¸ªå¸å¼•å­çŠ¶æ€ (å›¾ 3b ä¸‹) ï¼Œå¯¼è‡´åŠ¨ç‰©åšå‡ºé”™è¯¯åŠ¨ä½œï¼Œè¿™è¡¨æ˜ ALM ä¸­å­˜åœ¨ç±»ä¼¼å›¾ 1b æˆ–å›¾ 2e æ‰€ç¤ºæœºåˆ¶çš„åŒç¨³æ€åˆ‡æ¢åŠ¨åŠ›å­¦ã€‚\nGiven the long training time required for the task and the resulting tailoring of the ALM dynamics to the specific task structure â€” bistability for a two-choice task â€” it is likely that this system acquires its dynamics through slow plasticity and, thus, that the networkâ€™s recurrent structure is malleable in adult animals. New results showing the existence of small (on the scale of about $100 \\mu\\text{m}$) clusters of locally recurrent neurons in the ALM that can maintain persistent responses to microstimulation may provide experimental evidence of the theoretically posited mixed modular networks (below) that are hypothesized to support robust and high-capacity memory states.\né‰´äºè¯¥ä»»åŠ¡æ‰€éœ€çš„é•¿æ—¶é—´è®­ç»ƒä»¥åŠ ALM åŠ¨åŠ›å­¦å¯¹ç‰¹å®šä»»åŠ¡ç»“æ„çš„è°ƒæ•´â€”â€”åŒç¨³æ€ç”¨äºä¸¤ç§é€‰æ‹©ä»»åŠ¡â€”â€”è¯¥ç³»ç»Ÿå¾ˆå¯èƒ½é€šè¿‡ç¼“æ…¢çš„å¯å¡‘æ€§è·å¾—å…¶åŠ¨åŠ›å­¦, å› æ­¤ç½‘ç»œçš„é€’å½’ç»“æ„åœ¨æˆå¹´åŠ¨ç‰©ä¸­æ˜¯å¯å¡‘çš„. æ–°çš„ç»“æœæ˜¾ç¤º, åœ¨ ALM ä¸­å­˜åœ¨å°è§„æ¨¡ (çº¦ $100 \\mu\\text{m}$ è§„æ¨¡) çš„å±€éƒ¨é€’å½’ç¥ç»å…ƒé›†ç¾¤, è¿™äº›é›†ç¾¤å¯ä»¥ç»´æŒå¯¹å¾®åˆºæ¿€çš„æŒç»­ååº”, å¯èƒ½ä¸ºç†è®ºä¸Šå‡è®¾çš„æ··åˆæ¨¡å—åŒ–ç½‘ç»œæä¾›äº†å®éªŒè¯æ®, è¿™äº›ç½‘ç»œè¢«å‡è®¾æ”¯æŒç¨³å¥ä¸”é«˜å®¹é‡çš„è®°å¿†çŠ¶æ€.\nDiscrete multistability Hopfield networks and WTA networks (which can be viewed as a special type of Hopfield network, with bistable switch networks as a special type of WTA network) are models of multistability beyond bistability.\nHopfield ç½‘ç»œå’Œ WTA ç½‘ç»œ (å¯ä»¥å°†å…¶è§†ä¸º Hopfield ç½‘ç»œçš„ä¸€ç§ç‰¹æ®Šç±»å‹, åŒç¨³æ€å¼€å…³ç½‘ç»œä½œä¸º WTA ç½‘ç»œçš„ä¸€ç§ç‰¹æ®Šç±»å‹) æ˜¯è¶…è¶ŠåŒç¨³æ€çš„å¤šç¨³æ€æ¨¡å‹.\nAt present, the evidence for discrete multistability as a circuitlevel brain process is less direct and less exhaustive than that for continuous-attractor networks (described below).\nHowever, there are many likely candidate systems and brain regions with dynamics that are suggestive of and consistent with discrete multistability, at least of the special case of WTA attractor dynamics â€” including in the mammalian hippocampus and auditory cortex, and in the fly and mammalian olfactory system.\nIn particular, many of these circuits exhibit global inhibition that clearly narrows and refines activity in the circuit (Fig. 3c, left), and also show evidence of selective recurrent excitation that leads to multiple distinct and stably correlated input responses in distinct subpopulations of cells (Fig. 3c, middle and right).\nIn our view, it is likely that these circuits exhibit multiple discrete attractor states, but quantitative testing of the first three predictions of attractor dynamics and direct demonstration of these states as stable and invariant remain an important future direction for characterizing these circuits.\nç›®å‰, ä½œä¸ºå›è·¯çº§å¤§è„‘è¿‡ç¨‹çš„ç¦»æ•£å¤šç¨³æ€çš„è¯æ®ä¸å¦‚è¿ç»­å¸å¼•å­ç½‘ç»œ (ä¸‹é¢æè¿°çš„) ç›´æ¥å’Œè¯¦å°½.\nç„¶è€Œ, æœ‰è®¸å¤šå¯èƒ½çš„å€™é€‰ç³»ç»Ÿå’Œå¤§è„‘åŒºåŸŸ, å…¶åŠ¨åŠ›å­¦æš—ç¤ºå¹¶ä¸ç¦»æ•£å¤šç¨³æ€ä¸€è‡´, è‡³å°‘æ˜¯ WTA å¸å¼•å­åŠ¨åŠ›å­¦çš„ç‰¹æ®Šæƒ…å†µâ€”â€”åŒ…æ‹¬ å“ºä¹³åŠ¨ç‰©æµ·é©¬ä½“ å’Œ å¬è§‰çš®å±‚, ä»¥åŠæœè‡å’Œå“ºä¹³åŠ¨ç‰©çš„å—…è§‰ç³»ç»Ÿ.\nç‰¹åˆ«æ˜¯, è¿™äº›å›è·¯ä¸­çš„è®¸å¤šéƒ½è¡¨ç°å‡º å…¨å±€æŠ‘åˆ¶, æ˜æ˜¾ç¼©å°å’Œç»†åŒ–äº†å›è·¯ä¸­çš„æ´»åŠ¨ (å›¾ 3c, å·¦) , å¹¶ä¸”è¿˜æ˜¾ç¤ºå‡ºé€‰æ‹©æ€§é€’å½’å…´å¥‹çš„è¯æ®, å¯¼è‡´ç»†èƒä¸åŒå­ç¾¤ä¸­å¤šä¸ªä¸åŒä¸”ç¨³å®šç›¸å…³çš„è¾“å…¥å“åº” (å›¾ 3c, ä¸­é—´å’Œå³).\nåœ¨æˆ‘ä»¬çœ‹æ¥, è¿™äº›å›è·¯å¾ˆå¯èƒ½è¡¨ç°å‡ºå¤šä¸ªç¦»æ•£å¸å¼•å­çŠ¶æ€, ä½†å¯¹å¸å¼•å­åŠ¨åŠ›å­¦å‰ä¸‰ä¸ªé¢„æµ‹çš„å®šé‡æµ‹è¯•ä»¥åŠå°†è¿™äº›çŠ¶æ€ç›´æ¥è¯æ˜ä¸ºç¨³å®šä¸”ä¸å˜ä»ç„¶æ˜¯è¡¨å¾è¿™äº›å›è·¯çš„é‡è¦æœªæ¥æ–¹å‘.\na, Multi-unit activity (MUA) and single-unit activity ($V_{m}$) during cortical up states and down states show signatures of bistability (clusters and histograms at bottom).\na, çš®å±‚ â€œä¸Šæ€â€ å’Œ â€œä¸‹æ€â€ æœŸé—´çš„å¤šå•å…ƒæ´»åŠ¨ (MUA) å’Œå•å…ƒæ´»åŠ¨(è†œç”µä½) ($V_{m}$) æ˜¾ç¤ºå‡ºåŒç¨³æ€çš„ç‰¹å¾ (åº•éƒ¨çš„ç°‡å’Œç›´æ–¹å›¾).\nb, Delay-period dynamics in rodent premotor area (anterolateral motor cortex (ALM)) during a binary decision task (blue and red correspond to correct and incorrect direction choices, respectively). Before the animal makes a motor report of its decision (at the â€˜goâ€™ cue delivery), ALM activity seems to converge to one of two discrete end points (blue and red curves and histograms, top). Perturbations (optogenetic inhibition, denoted by pale blue) are either robustly erased (top; dashed lines show the unperturbed trajectory, and solid line shows a return to the unperturbed trajectory) or flip the dynamics so that the end points are reversed (bottom) and the animal reports the incorrect decision.\nb, åœ¨å•®é½¿åŠ¨ç‰©é¢å‰è¿åŠ¨åŒº (anterolateral motor cortexï¼ŒALM) æ‰§è¡ŒäºŒé€‰å†³ç­–ä»»åŠ¡çš„ç­‰å¾…æœŸ (delay-period) åŠ¨åŠ›å­¦ (è“è‰²å’Œçº¢è‰²åˆ†åˆ«å¯¹åº”æ­£ç¡®å’Œé”™è¯¯çš„æ–¹å‘é€‰æ‹©) ã€‚åœ¨åŠ¨ç‰©ä½œå‡ºè¿åŠ¨æ€§æŠ¥å‘Š (åœ¨ â€œgoâ€ æç¤ºä¹‹å‰) ï¼ŒALM æ´»åŠ¨çœ‹èµ·æ¥ä¼šæ”¶æ•›åˆ°ä¸¤ä¸ªç¦»æ•£ç»ˆç‚¹ä¹‹ä¸€ (é¡¶å›¾çš„è“è‰²å’Œçº¢è‰²æ›²çº¿ä¸ç›´æ–¹å›¾) ã€‚å¯¹ç½‘ç»œçš„æ‰°åŠ¨ (ç”¨å…‰é—ä¼ å­¦æŠ‘åˆ¶ï¼Œå›¾ä¸­ä»¥æ·¡è“è‰²è¡¨ç¤º) è¦ä¹ˆè¢«ç½‘ç»œç¨³å¥åœ°æŠ¹å» (é¡¶éƒ¨ï¼›è™šçº¿è¡¨ç¤ºæœªæ‰°åŠ¨æ—¶çš„è½¨è¿¹ï¼Œå®çº¿è¡¨ç¤ºæ‰°åŠ¨åè¿”å›åˆ°æœªæ‰°åŠ¨è½¨è¿¹) ï¼Œè¦ä¹ˆä½¿åŠ¨åŠ›å­¦ç¿»è½¬ï¼Œä½¿ç»ˆç‚¹äº’æ¢ (åº•éƒ¨) ï¼Œä»è€ŒåŠ¨ç‰©æŠ¥å‘Šé”™è¯¯çš„å†³ç­–ã€‚\nc, Evidence of all-to-all inhibition and competitive winner-takes-all (WTA) recurrent dynamics in the fly olfactory system. Kenyon cells (KCs) activate anterior paired lateral (APL) inhibitory neurons, which in turn globally inhibit KCs. KC responses to odours, when input from the APL neurons is intact, are sparse: top-left image shows calcium fluorescence responses of KCs to odorant isoamyl acetate. KC responses are also decorrelated across odours (left). Blocking either KC drive to APL neurons or APL inhibition of KCs results in dense and correlated odour responses (middle, right).\nc, åœ¨æœè‡å—…è§‰ç³»ç»Ÿä¸­ï¼Œæœ‰è¯æ®æ”¯æŒ â€œå…¨è¿é€šæŠ‘åˆ¶ + ç«äº‰æ€§èƒœè€…é€šåƒ (WTA) â€ çš„å›é¦ˆåŠ¨åŠ›å­¦ã€‚Kenyon ç»†èƒ (KCs) æ¿€æ´»å‰å¯¹ä¾§å¯¹ç§°çš„å‰è…¹ä¾§æŠ‘åˆ¶æ€§ç¥ç»å…ƒ (anterior paired lateralï¼ŒAPL) ï¼ŒAPL åè¿‡æ¥å¯¹æ‰€æœ‰ KCs æ–½åŠ å…¨å±€æŠ‘åˆ¶ã€‚å½“ APL çš„æŠ‘åˆ¶é€šè·¯å®Œæ•´æ—¶ï¼ŒKCs å¯¹æ°”å‘³çš„å“åº”æ˜¯ç¨€ç–çš„ï¼šå·¦ä¸Šå›¾æ˜¾ç¤º KCs å¯¹æ°”å‘³å¼‚æˆŠé†‡ (isoamyl acetate) çš„é’™è§å…‰å“åº”ã€‚KCs å¯¹ä¸åŒæ°”å‘³ä¹‹é—´çš„å“åº”ä¹Ÿè¢«å»ç›¸å…³ (å·¦) ã€‚é˜»æ–­ KC é©±åŠ¨ APL æˆ–é˜»æ–­ APL å¯¹ KCs çš„æŠ‘åˆ¶éƒ½ä¼šå¯¼è‡´å¯¹æ°”å‘³çš„å“åº”å˜å¾—å¯†é›†ä¸”é«˜åº¦ç›¸å…³ (ä¸­ã€å³å›¾) ã€‚\nè‹¥ç³»ç»Ÿæ˜¯çœŸåŒç¨³æ€ï¼Œæ•°æ®ä¼šèšæˆä¸¤ç°‡ (ä¸Šæ€ç°‡ã€ä¸‹æ€ç°‡) ï¼Œå¯¹åº”ä¸‹å›¾ä¸­ â€œèšç±»â€ å’Œ â€œç›´æ–¹å›¾â€ çš„åŒå³°å½¢æ€ã€‚\nContinuous attractors The oculomotor integrator The oculomotor integrator, together with the head-direction circuit, was one of the first systems in neuroscience to be studied theoretically and experimentally as a continuous-attractor network â€” specifically as a line attractor (Fig. 1e). This network, which is presynaptic to the motor neurons that control horizontal eye position, is highly conserved across vertebrates, from fish to primates.\nIt integrates pulse-like saccadic eye movement-command signals to generate step-like stable muscle tension command signals (Fig. 4a) that persist autonomously at graded activity levels after removal of the movement cue and even in the dark in the absence of visual feedback (Fig. 4b; third prediction), and thus enable stable gaze fixation at various degrees of eccentricity. Saccadic inputs knock the system slightly off the linear response states, but the neural responses rapidly decay back towards the persistent firing states (in line with the second prediction). Remarkably, the same system also integrates smooth head-velocity signals to permit gaze stabilization during head movement.\nçœ¼åŠ¨ç§¯åˆ†å™¨ ä¸ å¤´æœå‘å›è·¯ ä¸€èµ·, æ˜¯ç¥ç»ç§‘å­¦ä¸­æœ€æ—©ä½œä¸ºè¿ç»­å¸å¼•å­ç½‘ç»œè¿›è¡Œç†è®ºå’Œå®éªŒç ”ç©¶çš„ç³»ç»Ÿä¹‹ä¸€â€”â€”å°¤å…¶æ˜¯çº¿æ€§å¸å¼•å­ (å›¾ 1e). è¯¥ç½‘ç»œä½äºæ§åˆ¶æ°´å¹³çœ¼ä½çš„è¿åŠ¨ç¥ç»å…ƒçš„å‰çªè§¦å¤„, åœ¨ä»é±¼ç±»åˆ°çµé•¿ç±»åŠ¨ç‰©çš„è„Šæ¤åŠ¨ç‰©ä¸­é«˜åº¦ä¿å®ˆ.\nå®ƒç§¯åˆ†è„‰å†²çŠ¶çš„æ‰«è§†çœ¼åŠ¨æŒ‡ä»¤ä¿¡å·, ä»¥ç”Ÿæˆé˜¶æ¢¯çŠ¶ç¨³å®šçš„è‚Œè‚‰å¼ åŠ›æŒ‡ä»¤ä¿¡å· (å›¾ 4a) , åœ¨å»é™¤è¿åŠ¨æç¤ºåç”šè‡³åœ¨é»‘æš—ä¸­æ²¡æœ‰è§†è§‰åé¦ˆçš„æƒ…å†µä¸‹ä»¥åˆ†çº§æ´»åŠ¨æ°´å¹³è‡ªä¸»æŒç»­å­˜åœ¨ (å›¾ 4b; ç¬¬ä¸‰ä¸ªé¢„æµ‹) , ä»è€Œå®ç°å„ç§ åå¿ƒåº¦ çš„ç¨³å®šå‡è§†å›ºå®š. æ‰«è§†è¾“å…¥ä¼šä½¿ç³»ç»Ÿç•¥å¾®åç¦»çº¿æ€§å“åº”çŠ¶æ€, ä½†ç¥ç»å“åº”ä¼šè¿…é€Ÿè¡°å‡å›æŒç»­å‘å°„çŠ¶æ€ (ç¬¦åˆç¬¬äºŒä¸ªé¢„æµ‹). å€¼å¾—æ³¨æ„çš„æ˜¯, åŒä¸€ç³»ç»Ÿè¿˜ç§¯åˆ†å¹³æ»‘çš„å¤´éƒ¨é€Ÿåº¦ä¿¡å·, ä»¥å…è®¸åœ¨å¤´éƒ¨è¿åŠ¨æœŸé—´ç¨³å®šå‡è§†.\na, In the goldfish, the positions of the ipsilateral and contralateral eyes ($E_{\\text{ipsi}}$ and $E_{\\text{contra}}$, respectively) can be maintained for a stable horizontal gaze during inter-saccadic fixation at different angular positions (top two traces). This is supported by stable steps in firing rate by oculomotor integrator neurons (bottom two traces show extracellularly recorded firing rate and voltage (V)), which integrate transient (in the order of about 100 ms) saccadic command bursts.\na, åœ¨é‡‘é±¼ä¸­, åŒä¾§çœ¼ç›çš„ä½ç½® (åˆ†åˆ«ä¸ºåŒä¾§çœ¼ $E_{\\text{ipsi}}$ å’Œå¯¹ä¾§çœ¼ $E_{\\text{contra}}$) å¯ä»¥åœ¨ä¸åŒè§’åº¦ä½ç½®çš„æ‰«è§†é—´å›ºå®šæœŸé—´ä¿æŒç¨³å®šçš„æ°´å¹³å‡è§† (é¡¶éƒ¨ä¸¤æ¡æ›²çº¿). è¿™å¾—ç›Šäºçœ¼åŠ¨ç§¯åˆ†å™¨ç¥ç»å…ƒçš„ç¨³å®šé˜¶è·ƒå‘å°„ç‡ (åº•éƒ¨ä¸¤æ¡æ›²çº¿æ˜¾ç¤ºç»†èƒå¤–è®°å½•çš„å‘å°„ç‡å’Œç”µå‹ (V)) , å®ƒä»¬ç§¯åˆ†ç¬æ—¶ (å¤§çº¦ 100 æ¯«ç§’) æ‰«è§†æŒ‡ä»¤çˆ†å‘.\nb, Oculomotor neurons drive eye position with linearly ramping tuning curves (bottom). Their responses are the same in the light and the dark (top), and thus do not depend on visual input for gaze stabilization on the timescale of seconds.\nb, çœ¼åŠ¨ç¥ç»å…ƒé€šè¿‡çº¿æ€§æ–œå¡è°ƒåˆ¶æ›²çº¿é©±åŠ¨çœ¼ä½ (åº•éƒ¨). å®ƒä»¬çš„å“åº”åœ¨å…‰ç…§å’Œé»‘æš—ä¸­æ˜¯ç›¸åŒçš„ (é¡¶éƒ¨) , å› æ­¤åœ¨æ•°ç§’çš„æ—¶é—´å°ºåº¦ä¸Šä¸ä¾èµ–äºè§†è§‰è¾“å…¥æ¥ç¨³å®šå‡è§†.\nc, Transient current injection into individual oculomotor neurons results in only a transient (that is, not persistent)** decrease (left)** or increase (right) in firing rate, consistent with lack of a cellular origin for persistent intersaccadic firing.\nc, å¯¹å•ä¸ªçœ¼åŠ¨ç¥ç»å…ƒè¿›è¡Œç¬æ—¶ç”µæµæ³¨å…¥ä»…å¯¼è‡´å‘å°„ç‡çš„ç¬æ—¶ (å³éæŒç»­)** é™ä½ (å·¦)** æˆ– å¢åŠ  (å³) , è¿™ä¸æŒç»­æ‰«è§†é—´å‘å°„ç¼ºä¹ç»†èƒæ¥æºä¸€è‡´.\nd, Injection of kainic acid into the oculomotor integrator produces leaky dynamics in horizontal eye position, consistent with network models. The leak is pronounced in the dark and is still present although reduced, presumably because of visual feedback, during illumination (triangles).\nd, å‘çœ¼åŠ¨ç§¯åˆ†å™¨æ³¨å…¥ kainic acid ä¼šåœ¨æ°´å¹³çœ¼ä½ä¸­äº§ç”Ÿæ³„æ¼åŠ¨åŠ›å­¦, è¿™ä¸ç½‘ç»œæ¨¡å‹ä¸€è‡´. æ³„æ¼åœ¨é»‘æš—ä¸­å°¤ä¸ºæ˜æ˜¾, åœ¨å…‰ç…§æœŸé—´ (ç”¨ä¸‰è§’å½¢è¡¨ç¤º) è™½ç„¶æœ‰æ‰€å‡å°‘ä½†ä»ç„¶å­˜åœ¨, è¿™å¯èƒ½æ˜¯ç”±äºè§†è§‰åé¦ˆçš„ç¼˜æ•….\nç†æƒ³çš„ç§¯åˆ†å™¨åº”è¯¥ä½¿å¾—çœ¼ç›ä½ç½®ä¿æŒç¨³å®š. è€Œä¸å®Œç¾çš„ç§¯åˆ†å™¨ä¼šä½¿å¾—çœ¼ç›æŒ‡æ•°è¡°å‡å›ä¸­å¿ƒ:\n$$ \\dot{x} = -\\frac{1}{\\tau} x\\Rightarrow x(t) = x(0) e^{-t/\\tau} $$\ne, Visual training (here, from the motion of dots of light in a planetarium-like set-up) that mimics leaky or unstable eye positions in goldfish can mistune the oculomotor integrator, making it unstable or leaky, respectively. Arrows highlight fixations following saccades towards the mid position.\ne, æ¨¡æ‹Ÿé‡‘é±¼ä¸­æ³„æ¼æˆ–ä¸ç¨³å®šçœ¼ä½çš„è§†è§‰è®­ç»ƒ (è¿™é‡Œæ˜¯é€šè¿‡åœ¨ç±»ä¼¼å¤©æ–‡é¦†çš„è®¾ç½®ä¸­ç‚¹å…‰æºçš„è¿åŠ¨) å¯ä»¥ä½¿çœ¼åŠ¨ç§¯åˆ†å™¨å¤±è°ƒ, åˆ†åˆ«ä½¿å…¶å˜å¾—ä¸ç¨³å®šæˆ–æ³„æ¼. ç®­å¤´çªå‡ºæ˜¾ç¤ºäº†æ‰«è§†å‘ä¸­é—´ä½ç½®åçš„å‡è§†.\nIntegration functionality is a network-level rather than single-cell process: single neurons do not generate persistent responses to transient current injections (Fig. 4c, inset), whereas decreasing network feedback through the use of synaptic blockers reduces the time constant of integration and results in a leaky integrator (Fig. 4c).\nIt is possible to reduce or increase network feedback through training with a virtual surround that generates an artificial retinal-slip percept (Fig. 4d), implying that the system is capable of error-driven fine-tuning to maintain a high degree of persistence144.\nFinally, a recent electron microscopy reconstruction finds recurrent synaptic interconnectivity between integrator neurons, with excitatory connections between ipsilateral neurons and primarily inhibitory contralateral projections, in excellent agreement with line-attractor models of the oculomotor circuit (Fig. 1e).\nç§¯åˆ†åŠŸèƒ½æ˜¯ç½‘ç»œçº§è€Œéå•ç»†èƒè¿‡ç¨‹: å•ä¸ªç¥ç»å…ƒå¯¹ç¬æ—¶ç”µæµè¾“å…¥ä¸ä¼šäº§ç”ŸæŒç»­å“åº” (å›¾ 4c, æ’å›¾) , è€Œé€šè¿‡ä½¿ç”¨çªè§¦é˜»æ–­å‰‚å‡å°‘ç½‘ç»œåé¦ˆä¼šé™ä½ç§¯åˆ†çš„æ—¶é—´å¸¸æ•°, å¹¶å¯¼è‡´æ³„æ¼ç§¯åˆ†å™¨ (å›¾ 4c).\né€šè¿‡ä½¿ç”¨äº§ç”Ÿäººå·¥è§†ç½‘è†œæ»‘åŠ¨çŸ¥è§‰çš„è™šæ‹Ÿç¯å¢ƒè¿›è¡Œè®­ç»ƒ, å¯ä»¥å‡å°‘æˆ–å¢åŠ ç½‘ç»œåé¦ˆ (å›¾ 4d) , è¿™æ„å‘³ç€è¯¥ç³»ç»Ÿèƒ½å¤Ÿè¿›è¡Œè¯¯å·®é©±åŠ¨çš„å¾®è°ƒä»¥ä¿æŒé«˜åº¦çš„æŒç»­æ€§.\næœ€å, æœ€è¿‘çš„ä¸€é¡¹ç”µå­æ˜¾å¾®é•œé‡å»ºå‘ç°äº†ç§¯åˆ†å™¨ç¥ç»å…ƒä¹‹é—´çš„é€’å½’çªè§¦äº’è¿, åŒä¾§ç¥ç»å…ƒä¹‹é—´å­˜åœ¨å…´å¥‹æ€§è¿æ¥, è€Œä¸»è¦æ˜¯å¯¹ä¾§æŠ•å°„æŠ‘åˆ¶, è¿™ä¸çœ¼åŠ¨å›è·¯çš„çº¿æ€§å¸å¼•å­æ¨¡å‹éå¸¸å»åˆ (å›¾ 1e).\nHead-direction cells Some of the earliest experiments to suggest the existence of low-dimensional continuous-attractor dynamics were done in the rodent head-direction circuit (Fig. 5a,b).\nThe headdirection circuit in mammals maintains an updated internal compass estimate of the heading direction, relative to some arbitrary external reference, as animals move around. It does so by integrating internal rotational velocity estimates during navigation and incorporating information from external cues. The head-direction circuit is modelled as a ring-attractor network (Fig. 1c,g, left).\nBefore large population recordings became available, cell-cell correlations established that the network states remained invariant on a very lowdimensional manifold across environments (Fig. 5a), in line with the first and third predictions. The complete set of states of the several thousand-neuron mammalian head-direction network was shown to consist solely of a one-dimensional ring (Fig. 5b) (in line with the first prediction), revealing that the brain has completely factorized its navigational representations to dedicate a circuit only to head direction.\nFurthermore, intervals in the state-space ring manifold map isometrically to intervals of head direction (in line with the fourth prediction), as evidenced by a close match between the isometrically parameterized internal ring states and the measured head direction (Fig. 5b, inset and right).\næœ€æ—©çš„ä¸€äº›è¡¨æ˜å­˜åœ¨ä½ç»´è¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦çš„å®éªŒæ˜¯åœ¨ å•®é½¿åŠ¨ç‰© å¤´éƒ¨æ–¹å‘å›è·¯ä¸­å®Œæˆçš„ (å›¾ 5a, b).\nå“ºä¹³åŠ¨ç‰©çš„å¤´éƒ¨æ–¹å‘å›è·¯åœ¨åŠ¨ç‰©å››å¤„ç§»åŠ¨æ—¶, ä¿æŒç›¸å¯¹äºæŸä¸ªä»»æ„å¤–éƒ¨å‚è€ƒçš„èˆªå‘æ–¹å‘çš„æ›´æ–°å†…éƒ¨æŒ‡å—é’ˆä¼°è®¡. å®ƒé€šè¿‡åœ¨å¯¼èˆªè¿‡ç¨‹ä¸­ç§¯åˆ†å†…éƒ¨æ—‹è½¬é€Ÿåº¦ä¼°è®¡å¹¶ç»“åˆæ¥è‡ªå¤–éƒ¨çº¿ç´¢çš„ä¿¡æ¯æ¥å®ç°è¿™ä¸€ç‚¹. å¤´éƒ¨æ–¹å‘å›è·¯è¢«å»ºæ¨¡ä¸ºç¯å½¢å¸å¼•å­ç½‘ç»œ (å›¾ 1c, g, å·¦).\nåœ¨å¤§è§„æ¨¡é›†ç¾¤è®°å½•å®ç°å‰, ç»†èƒ-ç»†èƒç›¸å…³æ€§ç¡®å®šäº†ç½‘ç»œçŠ¶æ€åœ¨éå¸¸ä½ç»´æµå½¢ä¸Šåœ¨ä¸åŒç¯å¢ƒä¸­ä¿æŒä¸å˜ (å›¾ 5a) , ç¬¦åˆç¬¬ä¸€ä¸ªå’Œç¬¬ä¸‰ä¸ªé¢„æµ‹. å“ºä¹³åŠ¨ç‰©å¤´éƒ¨æ–¹å‘ç½‘ç»œçš„å‡ åƒä¸ªç¥ç»å…ƒçš„å®Œæ•´çŠ¶æ€é›†è¢«è¯æ˜ä»…ç”±ä¸€ç»´ç¯ç»„æˆ (å›¾ 5b) (ç¬¦åˆç¬¬ä¸€ä¸ªé¢„æµ‹) , æ­ç¤ºäº†å¤§è„‘å·²ç»å®Œå…¨åˆ†è§£äº†å…¶å¯¼èˆªè¡¨ç¤º, ä»¥ä¸“é—¨ç”¨äºå¤´éƒ¨æ–¹å‘çš„å›è·¯.\næ­¤å¤–, çŠ¶æ€ç©ºé—´ç¯æµå½¢ä¸­çš„é—´éš”ä¸å¤´éƒ¨æ–¹å‘çš„é—´éš”ç­‰è·æ˜ å°„ (ç¬¦åˆç¬¬å››ä¸ªé¢„æµ‹) , è¿™å¯ä»¥é€šè¿‡å†…ç¯çŠ¶æ€ä¸æµ‹é‡çš„å¤´éƒ¨æ–¹å‘ä¹‹é—´çš„å¯†åˆ‡åŒ¹é…æ¥è¯æ˜ (å›¾ 5b, æ’å›¾å’Œå³ä¾§).\na, Activity of two cells in the rat head-direction circuit during free foraging in a two-dimensional circular arena with a globally orienting cue (top). When the cue is removed (bottom), the fields rotate, but the cells maintain their tuning shapes and relative tuning angles (pale curves show the cellsâ€™ activity from the top plot, but globally rotated).\na, å¤§é¼ å¤´éƒ¨æ–¹å‘å›è·¯ä¸­ä¸¤ä¸ªç»†èƒåœ¨å¸¦æœ‰å…¨å±€å®šå‘æç¤ºçš„äºŒç»´åœ†å°ä¸­è‡ªç”±è§…é£Ÿæ—¶çš„æ´»åŠ¨ (é¡¶éƒ¨). å½“æç¤ºè¢«ç§»é™¤æ—¶ (åº•éƒ¨) , ç»†èƒçš„è°ƒåˆ¶åœºä¼šæ—‹è½¬, ä½†ç»†èƒä¿æŒå…¶è°ƒåˆ¶å½¢çŠ¶å’Œç›¸å¯¹è°ƒåˆ¶è§’åº¦ (æ·¡è‰²æ›²çº¿æ˜¾ç¤ºé¡¶éƒ¨å›¾ä¸­çš„ç»†èƒæ´»åŠ¨, ä½†è¿›è¡Œäº†å…¨å±€æ—‹è½¬).\nb, The population-level states of the anterodorsal thalamus during free-foraging and other natural behaviour in a two-dimensional environment, shown through nonlinear embedding in two dimensions and independently validated by topological data analysis, are confined to a onedimensional ring (as in Fig. 1c). Inset: another view of the same ring manifold in three dimensions (left). The manifold is colourized based on a computational approach called SPUD (spline parameterization for unsupervised decoding): the manifold is fit by a spline of matching dimension and topology (middle), and the spline is parameterized isometrically; equal changes in parameter value for equal distances along the manifold (right). Parameter changes are indicated by colour.\nb, åœ¨äºŒç»´ç¯å¢ƒä¸­è‡ªç”±è§…é£Ÿå’Œå…¶ä»–è‡ªç„¶è¡Œä¸ºæœŸé—´, å‰èƒŒä¸˜è„‘çš„é›†ç¾¤æ°´å¹³çŠ¶æ€é€šè¿‡äºŒç»´éçº¿æ€§åµŒå…¥æ˜¾ç¤ºï¼Œå¹¶é€šè¿‡æ‹“æ‰‘æ•°æ®åˆ†æç‹¬ç«‹éªŒè¯ï¼Œé™åˆ¶åœ¨ä¸€ç»´ç¯ä¸Š (å¦‚å›¾ 1c æ‰€ç¤º). æ’å›¾: ä¸‰ç»´ä¸­åŒä¸€ç¯æµå½¢çš„å¦ä¸€ç§è§†å›¾ (å·¦). æµå½¢åŸºäºä¸€ç§ç§°ä¸º SPUD (æ— ç›‘ç£è§£ç çš„æ ·æ¡å‚æ•°åŒ–) çš„è®¡ç®—æ–¹æ³•è¿›è¡Œç€è‰²: æµå½¢ç”±å…·æœ‰åŒ¹é…ç»´åº¦å’Œæ‹“æ‰‘çš„æ ·æ¡æ‹Ÿåˆ (ä¸­é—´) , å¹¶ä¸”æ ·æ¡æ˜¯ç­‰è·å‚æ•°åŒ–çš„; æµå½¢ä¸Šç›¸ç­‰è·ç¦»çš„å‚æ•°å€¼å˜åŒ–ç›¸ç­‰ (å³). å‚æ•°å˜åŒ–ç”±é¢œè‰²è¡¨ç¤º.\nc, There is a close match between unsupervised isometric parametrization of the manifold from part b and the externally measured head direction of the rodent.\nc, æ¥è‡ª b éƒ¨åˆ†çš„æµå½¢çš„æ— ç›‘ç£ç­‰è·å‚æ•°åŒ–ä¸å•®é½¿åŠ¨ç‰©å¤–éƒ¨æµ‹é‡çš„å¤´éƒ¨æ–¹å‘ä¹‹é—´å­˜åœ¨å¯†åˆ‡åŒ¹é….\nd,e, The same cells as in part b were recorded during rapid eye movement (REM) sleep (green): the states during REM sleep remain confined to a one-dimensional ring that precisely overlays the ring of waking states (blue, part e), and states off the ring exhibit large flows (black arrows) back towards the ring (part d).\nd,e, ä¸ b éƒ¨åˆ†ç›¸åŒçš„ç»†èƒåœ¨å¿«é€Ÿçœ¼åŠ¨ (REM) ç¡çœ æœŸé—´è¢«è®°å½• (ç»¿è‰²) : REM ç¡çœ æœŸé—´çš„çŠ¶æ€ä»ç„¶é™åˆ¶åœ¨ä¸€ä¸ªä¸€ç»´ç¯ä¸Šï¼Œè¯¥ç¯ç²¾ç¡®åœ°è¦†ç›–äº†æ¸…é†’çŠ¶æ€çš„ç¯ (è“è‰²ï¼Œe éƒ¨åˆ†) , è€Œç¯å¤–çš„çŠ¶æ€è¡¨ç°å‡ºå¤§å¹…åº¦çš„æµåŠ¨ (é»‘è‰²ç®­å¤´) å›åˆ°ç¯ä¸Š (d éƒ¨åˆ†).\nf, Calcium imaging of activity in the physically ring-shaped Drosophila ellipsoid body reveals a localized bump of excitation that follows the movement of a cue in the flyâ€™s visual field.\nf, å¯¹ç‰©ç†ç¯å½¢æœè‡æ¤­åœ†ä½“ä¸­æ´»åŠ¨çš„é’™æˆåƒæ­ç¤ºäº†ä¸€ä¸ªå±€éƒ¨çš„å…´å¥‹å³°, å®ƒè·Ÿéšæœè‡è§†è§‰åœºä¸­çº¿ç´¢çš„è¿åŠ¨.\ng, A combination of electrophysiology and electron microscopy imaging of the central complex in flies has provided detailed layout and connectivity data for comparison with predicted connectivity in ring attractor models.\ng, å¯¹æœè‡ä¸­å¤®å¤åˆä½“çš„ç”µç”Ÿç†å­¦å’Œç”µå­æ˜¾å¾®é•œæˆåƒçš„ç»“åˆæä¾›äº†è¯¦ç»†çš„å¸ƒå±€å’Œè¿æ¥æ•°æ®, ä»¥ä¸ç¯å½¢å¸å¼•å­æ¨¡å‹ä¸­é¢„æµ‹çš„è¿æ¥è¿›è¡Œæ¯”è¾ƒ.\nAfter natural perturbations away from the ring attractor, the activity of the head-direction circuit flowed back to it (Fig. 5d), meeting the second prediction, and the ring manifold was invariant across waking and rapid eye movement (REM) sleep (Fig. 5e), meeting the third prediction.\nThese findings explicitly validate the most fundamental predictions of ring attractor models and continuous attractor-based integrators, providing (together with the grid cell system; see below) the most direct and compelling evidence of continuous-attractor dynamics in the brain.\nåœ¨è¿œç¦»ç¯å½¢å¸å¼•å­è¿›è¡Œè‡ªç„¶æ‰°åŠ¨å, å¤´éƒ¨æ–¹å‘å›è·¯çš„æ´»åŠ¨æµå›åˆ°å®ƒ (å›¾ 5d) , æ»¡è¶³äº†ç¬¬äºŒä¸ªé¢„æµ‹, å¹¶ä¸”ç¯æµå½¢åœ¨æ¸…é†’å’Œå¿«é€Ÿçœ¼åŠ¨ (REM) ç¡çœ æœŸé—´æ˜¯ä¸å˜çš„ (å›¾ 5e) , æ»¡è¶³äº†ç¬¬ä¸‰ä¸ªé¢„æµ‹.\nè¿™äº›å‘ç°æ˜ç¡®éªŒè¯äº†ç¯å½¢å¸å¼•å­æ¨¡å‹å’ŒåŸºäºè¿ç»­å¸å¼•å­çš„ç§¯åˆ†å™¨çš„æœ€åŸºæœ¬é¢„æµ‹, æä¾›äº† (ä¸ç½‘æ ¼ç»†èƒç³»ç»Ÿä¸€èµ·; è§ä¸‹æ–‡) å¤§è„‘ä¸­è¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦çš„æœ€ç›´æ¥å’Œä»¤äººä¿¡æœçš„è¯æ®.\nIn a striking example of convergent evolution, Drosophila compute head-direction estimates using apparently very similar dynamics to mammals. The fly neural compass circuit is topographically organized such that the neuropil forms a physical ring-shaped structure in the ellipsoid body, with a local moving activity peak that tracks head direction as the fly turns (Fig. 5f).\nOther notable advantages of the fly circuit in the effort to characterize its mechanisms are that the number of neurons is small and their morphology and connectivity have been fully traced (Fig. 5g). This detailed view of the circuit permits quantitative, not just qualitative, comparisons with ring-attractor models.\nåœ¨ è¶‹åŒè¿›åŒ– çš„æ˜¾è‘—ä¾‹è¯ä¸­ï¼Œæœè‡ ä¸ å“ºä¹³åŠ¨ç‰© é‡‡ç”¨æä¸ºç›¸ä¼¼çš„åŠ¨åŠ›å­¦æœºåˆ¶æ¥è®¡ç®—å¤´éƒ¨æ–¹å‘ä¼°è®¡å€¼ã€‚æœè‡ç¥ç»æŒ‡å—é’ˆå›è·¯å…·æœ‰æ‹“æ‰‘ç»“æ„ç‰¹å¾ï¼šç¥ç»èƒ¶è´¨åœ¨æ¤­åœ†ä½“ä¸­å½¢æˆç‰©ç†ç¯çŠ¶ç»“æ„ï¼Œå…¶å±€éƒ¨æ´»åŠ¨å³°å€¼éšæœè‡è½¬å‘è€Œå®æ—¶è¿½è¸ªå¤´éƒ¨æ–¹å‘ (å›¾5f) ã€‚\nåœ¨åŠªåŠ›è¡¨å¾å…¶æœºåˆ¶æ–¹é¢, æœè‡å›è·¯çš„å…¶ä»–æ˜¾è‘—ä¼˜åŠ¿æ˜¯ç¥ç»å…ƒæ•°é‡å°‘ä¸”å…¶å½¢æ€å’Œè¿æ¥æ€§å·²è¢«å®Œå…¨è¿½è¸ª (å›¾ 5g). å¯¹å›è·¯çš„è¿™ç§è¯¦ç»†äº†è§£å…è®¸å¯¹ç¯å½¢å¸å¼•å­æ¨¡å‹è¿›è¡Œå®šé‡è€Œä¸ä»…ä»…æ˜¯å®šæ€§çš„æ¯”è¾ƒ.\nThe combined activity and connectivity data reveal that the fly head-direction system quite literally implements the copy-and-offset double-ring network architecture that has been proposed for velocity integration. However, the dimensionality of the fly head-direction circuit and its full state-space dynamics remain to be characterized.\nNotably, although the circuit is organized physically as a ring network, recent evidence suggests that the insect head-direction circuit may be involved in performing two-dimensional path integration as well. Thus, unlike the anterodorsal thalamic nucleus network in mammals, the insect head-direction circuit may not be confined to a one-dimensional ring of attractor states that fully factorizes out the representation of head direction in its representation of spatial variables.\nç»“åˆæ´»åŠ¨å’Œè¿æ¥æ•°æ®è¡¨æ˜, æœè‡å¤´éƒ¨æ–¹å‘ç³»ç»Ÿå®é™…ä¸Šå®ç°äº†ä¸ºé€Ÿåº¦ç§¯åˆ†æå‡ºçš„å¤åˆ¶å’Œåç§»åŒç¯ç½‘ç»œæ¶æ„. ç„¶è€Œ, æœè‡å¤´éƒ¨æ–¹å‘å›è·¯çš„ç»´åº¦åŠå…¶å®Œæ•´çš„çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ä»æœ‰å¾…è¡¨å¾.\nå€¼å¾—æ³¨æ„çš„æ˜¯, å°½ç®¡è¯¥å›è·¯åœ¨ç‰©ç†ä¸Šç»„ç»‡ä¸ºç¯å½¢ç½‘ç»œ, ä½†æœ€è¿‘çš„è¯æ®è¡¨æ˜æ˜†è™«å¤´éƒ¨æ–¹å‘å›è·¯å¯èƒ½ä¹Ÿå‚ä¸æ‰§è¡ŒäºŒç»´è·¯å¾„ç§¯åˆ†. å› æ­¤, ä¸å“ºä¹³åŠ¨ç‰©çš„å‰èƒŒä¸˜è„‘æ ¸ç½‘ç»œä¸åŒ, æ˜†è™«å¤´éƒ¨æ–¹å‘å›è·¯å¯èƒ½ä¸é™äºä¸€ç»´ç¯å½¢å¸å¼•å­çŠ¶æ€, è¿™äº›çŠ¶æ€åœ¨å…¶ç©ºé—´å˜é‡è¡¨ç¤ºä¸­å®Œå…¨åˆ†è§£äº†å¤´éƒ¨æ–¹å‘çš„è¡¨ç¤º.\nFinally, the head-direction system of both insects and mammals can be re-anchored and reset based on tuned external cues, and this can change the orientation tuning curves of cells and moment by moment firing rates of cells in a way that remains consistent with the third prediction for attractor dynamics.\næœ€å, æ˜†è™«å’Œå“ºä¹³åŠ¨ç‰©çš„å¤´éƒ¨æ–¹å‘ç³»ç»Ÿéƒ½å¯ä»¥åŸºäºè°ƒåˆ¶çš„å¤–éƒ¨çº¿ç´¢é‡æ–°é”šå®šå’Œé‡ç½®, è¿™å¯ä»¥ä»¥ä¸€ç§ä¸å¸å¼•å­åŠ¨åŠ›å­¦ç¬¬ä¸‰ä¸ªé¢„æµ‹ä¿æŒä¸€è‡´çš„æ–¹å¼æ”¹å˜ç»†èƒçš„æ–¹å‘è°ƒåˆ¶æ›²çº¿å’Œç»†èƒçš„ç¬æ—¶å‘å°„ç‡.\nGrid cells A grid cell encodes spatial location through a periodic triangular-lattice discharge pattern that tiles explored two-dimensional spaces. Grid cell phases update during movement in the light and in the dark to reflect the animalâ€™s current position, as a two-dimensional phase.\nContinuous-attractor models of grid cells are based on collective Turing pattern formation, explain their velocity integration function and predict that grid cells should exist in large sets with identical spatial periodicity and orientation, but tile all possible twodimensional phases.\nAs with the first general prediction of continuousattractor models, they specifically predict that the population states of such a set of cells should be confined to merely two dimensions along a torus-shaped manifold that remains unchanged across environments and behavioural states15 (Fig. 1d, rightmost column).\nç½‘æ ¼ç»†èƒé€šè¿‡å‘¨æœŸæ€§çš„ ä¸‰è§’æ™¶æ ¼ æ”¾ç”µæ¨¡å¼ç¼–ç ç©ºé—´ä½ç½®, è¯¥æ¨¡å¼å¹³é“ºäº†æ¢ç´¢çš„äºŒç»´ç©ºé—´. ç½‘æ ¼ç»†èƒåœ¨æœ‰å…‰å’Œé»‘æš—ä¸­è¿åŠ¨æ—¶æ›´æ–°å…¶ç›¸ä½, å³åŠ¨ç‰©çš„å½“å‰ä½ç½®æ˜ å°„ä¸ºäºŒç»´ç›¸ä½.\nç½‘æ ¼ç»†èƒçš„è¿ç»­å¸å¼•å­æ¨¡å‹åŸºäºé›†ä½“ Turing æ¨¡å¼å½¢æˆ, è§£é‡Šäº†å®ƒä»¬çš„é€Ÿåº¦ç§¯åˆ†åŠŸèƒ½, å¹¶é¢„æµ‹ç½‘æ ¼ç»†èƒåº”è¯¥å­˜åœ¨äºå…·æœ‰ç›¸åŒç©ºé—´å‘¨æœŸæ€§å’Œæ–¹å‘çš„å¤§é›†åˆä¸­, ä½†å¹³é“ºæ‰€æœ‰å¯èƒ½çš„äºŒç»´ç›¸ä½.\nä¸è¿ç»­å¸å¼•å­æ¨¡å‹çš„ç¬¬ä¸€ä¸ªä¸€èˆ¬é¢„æµ‹ä¸€æ ·, å®ƒä»¬ç‰¹åˆ«é¢„æµ‹: è¿™æ ·ä¸€ç»„ç»†èƒçš„ç¾¤ä½“çŠ¶æ€åº”ä»…é™äºæ²¿ç€ç¯é¢å½¢æµå½¢çš„ä¸¤ä¸ªç»´åº¦, è¯¥æµå½¢åœ¨ä¸åŒç¯å¢ƒå’Œè¡Œä¸ºçŠ¶æ€ä¸‹ä¿æŒä¸å˜ (å›¾ 1d, æœ€å³åˆ—).\nAnalyses of simultaneously recorded grid cells with similar periods revealed that their periods and orientations are identical down to estimation noise (thus defining a discrete population, subsequently called a â€˜moduleâ€™) and that they tile all possible two-dimensional phases, strongly suggesting a two-dimensional torus in line with the first prediction.\nMoreover, the relative firing phases and grid parameter ratios of co-modular cells are tightly conserved even as the spatial tuning of cells varies across time and environments (Fig. 6a), with the dimensionality of the spatial environment (Fig. 6b) and with large environmental rescaling-driven deformations of grid tuning, confirming the prediction of invariance. In addition, the detailed cell-cell relationships seen in waking exploration that define the low-dimensional response of a grid module are conserved across overnight sleep in grid cells but not in place cells (Fig. 6c), establishing that the low-dimensional states are autonomously generated.\nIn line with all of the fundamental predictions of continuous-attractor dynamics, these findings established that each grid moduleâ€™s response is very low-dimensional; is invariant across environments, time and behavioural states; and is internally stabilized and autonomously generated.\nMost recently, these findings were confirmed by large-scale recordings of grid cells that made it possible to directly characterize the grid cell population response by applying the topological analyses of statespace structure pioneered earlier to grid cells (Fig. 6e), directly illustrating the low-dimensional, toroidal and invariant state-space structure of grid cell modules.\nå¯¹åŒæ­¥è®°å½•çš„å…·æœ‰ç›¸ä¼¼å‘¨æœŸçš„ç½‘æ ¼ç»†èƒçš„åˆ†æè¡¨æ˜, å®ƒä»¬çš„å‘¨æœŸå’Œæ–¹å‘åœ¨ä¼°è®¡å™ªå£°èŒƒå›´å†…æ˜¯ç›¸åŒçš„ (ä»è€Œå®šä¹‰äº†ä¸€ä¸ªç¦»æ•£ç¾¤ä½“, éšåç§°ä¸º â€œæ¨¡å—â€ ) , å¹¶ä¸”å®ƒä»¬å¹³é“ºäº†æ‰€æœ‰å¯èƒ½çš„äºŒç»´ç›¸ä½, è¿™å¼ºçƒˆæš—ç¤ºäº†ä¸ç¬¬ä¸€ä¸ªé¢„æµ‹ä¸€è‡´çš„äºŒç»´ç¯é¢.\næ­¤å¤–, å³ä½¿éšç€æ—¶é—´å’Œç¯å¢ƒ (å›¾ 6a) ä¸­ç»†èƒç©ºé—´è°ƒåˆ¶çš„å˜åŒ–, ä»¥åŠç»†èƒç½‘æ ¼è°ƒåˆ¶çš„å¤§è§„æ¨¡ç¯å¢ƒé‡ç¼©æ”¾é©±åŠ¨å˜å½¢, å…±æ¨¡ç»†èƒçš„ç›¸å¯¹å‘å°„ç›¸ä½å’Œç½‘æ ¼å‚æ•°æ¯”ç‡ä¹Ÿå¾—åˆ°äº†ç´§å¯†ä¿å­˜, ç¡®è®¤äº†ä¸å˜æ€§çš„é¢„æµ‹. æ­¤å¤–, åœ¨ç½‘æ ¼ç»†èƒä¸­è¿‡å¤œç¡çœ æœŸé—´, å®šä¹‰ç½‘æ ¼æ¨¡å—ä½ç»´å“åº”çš„æ¸…é†’æ¢ç´¢ä¸­è§‚å¯Ÿåˆ°çš„è¯¦ç»†ç»†èƒ-ç»†èƒå…³ç³»å¾—ä»¥ä¿å­˜, è€Œåœ¨ä½ç½®ç»†èƒä¸­åˆ™æ²¡æœ‰ (å›¾ 6c) , ç¡®ç«‹äº†ä½ç»´çŠ¶æ€æ˜¯è‡ªä¸»ç”Ÿæˆçš„.\nç¬¦åˆè¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦çš„æ‰€æœ‰åŸºæœ¬é¢„æµ‹, è¿™äº›å‘ç°ç¡®ç«‹äº†æ¯ä¸ªç½‘æ ¼æ¨¡å—å“åº”\næ˜¯éå¸¸ä½ç»´çš„; åœ¨ç¯å¢ƒã€æ—¶é—´å’Œè¡Œä¸ºçŠ¶æ€ä¸‹æ˜¯ä¸å˜çš„; æ˜¯å†…éƒ¨ç¨³å®šå’Œè‡ªä¸»ç”Ÿæˆçš„. æœ€è¿‘, é€šè¿‡å¯¹ç½‘æ ¼ç»†èƒçš„å¤§å°ºåº¦è®°å½•è¯å®äº†è¿™äº›å‘ç°, è¿™ä½¿å¾—é€šè¿‡åº”ç”¨æ—©å…ˆåœ¨ç½‘æ ¼ç»†èƒä¸Šå¼€åˆ›çš„ çŠ¶æ€ç©ºé—´ç»“æ„æ‹“æ‰‘åˆ†æ æ¥ç›´æ¥è¡¨å¾ç½‘æ ¼ç»†èƒç¾¤ä½“å“åº”æˆä¸ºå¯èƒ½ (å›¾ 6e) , ç›´æ¥è¯´æ˜äº†ç½‘æ ¼ç»†èƒæ¨¡å—çš„ä½ç»´ã€ç¯é¢å’Œä¸å˜çŠ¶æ€ç©ºé—´ç»“æ„.\na, The spatial tuning periods and orientations of grid cells reconfigure substantially in novel environments (left: firing patterns of an example pair of grid cells in a familiar and a novel environment), but cellâ€“cell relationships remain the same, as seen from the tight covariance of changes across cells (right), implying an internally generated low-dimensional structure. Each colour corresponds to a variable that describes the lattice of the spatial tuning curve of the cell, as shown in the schematic.\na, ç½‘æ ¼ç»†èƒåœ¨æ–°ç¯å¢ƒä¸­çš„ç©ºé—´è°ƒåˆ¶å‘¨æœŸå’Œæ–¹å‘å‘ç”Ÿäº†å®è´¨æ€§é‡æ„ (å·¦å›¾: ä¾‹å­ä¸­ä¸€å¯¹ç½‘æ ¼ç»†èƒåœ¨ç†Ÿæ‚‰å’Œæ–°ç¯å¢ƒä¸­çš„å‘å°„æ¨¡å¼) , ä½†ç»†èƒ-ç»†èƒå…³ç³»ä¿æŒä¸å˜, ä»ç»†èƒé—´å˜åŒ–çš„ç´§å¯†åæ–¹å·®ä¸­å¯ä»¥çœ‹å‡º (å³å›¾) , è¿™æ„å‘³ç€ä¸€ä¸ªå†…éƒ¨ç”Ÿæˆçš„ä½ç»´ç»“æ„. æ¯ç§é¢œè‰²å¯¹åº”æè¿°ç»†èƒç©ºé—´è°ƒåˆ¶æ›²çº¿æ™¶æ ¼çš„å˜é‡, å¦‚ç¤ºæ„å›¾æ‰€ç¤º.\nb, The non-periodic responses of two example co-modular cells (dark blue) on a one-dimensional linear track do not look like simple offsets of one another, raising the question of whether cellâ€“cell relationships have reconfigured and the grid cell dynamics are not low-dimensional and invariant. However, the responses of the cells can be predicted (light blue) as parallel slices through the two-dimensional grid (bottom), and their two-dimensional relative phase offset is predicted by the separation of the one-dimensional response slices, showing that the cell relationships and two-dimensional circuit dynamics are preserved across diverse conditions.\nb, ä¸¤ä¸ªä¾‹å­ä¸­å…±æ¨¡ç»†èƒ (æ·±è“è‰²) åœ¨ä¸€ç»´çº¿æ€§è½¨é“ä¸Šçš„éå‘¨æœŸæ€§å“åº”çœ‹èµ·æ¥ä¸åƒå½¼æ­¤çš„ç®€å•åç§», è¿™å¼•å‘äº†ç»†èƒ-ç»†èƒå…³ç³»æ˜¯å¦å·²é‡æ–°é…ç½®ä»¥åŠç½‘æ ¼ç»†èƒåŠ¨åŠ›å­¦æ˜¯å¦ä¸æ˜¯ä½ç»´å’Œä¸å˜çš„é—®é¢˜. ç„¶è€Œ, è¿™äº›ç»†èƒçš„å“åº”å¯ä»¥é¢„æµ‹ (æµ…è“è‰²) ä¸ºäºŒç»´ç½‘æ ¼çš„å¹³è¡Œåˆ‡ç‰‡ (åº•éƒ¨) , å®ƒä»¬çš„äºŒç»´ç›¸å¯¹ç›¸ä½åç§»ç”±ä¸€ç»´å“åº”åˆ‡ç‰‡çš„åˆ†ç¦»é¢„æµ‹, æ˜¾ç¤ºç»†èƒå…³ç³»å’ŒäºŒç»´å›è·¯åŠ¨åŠ›å­¦åœ¨å„ç§æ¡ä»¶ä¸‹å¾—ä»¥ä¿å­˜.\nc, Pairwise correlations between grid cells in the medial entorhinal cortex (MEC) measured during navigation are preserved across overnight rapid eye movement (REM) sleep and non-REM (NREM) sleep, whereas those of place cells in hippocampal area CA1 are not.\nc, åœ¨å¯¼èˆªæœŸé—´æµ‹é‡çš„å†…å—…çš®å±‚ (MEC) ä¸­ç½‘æ ¼ç»†èƒä¹‹é—´çš„æˆå¯¹ç›¸å…³æ€§åœ¨è¿‡å¤œå¿«é€Ÿçœ¼åŠ¨ (REM) ç¡çœ å’Œé REM (NREM) ç¡çœ æœŸé—´å¾—ä»¥ä¿å­˜, è€Œæµ·é©¬åŒº CA1 ä¸­ä½ç½®ç»†èƒçš„ç›¸å…³æ€§åˆ™æ²¡æœ‰.\nd, Grid cells are anatomically arranged according to their relative spatial firing phases. Left: cell positions in a field of view of the MEC coloured according to the phase of their spatial tuning curves. The relative cortical positions of same-phase cells make a triangular lattice pattern (middle), with a grid-like autocorrelation pattern (right).\nd, ç½‘æ ¼ç»†èƒæ ¹æ®å…¶ç›¸å¯¹ç©ºé—´å‘å°„ç›¸ä½è¿›è¡Œè§£å‰–æ’åˆ—. å·¦å›¾: MEC è§†é‡ä¸­ç»†èƒçš„ä½ç½®æ ¹æ®å…¶ç©ºé—´è°ƒåˆ¶æ›²çº¿çš„ç›¸ä½è¿›è¡Œç€è‰². åŒç›¸ä½ç»†èƒçš„ç›¸å¯¹çš®å±‚ä½ç½®å½¢æˆä¸‰è§’å½¢æ™¶æ ¼æ¨¡å¼ (ä¸­é—´) , å…·æœ‰ç½‘æ ¼çŠ¶è‡ªç›¸å…³æ¨¡å¼ (å³ä¾§).\ne, The population-level states of grid cells from one module (each dot represents the population state at one point in time) during free foraging in a two-dimensional environment are shown through nonlinear dimensionality reduction and confirmed by topological data analysis to lie on a two-dimensional torus (left) as predicted by models15. As the animal follows a spatial trajectory (right), the state moves along the torus manifold (left). Manifold colouring is a gradient along the first principal component of the data.\ne, åœ¨äºŒç»´ç¯å¢ƒä¸­è‡ªç”±è§…é£ŸæœŸé—´, æ¥è‡ªä¸€ä¸ªæ¨¡å—çš„ç½‘æ ¼ç»†èƒçš„ç¾¤ä½“æ°´å¹³çŠ¶æ€ (æ¯ä¸ªç‚¹è¡¨ç¤ºæŸä¸€æ—¶é—´ç‚¹çš„ç¾¤ä½“çŠ¶æ€) é€šè¿‡éçº¿æ€§é™ç»´æ˜¾ç¤º, å¹¶é€šè¿‡æ‹“æ‰‘æ•°æ®åˆ†æç¡®è®¤ä½äºäºŒç»´ç¯é¢ä¸Š (å·¦ä¾§) , æ­£å¦‚æ¨¡å‹æ‰€é¢„æµ‹çš„é‚£æ ·. å½“åŠ¨ç‰©æ²¿ç€ç©ºé—´è½¨è¿¹ç§»åŠ¨æ—¶ (å³ä¾§) , çŠ¶æ€æ²¿ç€ç¯é¢æµå½¢ç§»åŠ¨ (å·¦ä¾§). æµå½¢ç€è‰²æ˜¯æ•°æ®ç¬¬ä¸€ä¸»æˆåˆ†çš„æ¸å˜.\nA corollary is that the grid cell response is not derived from upstream place cells, which remap across environments and during sleep (Fig. 6c): as shown in ref.101, this finding renders models in which the place cell response is primary to grid cells inconsistent with the data. Another corollary of the population states of grid cells remaining strictly preserved, even when their spatial tuning curves in two-dimensional and three-dimensional environments are altered so they do not form equilateral triangular grids, is that these variations must result from changes in how the invariant internal states are mapped to external states. Such changes may arise from, for example, alterations in velocity estimation that stretch the grid or from external cues that shift the phase of the grid cell network, rather than because of alterations in the internal grid network dynamics.\nä¸€ä¸ªæ¨è®ºæ˜¯, ç½‘æ ¼ç»†èƒçš„å“åº”ä¸æ˜¯æ¥è‡ªä¸Šæ¸¸çš„ä½ç½®ç»†èƒ, è¿™äº›ç»†èƒåœ¨ç¯å¢ƒå’Œç¡çœ æœŸé—´ä¼šé‡æ–°æ˜ å°„ (å›¾ 6c) : æ­£å¦‚å‚è€ƒæ–‡çŒ® 101 æ‰€ç¤º, è¿™ä¸€å‘ç°ä½¿å¾—ä½ç½®ç»†èƒå“åº”å¯¹ç½‘æ ¼ç»†èƒèµ·ä¸»è¦ä½œç”¨çš„æ¨¡å‹ä¸æ•°æ®ä¸ä¸€è‡´. ç½‘æ ¼ç»†èƒçš„ç¾¤ä½“çŠ¶æ€ä¸¥æ ¼ä¿æŒä¸å˜, å³ä½¿å®ƒä»¬åœ¨äºŒç»´å’Œä¸‰ç»´ç¯å¢ƒä¸­çš„ç©ºé—´è°ƒåˆ¶æ›²çº¿è¢«æ”¹å˜, ä»¥è‡³äºå®ƒä»¬ä¸å½¢æˆç­‰è¾¹ä¸‰è§’å½¢ç½‘æ ¼, å…¶å¦ä¸€ä¸ªæ¨è®ºæ˜¯, è¿™äº›å˜åŒ–å¿…é¡»æ˜¯ç”±äºä¸å˜çš„å†…éƒ¨çŠ¶æ€å¦‚ä½•æ˜ å°„åˆ°å¤–éƒ¨çŠ¶æ€çš„å˜åŒ–æ‰€è‡´. è¿™äº›å˜åŒ–å¯èƒ½æºè‡ªä¾‹å¦‚é€Ÿåº¦ä¼°è®¡çš„æ”¹å˜, ä»è€Œæ‹‰ä¼¸ç½‘æ ¼, æˆ–æ¥è‡ªå¤–éƒ¨çº¿ç´¢, ä»è€Œç§»åŠ¨ç½‘æ ¼ç»†èƒç½‘ç»œçš„ç›¸ä½, è€Œä¸æ˜¯ç”±äºå†…éƒ¨ç½‘æ ¼ç½‘ç»œåŠ¨åŠ›å­¦çš„æ”¹å˜.\nDespite having periodic representations, and thus each only representing position as an ambiguous two-dimensional phase, collectively grid cells form a discrete set of modules with distinct but similar periodicities164. This allows grid cells to unambiguously represent position over a scale that grows exponentially in the number of grid modules.\nå°½ç®¡å…·æœ‰å‘¨æœŸæ€§è¡¨ç¤º, å› æ­¤æ¯ä¸ªä»…å°†ä½ç½®è¡¨ç¤ºä¸ºæ¨¡ç³Šçš„äºŒç»´ç›¸ä½, ä½†ç½‘æ ¼ç»†èƒé›†ä½“å½¢æˆäº†ä¸€ç»„å…·æœ‰ä¸åŒä½†ç›¸ä¼¼å‘¨æœŸæ€§çš„ç¦»æ•£æ¨¡å—. è¿™å…è®¸ç½‘æ ¼ç»†èƒä»¥éšç€ç½‘æ ¼æ¨¡å—æ•°é‡å‘ˆæŒ‡æ•°å¢é•¿çš„æ¯”ä¾‹æ˜ç¡®åœ°è¡¨ç¤ºä½ç½®.\nIn sum, the head-direction cell and grid cell systems show that the same pattern formation principle â€” based on local excitation or disinhibition, with broader inhibition â€” that is pivotal for morphogenesis in plants and animals is also fundamental to the genesis of stationary continuous-attractor states for computation and representation in the brain.\næ€»ä¹‹, å¤´éƒ¨æ–¹å‘ç»†èƒå’Œç½‘æ ¼ç»†èƒç³»ç»Ÿè¡¨æ˜, åŒæ ·çš„æ¨¡å¼å½¢æˆåŸç†â€”â€”åŸºäºå±€éƒ¨å…´å¥‹æˆ–å»æŠ‘åˆ¶, ä¼´éšç€æ›´å¹¿æ³›çš„æŠ‘åˆ¶â€”â€”å¯¹äºæ¤ç‰©å’ŒåŠ¨ç‰©çš„å½¢æ€å‘ç”Ÿè‡³å…³é‡è¦, å¯¹äºå¤§è„‘ä¸­è®¡ç®—å’Œè¡¨ç¤ºçš„é™æ€è¿ç»­å¸å¼•å­çŠ¶æ€çš„äº§ç”Ÿä¹Ÿæ˜¯åŸºæœ¬çš„.\nGraded working memory networks In monkeys trained to saccade to a remembered cued location (selected from a set arranged in a circle), cells in the prefrontal cortex and posterior parietal cortex exhibit persistent activity across the delay period that is selective for the direction of the cue, consistent with the first and third predictions of attractor dynamics.\nThe delay period activity in the prefrontal cortex is a bump that moves apparently randomly along a onedimensional manifold with the characteristics of a diffusion process. Thus, the variance in bump location grows linearly with time during the delay, as predicted by continuous-attractor models, but the bump profile remains largely invariant (first and second predictions).\nBump movement predicts subsequent behavioural errors, suggesting that these states are repositories or read-outs of the memory.\nåœ¨æ¥å—è®­ç»ƒä»¥æ‰«è§†è®°å¿†æç¤ºä½ç½® (ä»æ’åˆ—æˆåœ†å½¢çš„ä¸€ç»„ä¸­é€‰æ‹©) çš„çŒ´å­ä¸­, å‰é¢å¶çš®å±‚ å’Œ åé¡¶å¶çš®å±‚ çš„ç»†èƒåœ¨å»¶è¿ŸæœŸé—´è¡¨ç°å‡ºå¯¹æç¤ºæ–¹å‘é€‰æ‹©æ€§çš„æŒç»­æ´»åŠ¨, è¿™ä¸å¸å¼•å­åŠ¨åŠ›å­¦çš„ç¬¬ä¸€ä¸ªå’Œç¬¬ä¸‰ä¸ªé¢„æµ‹ä¸€è‡´.\nå‰é¢å¶çš®å±‚ ä¸­çš„å»¶è¿ŸæœŸæ´»åŠ¨æ˜¯ä¸€ä¸ªæ²¿ç€ä¸€ç»´æµå½¢éšæœºç§»åŠ¨çš„å³°, å…¶ç‰¹å¾ç±»ä¼¼äºæ‰©æ•£è¿‡ç¨‹. å› æ­¤, åœ¨å»¶è¿ŸæœŸé—´, å³°ä½ç½®çš„æ–¹å·®éšç€æ—¶é—´çº¿æ€§å¢é•¿, æ­£å¦‚è¿ç»­å¸å¼•å­æ¨¡å‹æ‰€é¢„æµ‹çš„é‚£æ ·, ä½†å³°å€¼è½®å»“åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¿æŒä¸å˜ (ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªé¢„æµ‹).\nå³°å€¼ç§»åŠ¨é¢„æµ‹éšåçš„è¡Œä¸ºé”™è¯¯, è¡¨æ˜è¿™äº›çŠ¶æ€æ˜¯è®°å¿†çš„å­˜å‚¨åº“æˆ–è¯»å‡º.\nThe need for extensive training and the resulting tailoring of the attractor states to this specific but not naturally encountered multi-cue task suggests that this attractor forms through learning in a flexible system. We might therefore also expect a loss of the neural correlation structure if the animal is subsequently trained on other tasks, unlike with the grid and head-direction cell networks.\néœ€è¦å¤§é‡è®­ç»ƒä»¥åŠå¯¹å¸å¼•å­çŠ¶æ€çš„è°ƒæ•´ä»¥é€‚åº”è¿™ä¸€ç‰¹å®šä½†éè‡ªç„¶é‡åˆ°çš„å¤šæç¤ºä»»åŠ¡, è¡¨æ˜è¯¥å¸å¼•å­æ˜¯é€šè¿‡çµæ´»ç³»ç»Ÿä¸­çš„å­¦ä¹ å½¢æˆçš„. å› æ­¤, å¦‚æœåŠ¨ç‰©éšåæ¥å—å…¶ä»–ä»»åŠ¡çš„è®­ç»ƒ, æˆ‘ä»¬ä¹Ÿå¯èƒ½æœŸæœ›ç¥ç»ç›¸å…³ç»“æ„çš„ä¸¢å¤±, è¿™ä¸ç½‘æ ¼å’Œå¤´éƒ¨æ–¹å‘ç»†èƒç½‘ç»œä¸åŒ.\nLimit-cycle attractors The CNS and peripheral nervous system contain numerous instances of periodic dynamics, from the spiking of single neurons to circadian rhythms and sleep-cycle generation, to rhythmic activity in motor circuits.\nThe amplitude of a linear oscillator is set by the initial condition (for example, the height at which a pendulum is released), whereas limit-cycle oscillators have an invariant intrinsic amplitude. Thus, oscillations that decay or whose long-term amplitude or frequency changes after transient perturbation are not limit cycles.\nä¸­æ¢ç¥ç»ç³»ç»Ÿå’Œ å¤–å‘¨ç¥ç»ç³»ç»Ÿ åŒ…å«è®¸å¤šå‘¨æœŸæ€§åŠ¨åŠ›å­¦çš„å®ä¾‹, ä»å•ä¸ªç¥ç»å…ƒçš„å°–å³°å‘å°„åˆ°æ˜¼å¤œèŠ‚å¾‹å’Œç¡çœ å‘¨æœŸç”Ÿæˆ, å†åˆ°è¿åŠ¨å›è·¯ä¸­çš„èŠ‚å¾‹æ´»åŠ¨.\nçº¿æ€§æŒ¯è¡å™¨çš„æŒ¯å¹…ç”±åˆå§‹æ¡ä»¶è®¾å®š (ä¾‹å¦‚, å•æ‘†é‡Šæ”¾æ—¶çš„é«˜åº¦) , è€Œæé™ç¯æŒ¯è¡å™¨å…·æœ‰ä¸å˜çš„å†…ç¦€æŒ¯å¹…. å› æ­¤, åœ¨ç¬æ—¶æ‰°åŠ¨åè¡°å‡æˆ–å…¶é•¿æœŸæŒ¯å¹…æˆ–é¢‘ç‡å‘ç”Ÿå˜åŒ–çš„æŒ¯è¡ä¸æ˜¯æé™ç¯.\nMany of the oscillations noted above maintain their amplitude over time and, given their robustness, are probably generated through attractor dynamics.\nExperimentally well-characterized examples of sustained periodic dynamics are central pattern generators in spinal motor circuits that drive swimming, crawling, walking, breathing and digestion; these differ in specifics across species but have common principles of mechanism and operation, including high robustness.\nCentral pattern generator circuits typically integrate external feedback, but can operate in isolation without external drive189. However, driven (non-autonomous) systems could exhibit limit cycles that are attributable to their inputs rather than to intrinsic attractor dynamics.\nè®¸å¤šä¸Šè¿°æŒ¯è¡éšç€æ—¶é—´çš„æ¨ç§»ä¿æŒå…¶æŒ¯å¹…, å¹¶ä¸”é‰´äºå…¶ç¨³å¥æ€§, å¯èƒ½æ˜¯é€šè¿‡å¸å¼•å­åŠ¨åŠ›å­¦äº§ç”Ÿçš„.\nå®éªŒä¸Šè¡¨å¾è‰¯å¥½çš„æŒç»­å‘¨æœŸåŠ¨åŠ›å­¦çš„ä¾‹å­æ˜¯è„Šé«“è¿åŠ¨å›è·¯ä¸­çš„ ä¸­å¤®æ¨¡å¼å‘ç”Ÿå™¨, å®ƒä»¬é©±åŠ¨æ¸¸æ³³ã€çˆ¬è¡Œã€è¡Œèµ°ã€å‘¼å¸å’Œæ¶ˆåŒ–; è¿™äº›åœ¨ä¸åŒç‰©ç§ä¸­å…·ä½“æƒ…å†µä¸åŒ, ä½†åœ¨æœºåˆ¶å’Œæ“ä½œåŸç†ä¸Šå…·æœ‰å…±åŒç‚¹, åŒ…æ‹¬é«˜åº¦çš„ç¨³å¥æ€§.\nä¸­å¤®æ¨¡å¼å‘ç”Ÿå™¨å›è·¯é€šå¸¸ç§¯åˆ†å¤–éƒ¨åé¦ˆ, ä½†ä¹Ÿå¯ä»¥åœ¨æ²¡æœ‰å¤–éƒ¨é©±åŠ¨çš„æƒ…å†µä¸‹ç‹¬ç«‹è¿è¡Œ. ç„¶è€Œ, å—é©±åŠ¨ (éè‡ªä¸») ç³»ç»Ÿå¯èƒ½è¡¨ç°å‡ºæé™ç¯, å…¶å½’å› äº(å¤–éƒ¨)è¾“å…¥è€Œä¸æ˜¯å†…ç¦€çš„å¸å¼•å­åŠ¨åŠ›å­¦.\nGiven the sizeable literature on these topics, we refer the reader to some excellent papers and reviews.\né‰´äºè¿™äº›ä¸»é¢˜çš„åºå¤§æ–‡çŒ®, æˆ‘ä»¬å»ºè®®è¯»è€…å‚è€ƒä¸€äº›ä¼˜ç§€çš„è®ºæ–‡å’Œç»¼è¿°æ–‡ç« .\nDepartures from attractor dynamics Not all circuits hypothesized to exhibit low-dimensional attractor dynamics seem under further experimentation to do so, or currently lack sufficient evidence to establish such dynamics in the circuit. We discuss three such examples.\nå¹¶éæ‰€æœ‰å‡è®¾è¡¨ç°å‡ºä½ç»´å¸å¼•å­åŠ¨åŠ›å­¦çš„å›è·¯åœ¨è¿›ä¸€æ­¥å®éªŒä¸­ä¼¼ä¹éƒ½è¿™æ ·åš, æˆ–è€…ç›®å‰ç¼ºä¹è¶³å¤Ÿçš„è¯æ®æ¥ç¡®ç«‹å›è·¯ä¸­çš„è¿™ç§åŠ¨åŠ›å­¦. æˆ‘ä»¬è®¨è®ºä¸‰ä¸ªè¿™æ ·çš„ä¾‹å­.\nOrientation tuning in visual cortex The circuit of simple cells in the primary visual cortex (V1) satisfies some key properties of attractor networks: V1 and V2 cells exhibit orientation-tuned responses to real and illusory edges, and in V1 the activity of neurons with similar orientation tuning is correlated during spontaneous activity.\nHowever, changing the state of an attractor requires strong inputs and is slow, inconsistent with the need for perceptual systems to respond sensitively and rapidly.\nMoreover, the responses to illusory edges in V1 tend to occur at longer latency than responses to real edges, suggestive of top-down inputs rather than within-V1 dynamics. These observations lend weight to the possibility that responses might be dominated by feedforward drive, potentially with non-normal amplification processes. Quantitative characterizations of response speed will be important to draw clear conclusions about V1 circuit dynamics.\nåˆçº§è§†è§‰çš®å±‚ (V1) ä¸­ç®€å•ç»†èƒçš„å›è·¯æ»¡è¶³å¸å¼•å­ç½‘ç»œçš„ä¸€äº›å…³é”®å±æ€§: V1 å’Œ V2 ç»†èƒå¯¹çœŸå®å’Œé”™è§‰è¾¹ç¼˜è¡¨ç°å‡ºæ–¹å‘è°ƒåˆ¶å“åº”, å¹¶ä¸”åœ¨ V1 ä¸­, å…·æœ‰ç›¸ä¼¼æ–¹å‘è°ƒåˆ¶çš„ç¥ç»å…ƒåœ¨è‡ªå‘æ´»åŠ¨æœŸé—´çš„æ´»åŠ¨æ˜¯ç›¸å…³çš„.\nç„¶è€Œ, å¸å¼•å­çŠ¶æ€æ”¹å˜éœ€è¦å¼ºè¾“å…¥å¹¶ä¸”å¾ˆæ…¢, è¿™ä¸æ„ŸçŸ¥ç³»ç»Ÿéœ€è¦æ•æ„Ÿä¸”å¿«é€Ÿå“åº”çš„éœ€æ±‚ä¸ä¸€è‡´.\næ­¤å¤–, V1 ä¸­å¯¹é”™è§‰è¾¹ç¼˜çš„å“åº”å¾€å¾€æ¯”å¯¹çœŸå®è¾¹ç¼˜çš„å“åº”å‘ç”Ÿå¾—æ›´æ™š, è¿™è¡¨æ˜æ˜¯è‡ªä¸Šè€Œä¸‹çš„è¾“å…¥è€Œä¸æ˜¯ V1 å†…éƒ¨åŠ¨åŠ›å­¦. è¿™äº›è§‚å¯Ÿç»“æœæ”¯æŒè¿™æ ·ä¸€ç§å¯èƒ½æ€§, å³å“åº”å¯èƒ½ä»¥å‰é¦ˆé©±åŠ¨ä¸ºä¸», å¯èƒ½ä¼´éšç€éæ­£è§„æ”¾å¤§è¿‡ç¨‹. å¯¹å“åº”é€Ÿåº¦çš„å®šé‡è¡¨å¾å¯¹äºå¾—å‡ºå…³äº V1 å›è·¯åŠ¨åŠ›å­¦çš„æ˜ç¡®ç»“è®ºå°†éå¸¸é‡è¦.\nPlace cells Place cells form stable representations of space that can persist in the dark and shortly after the animal has fallen asleep. In any particular environment, the population response lies on a lowdimensional manifold in state space.\nAccordingly, the place cell circuit has been modelled as a continuous-attractor network with one or multiple overlapping maps, whereby each map is a different assignment of cells to spatial locations.\nHowever, the storage of multiple high-resolution maps in a homogeneous attractor network severely limits capacity. Cell-cell correlations are not preserved across environments, as implied by the phenomenon of remapping. Similar to V1 neurons, place cells might be better described as deriving their tuning by forming conjunctions between multiple feedforward inputs, including those from grid cells and cells that encode external cues such as borders, landmarks and reward sites.\nAt the same time, place cells exhibit sequential activation of previous trajectories during activity hippocampal replay. This sequential activation is hypothesized to be generated by recurrent connections in hippocampal area CA3, suggesting that recurrent and feedforward dynamics may collaborate in the generation of place cell states; more recent models are beginning to capture this interplay. Closing the book on the question of autonomous low-dimensional dynamics in what, in our view, is the far more complex response of place cells than grid cells requires more detailed experimentation, analysis and modelling.\nä½ç½®ç»†èƒå½¢æˆç©ºé—´çš„ç¨³å®šè¡¨ç¤º, è¿™äº›è¡¨ç¤ºå¯ä»¥åœ¨é»‘æš—ä¸­æŒç»­å­˜åœ¨, å¹¶ä¸”åœ¨åŠ¨ç‰©å…¥ç¡åä¸ä¹…ä¹Ÿèƒ½æŒç»­å­˜åœ¨. åœ¨ä»»ä½•ç‰¹å®šç¯å¢ƒä¸­, ç¾¤ä½“å“åº”ä½äºçŠ¶æ€ç©ºé—´ä¸­çš„ä½ç»´æµå½¢ä¸Š.\nå› æ­¤, ä½ç½®ç»†èƒå›è·¯è¢«å»ºæ¨¡ä¸ºå…·æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªé‡å åœ°å›¾çš„è¿ç»­å¸å¼•å­ç½‘ç»œ, å…¶ä¸­æ¯ä¸ªåœ°å›¾æ˜¯ç»†èƒä»¬åˆ°ç©ºé—´ä½ç½®çš„ä¸åŒåˆ†é….\nç„¶è€Œ, åœ¨å‡åŒ€å¸å¼•å­ç½‘ç»œä¸­å­˜å‚¨å¤šä¸ªé«˜åˆ†è¾¨ç‡åœ°å›¾æ—¶, å®¹é‡è¢«ä¸¥é‡é™åˆ¶. ç»†èƒ-ç»†èƒç›¸å…³æ€§åœ¨ä¸åŒç¯å¢ƒä¸­æ²¡æœ‰å¾—åˆ°ä¿å­˜, è¿™ä¸é‡æ–°æ˜ å°„ç°è±¡æ‰€æš—ç¤ºçš„ä¸€è‡´. ç±»ä¼¼äº V1 ç¥ç»å…ƒ, ä½ç½®ç»†èƒå¯èƒ½æ›´å¥½åœ°æè¿°ä¸ºé€šè¿‡å½¢æˆå¤šä¸ªå‰é¦ˆè¾“å…¥çš„ç»“åˆæ¥è·å¾—å…¶è°ƒåˆ¶, åŒ…æ‹¬æ¥è‡ªç½‘æ ¼ç»†èƒå’Œç¼–ç å¤–éƒ¨çº¿ç´¢ (å¦‚è¾¹ç•Œã€åœ°æ ‡å’Œå¥–åŠ±åœ°ç‚¹) çš„ç»†èƒçš„è¾“å…¥.\nä¸æ­¤åŒæ—¶, ä½ç½®ç»†èƒåœ¨æµ·é©¬é‡æ”¾æœŸé—´è¡¨ç°å‡ºå…ˆå‰è½¨è¿¹çš„åºåˆ—æ¿€æ´». è¿™ç§åºåˆ—æ¿€æ´»è¢«å‡è®¾æ˜¯ç”±æµ·é©¬ä½“ CA3 åŒºåŸŸä¸­çš„é€’å½’è¿æ¥äº§ç”Ÿçš„, è¿™è¡¨æ˜é€’å½’å’Œå‰é¦ˆåŠ¨åŠ›å­¦å¯èƒ½åœ¨ä½ç½®ç»†èƒçŠ¶æ€çš„ç”Ÿæˆä¸­ååŒå·¥ä½œ; æœ€è¿‘çš„æ¨¡å‹å¼€å§‹æ•æ‰è¿™ç§ç›¸äº’ä½œç”¨. è¦è§£å†³æˆ‘ä»¬è®¤ä¸ºä½ç½®ç»†èƒçš„å“åº”æ¯”ç½‘æ ¼ç»†èƒå¤æ‚å¾—å¤šçš„é—®é¢˜ä¸­è‡ªä¸»ä½ç»´åŠ¨åŠ›å­¦çš„é—®é¢˜, éœ€è¦æ›´è¯¦ç»†çš„å®éªŒã€åˆ†æå’Œå»ºæ¨¡.\nMotor cortical trajectories Finally, recordings of motor cortical activity during stereotyped arm movements in primates reveal the existence of stable low-dimensional trajectories, similar to the trajectories in state space that were originally characterized in olfactory circuit responses to different odours.\nLimit cycles and other low-dimensional attractors have been hypothesized to have a key role in cortical movement generation. The behaviours typically performed during these neural recordings are themselves restricted to be stereotyped and low-dimensional, and thus it remains unclear whether activity would remain equally low-dimensional across richer behaviours (for example, over the set of all possible arm movements).\nRecent evidence from perturbation experiments suggests that neural trajectories in the motor cortex during skilled movements are driven by input from the thalamus, and thus that the circuits for motor pattern generation in the CNS might be distributed across multiple brain regions.\nCharacterizing the intrinsic dimensionality of motor cortical activity, and determining whether the command to make more-complex motions involves multiple upstream or distributed primitive attractors, remain important open questions for both clinical brain-machine interfaces and neuroscience.\næœ€å, åœ¨çµé•¿ç±»åŠ¨ç‰©è¿›è¡Œåˆ»æ¿è‡‚éƒ¨è¿åŠ¨æœŸé—´è®°å½•çš„ è¿åŠ¨çš®å±‚æ´»åŠ¨ æ­ç¤ºäº†ç¨³å®šçš„ä½ç»´è½¨è¿¹çš„å­˜åœ¨, ç±»ä¼¼äºæœ€åˆåœ¨å¯¹ä¸åŒæ°”å‘³çš„å—…è§‰å›è·¯å“åº”ä¸­è¡¨å¾çš„çŠ¶æ€ç©ºé—´ä¸­çš„è½¨è¿¹.\næé™ç¯å’Œå…¶ä»–ä½ç»´å¸å¼•å­è¢«å‡è®¾åœ¨çš®å±‚è¿åŠ¨ç”Ÿæˆä¸­èµ·å…³é”®ä½œç”¨. è¿™äº›ç¥ç»è®°å½•æœŸé—´é€šå¸¸æ‰§è¡Œçš„è¡Œä¸ºæœ¬èº«è¢«é™åˆ¶ä¸ºåˆ»æ¿ä¸”ä½ç»´, å› æ­¤å°šä¸æ¸…æ¥šåœ¨æ›´ä¸°å¯Œçš„è¡Œä¸º (ä¾‹å¦‚, åœ¨æ‰€æœ‰å¯èƒ½çš„æ‰‹è‡‚è¿åŠ¨é›†åˆä¸Š) ä¸­æ´»åŠ¨æ˜¯å¦ä»ç„¶ä¿æŒåŒæ ·çš„ä½ç»´.\næ¥è‡ªæ‰°åŠ¨å®éªŒçš„æœ€æ–°è¯æ®è¡¨æ˜, ç†Ÿç»ƒè¿åŠ¨æœŸé—´è¿åŠ¨çš®å±‚ä¸­çš„ç¥ç»è½¨è¿¹æ˜¯ç”±ä¸˜è„‘è¾“å…¥é©±åŠ¨çš„, å› æ­¤ CNS ä¸­è¿åŠ¨æ¨¡å¼ç”Ÿæˆçš„å›è·¯å¯èƒ½åˆ†å¸ƒåœ¨å¤šä¸ªå¤§è„‘åŒºåŸŸ.\nè¡¨å¾è¿åŠ¨çš®å±‚æ´»åŠ¨çš„å†…åœ¨ç»´åº¦, å¹¶ç¡®å®šå‘å‡ºæ›´å¤æ‚è¿åŠ¨å‘½ä»¤æ˜¯å¦æ¶‰åŠå¤šä¸ªä¸Šæ¸¸æˆ–åˆ†å¸ƒå¼åŸå§‹å¸å¼•å­, ä»ç„¶æ˜¯ä¸´åºŠè„‘-æœºæ¥å£å’Œç¥ç»ç§‘å­¦çš„é‡è¦æœªè§£ä¹‹è°œ.\nFlexibility despite rigidity The attractor networks we have described in this Review are typically rigid across time and conditions. However, recent experimental and theoretical work has suggested that low-dimensional and rigid attractor states could be reused and recombined to create versatile and efficient systems for representation and computation in new situations.\næˆ‘ä»¬åœ¨æœ¬ç»¼è¿°ä¸­æè¿°çš„å¸å¼•å­ç½‘ç»œé€šå¸¸åœ¨æ—¶é—´å’Œæ¡ä»¶ä¸Šæ˜¯åˆšæ€§çš„. ç„¶è€Œ, æœ€è¿‘çš„å®éªŒå’Œç†è®ºå·¥ä½œè¡¨æ˜, ä½ç»´ä¸”åˆšæ€§çš„å¸å¼•å­çŠ¶æ€å¯ä»¥è¢«é‡å¤ä½¿ç”¨å’Œé‡æ–°ç»„åˆ, ä»¥åœ¨æ–°æƒ…å†µä¸‹åˆ›å»ºç”¨äºè¡¨ç¤ºå’Œè®¡ç®—çš„å¤šåŠŸèƒ½ä¸”é«˜æ•ˆçš„ç³»ç»Ÿ.\nBuilding a representation (Fig. 2a) could proceed by painstakingly constructing a large set of associative feedforward correspondences, equivalent to a look-up table.\nBy contrast, an attractor that is an integrator requires only two feedforward correspondences: an anchoring process that identifies one external state to one internal one, and then an association of external movement-based velocities with the internal shift mechanism in the integrator (Fig. 2f).\nThus, continuous attractors that are also integrators could enable, for example, the rapid construction and even inference of states visited for the first time through a new trajectory, and could be reused to represent multiple variables.\nIndeed, the brain seems to (re)use grid cells and place cells when navigating in space and in non-spatial domains; recent work shows how the dimensionality of the represented variable could be greater than the individual attractor networks.\næ„å»ºè¡¨ç¤º (å›¾ 2a) å¯ä»¥é€šè¿‡è¾›è‹¦åœ°æ„å»ºå¤§é‡å…³è”çš„å‰é¦ˆå¯¹åº”å…³ç³»æ¥è¿›è¡Œ, è¿™ç›¸å½“äºä¸€ä¸ªæŸ¥æ‰¾è¡¨.\nç›¸æ¯”ä¹‹ä¸‹, ä½œä¸ºç§¯åˆ†å™¨çš„å¸å¼•å­åªéœ€è¦ä¸¤ä¸ªå‰é¦ˆå¯¹åº”å…³ç³»: ä¸€ä¸ªé”šå®šè¿‡ç¨‹, å°†ä¸€ä¸ªå¤–éƒ¨çŠ¶æ€è¯†åˆ«ä¸ºä¸€ä¸ªå†…éƒ¨çŠ¶æ€, ç„¶åå°†åŸºäºå¤–éƒ¨è¿åŠ¨çš„é€Ÿåº¦ä¸ç§¯åˆ†å™¨ä¸­çš„å†…éƒ¨ç§»ä½æœºåˆ¶ç›¸å…³è” (å›¾ 2f).\nå› æ­¤, æ—¢æ˜¯è¿ç»­å¸å¼•å­åˆæ˜¯ç§¯åˆ†å™¨çš„å¸å¼•å­å¯ä»¥å®ç°, ä¾‹å¦‚, é€šè¿‡æ–°çš„è½¨è¿¹å¿«é€Ÿæ„å»ºç”šè‡³æ¨æ–­é¦–æ¬¡è®¿é—®çš„çŠ¶æ€, å¹¶ä¸”å¯ä»¥é‡å¤ä½¿ç”¨ä»¥è¡¨ç¤ºå¤šä¸ªå˜é‡.\näº‹å®ä¸Š, å¤§è„‘ä¼¼ä¹çš„ç¡®åœ¨ç©ºé—´å’Œéç©ºé—´é¢†åŸŸå¯¼èˆªæ—¶ (é‡æ–°) ä½¿ç”¨ç½‘æ ¼ç»†èƒå’Œä½ç½®ç»†èƒ; æœ€è¿‘çš„å·¥ä½œæ˜¾ç¤º, æ‰€è¡¨ç¤ºå˜é‡çš„ç»´åº¦å¯èƒ½å¤§äºå•ä¸ªå¸å¼•å­ç½‘ç»œ.\nA further line of work has posited that networks composed of modular subnetworks, each an attractor network, enable a given number of neurons to represent an exponentially larger number of representational or memory states through combinations of states than fully connected, Hopfield-like networks can.\nAlthough the combinatorial states expressed by the set of attractor networks are not themselves attractors, it is possible to couple together these subnetworks to generate an exponential number of attractor states such that they each have a reasonably sized basin and are thus robust (Fig. 2). The states in these networks cannot have arbitrary form and content; they are defined by the rigid states of each module.\nThus, a crucial question is how they could be leveraged for memory. Such high-capacity sets of attractor states have been shown to provide possible models for high-capacity and robust action selection, robust classification62 and smoothly decaying associative memory. Moreover, the principles described in this paragraph can be combined in a â€˜mixed modular coding schemeâ€™ to represent and store inputs of any dimensionality relative to the individual attractor networks, so long as it is lower than the summed attractor dimension across networks, without needing to reconfigure the recurrent network (Fig. 2h). Much of the potential for alternative uses, configurations or combinations of attractor networks remains unexplored and is ripe for further study.\nè¿›ä¸€æ­¥çš„å·¥ä½œè·¯çº¿å‡è®¾, ç”±æ¨¡å—åŒ–å­ç½‘ç»œç»„æˆçš„ç½‘ç»œ, æ¯ä¸ªéƒ½æ˜¯ä¸€ä¸ªå¸å¼•å­ç½‘ç»œ, ä½¿å¾—ç»™å®šæ•°é‡çš„ç¥ç»å…ƒèƒ½å¤Ÿé€šè¿‡çŠ¶æ€ç»„åˆè¡¨ç¤ºæ¯”å®Œå…¨è¿æ¥çš„ç±»ä¼¼ Hopfield ç½‘ç»œæ›´å¤šçš„è¡¨ç¤ºæˆ–è®°å¿†çŠ¶æ€.\nå°½ç®¡ç”±ä¸€ç»„å¸å¼•å­ç½‘ç»œè¡¨è¾¾çš„ç»„åˆçŠ¶æ€æœ¬èº«ä¸æ˜¯å¸å¼•å­, ä½†å¯ä»¥å°†è¿™äº›å­ç½‘ç»œè€¦åˆåœ¨ä¸€èµ·ä»¥ç”ŸæˆæŒ‡æ•°æ•°é‡çš„å¸å¼•å­çŠ¶æ€, ä½¿å®ƒä»¬æ¯ä¸ªéƒ½æœ‰ä¸€ä¸ªåˆç†å¤§å°çš„è°·åº•, å› æ­¤æ˜¯ç¨³å¥çš„ (å›¾ 2). è¿™äº›ç½‘ç»œä¸­çš„çŠ¶æ€ä¸èƒ½å…·æœ‰ä»»æ„å½¢å¼å’Œå†…å®¹; å®ƒä»¬ç”±æ¯ä¸ªæ¨¡å—çš„åˆšæ€§çŠ¶æ€å®šä¹‰.\nå› æ­¤, ä¸€ä¸ªå…³é”®é—®é¢˜æ˜¯å¦‚ä½•åˆ©ç”¨å®ƒä»¬è¿›è¡Œè®°å¿†. å·²ç»è¯æ˜, è¿™äº›é«˜å®¹é‡çš„å¸å¼•å­çŠ¶æ€é›†ä¸ºé«˜å®¹é‡å’Œç¨³å¥çš„åŠ¨ä½œé€‰æ‹©ã€ç¨³å¥åˆ†ç±»å’Œå¹³æ»‘è¡°å‡çš„å…³è”è®°å¿†æä¾›äº†å¯èƒ½çš„æ¨¡å‹. æ­¤å¤–, æœ¬æ®µä¸­æè¿°çš„åŸç†å¯ä»¥ç»“åˆåœ¨ â€œæ··åˆæ¨¡å—ç¼–ç æ–¹æ¡ˆâ€ ä¸­, ä»¥ç›¸å¯¹äºå•ä¸ªå¸å¼•å­ç½‘ç»œè¡¨ç¤ºå’Œå­˜å‚¨ä»»ä½•ç»´åº¦çš„è¾“å…¥, åªè¦å®ƒä½äºè·¨ç½‘ç»œçš„æ€»å¸å¼•å­ç»´åº¦, è€Œæ— éœ€é‡æ–°é…ç½®é€’å½’ç½‘ç»œ (å›¾ 2h). å¯¹å¸å¼•å­ç½‘ç»œçš„æ›¿ä»£ç”¨é€”ã€é…ç½®æˆ–ç»„åˆçš„æ½œåŠ›ä»æœ‰å¾ˆå¤§ä¸€éƒ¨åˆ†æœªè¢«æ¢ç´¢, å€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶.\nLooking ahead The theory of attractor dynamics in the brain has provided a powerful and unifying conceptual framework for understanding integration, representation, memory, error correction and efficient learning and inference in the brain. The experimental effort to study candidate attractor circuits and test their predictions has been a fertile field of research, and population-wide physiology techniques have led to breath-taking direct visualizations of attractor dynamics at work in the brain.\nå¸å¼•å­åŠ¨åŠ›å­¦ç†è®ºä¸ºç†è§£å¤§è„‘ä¸­çš„ç§¯åˆ†ã€è¡¨ç¤ºã€è®°å¿†ã€è¯¯å·®æ ¡æ­£ä»¥åŠé«˜æ•ˆå­¦ä¹ å’Œæ¨ç†æä¾›äº†ä¸€ä¸ªå¼ºå¤§è€Œç»Ÿä¸€çš„æ¦‚å¿µæ¡†æ¶. ç ”ç©¶å€™é€‰å¸å¼•å­å›è·¯å¹¶æµ‹è¯•å…¶é¢„æµ‹çš„å®éªŒå·¥ä½œä¸€ç›´æ˜¯ä¸€ä¸ªå¯Œæœ‰æˆæœçš„ç ”ç©¶é¢†åŸŸ, ç¾¤ä½“èŒƒå›´çš„ç”Ÿç†æŠ€æœ¯å·²ç»å¯¼è‡´äº†å¯¹å¤§è„‘ä¸­å¸å¼•å­åŠ¨åŠ›å­¦å·¥ä½œçš„æƒŠäººç›´æ¥å¯è§†åŒ–.\nThe theory is also proving to be a powerful tool in interpreting how artificial neural networks (ANNs) solve complex tasks. ANNs trained to robustly solve memory, integration and decision-making tasks in domains as diverse as spatial navigation, vision and language develop attractor dynamics, suggesting that attractor networks not only are able to solve such problems but also might be necessary when the computing elements are memoryless neurons. Furthermore, equipping ANNs with preconfigured attractor networks can help produce faster, more data-efficient and generalizable learning. Because ANNs can be trained on complex tasks and then fully examined after learning, they will potentially more readily contribute to the next chapter in our understanding of how continuous-attractor networks can interact and combine with other mechanisms to enable the brain to solve rich problems associated with intelligence.\nè¯¥ç†è®ºä¹Ÿè¢«è¯æ˜æ˜¯è§£é‡Šäººå·¥ç¥ç»ç½‘ç»œ (ANN) å¦‚ä½•è§£å†³å¤æ‚ä»»åŠ¡çš„å¼ºå¤§å·¥å…·. åœ¨ç©ºé—´å¯¼èˆªã€è§†è§‰å’Œè¯­è¨€ç­‰å¤šç§é¢†åŸŸä¸­, ç»è¿‡è®­ç»ƒä»¥ç¨³å¥åœ°è§£å†³è®°å¿†ã€ç§¯åˆ†å’Œå†³ç­–ä»»åŠ¡çš„ ANN å‘å±•å‡ºå¸å¼•å­åŠ¨åŠ›å­¦, è¿™è¡¨æ˜å¸å¼•å­ç½‘ç»œä¸ä»…èƒ½å¤Ÿè§£å†³æ­¤ç±»é—®é¢˜, è€Œä¸”å½“è®¡ç®—å…ƒç´ æ˜¯æ— è®°å¿†ç¥ç»å…ƒæ—¶å¯èƒ½æ˜¯å¿…è¦çš„. æ­¤å¤–, ä¸º ANN é…å¤‡é¢„é…ç½®çš„å¸å¼•å­ç½‘ç»œå¯ä»¥å¸®åŠ©äº§ç”Ÿæ›´å¿«ã€æ›´é«˜æ•ˆå’Œæ›´å…·æ³›åŒ–èƒ½åŠ›çš„å­¦ä¹ . ç”±äº ANN å¯ä»¥åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒ, ç„¶ååœ¨å­¦ä¹ åè¿›è¡Œå…¨é¢æ£€æŸ¥, å®ƒä»¬æœ‰å¯èƒ½æ›´å®¹æ˜“ä¸ºæˆ‘ä»¬ç†è§£è¿ç»­å¸å¼•å­ç½‘ç»œå¦‚ä½•ä¸å…¶ä»–æœºåˆ¶ç›¸äº’ä½œç”¨å’Œç»“åˆä»¥ä½¿å¤§è„‘èƒ½å¤Ÿè§£å†³ä¸æ™ºèƒ½ç›¸å…³çš„ä¸°å¯Œé—®é¢˜çš„ä¸‹ä¸€ç« åšå‡ºè´¡çŒ®.\nNotable mechanistic questions about attractor networks also remain open. One avenue may involve moving away from the high firing-rate asynchronous spiking regimens to better understand whether low firing-rate synchronous spiking networks might support attractor dynamics â€” and thus permit a combination of fast timescale dynamics such as spike synchronization and oscillatory phase dynamics. For continuous attractors, understanding how the brain deals with the problem of fine-tuning in linear networks or the imposition and maintenance of a continuous symmetry across neurons remains unknown and is ripe for resolution.\nå…³äºå¸å¼•å­ç½‘ç»œçš„æ˜¾è‘—æœºåˆ¶é—®é¢˜ä»ç„¶æ‚¬è€Œæœªå†³. ä¸€ä¸ªé€”å¾„å¯èƒ½æ¶‰åŠè¿œç¦»é«˜å‘å°„ç‡å¼‚æ­¥å°–å³°å‘å°„æ–¹æ¡ˆ, ä»¥æ›´å¥½åœ°ç†è§£ä½å‘å°„ç‡åŒæ­¥å°–å³°å‘å°„ç½‘ç»œæ˜¯å¦å¯èƒ½æ”¯æŒå¸å¼•å­åŠ¨åŠ›å­¦â€”â€”ä»è€Œå…è®¸å¿«é€Ÿæ—¶é—´å°ºåº¦åŠ¨åŠ›å­¦ (å¦‚å°–å³°åŒæ­¥å’ŒæŒ¯è¡ç›¸ä½åŠ¨åŠ›å­¦) çš„ç»„åˆ. å¯¹äºè¿ç»­å¸å¼•å­, äº†è§£å¤§è„‘å¦‚ä½•å¤„ç†çº¿æ€§ç½‘ç»œä¸­çš„å¾®è°ƒé—®é¢˜æˆ–åœ¨ç¥ç»å…ƒä¹‹é—´å¼ºåŠ å’Œç»´æŒè¿ç»­å¯¹ç§°æ€§ä»ç„¶æœªçŸ¥, å¹¶ä¸”æœ‰å¾…è§£å†³.\nA few models of the development of continuous attractors show how they could emerge simply through unsupervised associative plasticity, whereas others are based on combining feedback of known or plausible error signals with neural activity in relatively simple learning rules. The rest of such models train networks on a high-level goal through error backpropagation, combined with several other constraints on architecture or the form the solutions should take. As recent work suggests, however, training ANNs to solve tasks is not a panacea for understanding the brainâ€™s solutions. All models of attractor network development are incomplete for different reasons: the unsupervised models require uniform exploration of the input variable space and suppression of recurrent weights during their training, whereas backpropagation models do not offer an account of how loss functions, learning and additional constraints might be generated and implemented in biological systems.\nä¸€äº›è¿ç»­å¸å¼•å­çš„å‘å±•æ¨¡å‹è¡¨æ˜, å®ƒä»¬å¯ä»¥é€šè¿‡æ— ç›‘ç£å…³è”å¯å¡‘æ€§ç®€å•åœ°å‡ºç°, è€Œå…¶ä»–æ¨¡å‹åˆ™åŸºäºå°†å·²çŸ¥æˆ–åˆç†çš„è¯¯å·®ä¿¡å·åé¦ˆä¸ç›¸å¯¹ç®€å•çš„å­¦ä¹ è§„åˆ™ä¸­çš„ç¥ç»æ´»åŠ¨ç›¸ç»“åˆ. å…¶ä½™æ­¤ç±»æ¨¡å‹é€šè¿‡è¯¯å·®åå‘ä¼ æ’­å¯¹ç½‘ç»œè¿›è¡Œè®­ç»ƒ, ä»¥å®ç°é«˜çº§ç›®æ ‡, å¹¶ç»“åˆå¯¹æ¶æ„æˆ–è§£å†³æ–¹æ¡ˆåº”é‡‡å–çš„å½¢å¼çš„å…¶ä»–å‡ ä¸ªçº¦æŸ. ç„¶è€Œ, æ­£å¦‚æœ€è¿‘çš„å·¥ä½œæ‰€è¡¨æ˜çš„é‚£æ ·, è®­ç»ƒ ANN ä»¥è§£å†³ä»»åŠ¡å¹¶ä¸æ˜¯ç†è§£å¤§è„‘è§£å†³æ–¹æ¡ˆçš„çµä¸¹å¦™è¯. æ‰€æœ‰å¸å¼•å­ç½‘ç»œå‘å±•çš„æ¨¡å‹ç”±äºä¸åŒçš„åŸå› éƒ½æ˜¯ä¸å®Œæ•´çš„: æ— ç›‘ç£æ¨¡å‹éœ€è¦å¯¹è¾“å…¥å˜é‡ç©ºé—´è¿›è¡Œå‡åŒ€æ¢ç´¢å¹¶åœ¨è®­ç»ƒæœŸé—´æŠ‘åˆ¶é€’å½’æƒé‡, è€Œåå‘ä¼ æ’­æ¨¡å‹åˆ™æ²¡æœ‰è§£é‡Šå¦‚ä½•åœ¨ç”Ÿç‰©ç³»ç»Ÿä¸­ç”Ÿæˆå’Œå®ç°æŸå¤±å‡½æ•°ã€å­¦ä¹ å’Œå…¶ä»–çº¦æŸ.\nThere is much left to do in the field and an exciting vista ahead. On the experimental side, tools for high-resolution population-level neural recordings and perturbation across multiple brain areas enable us to peer further and deeper than ever. On the theory side, future developments will help us conceptualize how such circuits could help underwrite intelligent computation through the formation, interaction and reuse of multiple low-dimensional attractors or attractor-like structures.\nè¯¥é¢†åŸŸè¿˜æœ‰å¾ˆå¤šå·¥ä½œè¦åš, å‰æ™¯ä»¤äººå…´å¥‹. åœ¨å®éªŒæ–¹é¢, ç”¨äºé«˜åˆ†è¾¨ç‡ç¾¤ä½“æ°´å¹³ç¥ç»è®°å½•å’Œè·¨å¤šä¸ªå¤§è„‘åŒºåŸŸæ‰°åŠ¨çš„å·¥å…·ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´æ·±å…¥åœ°çª¥è§†. åœ¨ç†è®ºæ–¹é¢, æœªæ¥çš„å‘å±•å°†å¸®åŠ©æˆ‘ä»¬æ¦‚å¿µåŒ–è¿™äº›å›è·¯å¦‚ä½•é€šè¿‡å½¢æˆã€ç›¸äº’ä½œç”¨å’Œé‡å¤ä½¿ç”¨å¤šä¸ªä½ç»´å¸å¼•å­æˆ–ç±»ä¼¼å¸å¼•å­ç»“æ„æ¥æ”¯æŒæ™ºèƒ½è®¡ç®—.\n",
  "wordCount" : "34560",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/11/23/gm4Vu15dJcpnh9M.png","datePublished": "2025-11-23T00:18:23+08:00",
  "dateModified": "2025-11-23T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/reference/attractor-and-integrator-networks-in-the-brain/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "æ— å¤„æƒ¹å°˜åŸƒ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿— (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿—</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/">ğŸ“• é˜…è¯»</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/">ğŸ“• æ–‡çŒ®</a></div>
            <h1 class="post-title">
                Attractor and integrator networks in the brain
            </h1>
            <div class="post-description">
                å¤§è„‘ä¸­çš„å¸å¼•å­ä¸ç§¯åˆ†ç½‘ç»œ
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-11-23
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>34560å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>69åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/11/23/gm4Vu15dJcpnh9M.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#what-are-attractors" aria-label="What are attractors?">What are attractors?</a><ul>
                        
                <li>
                    <a href="#defining-the-state-of-a-neural-system" aria-label="Defining the state of a neural system">Defining the state of a neural system</a></li>
                <li>
                    <a href="#attractors-in-the-presence-of-noise" aria-label="Attractors in the presence of noise">Attractors in the presence of noise</a></li></ul>
                </li>
                <li>
                    <a href="#construction-and-mechanisms" aria-label="Construction and mechanisms">Construction and mechanisms</a><ul>
                        
                <li>
                    <a href="#discrete-attractors" aria-label="Discrete attractors">Discrete attractors</a></li>
                <li>
                    <a href="#continuous-attractors" aria-label="Continuous attractors">Continuous attractors</a></li>
                <li>
                    <a href="#non-stationary-continuous-attractors" aria-label="Non-stationary continuous attractors">Non-stationary continuous attractors</a></li></ul>
                </li>
                <li>
                    <a href="#attractors-for-neural-computation" aria-label="Attractors for neural computation">Attractors for neural computation</a><ul>
                        
                <li>
                    <a href="#representation-and-memory" aria-label="Representation and memory">Representation and memory</a></li>
                <li>
                    <a href="#de-noising-representations-and-memories" aria-label="De-noising representations and memories">De-noising representations and memories</a></li>
                <li>
                    <a href="#robust-classification" aria-label="Robust classification">Robust classification</a></li>
                <li>
                    <a href="#integration" aria-label="Integration">Integration</a></li>
                <li>
                    <a href="#decision-making" aria-label="Decision-making">Decision-making</a></li>
                <li>
                    <a href="#sequence-generation" aria-label="Sequence generation">Sequence generation</a></li></ul>
                </li>
                <li>
                    <a href="#evidence-of-attractors-in-the-brain" aria-label="Evidence of attractors in the brain">Evidence of attractors in the brain</a><ul>
                        
                <li>
                    <a href="#criteria-for-attractor-dynamics" aria-label="Criteria for attractor dynamics">Criteria for attractor dynamics</a></li>
                <li>
                    <a href="#discrete-attractors-1" aria-label="Discrete attractors">Discrete attractors</a><ul>
                        
                <li>
                    <a href="#up-and-down-states" aria-label="Up and down states.">Up and down states.</a></li>
                <li>
                    <a href="#perceptual-bistability" aria-label="Perceptual bistability">Perceptual bistability</a></li>
                <li>
                    <a href="#bistability-in-a-premotor-area" aria-label="Bistability in a premotor area">Bistability in a premotor area</a></li>
                <li>
                    <a href="#discrete-multistability" aria-label="Discrete multistability">Discrete multistability</a></li></ul>
                </li>
                <li>
                    <a href="#continuous-attractors-1" aria-label="Continuous attractors">Continuous attractors</a><ul>
                        
                <li>
                    <a href="#the-oculomotor-integrator" aria-label="The oculomotor integrator">The oculomotor integrator</a></li>
                <li>
                    <a href="#head-direction-cells" aria-label="Head-direction cells">Head-direction cells</a></li>
                <li>
                    <a href="#grid-cells" aria-label="Grid cells">Grid cells</a></li>
                <li>
                    <a href="#graded-working-memory-networks" aria-label="Graded working memory networks">Graded working memory networks</a></li></ul>
                </li>
                <li>
                    <a href="#limit-cycle-attractors" aria-label="Limit-cycle attractors">Limit-cycle attractors</a></li></ul>
                </li>
                <li>
                    <a href="#departures-from-attractor-dynamics" aria-label="Departures from attractor dynamics">Departures from attractor dynamics</a><ul>
                        
                <li>
                    <a href="#orientation-tuning-in-visual-cortex" aria-label="Orientation tuning in visual cortex">Orientation tuning in visual cortex</a></li>
                <li>
                    <a href="#place-cells" aria-label="Place cells">Place cells</a></li>
                <li>
                    <a href="#motor-cortical-trajectories" aria-label="Motor cortical trajectories">Motor cortical trajectories</a></li></ul>
                </li>
                <li>
                    <a href="#flexibility-despite-rigidity" aria-label="Flexibility despite rigidity">Flexibility despite rigidity</a></li>
                <li>
                    <a href="#looking-ahead" aria-label="Looking ahead">Looking ahead</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><h1 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h1>
<blockquote>
<p>In this Review, we describe the singular success of attractor neural network models in describing how the brain maintains persistent activity states for working memory, corrects errors and integrates noisy cues. We consider the mechanisms by which simple and forgetful units can organize to collectively generate dynamics on the long timescales required for such computations.</p>
<p>We discuss the myriad potential uses of attractor dynamics for computation in the brain, and showcase notable examples of brain systems in which inherently low-dimensional continuous-attractor dynamics have been concretely and rigorously identified. Thus, it is now possible to conclusively state that the brain constructs and uses such systems for computation.</p>
<p>Finally, we highlight recent theoretical advances in understanding how the fundamental trade-offs between robustness and capacity and between structure and flexibility can be overcome by reusing and recombining the same set of modular attractors for multiple functions, so they together produce representations that are structurally constrained and robust but exhibit high capacity and are flexible.</p>
</blockquote>
<p>åœ¨è¿™ç¯‡ç»¼è¿°ä¸­, æˆ‘ä»¬æè¿°äº†å¸å¼•å­ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨æè¿°å¤§è„‘å¦‚ä½•ç»´æŒå·¥ä½œè®°å¿†çš„æŒç»­æ´»åŠ¨çŠ¶æ€ã€çº æ­£é”™è¯¯å’Œç§¯åˆ†å™ªå£°çº¿ç´¢æ–¹é¢çš„ç‹¬ç‰¹æˆåŠŸ. æˆ‘ä»¬è€ƒè™‘äº†ç®€å•ä¸”æ˜“å¿˜çš„å•å…ƒå¦‚ä½•ç»„ç»‡èµ·æ¥, å…±åŒç”Ÿæˆæ‰€éœ€çš„é•¿æ—¶é—´å°ºåº¦åŠ¨åŠ›å­¦æœºåˆ¶, ä»¥è¿›è¡Œæ­¤ç±»è®¡ç®—.</p>
<p>æˆ‘ä»¬è®¨è®ºäº†å¸å¼•å­åŠ¨åŠ›å­¦åœ¨å¤§è„‘è®¡ç®—ä¸­çš„æ— æ•°æ½œåœ¨ç”¨é€”, å¹¶å±•ç¤ºäº†åœ¨å…¶ä¸­å›ºæœ‰çš„ä½ç»´è¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦å·²è¢«å…·ä½“ä¸”ä¸¥æ ¼è¯†åˆ«çš„æ˜¾è‘—å¤§è„‘ç³»ç»Ÿç¤ºä¾‹. å› æ­¤, ç°åœ¨å¯ä»¥æ˜ç¡®åœ°è¯´, å¤§è„‘æ„å»ºå¹¶ä½¿ç”¨è¿™æ ·çš„ç³»ç»Ÿè¿›è¡Œè®¡ç®—.</p>
<p>æœ€å, æˆ‘ä»¬å¼ºè°ƒäº†æœ€è¿‘åœ¨ç†è§£ç¨³å¥æ€§ä¸å®¹é‡ä¹‹é—´ä»¥åŠç»“æ„ä¸çµæ´»æ€§ä¹‹é—´çš„åŸºæœ¬æƒè¡¡æ–¹é¢çš„ç†è®ºè¿›å±•, è¿™äº›è¿›å±•å¯ä»¥é€šè¿‡é‡å¤ä½¿ç”¨å’Œé‡æ–°ç»„åˆåŒä¸€ç»„æ¨¡å—åŒ–å¸å¼•å­æ¥å…‹æœ, ä»è€Œå…±åŒäº§ç”Ÿç»“æ„å—é™ä¸”ç¨³å¥ä½†å…·æœ‰é«˜å®¹é‡ä¸”çµæ´»çš„è¡¨ç¤º.</p>
<h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<blockquote>
<p>One of biology&rsquo;s grand challenges is to explain how order and complex function spring from inanimate physical systems composed of much simpler parts. The brain creates order in its representations of the world and performs complex functions through the collective interactions of simpler elements.</p>
<p>In this Review, we describe and evaluate the hypothesis that attractor dynamics in widespread regions of the CNS have a key role in constructing some of these representations, generating long timescales to support integration and memory functions and endowing all these functions with robustness.</p>
<p>We review the specific predictions of attractor-based models and the now extensive body of work testing these predictions. Thus, we illustrate that the theory and validation of computation with attractor dynamics in the brain is one of the biggest success stories in systems neuroscience.</p>
</blockquote>
<p>ç”Ÿç‰©å­¦çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜æ˜¯è§£é‡Šæ— ç”Ÿå‘½çš„ç‰©ç†ç³»ç»Ÿå¦‚ä½•ä»æ›´ç®€å•çš„éƒ¨åˆ†ä¸­äº§ç”Ÿç§©åºå’Œå¤æ‚åŠŸèƒ½. å¤§è„‘é€šè¿‡æ›´ç®€å•å…ƒç´ çš„é›†ä½“ç›¸äº’ä½œç”¨, åœ¨å…¶å¯¹ä¸–ç•Œçš„è¡¨å¾ä¸­åˆ›é€ ç§©åºå¹¶æ‰§è¡Œå¤æ‚åŠŸèƒ½.</p>
<p>åœ¨è¿™ç¯‡ç»¼è¿°ä¸­, æˆ‘ä»¬æè¿°å¹¶è¯„ä¼°äº†è¿™æ ·ä¸€ä¸ªå‡è®¾: ä¸­æ¢ç¥ç»ç³»ç»Ÿ (CNS) å¹¿æ³›åŒºåŸŸä¸­çš„å¸å¼•å­åŠ¨åŠ›å­¦åœ¨æ„å»ºè¿™äº›è¡¨å¾ã€ç”Ÿæˆæ”¯æŒæ•´åˆå’Œè®°å¿†åŠŸèƒ½çš„é•¿æ—¶é—´å°ºåº¦ä»¥åŠèµ‹äºˆæ‰€æœ‰è¿™äº›åŠŸèƒ½ä»¥ç¨³å¥æ€§æ–¹é¢èµ·ç€å…³é”®ä½œç”¨.</p>
<p>æˆ‘ä»¬å›é¡¾äº†åŸºäºå¸å¼•å­çš„æ¨¡å‹çš„å…·ä½“é¢„æµ‹ä»¥åŠç°åœ¨å¹¿æ³›çš„å·¥ä½œæ¥æµ‹è¯•è¿™äº›é¢„æµ‹. å› æ­¤, æˆ‘ä»¬è¯´æ˜äº†åœ¨å¤§è„‘ä¸­ä½¿ç”¨å¸å¼•å­åŠ¨åŠ›å­¦è¿›è¡Œè®¡ç®—çš„ç†è®ºå’ŒéªŒè¯æ˜¯ç³»ç»Ÿç¥ç»ç§‘å­¦ä¸­æœ€å¤§çš„æˆåŠŸæ•…äº‹ä¹‹ä¸€.</p>
<blockquote>
<p>Some of the first formal circuit-level models of brain function focused on the problem of <strong>associative memory</strong> and how neural circuits might generate spatially distributed, stable patterns of activity that could function as such a memory.</p>
<p>Hopfield networks, with multiple stable states constructed by <strong>inscribing</strong> input patterns into connection weights, were proposed more than four decades ago.</p>
<p>Network models possessing a continuous set of stable states that could be used to represent continuous variables were also first proposed in the same period. Subsequently, many canonical brain circuits for motor control, sensory amplification and memory, <strong>motion integration</strong>, <strong>evidence integration</strong>, decision-making and spatial navigation have been modelled using the same general principle â€” that a set of states can be stabilized through collective positive feedback.</p>
</blockquote>
<p>ä¸€äº›æœ€æ—©çš„æ­£å¼å›è·¯çº§å¤§è„‘åŠŸèƒ½æ¨¡å‹é›†ä¸­åœ¨ <strong>è”æƒ³è®°å¿†</strong> çš„é—®é¢˜ä¸Š, ä»¥åŠç¥ç»å›è·¯å¦‚ä½•äº§ç”Ÿç©ºé—´åˆ†å¸ƒçš„ç¨³å®šæ´»åŠ¨æ¨¡å¼, è¿™äº›æ¨¡å¼å¯ä»¥ä½œä¸ºè¿™æ ·çš„è®°å¿†.</p>
<p>Hopfield ç½‘ç»œé€šè¿‡å°†è¾“å…¥æ¨¡å¼ <strong>é“­åˆ»</strong> åˆ°è¿æ¥æƒé‡ä¸­, æ„å»ºäº†å¤šä¸ªç¨³å®šçŠ¶æ€, è¿™ä¸€æ¦‚å¿µåœ¨å››åå¤šå¹´å‰å°±è¢«æå‡ºäº†.</p>
<p>å…·æœ‰ä¸€ç»„è¿ç»­ç¨³å®šçŠ¶æ€çš„ç½‘ç»œæ¨¡å‹ä¹Ÿé¦–æ¬¡åœ¨åŒä¸€æ—¶æœŸè¢«æå‡º, è¿™äº›çŠ¶æ€å¯ç”¨äºè¡¨ç¤ºè¿ç»­å˜é‡. éšå, è®¸å¤šç”¨äºè¿åŠ¨æ§åˆ¶ã€æ„Ÿè§‰æ”¾å¤§å’Œè®°å¿†ã€<strong>è¿åŠ¨ç§¯åˆ†</strong>ã€<strong>è¯æ®ç§¯åˆ†</strong>ã€å†³ç­–å’Œç©ºé—´å¯¼èˆªçš„å…¸å‹å¤§è„‘å›è·¯éƒ½ä½¿ç”¨äº†ç›¸åŒçš„ä¸€èˆ¬åŸç†è¿›è¡Œå»ºæ¨¡â€”â€”å³é€šè¿‡é›†ç¾¤æ­£åé¦ˆå¯ä»¥ç¨³å®šä¸€ç»„çŠ¶æ€.</p>
<blockquote>
<p>Because these are circuit-level models, but were typically inspired by experimental characterization of neurons recorded singly or a few at a time, the patterns of connectivity and the cell-activity correlations in the models automatically became novel and relatively specific predictions about the <strong>population dynamics</strong> and architecture of such circuits.</p>
<p>As we discuss below, the combination of these prediction-rich (yet conceptually simple) models, modern experimental breakthroughs in the acquisition of cellular-resolution population activity data and novel and rigorous analyses of such data on the basis of the model predictions has provided much evidence that the brain constructs and exploits attractor networks for performing several essential computations.</p>
</blockquote>
<p>ç”±äºè¿™äº›æ˜¯å›è·¯çº§æ¨¡å‹, ä½†é€šå¸¸æ˜¯å—å•ç‹¬æˆ–å°‘é‡è®°å½•çš„ç¥ç»å…ƒçš„å®éªŒè¡¨å¾å¯å‘, å› æ­¤æ¨¡å‹ä¸­çš„è¿æ¥æ¨¡å¼å’Œç»†èƒæ´»åŠ¨ç›¸å…³æ€§è‡ªç„¶è€Œç„¶æˆä¸ºå…³äºæ­¤ç±»å›è·¯çš„ <strong>ç¾¤ä½“åŠ¨åŠ›å­¦</strong> å’Œæ¶æ„çš„æ–°é¢–ä¸”ç›¸å¯¹å…·ä½“çš„é¢„æµ‹.</p>
<p>æ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·, è¿™äº›å¯Œæœ‰é¢„æµ‹åŠ› (ä½†æ¦‚å¿µä¸Šç®€å•) æ¨¡å‹çš„ç»“åˆã€åœ¨è·å–ç»†èƒåˆ†è¾¨ç‡ç¾¤ä½“æ´»åŠ¨æ•°æ®æ–¹é¢çš„ç°ä»£å®éªŒçªç ´ä»¥åŠåŸºäºæ¨¡å‹é¢„æµ‹å¯¹è¿™äº›æ•°æ®è¿›è¡Œçš„æ–°é¢–ä¸”ä¸¥æ ¼çš„åˆ†æ, æä¾›äº†å¤§é‡è¯æ®è¡¨æ˜å¤§è„‘æ„å»ºå¹¶åˆ©ç”¨å¸å¼•å­ç½‘ç»œæ¥æ‰§è¡Œå‡ é¡¹åŸºæœ¬è®¡ç®—.</p>
<blockquote>
<p>We begin by defining attractors, and then describe proposed mechanisms for the construction of attractor network models in neuroscience. We provide an overview of why attractor networks can be important for computation in the brain and highlight criteria for determining whether a system has non-trivial attractor dynamics.</p>
<p>We also discuss examples of brain circuits with non-trivial attractor dynamics. We end with a summary of new directions in our understanding of how these simple circuits could contribute to flexible computation through reuse in multiple contexts.</p>
</blockquote>
<p>æˆ‘ä»¬é¦–å…ˆå®šä¹‰å¸å¼•å­, ç„¶åæè¿°ç¥ç»ç§‘å­¦ä¸­å¸å¼•å­ç½‘ç»œæ¨¡å‹æ„å»ºçš„å‡å®šæœºåˆ¶. æˆ‘ä»¬æ¦‚è¿°äº†ä¸ºä»€ä¹ˆå¸å¼•å­ç½‘ç»œå¯¹äºå¤§è„‘ä¸­çš„è®¡ç®—å¯èƒ½å¾ˆé‡è¦, å¹¶å¼ºè°ƒäº†ç¡®å®šç³»ç»Ÿæ˜¯å¦å…·æœ‰éå¹³å‡¡å¸å¼•å­åŠ¨åŠ›å­¦çš„æ ‡å‡†.</p>
<p>æˆ‘ä»¬è¿˜è®¨è®ºäº†å…·æœ‰éå¹³å‡¡å¸å¼•å­åŠ¨åŠ›å­¦çš„å¤§è„‘å›è·¯ç¤ºä¾‹. æœ€å, æˆ‘ä»¬æ€»ç»“äº†æˆ‘ä»¬å¯¹è¿™äº›ç®€å•å›è·¯å¦‚ä½•é€šè¿‡åœ¨å¤šç§ç¯å¢ƒä¸­é‡å¤ä½¿ç”¨æ¥ä¿ƒè¿›çµæ´»è®¡ç®—çš„æ–°ç†è§£æ–¹å‘.</p>
<h1 id="what-are-attractors">What are attractors?<a hidden class="anchor" aria-hidden="true" href="#what-are-attractors">#</a></h1>
<blockquote>
<p>To define an attractor, we first define a dynamical system and its states.</p>
<p>A dynamical system is a set of variables together with all the rules that determine their changes in value with the passage of time.</p>
<p>The value of these variables at any given instant is called the state of the system at that moment. The state is a point (vector) in the state space of the dynamical system.</p>
<p>An attractor is the minimal set of states in a state space, to which all nearby states eventually flow with time. One simple example of an attractor is a stable fixed point: all neighbouring states flow to it.</p>
<p>Transferring these crisp mathematical definitions to the context of the brain involves challenges and simplifications that revolve around identifying a <strong>sufficiently self-contained system</strong> and the variables necessary to determine its dynamics.</p>
</blockquote>
<p>ä¸ºäº†å®šä¹‰å¸å¼•å­, æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªåŠ¨åŠ›ç³»ç»ŸåŠå…¶çŠ¶æ€.</p>
<p>åŠ¨åŠ›ç³»ç»Ÿæ˜¯ä¸€ç»„å˜é‡ä»¥åŠæ‰€æœ‰å†³å®šå®ƒä»¬éšæ—¶é—´å˜åŒ–çš„è§„åˆ™.</p>
<p>è¿™äº›å˜é‡åœ¨ä»»ä½•ç»™å®šæ—¶åˆ»çš„å€¼ç§°ä¸ºè¯¥æ—¶åˆ»ç³»ç»Ÿçš„çŠ¶æ€. çŠ¶æ€æ˜¯åŠ¨åŠ›ç³»ç»ŸçŠ¶æ€ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ (çŸ¢é‡).</p>
<p>å¸å¼•å­æ˜¯çŠ¶æ€ç©ºé—´ä¸­æœ€å°çš„çŠ¶æ€é›†, æ‰€æœ‰é™„è¿‘çŠ¶æ€æœ€ç»ˆéƒ½ä¼šéšç€æ—¶é—´æµå‘è¯¥çŠ¶æ€é›†. å¸å¼•å­çš„ä¸€ä¸ªç®€å•ä¾‹å­æ˜¯ç¨³å®šçš„ä¸åŠ¨ç‚¹: æ‰€æœ‰é‚»è¿‘çŠ¶æ€éƒ½æµå‘å®ƒ.</p>
<p>å°†è¿™äº›æ¸…æ™°çš„æ•°å­¦å®šä¹‰è½¬ç§»åˆ°å¤§è„‘çš„èƒŒæ™¯ä¸­æ¶‰åŠæŒ‘æˆ˜å’Œç®€åŒ–, è¿™äº›æŒ‘æˆ˜å’Œç®€åŒ–å›´ç»•è¯†åˆ«ä¸€ä¸ª <strong>è¶³å¤Ÿè‡ªåŒ…å«çš„ç³»ç»Ÿ</strong> ä»¥åŠç¡®å®šå…¶åŠ¨åŠ›å­¦æ‰€éœ€çš„å˜é‡å±•å¼€.</p>
<h2 id="defining-the-state-of-a-neural-system">Defining the state of a neural system<a hidden class="anchor" aria-hidden="true" href="#defining-the-state-of-a-neural-system">#</a></h2>
<blockquote>
<p>Inherent in the definition of a dynamical system is the assumption that there are no external dynamical inputs to the system (or, equivalently, that the system definition includes all such external variables).</p>
</blockquote>
<p>åœ¨åŠ¨åŠ›ç³»ç»Ÿçš„å®šä¹‰ä¸­, å›ºæœ‰åœ°å‡è®¾ç³»ç»Ÿæ²¡æœ‰å¤–éƒ¨åŠ¨åŠ›è¾“å…¥ (æˆ–è€…ç­‰æ•ˆåœ°, ç³»ç»Ÿå®šä¹‰åŒ…æ‹¬æ‰€æœ‰æ­¤ç±»å¤–éƒ¨å˜é‡).</p>
<blockquote>
<p>The first simplification in characterizing the dynamics of a neural circuit is to assume that, <u>at least on the timescale of interest, the system evolves in an <strong>autonomous</strong> way</u>.</p>
<p>Given that subcircuits in the brain are interconnected with others, and that the brain itself interacts with the world, it is impossible to isolate these circuits completely into autonomous systems.</p>
<p>However, we may define a notion of â€˜effectively autonomousâ€™ dynamics, whereby inputs do not vary over time and are untuned, in the sense that they do not provide differential drive to subsets of the putative set of attractor states.</p>
</blockquote>
<p>è¡¨å¾ç¥ç»å›è·¯åŠ¨åŠ›å­¦çš„ç¬¬ä¸€ä¸ªç®€åŒ–æ˜¯ <u>å‡è®¾è‡³å°‘åœ¨æ„Ÿå…´è¶£çš„æ—¶é—´å°ºåº¦ä¸Š, ç³»ç»Ÿä»¥ <strong>è‡ªä¸»</strong> çš„æ–¹å¼æ¼”åŒ–</u>.</p>
<p>é‰´äºå¤§è„‘ä¸­çš„å­å›è·¯ä¸å…¶ä»–å›è·¯ç›¸äº’è¿æ¥, å¹¶ä¸”å¤§è„‘æœ¬èº«ä¸ä¸–ç•Œäº’åŠ¨, å› æ­¤ä¸å¯èƒ½å°†è¿™äº›å›è·¯å®Œå…¨éš”ç¦»æˆè‡ªä¸»ç³»ç»Ÿ.</p>
<p>ç„¶è€Œ, æˆ‘ä»¬å¯ä»¥å®šä¹‰ &ldquo;ç­‰æ•ˆè‡ªä¸»&rdquo; åŠ¨åŠ›å­¦çš„æ¦‚å¿µ, å³è¾“å…¥ä¸éšæ—¶é—´å˜åŒ–ä¸”æœªå—è°ƒåˆ¶, ä»è€Œä¸ä¼šä¸ºå‡å®šçš„å¸å¼•å­çŠ¶æ€é›†çš„å­é›†æä¾›å·®å¼‚é©±åŠ¨.</p>
<blockquote>
<p>The second simplification is in defining the states of the system.</p>
<p>The changes in state of a circuit in the brain over time may depend on the detailed pattern of all the spikes in all neurons, the levels of associated ions, <strong>neurotransmitters</strong> and <strong>modulators</strong>, and even the states of the ion channels.</p>
<p>The weights and connections between neurons may be considered as parameters (rather than variables) on short timescales, but are themselves variables if considering a longer timescale.</p>
<p>One widely used simplification in describing a neural circuit on the timescale of seconds is to use just the <strong>spiking outputs</strong> of the neurons in the circuit as the states, often further simplified as time-varying spike rates.</p>
<p>If such a description is sufficient to predict the state changes of the system at the relevant timescales, it can be viewed as a reasonable dynamical system model of the circuit.</p>
<p>Although spike or spike-rate descriptions ignore subcellular and molecular variables to make the grossly simplifying assumption that the relevant circuit dynamics are governed by spikes, the state space of a <strong>vertebrate</strong> microcircuit described in this way is nevertheless very high-dimensional, comprising the number of neurons in the circuit, which can be in the order of $10^2-10^7$ cells.</p>
<p>As we discuss below, such simplified models can nevertheless yield rich and accurate predictions about neural circuits.</p>
</blockquote>
<p>ç¬¬äºŒä¸ªç®€åŒ–æ˜¯åœ¨å®šä¹‰ç³»ç»ŸçŠ¶æ€æ—¶.</p>
<p>å¤§è„‘ä¸­å›è·¯éšæ—¶é—´å˜åŒ–çš„çŠ¶æ€å˜åŒ–å¯èƒ½å–å†³äºæ‰€æœ‰ç¥ç»å…ƒä¸­æ‰€æœ‰è„‰å†²çš„è¯¦ç»†æ¨¡å¼ã€ç›¸å…³ç¦»å­ã€<strong>ç¥ç»é€’è´¨</strong> å’Œ <strong>è°ƒèŠ‚å‰‚</strong> çš„æ°´å¹³, ç”šè‡³ç¦»å­é€šé“çš„çŠ¶æ€.</p>
<p>åœ¨çŸ­æ—¶é—´å°ºåº¦ä¸Š, ç¥ç»å…ƒä¹‹é—´çš„æƒé‡å’Œè¿æ¥å¯ä»¥è¢«è§†ä¸ºå‚æ•° (è€Œä¸æ˜¯å˜é‡) , ä½†å¦‚æœè€ƒè™‘æ›´é•¿çš„æ—¶é—´å°ºåº¦, å®ƒä»¬æœ¬èº«å°±æ˜¯å˜é‡.</p>
<p>åœ¨æè¿°ç§’çº§æ—¶é—´å°ºåº¦ä¸Šçš„ç¥ç»å›è·¯æ—¶, ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„ç®€åŒ–æ˜¯ä»…ä½¿ç”¨å›è·¯ä¸­ç¥ç»å…ƒçš„ <strong>è„‰å†²è¾“å‡º</strong> ä½œä¸ºçŠ¶æ€, é€šå¸¸è¿›ä¸€æ­¥ç®€åŒ–ä¸ºå«æ—¶è„‰å†²ç‡.</p>
<p>å¦‚æœè¿™æ ·çš„æè¿°è¶³ä»¥é¢„æµ‹ç›¸å…³æ—¶é—´å°ºåº¦ä¸Šç³»ç»Ÿçš„çŠ¶æ€å˜åŒ–, åˆ™å¯ä»¥å°†å…¶è§†ä¸ºå›è·¯çš„åˆç†åŠ¨åŠ›ç³»ç»Ÿæ¨¡å‹.</p>
<p>å°½ç®¡è„‰å†²æˆ–è„‰å†²ç‡è¡¨è¿°å¿½ç•¥äº†äºšç»†èƒå’Œåˆ†å­å˜é‡, ä»¥åšå‡ºç›¸å…³å›è·¯åŠ¨åŠ›å­¦ç”±è„‰å†²æ§åˆ¶çš„ç²—ç•¥ç®€åŒ–å‡è®¾, ä½†ä»¥è¿™ç§æ–¹å¼æè¿°çš„ <strong>è„Šæ¤åŠ¨ç‰©</strong> å¾®å›è·¯çš„çŠ¶æ€ç©ºé—´ä»ç„¶æ˜¯éå¸¸é«˜ç»´çš„, åŒ…æ‹¬å›è·¯ä¸­çš„ç¥ç»å…ƒæ•°é‡, å¯èƒ½åœ¨ $10^2-10^7$ ä¸ªç»†èƒçš„èŒƒå›´å†….</p>
<p>æ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·, è¿™æ ·çš„ç®€åŒ–æ¨¡å‹ä»ç„¶å¯ä»¥äº§ç”Ÿå…³äºç¥ç»å›è·¯ä¸°å¯Œä¸”å‡†ç¡®çš„é¢„æµ‹.</p>
<blockquote>
<p>Attractors exist in various flavours: an attractor may consist of a single state, a set of discrete states, a set of states that effectively behave as a continuous set or many such near-continuous sets (Fig. 1).</p>
<p>If a set of attractor states traces out a shape in state space that is approximately continuous and <strong>locally Euclidean</strong>, it is known as an <strong>attractor manifold</strong>.</p>
<p>Nonlinear continuous-attractor manifolds can be curved and topologically complex (for example, resembling rings, tori and so on; Fig. 1c,d, rightmost column). States on an attractor may be stationary, or might flow along the attractor to trace out trajectories that are periodic (known as <strong>limit cycles</strong>; Fig. 1f, rightmost column) or <strong>chaotic</strong> (that is, with dynamics that are inherently unpredictable owing to high sensitivity to small changes in the state).</p>
</blockquote>
<p>å¸å¼•å­å­˜åœ¨å„ç§å½¢å¼: å¸å¼•å­å¯ä»¥ç”±å•ä¸ªçŠ¶æ€ã€ä¸€ç»„ç¦»æ•£çŠ¶æ€ã€ä¸€ç»„æœ‰æ•ˆåœ°è¡¨ç°ä¸ºè¿ç»­é›†çš„çŠ¶æ€æˆ–è®¸å¤šè¿™æ ·çš„è¿‘è¿ç»­é›†ç»„æˆ (å›¾ 1).</p>
<p>å¦‚æœä¸€ç»„å¸å¼•å­çŠ¶æ€åœ¨çŠ¶æ€ç©ºé—´ä¸­æç»˜å‡ºä¸€ä¸ªè¿‘ä¼¼è¿ç»­ä¸”å±€éƒ¨æ¬§å¼å‡ ä½•çš„å½¢çŠ¶, åˆ™ç§°å…¶ä¸º <strong>å¸å¼•å­æµå½¢</strong>.</p>
<p>éçº¿æ€§è¿ç»­å¸å¼•å­æµå½¢å¯ä»¥æ˜¯å¼¯æ›²ä¸”æ‹“æ‰‘å¤æ‚çš„ (ä¾‹å¦‚, ç±»ä¼¼äºç¯ã€ç¯é¢ç­‰; å›¾ 1cã€d, æœ€å³åˆ—).  å¸å¼•å­ä¸Šçš„çŠ¶æ€å¯ä»¥æ˜¯é™æ­¢çš„, æˆ–è€…å¯èƒ½æ²¿ç€å¸å¼•å­æµåŠ¨ä»¥æç»˜å‡ºå‘¨æœŸæ€§è½¨è¿¹ (ç§°ä¸º <strong>æé™ç¯</strong>; å›¾ 1f, æœ€å³åˆ—) æˆ– <strong>æ··æ²Œ</strong> (å³, ç”±äºå¯¹çŠ¶æ€å¾®å°å˜åŒ–çš„é«˜åº¦æ•æ„Ÿæ€§è€Œå…·æœ‰å›ºæœ‰ä¸å¯é¢„æµ‹æ€§çš„åŠ¨åŠ›å­¦).</p>
<blockquote>
<blockquote>
<p>Fig. 1 | Mechanisms of attractor formation.</p>
<p>Left columns: open grey circles represent neurons, and connections between them are <strong>excitatory</strong> (black lines ending in bars) or <strong>inhibitory</strong> (black lines ending in circles). For layout of neurons and connections, connectivity matrices are shown as the inset, with black to white colours indicating strongly inhibitory to excitatory interactions, respectively.</p>
<p>Middle columns: examples of stable population activity patterns.</p>
<p>Right columns: state-space views of population states and dynamics. Red circles with shades of blue rings indicate the activity states shown in middle column; grey lines denote transient dynamic trajectories and red denotes attracting states.</p>
</blockquote>
<p>å›¾ 1 | å¸å¼•å­å½¢æˆæœºåˆ¶.</p>
<p>å·¦åˆ—: å¼€æ”¾çš„ç°è‰²åœ†åœˆä»£è¡¨ç¥ç»å…ƒ, è¿æ¥å®ƒä»¬çš„æ˜¯ <strong>å…´å¥‹æ€§</strong> (ä»¥æ£’ç»“å°¾çš„é»‘çº¿) æˆ– <strong>æŠ‘åˆ¶æ€§</strong> (ä»¥åœ†åœˆç»“å°¾çš„é»‘çº¿).  å¯¹äºç¥ç»å…ƒå’Œè¿æ¥çš„å¸ƒå±€, æ’å›¾ä¸­æ˜¾ç¤ºäº†è¿æ¥çŸ©é˜µ, é»‘è‰²åˆ°ç™½è‰²è¡¨ç¤ºå¼ºæŠ‘åˆ¶åˆ°(å¼º)å…´å¥‹æ€§ç›¸äº’ä½œç”¨.</p>
<p>ä¸­é—´åˆ—: ç¨³å®šçš„ç¾¤ä½“æ´»åŠ¨æ¨¡å¼ç¤ºä¾‹.</p>
<p>å³åˆ—: ç¾¤ä½“çŠ¶æ€å’ŒåŠ¨åŠ›å­¦çš„çŠ¶æ€ç©ºé—´è§†å›¾. å¸¦æœ‰è“è‰²ç¯é˜´å½±çš„çº¢è‰²åœ†åœˆè¡¨ç¤ºä¸­é—´åˆ—ä¸­æ˜¾ç¤ºçš„æ´»åŠ¨çŠ¶æ€; ç°è‰²çº¿è¡¨ç¤ºç¬æ€åŠ¨æ€è½¨è¿¹, çº¢è‰²è¡¨ç¤ºå¸å¼•çŠ¶æ€.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/Gw38NKiaOWkxuYz.png" alt=""  /></p>
<p><strong>a</strong>, A network with dense symmetric connections determined by associative Hebbian learning on a set of input patterns (middle) stores them as stable attractor states. This defines a Hopfield network.</p>
</blockquote>
<p><strong>a</strong>ã€ç”±ä¸€ç»„è¾“å…¥æ¨¡å¼ä¸Šçš„è”æƒ³ Hebbian å­¦ä¹ ç¡®å®šçš„å¯†é›†å¯¹ç§°è¿æ¥ç½‘ç»œ (ä¸­é—´) å°†å®ƒä»¬å­˜å‚¨ä¸ºç¨³å®šçš„å¸å¼•å­çŠ¶æ€. è¿™å®šä¹‰äº† Hopfield ç½‘ç»œ.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/aNTnwrmjvK2i8qD.png" alt=""  /></p>
<p><strong>b</strong>, Disjoint groups of neurons that interact through <strong>within-group excitation</strong> and <strong>across-group inhibition</strong> lead to group winner-takes-all (WTA) dynamics. Stable states are any patterns with one winning group. The state-space plot collapses all activities of neurons in group $g_{i}$ along the axis $r_{gi}$.</p>
</blockquote>
<p><strong>b</strong>ã€é€šè¿‡ <strong>ç»„å†…å…´å¥‹</strong> å’Œ <strong>ç»„é—´æŠ‘åˆ¶</strong> ç›¸äº’ä½œç”¨çš„ä¸ç›¸äº¤ç¥ç»å…ƒç»„å¯¼è‡´ç»„èµ¢å®¶é€šåƒ (WTA) åŠ¨åŠ›å­¦. ç¨³å®šçŠ¶æ€æ˜¯å…·æœ‰ä¸€ä¸ªè·èƒœç»„çš„ä»»ä½•æ¨¡å¼. çŠ¶æ€ç©ºé—´å›¾æ²¿è½´ $r_{g_{i}}$ æŠ˜å äº†ç»„ $g_{i}$ ä¸­ç¥ç»å…ƒçš„æ‰€æœ‰æ´»åŠ¨.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/9y6DwkLXaIgb4UG.png" alt=""  /></p>
<p><strong>c</strong>, Neurons arranged in a ring with <strong>global inhibition</strong> and either <strong>local excitation</strong> or a <strong>lack of local inhibition</strong>, combined with uniform excitatory input to all neurons, produce localized activity bumps (middle) as the stable states.</p>
<p>Bumps may be centred anywhere on the neural ring, defining a near-continuum of attractor states that form a ring in state space (right).</p>
</blockquote>
<p><strong>c</strong>ã€åœ¨ç¯ä¸Šæ’åˆ—çš„ç¥ç»å…ƒå…·æœ‰ <strong>å…¨å±€æŠ‘åˆ¶</strong> å’Œ <strong>å±€éƒ¨å…´å¥‹</strong> æˆ– <strong>å±€éƒ¨æŠ‘åˆ¶ç¼ºä¹</strong>, ç»“åˆå¯¹æ‰€æœ‰ç¥ç»å…ƒçš„å‡åŒ€å…´å¥‹è¾“å…¥, äº§ç”Ÿå±€éƒ¨æ´»åŠ¨å³°å€¼ (ä¸­é—´) ä½œä¸ºç¨³å®šçŠ¶æ€.</p>
<p>å³°å€¼å¯ä»¥ä½äºç¥ç»ç¯ä¸Šçš„ä»»ä½•ä½ç½®, åœ¨çŠ¶æ€ç©ºé—´ä¸­å®šä¹‰äº†ä¸€ä¸ªè¿‘è¿ç»­çš„å¸å¼•å­çŠ¶æ€ç¯ (å³).</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/1xkeypCVAXG5J8B.png" alt=""  /></p>
<p><strong>d</strong>, Neurons arranged on a two-dimensional neural sheet, interacting through <strong>local inhibition</strong> and either <strong>centre excitation</strong> or a <strong>lack of inhibition</strong> near the centre with uniform excitatory input to all neurons, result in a pattern of multiple periodically spaced activity bumps (middle).</p>
<p>Any two-dimensional phase shift of the periodic pattern up to the lattice periodicity results in distinct but equivalent stable states, and then the states repeat; thus, the result is a torus of stable states.</p>
</blockquote>
<p><strong>d</strong>ã€åœ¨äºŒç»´ç¥ç»ç‰‡ä¸Šæ’åˆ—çš„ç¥ç»å…ƒé€šè¿‡ <strong>å±€éƒ¨æŠ‘åˆ¶</strong> ç›¸äº’ä½œç”¨, å¹¶ä¸”åœ¨ä¸­å¿ƒé™„è¿‘å…·æœ‰ <strong>ä¸­å¿ƒå…´å¥‹</strong> æˆ– <strong>æŠ‘åˆ¶ç¼ºä¹</strong>, å¯¹æ‰€æœ‰ç¥ç»å…ƒè¿›è¡Œå‡åŒ€å…´å¥‹è¾“å…¥, å¯¼è‡´å¤šä¸ªå‘¨æœŸæ€§é—´éš”æ´»åŠ¨å³°å€¼çš„æ¨¡å¼ (ä¸­é—´).</p>
<p>å‘¨æœŸæ€§æ¨¡å¼çš„ä»»ä½•(æ™¶æ ¼å‘¨æœŸæ€§çš„)äºŒç»´ç›¸ç§»éƒ½ä¼šå¯¼è‡´ä¸åŒä½†ç­‰æ•ˆçš„ç¨³å®šçŠ¶æ€, ç„¶åçŠ¶æ€é‡å¤; å› æ­¤, ç»“æœæ˜¯ä¸€ä¸ªç¨³å®šçŠ¶æ€ç¯é¢.</p>
</blockquote>
<blockquote>
<p>åœ†ç¯å’Œåœ†ç¯çš„ç›´ç§¯å½¢æˆç¯é¢(torus), å³ $S^{1}\otimes S^{1} = T^{2}$</p>
<p>ç”¨åˆ°çš„è¿æ¥æ–¹å¼è¢«æ¯”å–»ä¸º Mexican-hat.
<img loading="lazy" src="https://i.ebayimg.com/images/g/nBEAAOSw-xVaRPE9/s-l1600.webp" alt=""  /></p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/NJSXpH1TBxO2WbV.png" alt=""  /></p>
<p><strong>e</strong>, Two neuron groups with <strong>in-group excitation</strong> and <strong>across-group inhibition</strong>, precisely tuned interaction strengths and quasi-linear neural input-output responses can counteract activity decay in the network and produce persistent activity over a continuum of activity levels in the two populations, defining ramp-like neural tuning and a line of attractor states.</p>
</blockquote>
<p><strong>e</strong>ã€ä¸¤ä¸ªå…·æœ‰ <strong>ç»„å†…å…´å¥‹</strong> å’Œ <strong>ç»„é—´æŠ‘åˆ¶</strong>ã€ç²¾ç¡®è°ƒåˆ¶ç›¸äº’ä½œç”¨å¼ºåº¦å’Œå‡†çº¿æ€§ç¥ç»è¾“å…¥-è¾“å‡ºå“åº”çš„ç¥ç»å…ƒç»„å¯ä»¥æŠµæ¶ˆç½‘ç»œä¸­çš„æ´»åŠ¨è¡°å‡, å¹¶åœ¨ä¸¤ä¸ªç¾¤ä½“ä¸­äº§ç”Ÿè¿ç»­çš„æ´»åŠ¨æ°´å¹³ä¸Šçš„æŒç»­æ´»åŠ¨, å®šä¹‰äº†é˜¶æ¢¯çŠ¶ç¥ç»è°ƒåˆ¶å’Œä¸€ç³»åˆ—å¸å¼•å­çŠ¶æ€.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/28/qrNG32jeMbYJQTZ.png" alt=""  /></p>
<p><strong>f</strong>, Neurons arranged on a ring with asymmetric connections drive a flow of neural activity in a particular direction.</p>
<p>The network forms localized activity bumps that sequentially move around the ring in that direction (middle). The state space contains a <strong>limit-cycle attractor</strong> (right).</p>
</blockquote>
<p><strong>f</strong>ã€åœ¨ç¯ä¸Šæ’åˆ—çš„ç¥ç»å…ƒå…·æœ‰ä¸å¯¹ç§°è¿æ¥, é©±åŠ¨ç¥ç»æ´»åŠ¨æœç‰¹å®šæ–¹å‘æµåŠ¨.</p>
<p>ç½‘ç»œå½¢æˆå±€éƒ¨æ´»åŠ¨å³°å€¼, ä¾æ¬¡å›´ç»•è¯¥ç¯ç§»åŠ¨ (ä¸­é—´). çŠ¶æ€ç©ºé—´åŒ…å«ä¸€ä¸ª <strong>æé™ç¯å¸å¼•å­</strong> (å³).</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/28/YjgLM48nKCztaVH.png" alt=""  /></p>
<p><strong>g</strong>, The <strong>copy-and-offset mechanism</strong> for constructing integrators, illustrated for the ring (left) and grid (right) attractor circuits. Each network copy receives velocity inputs tuned to the corresponding shift direction.</p>
</blockquote>
<p><strong>g</strong>ã€æ„å»ºç§¯åˆ†å™¨çš„ <strong>å¤åˆ¶å’Œåç§»æœºåˆ¶</strong>, è¯´æ˜äº†ç¯ (å·¦) å’Œç½‘æ ¼ (å³) å¸å¼•å­å›è·¯. æ¯ä¸ªç½‘ç»œå‰¯æœ¬æ¥æ”¶è°ƒåˆ¶äºç›¸åº”åç§»æ–¹å‘çš„é€Ÿåº¦è¾“å…¥.</p>
</blockquote>
<blockquote>
<p>Various combinations of such attractors, of different dimensions, geometries and topologies, may coexist in different regions of the state space of a single dynamical system.</p>
<p>Typically, the set of attractors in a dynamical system comprises a small subset of the state space, and attractor manifolds are usually much lower-dimensional than the state space.</p>
<p>In cases in which a system has multiple attractor states, the initial condition determines the attractor state to which the system flows.</p>
</blockquote>
<p>å„ç§ä¸åŒç»´åº¦ã€å‡ ä½•å½¢çŠ¶å’Œæ‹“æ‰‘ç»“æ„çš„å¸å¼•å­ç»„åˆå¯èƒ½å…±å­˜äºå•ä¸€åŠ¨åŠ›ç³»ç»ŸçŠ¶æ€ç©ºé—´çš„ä¸åŒåŒºåŸŸä¸­.</p>
<p>é€šå¸¸, åŠ¨åŠ›ç³»ç»Ÿä¸­çš„å¸å¼•å­é›†åˆæ„æˆçŠ¶æ€ç©ºé—´çš„ä¸€ä¸ªå°å­é›†, å¹¶ä¸”å¸å¼•å­æµå½¢é€šå¸¸æ¯”çŠ¶æ€ç©ºé—´ç»´æ•°ä½å¾—å¤š.</p>
<p>è‹¥ç³»ç»Ÿå…·æœ‰å¤šä¸ªå¸å¼•å­çŠ¶æ€, åˆå§‹æ¡ä»¶å†³å®šäº†ç³»ç»Ÿæµå‘çš„å¸å¼•å­çŠ¶æ€.</p>
<h2 id="attractors-in-the-presence-of-noise">Attractors in the presence of noise<a hidden class="anchor" aria-hidden="true" href="#attractors-in-the-presence-of-noise">#</a></h2>
<blockquote>
<p>Any real physical system unavoidably behaves non-deterministically from the perspective of a model of the system.</p>
<p>This is because one cannot observe and describe all variables, and all uncharacterized variables together with true stochastic sources of variation <em>(such as <strong>synaptic signalling noise</strong> from stochastic vesicle release; fluctuations in ion concentrations during processes such as spike initiation and calcium signalling; or fluctuations in small copy numbers of proteins)</em> serve as effective sources of noise in the model.</p>
<p>Noise can disrupt states so they do not strictly <strong>localize</strong> to the attractor described in a noise-free version of the model, and can drive the system to escape from an attractor over time.</p>
<p>However, the general idea of attractor states remains, in that, if the system is initialized near such a state, it tends to flow towards it and subsequently remains localized around it, for extended periods.</p>
</blockquote>
<p>ä»ç³»ç»Ÿæ¨¡å‹çš„è§’åº¦æ¥çœ‹, ä»»ä½•çœŸå®çš„ç‰©ç†ç³»ç»Ÿä¸å¯é¿å…åœ°è¡¨ç°å‡ºä¸ç¡®å®šæ€§.</p>
<p>è¿™æ˜¯å› ä¸ºæ— æ³•è§‚å¯Ÿå’Œæè¿°æ‰€æœ‰å˜é‡, æ‰€æœ‰æœªè¡¨å¾çš„å˜é‡ä»¥åŠçœŸæ­£çš„éšæœºå˜åŒ–æºå…±åŒä½œä¸ºæ¨¡å‹ä¸­çš„ç­‰æ•ˆå™ªå£°æº <em>(ä¾‹å¦‚æ¥è‡ªéšæœºå›Šæ³¡é‡Šæ”¾çš„ <strong>çªè§¦ä¿¡å·å™ªå£°</strong>; åœ¨å°–å³°å¯åŠ¨å’Œé’™ä¿¡å·ç­‰è¿‡ç¨‹ä¸­ç¦»å­æµ“åº¦çš„æ³¢åŠ¨; æˆ–è›‹ç™½è´¨çš„å°æ‹·è´æ•°æ³¢åŠ¨)</em>.</p>
<p>å™ªå£°å¯èƒ½ä¼šå¹²æ‰°çŠ¶æ€, å› æ­¤(çŠ¶æ€)ä¸ä¼šä¸¥æ ¼ <strong>å®šåŸŸ</strong> åˆ°æ— å™ªå£°æ¨¡å‹ä¸­æè¿°çš„å¸å¼•å­, ä¸”éšç€æ—¶é—´çš„æ¨ç§», å™ªå£°å¯èƒ½é©±åŠ¨ç³»ç»Ÿé€ƒç¦»å¸å¼•å­.</p>
<p>ç„¶è€Œ, å¹¿ä¹‰çš„å¸å¼•å­çŠ¶æ€æ¦‚å¿µä»ç„¶å­˜åœ¨, å³å¦‚æœç³»ç»Ÿåˆæ€åœ¨è¿™æ ·çš„çŠ¶æ€é™„è¿‘, å®ƒå€¾å‘äºæµå‘å®ƒå¹¶éšåé•¿æ—¶é—´ä¿æŒåœ¨å…¶å‘¨å›´.</p>
<blockquote>
<p>Because attractor states are where systems tend to localize (when not externally driven), they should be observable in the autonomous dynamics of real systems.</p>
<p>This basic property is the basis for the most fundamental and robust tests of attractor dynamics in neural systems, as we discuss below.</p>
<p>In a nutshell, the central signatures of attractors in real systems (discussed in more detail in later sections of this Review) can be summarized as: <strong>the localization of the states of a system to a lower-dimensional subset; the flow of the states towards the subset after perturbation; and the long-time and (effectively) autonomous stability of states in that subset.</strong></p>
</blockquote>
<p>ç”±äºå¸å¼•å­çŠ¶æ€æ˜¯ç³»ç»Ÿ (æ²¡æœ‰å¤–éƒ¨é©±åŠ¨æ—¶) å€¾å‘äºå®šåŸŸçš„åœ°æ–¹, å› æ­¤å®ƒä»¬åº”è¯¥å¯ä»¥åœ¨çœŸå®ç³»ç»Ÿçš„è‡ªä¸»åŠ¨åŠ›å­¦ä¸­è§‚å¯Ÿåˆ°.</p>
<p>è¿™ä¸€åŸºæœ¬å±æ€§æ˜¯ç¥ç»ç³»ç»Ÿä¸­å¸å¼•å­åŠ¨åŠ›å­¦æœ€åŸºæœ¬å’Œæœ€ç¨³å¥æµ‹è¯•çš„åŸºç¡€, æ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·.</p>
<p>ç®€è€Œè¨€ä¹‹, çœŸå®ç³»ç»Ÿä¸­å¸å¼•å­çš„ä¸­å¿ƒç‰¹å¾ (åœ¨æœ¬ç»¼è¿°çš„åç»­éƒ¨åˆ†ä¸­å°†æ›´è¯¦ç»†åœ°è®¨è®º) å¯ä»¥æ€»ç»“ä¸º: <strong>ç³»ç»ŸçŠ¶æ€å®šåŸŸè‡³ä½ç»´å­é›†; å¾®æ‰°åçŠ¶æ€æµå‘è¯¥å­é›†; ä»¥åŠè¯¥å­é›†ä¸­çŠ¶æ€çš„é•¿æœŸå’Œ (æœ‰æ•ˆåœ°) è‡ªä¸»ç¨³å®šæ€§.</strong></p>
<h1 id="construction-and-mechanisms">Construction and mechanisms<a hidden class="anchor" aria-hidden="true" href="#construction-and-mechanisms">#</a></h1>
<blockquote>
<p>The general principle underlying the formation of non-trivial attractor states in neural circuits is <strong>strong recurrent positive feedback</strong>. Positive feedback fights activity decay to stabilize certain states, and has been posited to be the basis for the stabilization of memory traces and persistent activity in the brain.</p>
<p>Which states become stabilized into attractors depends on how the network sculpts the positive feedback, which, according to the synaptic hypothesis, is determined by synaptic weights.</p>
</blockquote>
<p>åœ¨ç¥ç»å›è·¯ä¸­å½¢æˆéå¹³å‡¡å¸å¼•å­çŠ¶æ€çš„åŸºæœ¬åŸç†æ˜¯ <strong>å¼ºçƒˆçš„é€’å½’æ­£åé¦ˆ</strong>. æ­£åé¦ˆæŠµæŠ—æ¿€æ´»è¡°å‡ä»¥ç¨³å®šæŸäº›çŠ¶æ€, è¿™è¢«å‡å®šä¸ºå¤§è„‘ä¸­è®°å¿†ç—•è¿¹å’ŒæŒç»­æ¿€æ´»ç¨³å®šåŒ–çš„åŸºç¡€.</p>
<p>æ ¹æ®çªè§¦å‡è¯´, &ldquo;ä½•ç§çŠ¶æ€è¢«ç¨³å®šä¸ºå¸å¼•å­&rdquo; å–å†³äº &ldquo;ç½‘ç»œå¦‚ä½•å¡‘é€ æ­£åé¦ˆ&rdquo;, éœ€è¦è¢«çªè§¦æƒé‡å†³å®š.</p>
<blockquote>
<p>In general, characterizing the relationship between structure and function in a large collection of interacting elements is extremely difficult.</p>
<p>For example, a large collection of simple polar three-atom molecules of hydrogen and oxygen give rise to the <strong>emergent phenomena</strong> we associate with water â€” such as liquidness, wetness and freezing into a solid â€” that cannot be predicted through intuition or by drawing box and arrow diagrams.</p>
<p>Nevertheless, the transitions and properties of <strong>emergent states</strong> can be described relatively simply, with very few key parameters and variables.</p>
</blockquote>
<p>ä¸€èˆ¬æ¥è¯´, è¡¨å¾å¤§é‡ç›¸äº’ä½œç”¨å…ƒç´ ä¹‹é—´çš„ç»“æ„ä¸åŠŸèƒ½å…³ç³»æ˜¯éå¸¸å›°éš¾çš„.</p>
<p>ä¾‹å¦‚, å¤§é‡ç”±æ°¢å’Œæ°§ç»„æˆçš„ç®€å•ææ€§ä¸‰åŸå­åˆ†å­äº§ç”Ÿäº†æˆ‘ä»¬ä¸æ°´ç›¸å…³çš„ <strong>æ¶Œç°ç°è±¡</strong>â€”â€”æ¯”å¦‚æ¶²æ€ã€æ¹¿æ¶¦å’Œå†»ç»“æˆå›ºä½“â€”â€”è¿™äº›ç°è±¡æ— æ³•é€šè¿‡ç›´è§‰æˆ–ç»˜åˆ¶æ¡†å›¾å’Œç®­å¤´å›¾æ¥é¢„æµ‹.</p>
<p>ç„¶è€Œ, <strong>æ¶Œç°çŠ¶æ€</strong> çš„è½¬å˜å’Œå±æ€§å¯ä»¥ç›¸å¯¹ç®€å•åœ°æè¿°, åªéœ€å¾ˆå°‘çš„å…³é”®å‚æ•°å’Œå˜é‡.</p>
<blockquote>
<p>One way to characterize the relationship between synaptic weights and attractor dynamics is to ask what attractor states a given set of weights produces (the â€˜forwardâ€™ problem).</p>
<p>With a given set of weights, one can simulate a circuit and explore the resulting dynamics to find attractors of the system. A more powerful method, the Lyapunov function approach, holds for symmetric weight matrices ($W_{ij} = W_{ji}$) and ratebased neural dynamics.</p>
<p>For this class of models, a generalized energy function (the Lyapunov function), which is a function of the weights and neural activation function, analytically specifies the networkâ€™s dynamics.</p>
<p>Stable and unstable attractor states are the energy minima and maxima of the derived landscape, respectively, and the networkâ€™s state flows downhill towards the attractors (Fig. 2e) in the way a ball rolls down a gravitational potential.</p>
</blockquote>
<p>è¡¨å¾çªè§¦æƒé‡ä¸å¸å¼•å­åŠ¨åŠ›å­¦ä¹‹é—´å…³ç³»çš„ä¸€ç§æ–¹æ³•æ˜¯è¯¢é—®ç»™å®šæƒé‡é›†äº§ç”Ÿäº†å“ªäº›å¸å¼•å­çŠ¶æ€ ( &ldquo;æ­£å‘&rdquo; é—®é¢˜).</p>
<p>é€šè¿‡ç»™å®šä¸€ç»„æƒé‡, å¯ä»¥æ¨¡æ‹Ÿå›è·¯å¹¶æ¢ç´¢ç”±æ­¤äº§ç”Ÿçš„åŠ¨åŠ›å­¦ä»¥æ‰¾åˆ°ç³»ç»Ÿçš„å¸å¼•å­. æ›´å¼ºå¤§çš„æ–¹æ³•æ˜¯ Lyapunov å‡½æ•°æ–¹æ³•, å®ƒé€‚ç”¨äºå¯¹ç§°æƒé‡çŸ©é˜µ ($W_{ij} = W_{ji}$) å’ŒåŸºäºé€Ÿç‡çš„ç¥ç»åŠ¨åŠ›å­¦.</p>
<p>å¯¹äºè¿™ä¸€ç±»æ¨¡å‹, å¹¿ä¹‰èƒ½é‡å‡½æ•° (Lyapunov å‡½æ•°) , å®ƒæ˜¯æƒé‡å’Œç¥ç»æ¿€æ´»å‡½æ•°çš„å‡½æ•°, è§£æåœ°æŒ‡å®šäº†ç½‘ç»œçš„åŠ¨åŠ›å­¦.</p>
<p>ç¨³å®šå’Œä¸ç¨³å®šçš„å¸å¼•å­çŠ¶æ€åˆ†åˆ«æ˜¯å¯¼å‡ºæ™¯è§‚çš„èƒ½é‡æå°å€¼å’Œæå¤§å€¼, ç½‘ç»œçš„çŠ¶æ€æ²¿ç€å¸å¼•å­å‘ä¸‹æµåŠ¨ (å›¾ 2e) , å°±åƒçƒæ²¿ç€å¼•åŠ›åŠ¿æ»šåŠ¨ä¸€æ ·.</p>
<blockquote>
<blockquote>
<p>Fig. 2 | The utility of low-dimensional attractor networks.</p>
</blockquote>
<p>å›¾ 2 | ä½ç»´å¸å¼•å­ç½‘ç»œçš„å®ç”¨æ€§.</p>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/28/fqitK9ney4rMag1.png" alt=""  /></p>
<p><strong>a</strong>, Persistent and stable states generated by attractor networks (red) can be used to represent and remember external variables (blue) by constructing an appropriate mapping between them (vertical lines).</p>
</blockquote>
<p><strong>a</strong>ã€ å¸å¼•å­ç½‘ç»œ (çº¢è‰²) äº§ç”Ÿçš„æŒç»­ä¸”ç¨³å®šçš„çŠ¶æ€å¯ä»¥é€šè¿‡åœ¨å®ƒä»¬ä¹‹é—´æ„å»ºé€‚å½“çš„æ˜ å°„ (å‚ç›´çº¿) æ¥è¡¨ç¤ºå’Œè®°å¿†å¤–éƒ¨å˜é‡ (è“è‰²).</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/28/jeyWOA4gvI1Dw9k.png" alt=""  /></p>
<p><strong>b</strong>, Attractor networks can correct errors by mapping <strong>noisy states</strong> to the nearest attractor state.</p>
<p>$N$-dimensional noise drawn from the unit sphere centred on a one-dimensional attractor has a projection strength of only $1/N$ along the attractor: in this counter-intuitive high-dimensional geometry, a ball is more similar to a pancake, with the attractor orthogonal to the large dimensions.</p>
</blockquote>
<p><strong>b</strong>ã€å¸å¼•å­ç½‘ç»œå¯ä»¥é€šè¿‡å°† <strong>å—å™ªçŠ¶æ€</strong> æ˜ å°„åˆ°æœ€è¿‘çš„å¸å¼•å­çŠ¶æ€æ¥çº æ­£é”™è¯¯.</p>
<p>ä»ä»¥ä¸€ç»´å¸å¼•å­ä¸ºä¸­å¿ƒçš„å•ä½çƒä¸­ç»˜åˆ¶çš„ $N$ ç»´å™ªå£°åœ¨å¸å¼•å­ä¸Šçš„æŠ•å½±å¼ºåº¦ä»…ä¸º $1/N$: åœ¨è¿™ç§è¿åç›´è§‰çš„é«˜ç»´å‡ ä½•ä¸­, çƒæ›´ç±»ä¼¼äºç…é¥¼, å¸å¼•å­ä¸å¤§ç»´åº¦æ­£äº¤.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/28/976pKBAic3ueDVF.png" alt=""  /></p>
<p><strong>c</strong>, Flow to the nearest (continuous or discrete) attractor can perform a <strong>nearest-neighbour computation</strong> and, thus, perform classification. For example, the two attractors may represent â€˜catâ€™ and â€˜dogâ€™ perceptual manifolds, and the blue dot a specific input data point.</p>
</blockquote>
<p><strong>c</strong>ã€æµå‘æœ€è¿‘çš„ (è¿ç»­æˆ–ç¦»æ•£) å¸å¼•å­å¯ä»¥æ‰§è¡Œ <strong>æœ€è¿‘é‚»è®¡ç®—</strong>, ä»è€Œæ‰§è¡Œåˆ†ç±». ä¾‹å¦‚, ä¸¤ä¸ªå¸å¼•å­å¯èƒ½ä»£è¡¨ &ldquo;çŒ«&rdquo; å’Œ &ldquo;ç‹—&rdquo; çš„æ„ŸçŸ¥æµå½¢, è“ç‚¹ä»£è¡¨ç‰¹å®šçš„è¾“å…¥æ•°æ®ç‚¹.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/28/5QHY6hwkMLGjPi4.png" alt=""  /></p>
<p><strong>d</strong>, Left: continuous attractors can become integrators if velocities or movements in the external space are inputs to the network and induce proportional shifts in the internal attractor state. The current state on the attractor is then the integral of past velocity inputs relative to the starting state.</p>
<p>Right: if the input to an integrating attractor consists of temporally varying <strong>evidence pulses</strong> (bottom, evidence about one option in dark blue and evidence about the opposing option in light blue), these will move the state on the attractor (top) so the systemâ€™s current state reflects the integral of the total evidence.</p>
</blockquote>
<p><strong>d</strong>ã€å·¦: å¦‚æœå¤–éƒ¨ç©ºé—´ä¸­çš„é€Ÿåº¦æˆ–è¿åŠ¨æ˜¯ç½‘ç»œçš„è¾“å…¥å¹¶å¼•èµ·å†…éƒ¨å¸å¼•å­çŠ¶æ€çš„æˆæ¯”ä¾‹åç§», åˆ™è¿ç»­å¸å¼•å­å¯ä»¥æˆä¸ºç§¯åˆ†å™¨. é‚£ä¹ˆ, å¸å¼•å­ä¸Šçš„å½“å‰çŠ¶æ€æ˜¯ç›¸å¯¹äºèµ·å§‹çŠ¶æ€çš„è¿‡å»é€Ÿåº¦è¾“å…¥çš„ç§¯åˆ†.</p>
<p>å³: å¦‚æœç§¯åˆ†å¸å¼•å­çš„è¾“å…¥ç”±æ—¶é—´å˜åŒ–çš„ <strong>è¯æ®è„‰å†²</strong> ç»„æˆ (åº•éƒ¨, æ·±è“è‰²è¡¨ç¤ºä¸€ä¸ªé€‰é¡¹çš„è¯æ®, æµ…è“è‰²è¡¨ç¤ºç›¸åé€‰é¡¹çš„è¯æ®) , è¿™äº›å°†ç§»åŠ¨å¸å¼•å­ä¸Šçš„çŠ¶æ€ (é¡¶éƒ¨) , å› æ­¤ç³»ç»Ÿçš„å½“å‰çŠ¶æ€åæ˜ äº†æ‰€æœ‰è¯æ®çš„ç§¯åˆ†.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/28/IZdituyoTnP7hv2.png" alt=""  /></p>
<p><strong>e</strong>, The energy ($E$) landscape of a combined integration and decision-making network: inputs push the state left or right, and as the system integrates, the network state also moves towards one of two discrete attractors (left and right; white arrows, two sample trajectories). Arrival in the basin of one of the discrete attractors is a decision point.</p>
</blockquote>
<p><strong>e</strong>ã€ç§¯åˆ†å’Œå†³ç­–ç½‘ç»œçš„èƒ½é‡ ($E$) æ™¯è§‚: è¾“å…¥å‘å·¦æˆ–å‘å³æ¨åŠ¨çŠ¶æ€, å¹¶ä¸”éšç€ç³»ç»Ÿç§¯åˆ†, ç½‘ç»œçŠ¶æ€ä¹Ÿç§»åŠ¨åˆ°ä¸¤ä¸ªç¦»æ•£å¸å¼•å­ä¹‹ä¸€ (å·¦å’Œå³; ç™½è‰²ç®­å¤´, ä¸¤ä¸ªæ ·æœ¬è½¨è¿¹). åˆ°è¾¾å…¶ä¸­ä¸€ä¸ªç¦»æ•£å¸å¼•å­çš„ç›†åœ°æ˜¯ä¸€ä¸ªå†³ç­–ç‚¹.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/28/GsgxXenmzELUtB1.png" alt=""  /></p>
<p><strong>f</strong>, An integrator can be quickly re-purposed to represent multiple different and new external variables simply by yoking its velocity shift mechanism to different external velocities cues through feedforward learning.</p>
<p>This mechanism also supports <strong>zero-shot learning</strong> and inference: given an initial state and an input velocity trajectory, it will generate a self-consistent representation for the current state even if the trajectory is different and new each time.</p>
</blockquote>
<p><strong>f</strong>ã€ç§¯åˆ†å™¨å¯ä»¥é€šè¿‡å°†å…¶é€Ÿåº¦åç§»æœºåˆ¶ä¸ä¸åŒçš„å¤–éƒ¨é€Ÿåº¦çº¿ç´¢é€šè¿‡å‰é¦ˆå­¦ä¹ è”ç³»èµ·æ¥, å¿«é€Ÿé‡æ–°ç”¨äºè¡¨ç¤ºå¤šä¸ªä¸åŒä¸”æ–°çš„å¤–éƒ¨å˜é‡.</p>
<p>è¿™ç§æœºåˆ¶è¿˜æ”¯æŒ <strong>é›¶æ ·æœ¬å­¦ä¹ </strong> å’Œæ¨ç†: ç»™å®šåˆå§‹çŠ¶æ€å’Œè¾“å…¥é€Ÿåº¦è½¨è¿¹, å³ä½¿æ¯æ¬¡è½¨è¿¹ä¸åŒä¸”å…¨æ–°, å®ƒä¹Ÿä¼šç”Ÿæˆå½“å‰çŠ¶æ€çš„è‡ªæ´½è¡¨ç¤º.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/28/Jgq9f6hFSsxbpUW.png" alt=""  /></p>
<p><strong>g</strong>, A set of (continuous or discrete) attractor subnetworks (red boxes at bottom) can interact bidirectionally with a shared network to form a high-capacity attractor network.</p>
</blockquote>
<p><strong>g</strong>ã€ä¸€ç»„ (è¿ç»­æˆ–ç¦»æ•£) å¸å¼•å­å­ç½‘ç»œ (åº•éƒ¨çš„çº¢è‰²æ¡†) å¯ä»¥ä¸å…±äº«ç½‘ç»œåŒå‘äº¤äº’ä»¥å½¢æˆé«˜å®¹é‡å¸å¼•å­ç½‘ç»œ.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/28/1JjOhakuyWidHxp.png" alt=""  /></p>
<p><strong>h</strong>, <strong>Mixed modular representations</strong> can enable representation of inputs of different dimensions, by reusing the same attractors of fixed dimension each. Velocities ($v_{i}$) from external spaces of potentially different dimension are selected by a set of selection signals ($s_{i}$). The selected velocity (green) is routed through random projections to a set of $M$ modular integrator networks of dimension $K$ each. This kind of mixed modular circuit can interchangeably represent various input spaces of dimension $D \leq MK$ while smoothly trading off resolution for dimension.</p>
</blockquote>
<p><strong>h</strong>ã€<strong>æ··åˆæ¨¡å—åŒ–è¡¨è±¡</strong> å¯ä»¥é€šè¿‡é‡å¤ä½¿ç”¨æ¯ä¸ªå›ºå®šç»´åº¦çš„ç›¸åŒå¸å¼•å­æ¥è¡¨ç¤ºä¸åŒç»´åº¦çš„è¾“å…¥. ä¸€ç»„é€‰æ‹©ä¿¡å· ($s_{i}$) é€‰æ‹©æ¥è‡ªæ½œåœ¨ä¸åŒç»´åº¦çš„å¤–éƒ¨ç©ºé—´çš„é€Ÿåº¦ ($v_{i}$). é€‰æ‹©çš„é€Ÿåº¦ (ç»¿è‰²) é€šè¿‡éšæœºæŠ•å½±è·¯ç”±åˆ°ä¸€ç»„æ¯ä¸ªç»´åº¦ä¸º $K$ çš„ $M$ ä¸ªæ¨¡å—åŒ–ç§¯åˆ†å™¨ç½‘ç»œ. è¿™ç§æ··åˆæ¨¡å—åŒ–å›è·¯å¯ä»¥äº’æ¢åœ°è¡¨ç¤ºå„ç§ç»´åº¦ä¸º $D \leq MK$ çš„è¾“å…¥ç©ºé—´, åŒæ—¶å¹³æ»‘åœ°æƒè¡¡åˆ†è¾¨ç‡ä¸ç»´åº¦.</p>
</blockquote>
<blockquote>
<p>Another way to characterize the relationship between attractors and network structure is to consider the â€˜inverseâ€™ problem: given a set of attractors, what network structure could generate it?</p>
<p>Neuroscientists want to solve the inverse problem to make predictions about underlying mechanisms and, because neural activations are more readily observed than synaptic weights, the inverse problem is more frequently encountered than the forward problem.</p>
<p>By contrast, evolution, the brain and artificially intelligent systems must solve the inverse problem to be able to perform computations that require a given type of attractor dynamics (discussed below). Theoretical neuroscience has discovered some solutions to the inverse problem for different types of attractors, as we describe below.</p>
</blockquote>
<p>å¦ä¸€ç§è¡¨å¾å¸å¼•å­ä¸ç½‘ç»œç»“æ„ä¹‹é—´å…³ç³»çš„æ–¹æ³•æ˜¯è€ƒè™‘ &ldquo;é€†&rdquo; é—®é¢˜: ç»™å®šä¸€ç»„å¸å¼•å­, ä»€ä¹ˆç½‘ç»œç»“æ„å¯ä»¥ç”Ÿæˆå®ƒï¼Ÿ</p>
<p>ç¥ç»ç§‘å­¦å®¶å¸Œæœ›è§£å†³é€†é—®é¢˜ä»¥å¯¹æ½œåœ¨æœºåˆ¶è¿›è¡Œé¢„æµ‹, å¹¶ä¸”ç”±äºç¥ç»æ¿€æ´»æ¯”çªè§¦æƒé‡æ›´å®¹æ˜“è§‚å¯Ÿåˆ°, å› æ­¤é€†é—®é¢˜æ¯”æ­£å‘é—®é¢˜æ›´å¸¸è§.</p>
<p>ç›¸æ¯”ä¹‹ä¸‹, è¿›åŒ–ã€å¤§è„‘å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿå¿…é¡»è§£å†³é€†é—®é¢˜, ä»¥ä¾¿èƒ½å¤Ÿæ‰§è¡Œéœ€è¦ç‰¹å®šç±»å‹å¸å¼•å­åŠ¨åŠ›å­¦çš„è®¡ç®— (ä¸‹é¢è®¨è®º).  æ­£å¦‚æˆ‘ä»¬ä¸‹é¢æè¿°çš„é‚£æ ·, ç†è®ºç¥ç»ç§‘å­¦å·²ç»å‘ç°äº†ä¸€äº›ä¸åŒç±»å‹å¸å¼•å­çš„é€†é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ.</p>
<h2 id="discrete-attractors">Discrete attractors<a hidden class="anchor" aria-hidden="true" href="#discrete-attractors">#</a></h2>
<blockquote>
<p>A well-known prescription for creating a set of discrete attractors at user-defined points is given by the Hopfield model5 (Fig. 1a).</p>
<p>Input patterns of neural activation are <strong>inscribed</strong> into the network weights through a Hebbian-like learning rule, such that co-active neurons are connected by excitatory interactions and inhibit all the rest. Thus, these patterns stabilize themselves and become attractor states. If a sufficiently small number of patterns are learned, they can be retrieved from partial or corrupted versions of the stored states, and thus the network can be said to store <strong>content-addressable memories</strong>.</p>
<p>More generally, the attractors of simple rate-based networks with arbitrary symmetric weight matrices and without communication delays consist entirely of <strong>fixed points</strong>.</p>
<p>Some non-symmetric networks can also support point attractors, but not generically, and they can require additional mechanisms such as homeostatic plasticity.</p>
</blockquote>
<p>ä¸€ç§ä¼—æ‰€å‘¨çŸ¥çš„åœ¨è‡ªå®šä¹‰çš„ç‚¹åˆ›å»ºä¸€ç»„ç¦»æ•£å¸å¼•å­çš„å¤„ç†æ–¹æ³•æ˜¯ Hopfield æ¨¡å‹ (å›¾ 1a).</p>
<p>é€šè¿‡ç±»ä¼¼ Hebbian çš„å­¦ä¹ è§„åˆ™å°†ç¥ç»æ¿€æ´»çš„è¾“å…¥æ¨¡å¼ <strong>é“­åˆ»</strong> åˆ°ç½‘ç»œæƒé‡ä¸­, ä½¿å¾—å…±åŒæ¿€æ´»çš„ç¥ç»å…ƒé€šè¿‡å…´å¥‹æ€§ç›¸äº’ä½œç”¨è¿æ¥å¹¶æŠ‘åˆ¶å…¶ä½™éƒ¨åˆ†. å› æ­¤, è¿™äº›æ¨¡å¼ç¨³å®šè‡ªèº«å¹¶æˆä¸ºå¸å¼•å­çŠ¶æ€. å¦‚æœå­¦ä¹ çš„æ¨¡å¼æ•°é‡è¶³å¤Ÿå°‘, åˆ™å¯ä»¥ä»å­˜å‚¨çŠ¶æ€çš„éƒ¨åˆ†æˆ–æŸåç‰ˆæœ¬ä¸­æ£€ç´¢å®ƒä»¬, å› æ­¤å¯ä»¥è¯´ç½‘ç»œå­˜å‚¨äº† <strong>å†…å®¹å¯å¯»å€è®°å¿†</strong>.</p>
<p>æ›´ä¸€èˆ¬åœ°è¯´, å…·æœ‰ä»»æ„å¯¹ç§°æƒé‡çŸ©é˜µä¸”æ²¡æœ‰é€šä¿¡å»¶è¿Ÿçš„ç®€å•åŸºäºé€Ÿç‡çš„ç½‘ç»œçš„å¸å¼•å­å®Œå…¨ç”± <strong>ä¸åŠ¨ç‚¹</strong> ç»„æˆ.</p>
<p>ä¸€äº›éå¯¹ç§°ç½‘ç»œä¹Ÿå¯ä»¥æ”¯æŒç‚¹å¸å¼•å­, ä½†ä¸æ˜¯é€šç”¨çš„, å¹¶ä¸”å®ƒä»¬å¯èƒ½éœ€è¦è¯¸å¦‚ç¨³æ€å¯å¡‘æ€§ä¹‹ç±»çš„é™„åŠ æœºåˆ¶.</p>
<blockquote>
<p>Attractor states in Hopfield-like networks typically have highly overlapping neural memberships, even when they are well separated in the state space (Fig. 1a, middle column). Thus, there is not a clear notion of distinct â€˜cell assembliesâ€™. In a special case of Hopfield networks, neurons are partitioned into largely disjointed groups with self-excitation within groups and inhibition between groups.</p>
<p>In these <strong>winner-take-all (WTA)</strong> networks, the attractor states consist of largely non-overlapping active cell groups, which might then be called <strong>â€˜assembliesâ€™</strong> (Fig. 1b).</p>
</blockquote>
<p>Hopfield ç±»ç½‘ç»œä¸­çš„å¸å¼•å­çŠ¶æ€é€šå¸¸å…·æœ‰é«˜åº¦é‡å çš„ç¥ç»å…ƒæˆå‘˜, å³ä½¿å®ƒä»¬åœ¨çŠ¶æ€ç©ºé—´ä¸­åˆ†ç¦»è‰¯å¥½ (å›¾ 1a, ä¸­é—´åˆ—).  å› æ­¤, æ²¡æœ‰æ˜ç¡®çš„ &ldquo;ç»†èƒé›†ç¾¤&rdquo; æ¦‚å¿µ. åœ¨ Hopfield ç½‘ç»œçš„ä¸€ä¸ªç‰¹æ®Šæƒ…å†µä¸‹, ç¥ç»å…ƒè¢«åˆ’åˆ†ä¸ºå¤§è‡´ä¸ç›¸äº¤çš„ç»„, ç»„å†…å…·æœ‰è‡ªæˆ‘å…´å¥‹, ç»„é—´å…·æœ‰æŠ‘åˆ¶.</p>
<p>åœ¨è¿™äº› <strong>èµ¢å®¶é€šåƒ (WTA)</strong>  ç½‘ç»œä¸­, å¸å¼•å­çŠ¶æ€ç”±å¤§è‡´ä¸é‡å çš„æ´»è·ƒç»†èƒç¾¤ç»„æˆ, ç„¶åå¯ä»¥ç§°ä¹‹ä¸º <strong>&ldquo;é›†ç¾¤&rdquo;</strong> (å›¾ 1b).</p>
<h2 id="continuous-attractors">Continuous attractors<a hidden class="anchor" aria-hidden="true" href="#continuous-attractors">#</a></h2>
<blockquote>
<p>How can one construct networks with a continuum of stationary attractor states?</p>
<p>Weight matrices with a particular symmetry (across the diagonal) give rise to discrete attractors, as we have seen.</p>
<p>If the weights instead exhibit a continuous symmetry â€” for example, if the weight profiles are invariant across neurons (they look the same at each neuron, thus the symmetry is translational) â€” then the set of formed attractors will be related by the same symmetry and could thus form a continuous set.</p>
</blockquote>
<p>å¦‚ä½•æ„å»ºå…·æœ‰è¿ç»­é™æ­¢å¸å¼•å­çŠ¶æ€çš„ç½‘ç»œï¼Ÿ</p>
<p>æˆ‘ä»¬å·²ç»çœ‹åˆ°, å…·æœ‰ç‰¹å®šå¯¹è§’çº¿å¯¹ç§°æ€§çš„æƒé‡çŸ©é˜µä¼šäº§ç”Ÿç¦»æ•£å¸å¼•å­.</p>
<p>å¦‚æœæƒé‡è¡¨ç°å‡ºè¿ç»­å¯¹ç§°æ€§â€”â€”ä¾‹å¦‚, å¦‚æœæƒé‡é…ç½®åœ¨ç¥ç»å…ƒä¹‹é—´æ˜¯ä¸å˜çš„ (å®ƒä»¬åœ¨æ¯ä¸ªç¥ç»å…ƒå¤„çœ‹èµ·æ¥ç›¸åŒ, å› æ­¤å¯¹ç§°æ€§æ˜¯å¹³ç§»çš„) â€”â€”é‚£ä¹ˆå½¢æˆçš„å¸å¼•å­é›†å°†ä¸ç›¸åŒçš„å¯¹ç§°æ€§ç›¸å…³è”, å› æ­¤å¯ä»¥å½¢æˆè¿ç»­é›†.</p>
<blockquote>
<p>The general principle for the formation of stationary continuous attractors is <strong>pattern formation</strong>. Simple and spatially local competitive interactions across the neural sheet lead to the emergence of spatially structured activity patterns that are stable states: neurons with excitatory coupling between them become co-active and suppress the rest of their neighbours through inhibition in what is known as a linear Turing instability.</p>
</blockquote>
<p>å½¢æˆé™æ­¢è¿ç»­å¸å¼•å­çš„åŸºæœ¬åŸç†æ˜¯ <strong>æ¨¡å¼å½¢æˆ</strong>. ç¥ç»ç‰‡ä¸Šç®€å•ä¸”ç©ºé—´å±€åŸŸçš„ç«äº‰æ€§ç›¸äº’ä½œç”¨å¯¼è‡´ç©ºé—´ç»“æ„åŒ–æ´»åŠ¨æ¨¡å¼çš„å‡ºç°, è¿™äº›æ¨¡å¼æ˜¯ç¨³å®šçŠ¶æ€: é€šè¿‡å…´å¥‹æ€§è€¦åˆçš„ç¥ç»å…ƒå˜å¾—å…±åŒæ´»è·ƒ, å¹¶é€šè¿‡æŠ‘åˆ¶æŠ‘åˆ¶å…¶ä½™è¿‘é‚», è¿™è¢«ç§°ä¸ºçº¿æ€§ Turing ä¸ç¨³å®šæ€§.</p>
<blockquote>
<p>Three conditions are generally sufficient (although not strictly necessary) to provide a solution to the inverse problem for forming stationary continuous attractors (Box 1).</p>
<p>First, the system must include nonlinear neurons with saturating responses or inhibition-dominated recurrent interactions and a uniform excitatory drive to keep network activity bounded.</p>
<p>Second, the system must involve sufficiently strong <strong>recurrent weights</strong> with competitive dynamics in the form of local excitation or disinhibition, with broader inhibition, to drive spontaneous pattern formation through the Turing instability; these patterns become the attractor states.</p>
<p>Last, <u>the system requires some continuous symmetry in the weights</u> (a continuous weight symmetry is one where as some variable is varied continuously, the weights remain invariant), such as translational or rotational invariance (Fig. 1c,d), to ensure a continuum of attractor states.</p>
</blockquote>
<p>é€šå¸¸æœ‰ä¸‰ä¸ªæ¡ä»¶è¶³ä»¥ (å°½ç®¡ä¸æ˜¯ä¸¥æ ¼å¿…è¦çš„) ä¸º &ldquo;å½¢æˆé™æ­¢è¿ç»­å¸å¼•å­&rdquo; æä¾›é€†é—®é¢˜çš„è§£ (æ¡† 1).</p>
<ul>
<li>
<p>é¦–å…ˆ, ç³»ç»Ÿå¿…é¡»åŒ…æ‹¬å…·æœ‰é¥±å’Œå“åº”çš„éçº¿æ€§ç¥ç»å…ƒ, æˆ–ä»¥æŠ‘åˆ¶ä¸ºä¸»å¯¼çš„é€’å½’ç›¸äº’ä½œç”¨ä»¥åŠå‡åŒ€çš„å…´å¥‹é©±åŠ¨ä»¥ä¿æŒç½‘ç»œæ´»åŠ¨æœ‰ç•Œ.</p>
</li>
<li>
<p>å…¶æ¬¡, ç³»ç»Ÿå¿…é¡»æ¶‰åŠè¶³å¤Ÿå¼ºçš„ <strong>é€’å½’æƒé‡</strong>, å…·æœ‰å±€éƒ¨å…´å¥‹æˆ–å»æŠ‘åˆ¶å½¢å¼çš„ç«äº‰åŠ¨åŠ›å­¦, ä»¥åŠæ›´å¹¿æ³›çš„æŠ‘åˆ¶, ä»¥é€šè¿‡ Turing ä¸ç¨³å®šæ€§é©±åŠ¨è‡ªå‘æ¨¡å¼å½¢æˆ; è¿™äº›æ¨¡å¼æˆä¸ºå¸å¼•å­çŠ¶æ€.</p>
</li>
<li>
<p>æœ€å, <u>ç³»ç»Ÿéœ€è¦æƒé‡ä¸­çš„æŸç§è¿ç»­å¯¹ç§°æ€§</u> (è¿ç»­æƒé‡å¯¹ç§°æ€§æ˜¯æŒ‡éšç€æŸä¸ªå˜é‡çš„è¿ç»­å˜åŒ–, æƒé‡ä¿æŒä¸å˜) , ä¾‹å¦‚å¹³ç§»æˆ–æ—‹è½¬ä¸å˜æ€§ (å›¾ 1cã€d) , ä»¥ç¡®ä¿å¸å¼•å­çŠ¶æ€çš„è¿ç»­æ€§.</p>
</li>
</ul>
<blockquote>
<blockquote>
<p><strong>BOX 1</strong></p>
<p><strong>Attractor dynamics, anatomical topography and weight symmetries</strong></p>
<p>Anatomical topography, in which functionally similar neurons are near one another, is neither a necessary nor a sufficient condition for the existence of an attractor, because any low-dimensional attractor network is mathematically unchanged if all weights are preserved but neuron locations are scrambled. However, if the network is merely a spatially scrambled version of the idealized model, then the symmetries of the weight matrix can be revealed after an appropriate reordering of the neurons. An advantage of anatomical topography from a biological perspective is that it can reduce the complexity of development, in that wiring decisions can be guided by spatial proximity rather than depending entirely on activity or other target cell-signalling mechanisms. For example, the locally competitive interactions of grid and head-direction circuit models could be largely constructed through local arborization. Anatomical topography also reduces overall wiring length in the mature circuit. However, a circuit with three-dimensional dynamics or higher that are represented in an unfactorizable form cannot be embedded topographically in a two-dimensional cell layout, limiting the feasibility of topographic layouts for circuits that represent higher-dimensional unfactorizable manifolds.</p>
</blockquote>
<p>åŠŸèƒ½ç›¸ä¼¼çš„ç¥ç»å…ƒå½¼æ­¤é è¿‘çš„è§£å‰–æ‹“æ‰‘æ—¢ä¸æ˜¯å¸å¼•å­å­˜åœ¨çš„å¿…è¦æ¡ä»¶, ä¹Ÿä¸æ˜¯å……åˆ†æ¡ä»¶, å› ä¸ºå¦‚æœä¿ç•™æ‰€æœ‰æƒé‡ä½†ç¥ç»å…ƒä½ç½®è¢«æ‰“ä¹±, ä»»ä½•ä½ç»´å¸å¼•å­ç½‘ç»œåœ¨æ•°å­¦ä¸Šéƒ½æ˜¯ä¸å˜çš„. ç„¶è€Œ, å¦‚æœç½‘ç»œä»…ä»…æ˜¯ç†æƒ³åŒ–æ¨¡å‹çš„ç©ºé—´æ··ä¹±ç‰ˆæœ¬, é‚£ä¹ˆåœ¨é€‚å½“é‡æ–°æ’åºç¥ç»å…ƒå, å¯ä»¥æ­ç¤ºæƒé‡çŸ©é˜µçš„å¯¹ç§°æ€§. ä»ç”Ÿç‰©å­¦è§’åº¦æ¥çœ‹, è§£å‰–æ‹“æ‰‘çš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯å®ƒå¯ä»¥å‡å°‘å‘å±•çš„å¤æ‚æ€§, å› ä¸ºå¸ƒçº¿å†³ç­–å¯ä»¥é€šè¿‡ç©ºé—´æ¥è¿‘æ€§æ¥æŒ‡å¯¼, è€Œä¸å®Œå…¨ä¾èµ–äºæ´»åŠ¨æˆ–å…¶ä»–ç›®æ ‡ç»†èƒä¿¡å·æœºåˆ¶. ä¾‹å¦‚, ç½‘æ ¼å’Œå¤´éƒ¨æ–¹å‘å›è·¯æ¨¡å‹çš„å±€éƒ¨ç«äº‰æ€§ç›¸äº’ä½œç”¨å¯ä»¥ä¸»è¦é€šè¿‡å±€éƒ¨æ ‘çªå½¢æˆæ¥æ„å»º. æˆç†Ÿå›è·¯ä¸­çš„è§£å‰–æ‹“æ‰‘è¿˜å‡å°‘äº†æ•´ä½“å¸ƒçº¿é•¿åº¦. ç„¶è€Œ, å…·æœ‰ä¸‰ç»´åŠ¨åŠ›å­¦æˆ–æ›´é«˜ç»´åº¦åŠ¨åŠ›å­¦ä¸”ä»¥ä¸å¯åˆ†è§£å½¢å¼è¡¨ç¤ºçš„å›è·¯æ— æ³•åœ¨äºŒç»´ç»†èƒå¸ƒå±€ä¸­è¿›è¡Œæ‹“æ‰‘åµŒå…¥, è¿™é™åˆ¶äº†è¡¨ç¤ºé«˜ç»´ä¸å¯åˆ†è§£æµå½¢çš„å›è·¯çš„æ‹“æ‰‘å¸ƒå±€çš„å¯è¡Œæ€§.</p>
<blockquote>
<p>In addition, the posited weight symmetries in simple models of attractors need not exist in a biological instance of the circuit with the same dynamics: unscrambling or reordering neurons may not be sufficient to reveal the symmetries. Consider, for  example, a scenario in which low-dimensional attractor dynamics are generated by a recurrent network of $N$ neurons, but are only needed downstream in a set of $M &lt; N$ neurons. In this situation, the weight symmetries needed for continuous-attractor dynamics can be spread across both the recurrent and readout networks, such that the weights of the recurrent network alone will not reflect the relevant symmetries. Unveiling the symmetry in the circuit weights will require combining the readout weights with the recurrent ones.</p>
</blockquote>
<p>æ­¤å¤–, ç®€å•å¸å¼•å­æ¨¡å‹ä¸­å‡å®šçš„æƒé‡å¯¹ç§°æ€§ä¸ä¸€å®šå­˜åœ¨äºå…·æœ‰ç›¸åŒåŠ¨åŠ›å­¦çš„å›è·¯çš„ç”Ÿç‰©å®ä¾‹ä¸­: æ‰“ä¹±æˆ–é‡æ–°æ’åºç¥ç»å…ƒå¯èƒ½ä¸è¶³ä»¥æ­ç¤ºå¯¹ç§°æ€§. ä¾‹å¦‚, è€ƒè™‘è¿™æ ·ä¸€ç§æƒ…å†µ: ä½ç»´å¸å¼•å­åŠ¨åŠ›å­¦ç”± $N$ ä¸ªç¥ç»å…ƒçš„é€’å½’ç½‘ç»œç”Ÿæˆ, ä½†ä»…åœ¨ä¸‹æ¸¸çš„ä¸€ç»„ $M &lt; N$ ä¸ªç¥ç»å…ƒä¸­éœ€è¦. åœ¨è¿™ç§æƒ…å†µä¸‹, è¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦æ‰€éœ€çš„æƒé‡å¯¹ç§°æ€§å¯ä»¥åˆ†å¸ƒåœ¨é€’å½’å’Œè¯»å‡ºç½‘ç»œä¸­, å› æ­¤ä»…é€’å½’ç½‘ç»œçš„æƒé‡å°†ä¸ä¼šåæ˜ ç›¸å…³çš„å¯¹ç§°æ€§. æ­ç¤ºå›è·¯æƒé‡ä¸­çš„å¯¹ç§°æ€§å°†éœ€è¦å°†è¯»å‡ºæƒé‡ä¸é€’å½’æƒé‡ç»“åˆèµ·æ¥.</p>
<blockquote>
<p>These considerations give rise to a hypothesis for circuits with continuous attractors of dimension $\leq 2$: evolutionarily conserved circuits that do not require extensive early experience should be topographically organized. We might thus predict that the circuit that originates head-direction signals in mammals should be topographically organized. By contrast, if low-dimensional dynamics only emerge on the basis of activity-dependent plasticity with repetitive training, we may not expect the circuit to be topographically organized (or even localized to a single brain region).</p>
</blockquote>
<p>è¿™äº›è€ƒè™‘ä¸ºç»´åº¦ $\leq 2$ çš„è¿ç»­å¸å¼•å­å›è·¯æå‡ºäº†ä¸€ä¸ªå‡è®¾: ä¸éœ€è¦å¹¿æ³›æ—©æœŸç»éªŒçš„è¿›åŒ–ä¿å®ˆå›è·¯åº”è¯¥æ˜¯æ‹“æ‰‘ç»„ç»‡çš„. å› æ­¤, æˆ‘ä»¬å¯ä»¥é¢„æµ‹èµ·æºäºå“ºä¹³åŠ¨ç‰©å¤´éƒ¨æ–¹å‘ä¿¡å·çš„å›è·¯åº”è¯¥æ˜¯æ‹“æ‰‘ç»„ç»‡çš„. ç›¸æ¯”ä¹‹ä¸‹, å¦‚æœä½ç»´åŠ¨åŠ›å­¦ä»…åŸºäºå…·æœ‰é‡å¤è®­ç»ƒçš„æ´»åŠ¨ä¾èµ–æ€§å¯å¡‘æ€§å‡ºç°, æˆ‘ä»¬å¯èƒ½ä¸æœŸæœ›è¯¥å›è·¯æ˜¯æ‹“æ‰‘ç»„ç»‡çš„ (ç”šè‡³ä¸å±€é™äºå•ä¸ªå¤§è„‘åŒºåŸŸ).</p>
<blockquote>
<p>Remarkably, despite these caveats, and in a beautiful example of the predictive power of simple theories in neuroscience, empirical evidence from the anatomy of the zebrafish oculomotor integrator and the fly head-direction circuit in the past few years shows that nature has used precisely the hypothesized constructions proposed in simple circuit models to build some integrator networks.</p>
</blockquote>
<p>å€¼å¾—æ³¨æ„çš„æ˜¯, å°½ç®¡å­˜åœ¨è¿™äº›è­¦å‘Š, å¹¶ä¸”åœ¨ç¥ç»ç§‘å­¦ä¸­ç®€å•ç†è®ºé¢„æµ‹èƒ½åŠ›çš„ä¸€ä¸ªç¾ä¸½ä¾‹å­ä¸­, è¿‡å»å‡ å¹´æ¥è‡ªæ–‘é©¬é±¼çœ¼åŠ¨ç§¯åˆ†å™¨å’Œæœè‡å¤´éƒ¨æ–¹å‘å›è·¯è§£å‰–å­¦çš„å®è¯è¯æ®è¡¨æ˜, è‡ªç„¶ç•Œå·²ç»ä½¿ç”¨äº†ç®€å•å›è·¯æ¨¡å‹ä¸­æå‡ºçš„å‡è®¾æ„é€ æ¥æ„å»ºä¸€äº›ç§¯åˆ†å™¨ç½‘ç»œ.</p>
</blockquote>
<blockquote>
<p>A special set of networks generate continuous-attractor dynamics without pattern formation: those with linear, planar or hyperplanar attractors that are generated by neurons with linear or near-linear response functions.</p>
<p>In circuits of linear neurons, the feedback within the network is a linear function of activity ($Wr$, where $W$ is the weight matrix and $r$ are the neural activities), as is the activity decay (given by $âˆ’r$). Such networks can stabilize non-zero activity states simply by tuning positive feedback to cancel the decay. The matrix $W$ can direct feedback in state space; if feedback is directed largely along one dimension, the network can support a line attractor (Fig. 1e). If it is directed equally along two or more dimensions, it can support a plane or hyperplane attractor. To create long-lived attractors requires that the network feedback magnitude is finely tuned to precisely cancel the decay, in contrast to pattern-forming continuous-attractor systems where the weight shapes (but not magnitudes) are tuned to maintain continuous symmetry across neurons.</p>
</blockquote>
<p>ä¸€ç»„ç‰¹æ®Šçš„ç½‘ç»œåœ¨æ²¡æœ‰æ¨¡å¼å½¢æˆçš„æƒ…å†µä¸‹äº§ç”Ÿè¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦: é‚£äº›ç”±å…·æœ‰çº¿æ€§æˆ–è¿‘çº¿æ€§å“åº”å‡½æ•°çš„ç¥ç»å…ƒç”Ÿæˆçš„çº¿æ€§ã€å¹³é¢æˆ–è¶…å¹³é¢å¸å¼•å­.</p>
<p>åœ¨çº¿æ€§ç¥ç»å…ƒå›è·¯ä¸­, ç½‘ç»œå†…çš„åé¦ˆæ˜¯æ´»åŠ¨çš„çº¿æ€§å‡½æ•° ($Wr$, å…¶ä¸­ $W$ æ˜¯æƒé‡çŸ©é˜µ, $r$ æ˜¯ç¥ç»æ´»åŠ¨) , æ´»åŠ¨è¡°å‡ä¹Ÿæ˜¯å¦‚æ­¤ (ç”± $âˆ’r$ ç»™å‡º).  é€šè¿‡è°ƒèŠ‚æ­£åé¦ˆä»¥æŠµæ¶ˆè¡°å‡, è¿™æ ·çš„ç½‘ç»œå¯ä»¥ç¨³å®šéé›¶æ´»åŠ¨çŠ¶æ€. çŸ©é˜µ $W$ å¯ä»¥åœ¨çŠ¶æ€ç©ºé—´ä¸­å¼•å¯¼åé¦ˆ; å¦‚æœåé¦ˆä¸»è¦æ²¿ä¸€ä¸ªç»´åº¦å¼•å¯¼, ç½‘ç»œå¯ä»¥æ”¯æŒçº¿æ€§å¸å¼•å­ (å›¾ 1e).  å¦‚æœå®ƒåœ¨ä¸¤ä¸ªæˆ–æ›´å¤šç»´åº¦ä¸Šå‡åŒ€å¼•å¯¼, å®ƒå¯ä»¥æ”¯æŒå¹³é¢æˆ–è¶…å¹³é¢å¸å¼•å­. è¦åˆ›å»ºé•¿å¯¿å‘½å¸å¼•å­, è¦æ±‚ç½‘ç»œåé¦ˆå¹…åº¦è¢«ç²¾ç»†è°ƒåˆ¶ä»¥ç²¾ç¡®æŠµæ¶ˆè¡°å‡, è¿™ä¸æ¨¡å¼å½¢æˆçš„è¿ç»­å¸å¼•å­ç³»ç»Ÿå½¢æˆå¯¹æ¯”, åœ¨è¿™äº›ç³»ç»Ÿä¸­, æƒé‡å½¢çŠ¶ (è€Œä¸æ˜¯å¹…åº¦) è¢«è°ƒåˆ¶ä»¥ä¿æŒç¥ç»å…ƒä¹‹é—´çš„è¿ç»­å¯¹ç§°æ€§.</p>
<blockquote>
<p>ç³»ç»Ÿçš„çº¿æ€§åŠ¨åŠ›å­¦:</p>
<p>$$
\frac{\mathrm{d}r}{\mathrm{d}t} = Wr - r
$$</p>
<p>$W$ çš„ä¸º 1 çš„ç‰¹å¾å€¼æ•°é‡ç¡®å®šäº†å¸å¼•å­çš„ç»´æ•°. æ¯”å¦‚çº¿å¸å¼•å­(1 ä¸ª), å¹³é¢å¸å¼•å­(2 ä¸ª)&hellip;</p>
<p>$&gt;1$ ä»£è¡¨æ¿€æ´»å‘æ•£, $&lt;1$ ä»£è¡¨æ¿€æ´»è¡°å‡.</p>
</blockquote>
<h2 id="non-stationary-continuous-attractors">Non-stationary continuous attractors<a hidden class="anchor" aria-hidden="true" href="#non-stationary-continuous-attractors">#</a></h2>
<blockquote>
<p>Large non-symmetric networks with nonlinear neurons and strong connectivity generically exhibit <strong>limit-cycle attractors</strong> or <strong>chaotic dynamics</strong>.</p>
<p>Just as point attractors emerge generically in large networks with strong symmetric weights and bounded state spaces, <strong>chaotic attractors</strong> emerge generically in large recurrent networks with strong asymmetric weights.</p>
<p>Adequate asymmetries are easily achieved if excitatory and inhibitory synapses emerge from distinct sets of neurons, as biologically necessitated by Daleâ€™s law.</p>
</blockquote>
<p>å«æœ‰éçº¿æ€§ç¥ç»å…ƒå’Œå¼ºè¿æ¥æ€§çš„éå¯¹ç§°å¤§å‹ç½‘ç»œé€šå¸¸è¡¨ç°å‡º <strong>æé™ç¯å¸å¼•å­</strong> æˆ– <strong>æ··æ²ŒåŠ¨åŠ›å­¦</strong>.</p>
<p>æ­£å¦‚åœ¨å…·æœ‰å¼ºå¯¹ç§°æƒé‡å’Œæœ‰ç•ŒçŠ¶æ€ç©ºé—´çš„å¤§å‹ç½‘ç»œä¸­ç‚¹å¸å¼•å­æ™®éå‡ºç°ä¸€æ ·, åœ¨å…·æœ‰å¼ºéå¯¹ç§°æƒé‡çš„å¤§å‹é€’å½’ç½‘ç»œä¸­ <strong>æ··æ²Œå¸å¼•å­</strong> ä¹Ÿæ™®éå‡ºç°.</p>
<p>å¦‚æœå…´å¥‹æ€§å’ŒæŠ‘åˆ¶æ€§çªè§¦æ¥è‡ªä¸åŒçš„ç¥ç»å…ƒç»„ (æ­£å¦‚ Dale å®šå¾‹åœ¨ç”Ÿç‰©å­¦ä¸Šæ‰€å¿…éœ€çš„é‚£æ ·) , åˆ™å¯ä»¥è½»æ¾å®ç°è¶³å¤Ÿçš„éå¯¹ç§°æ€§.</p>
<blockquote>
<p>Despite the complexity of chaotic dynamics, chaotic attractors are also highly structured in that they typically exist in a relatively low number of dimensions compared with the number of neurons in the network.</p>
<p>Non-symmetric networks that are dominated by inhibition exhibit a single attractor at zero activity, although the flow towards the attractor in response to perturbations can involve large transients in neural activation that temporarily move the state further away from the attractor.</p>
</blockquote>
<p>å°½ç®¡æ··æ²ŒåŠ¨åŠ›å­¦å¤æ‚, ä½†æ··æ²Œå¸å¼•å­ä¹Ÿå…·æœ‰é«˜åº¦ç»“æ„åŒ–çš„ç‰¹å¾, å› ä¸ºå®ƒä»¬é€šå¸¸å­˜åœ¨äºç›¸æ¯” ç½‘ç»œä¸­ç¥ç»å…ƒçš„æ•°é‡ ä½å¾—å¤šçš„ç»´æ•°ä¸­.</p>
<p>ä»¥æŠ‘åˆ¶ä¸ºä¸»å¯¼çš„éå¯¹ç§°ç½‘ç»œåœ¨é›¶æ¿€æ´»ä¸‹å±•ç°å‡ºå•ä¸€å¸å¼•å­, å°½ç®¡å“åº”æ‰°åŠ¨æ—¶, æœå‘å¸å¼•å­çš„æµåŠ¨å¯èƒ½æ¶‰åŠç¥ç»æ¿€æ´»ä¸­çš„å¤§ç¬æ€, è¿™äº›ç¬æ€ä¼šæš‚æ—¶ä½¿çŠ¶æ€è¿œç¦»å¸å¼•å­.</p>
<h1 id="attractors-for-neural-computation">Attractors for neural computation<a hidden class="anchor" aria-hidden="true" href="#attractors-for-neural-computation">#</a></h1>
<blockquote>
<p>A system could theoretically be perfectly <strong>tuned</strong> such that every point in state space is a neutrally stable attractor, and thus the system has maximally high-dimensional attractor dynamics.</p>
<p>However, because the robustness of attractor networks is related to the low-dimensionality of the attractor states (as discussed below), the system would lose most of its interesting computational properties: error correction or noise tolerance, nearest-neighbour computation, pattern completion and content-addressable memory. It could perform integration, but with no robustness to noise.</p>
<p>As such, networks with low-dimensional attractor dynamics exhibit myriad properties that can be vital for computation in the brain These include robust representation, memory, sequence generation, integration, and robust classification and decision-making ideas that have been extensively explored in the literature.</p>
<p>In a later section, we describe how, although attractor dynamics may be rigid and invariant as needed for the roles listed above, recent theoretical and experimental findings are beginning to reveal how these <strong>rigid constructions</strong> may also be exploited to perform flexible computation through reuse and recombination across tasks.</p>
</blockquote>
<p>ä¸€ä¸ªç³»ç»Ÿç†è®ºä¸Šå¯ä»¥è¢«å®Œç¾ <strong>è°ƒåˆ¶</strong>, ä½¿å¾—çŠ¶æ€ç©ºé—´ä¸­çš„æ¯ä¸ªç‚¹éƒ½æ˜¯ä¸€ä¸ªä¸­æ€§ç¨³å®šçš„å¸å¼•å­, å› æ­¤ç³»ç»Ÿå…·æœ‰æœ€å¤§ç»´åº¦çš„å¸å¼•å­åŠ¨åŠ›å­¦.</p>
<p>ç„¶è€Œ, ç”±äºå¸å¼•å­ç½‘ç»œçš„ç¨³å¥æ€§ä¸å¸å¼•å­çŠ¶æ€çš„ä½ç»´æ€§æœ‰å…³ (å¦‚ä¸‹æ‰€è¿°) , ç³»ç»Ÿå°†å¤±å»å…¶å¤§éƒ¨åˆ†æœ‰è¶£çš„è®¡ç®—å±æ€§: é”™è¯¯çº æ­£æˆ–å™ªå£°å®¹å¿ã€æœ€è¿‘é‚»è®¡ç®—ã€æ¨¡å¼å®Œæˆå’Œå†…å®¹å¯å¯»å€è®°å¿†. å®ƒå¯ä»¥æ‰§è¡Œç§¯åˆ†, ä½†å¯¹å™ªå£°æ²¡æœ‰é²æ£’æ€§.</p>
<p>å› æ­¤, å…·æœ‰ä½ç»´å¸å¼•å­åŠ¨åŠ›å­¦çš„ç½‘ç»œè¡¨ç°å‡ºæ— æ•°å¯¹äºå¤§è„‘è®¡ç®—è‡³å…³é‡è¦çš„å±æ€§. è¿™äº›åŒ…æ‹¬ç¨³å¥çš„è¡¨ç¤ºã€è®°å¿†ã€åºåˆ—ç”Ÿæˆã€ç§¯åˆ†, ä»¥åŠåœ¨æ–‡çŒ®ä¸­å·²è¢«å¹¿æ³›æ¢ç´¢çš„ç¨³å¥åˆ†ç±»å’Œå†³ç­–æ€æƒ³.</p>
<p>åœ¨åé¢çš„ç« èŠ‚ä¸­, æˆ‘ä»¬æè¿°äº†å°½ç®¡å¸å¼•å­åŠ¨åŠ›å­¦å¯èƒ½æ˜¯åˆšæ€§çš„ä¸”ä¸å˜çš„, ä»¥æ»¡è¶³ä¸Šè¿°éœ€è¦, ä½†æœ€è¿‘çš„ç†è®ºå’Œå®éªŒå‘ç°å¼€å§‹æ­ç¤ºå¦‚ä½•åˆ©ç”¨è¿™äº› <strong>åˆšæ€§ç»“æ„</strong> é€šè¿‡è·¨ä»»åŠ¡çš„å¤ç”¨å’Œé‡ç»„æ¥æ‰§è¡Œçµæ´»è®¡ç®—.</p>
<h2 id="representation-and-memory">Representation and memory<a hidden class="anchor" aria-hidden="true" href="#representation-and-memory">#</a></h2>
<blockquote>
<p>A representation of a set of inputs means the assignment of inputs to representational states (not necessarily on a one-to-one basis), with the ability to reproducibly retrieve those states (â€˜labelsâ€™) when cued.</p>
<p>Attractor networks provide a stable internal set of states that can be used for reproducible representation of discrete or analogue variables, by mapping states in the world to the attractor states. One way to achieve this mapping is through a <strong>feedforward learning process</strong> that associates each external state with an internal attractor state (Fig. 2a).</p>
</blockquote>
<p>å¯¹ä¸€ç»„è¾“å…¥çš„è¡¨ç¤ºæ„å‘³ç€å°†è¾“å…¥åˆ†é…ç»™è¡¨ç¤ºçŠ¶æ€ (ä¸ä¸€å®šæ˜¯ä¸€å¯¹ä¸€çš„åŸºç¡€) , å¹¶ä¸”åœ¨æç¤ºæ—¶èƒ½å¤Ÿå¯é‡å¤åœ°æ£€ç´¢è¿™äº›çŠ¶æ€ ( &ldquo;æ ‡ç­¾&rdquo; ).</p>
<p>å¸å¼•å­ç½‘ç»œæä¾›äº†ä¸€ç»„ç¨³å®šçš„å†…éƒ¨çŠ¶æ€, å¯ç”¨äºé€šè¿‡å°†ä¸–ç•Œä¸­çš„çŠ¶æ€æ˜ å°„åˆ°å¸å¼•å­çŠ¶æ€æ¥å¯é‡å¤åœ°è¡¨ç¤ºç¦»æ•£æˆ–æ¨¡æ‹Ÿå˜é‡. å®ç°è¿™ç§æ˜ å°„çš„ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡ <strong>å‰é¦ˆå­¦ä¹ è¿‡ç¨‹</strong>, å°†æ¯ä¸ªå¤–éƒ¨çŠ¶æ€ä¸å†…éƒ¨å¸å¼•å­çŠ¶æ€ç›¸å…³è” (å›¾ 2a).</p>
<blockquote>
<p>An attractor network can exhibit two kinds of memory.</p>
<p>The first is in the structure of the weights, which specify the set of all attractors. If these weights are specified through an input-driven learning process, this is a form of long-term memory about the inputs.</p>
<p>The second kind of memory is the ability to maintain persistent activity in a stationary attractor state: if a system with multiple stationary attractor states is initialized in one of them, it will tend to remain at or near the same state for some time. In other words, the activation levels of the neurons contributing to that state persist while the system remains in the state. This persistent activity response is thus a form of short-term memory of the input that initialized the circuit. If these persistent memory states can be activated without an <strong>explicit address</strong>, using just the content (or partial content) of the memory, they are content-addressable.</p>
</blockquote>
<p>å¸å¼•å­ç½‘ç»œå¯ä»¥è¡¨ç°å‡ºä¸¤ç§è®°å¿†.</p>
<p>ç¬¬ä¸€ç§æ˜¯å­˜å‚¨äºæƒé‡çš„ç»“æ„, å®ƒç¡®å®šäº†æ‰€æœ‰å¸å¼•å­çŠ¶æ€çš„é›†åˆ. å¦‚æœè¿™äº›æƒé‡æ˜¯é€šè¿‡è¾“å…¥é©±åŠ¨çš„å­¦ä¹ è¿‡ç¨‹ç¡®å®šçš„, è¿™æ˜¯ä¸€ç§å…³äºè¾“å…¥çš„é•¿æœŸè®°å¿†å½¢å¼.</p>
<p>ç¬¬äºŒç§è®°å¿†æ¥è‡ªç½‘ç»œåœ¨ä¸€é™æ­¢å¸å¼•å­çŠ¶æ€ä¸­ç»´æŒæŒç»­æ´»åŠ¨çš„èƒ½åŠ›: å¦‚æœä¸€ç³»ç»Ÿå…·æœ‰è‹¥å¹²é™æ­¢å¸å¼•å­çŠ¶æ€, å¹¶ä¸”è®¾ç½®åˆæ€ä¸ºå…¶ä¸­ä¸€ä¸ªå¸å¼•å­çŠ¶æ€, å®ƒå°†å€¾å‘äºåœ¨ä¸€æ®µæ—¶é—´å†…ä¿æŒåœ¨è¯¥çŠ¶æ€æˆ–é™„è¿‘. æ¢å¥è¯è¯´, è´¡çŒ®äºè¯¥çŠ¶æ€çš„ç¥ç»å…ƒçš„æ¿€æ´»æ°´å¹³åœ¨ç³»ç»Ÿä¿æŒåœ¨è¯¥çŠ¶æ€æ—¶æŒç»­å­˜åœ¨. å› æ­¤, è¿™ç§æŒç»­çš„æ´»åŠ¨å“åº”æ˜¯ä¸€ç§å¯¹åˆå§‹åŒ–å›è·¯çš„è¾“å…¥çš„çŸ­æœŸè®°å¿†å½¢å¼. å¦‚æœè¿™äº›æŒç»­çš„è®°å¿†çŠ¶æ€å¯ä»¥åœ¨æ²¡æœ‰ <strong>æ˜¾å¼åœ°å€</strong> çš„æƒ…å†µä¸‹è¢«æ¿€æ´», ä»…ä½¿ç”¨è®°å¿†çš„å†…å®¹ (æˆ–éƒ¨åˆ†å†…å®¹) , åˆ™å®ƒä»¬æ˜¯å†…å®¹å¯å¯»å€çš„.</p>
<blockquote>
<p>The short-term memory function of attractors depends on the prior formation of stable states through long-term plasticity.</p>
<p>For instance, in Hopfield-like networks, states cannot persist if they were not first trained to be attractor states. Even models of short-term memory that are based on <strong>presynaptic facilitation</strong>, rather than persistent activity, rely implicitly on prior long-term associative plasticity to construct recurrently stabilized neural ensembles that can be reinstated by random inputs. (Additionally, these models are not activity-silent in the delay period, in the sense that they would require ongoing activity to refresh the facilitation state over longer delays and to generate robustness against random background activity that would facilitate different synapses.)</p>
<p>In other words, these presynaptic facilitation models cannot  explain short-term memory for entirely novel inputs; however, combinations of attractors could enable more flexible short-term memory, as we discuss later.</p>
</blockquote>
<p>å¸å¼•å­çš„çŸ­æœŸè®°å¿†åŠŸèƒ½ä¾èµ–äºé€šè¿‡é•¿æœŸå¯å¡‘æ€§å½¢æˆç¨³å®šçŠ¶æ€çš„å…ˆéªŒ.</p>
<p>ä¾‹å¦‚, åœ¨ Hopfield ç±»ç½‘ç»œä¸­, å¦‚æœçŠ¶æ€æ²¡æœ‰é¦–å…ˆè¢«è®­ç»ƒä¸ºå¸å¼•å­çŠ¶æ€, åˆ™å®ƒä»¬æ— æ³•æŒç»­å­˜åœ¨. å³ä½¿æ˜¯åŸºäº <strong>çªè§¦å‰æ˜“åŒ–</strong> è€Œä¸æ˜¯æŒç»­æ´»åŠ¨çš„çŸ­æœŸè®°å¿†æ¨¡å‹, ä¹Ÿéšå¼åœ°ä¾èµ–äºå…ˆå‰çš„é•¿æœŸè”æƒ³å¯å¡‘æ€§æ¥æ„å»ºå¯ä»¥é€šè¿‡éšæœºè¾“å…¥é‡æ–°å»ºç«‹çš„é€’å½’ç¨³å®šç¥ç»å…ƒé›†åˆ.  (æ­¤å¤–, è¿™äº›æ¨¡å‹åœ¨å»¶è¿ŸæœŸé—´å¹¶éæ´»åŠ¨é™é»˜, å› ä¸ºå®ƒä»¬éœ€è¦æŒç»­çš„æ´»åŠ¨æ¥åˆ·æ–°ä¿ƒè¿›çŠ¶æ€ä»¥åº”å¯¹æ›´é•¿çš„å»¶è¿Ÿ, å¹¶ç”Ÿæˆå¯¹éšæœºèƒŒæ™¯æ´»åŠ¨çš„é²æ£’æ€§, è¿™äº›æ´»åŠ¨ä¼šä¿ƒè¿›ä¸åŒçš„çªè§¦. )</p>
<p>æ¢å¥è¯è¯´, è¿™äº›çªè§¦å‰ä¿ƒè¿›æ¨¡å‹æ— æ³•è§£é‡Šå¯¹å®Œå…¨æ–°é¢–è¾“å…¥çš„çŸ­æœŸè®°å¿†; ç„¶è€Œ, æ­£å¦‚æˆ‘ä»¬ç¨åè®¨è®ºçš„é‚£æ ·, å¸å¼•å­çš„ç»„åˆå¯ä»¥å®ç°æ›´çµæ´»çš„çŸ­æœŸè®°å¿†.</p>
<h2 id="de-noising-representations-and-memories">De-noising representations and memories<a hidden class="anchor" aria-hidden="true" href="#de-noising-representations-and-memories">#</a></h2>
<blockquote>
<p>If representational states are attractors, then the representations are robust in the sense that they perform de-noising: if the input cues or initial conditions reflect noisy or corrupted versions of an attractor state, the dynamics drive the state to a point on the representational attractor (Fig. 2b, inset).</p>
<p>When attractors form a continuous manifold of dimension $K\ll N$, where $N$ is the number of neurons in the circuit, all noise in $N-K$ dimensions is erased. A noise ball of unit radius in $N$ dimensions (corresponding to random independent noise per neuron) has a projection of size only $\sim\sqrt{K/N}\ll 1$ along $K$ dimensions.</p>
<p>If $K$ is low-dimensional, as is often the case, and $N$ ranges from $10^2$ to $10^7$ as estimated before for common microcircuits, this constitutes a massive reduction in the sensitivity of the state to internal or input noise (Fig. 2b). Thus, most noise is rendered impotent by attractor dynamics.</p>
</blockquote>
<p>å¦‚æœè¡¨è±¡çŠ¶æ€æ˜¯å¸å¼•å­, é‚£ä¹ˆè¡¨è±¡åœ¨æŸç§æ„ä¹‰ä¸Šæ˜¯ç¨³å¥çš„, å› ä¸ºå®ƒä»¬æ‰§è¡Œå»å™ª: å¦‚æœè¾“å…¥æç¤ºæˆ–åˆå§‹æ¡ä»¶åæ˜ äº†åŠ å™ªæˆ–æŸåçš„å¸å¼•å­çŠ¶æ€, åŠ¨åŠ›å­¦ä¼šå°†çŠ¶æ€é©±åŠ¨åˆ°è¡¨è±¡çš„å¸å¼•å­ (å›¾ 2b, æ’å›¾).</p>
<p>å½“å¸å¼•å­å½¢æˆç»´åº¦ä¸º $K\ll N$ çš„è¿ç»­æµå½¢æ—¶, å…¶ä¸­ $N$ æ˜¯å›è·¯ä¸­ç¥ç»å…ƒçš„æ•°é‡, $N-K$ ç»´ä¸­çš„æ‰€æœ‰å™ªå£°éƒ½è¢«æŠ¹å». åœ¨ $N$ ç»´ä¸­çš„å•ä½åŠå¾„å™ªå£°çƒ (å¯¹åº”äºæ¯ä¸ªç¥ç»å…ƒçš„éšæœºç‹¬ç«‹å™ªå£°) åœ¨ $K$ ç»´ä¸Šçš„æŠ•å½±å¤§å°ä»…ä¸º $\sim\sqrt{K/N}\ll 1$.</p>
<p>å¦‚æœ $K$ æ˜¯ä½ç»´æ•°, æ­£å¦‚é€šå¸¸æƒ…å†µä¸€æ ·, å¹¶ä¸” $N$ èŒƒå›´ä»ä¹‹å‰ä¼°è®¡çš„å¸¸è§å¾®å›è·¯çš„ $10^{2}$ åˆ° $10^{7}$, è¿™æ„æˆäº†å¯¹çŠ¶æ€å¯¹å†…éƒ¨æˆ–è¾“å…¥å™ªå£°æ•æ„Ÿæ€§çš„å·¨å¤§é™ä½ (å›¾ 2b).  å› æ­¤, å¤§å¤šæ•°å™ªå£°é€šè¿‡å¸å¼•å­åŠ¨åŠ›å­¦å˜å¾—æ— æ•ˆ.</p>
<blockquote>
<p>De-noising owing to attractor dynamics is especially important for memory maintenance as, otherwise, noise-induced deviations would accumulate and grow over time.</p>
<p>Discrete attractors continually erase all noise by mapping perturbed states back to the point attractor, resulting in zero drift. With continuous attractors as memory states, all noise orthogonal to the manifold is corrected; thus, there is a net reduction of the effects of noise by the factor $\sim\sqrt{K/N}\ll 1$ (refs.45,55).</p>
<p>However, all states on the attractor manifold are neutrally stable, so the state can drift along the attractor. As such, components of noise along the $K$ attractor dimensions are not internally corrected and cause an accumulating drift away from the initial state, with variance proportional to $KT/N$, where $T$ is the elapsed time. Thus, through the $1/N$ decrease in variance, even continuous memory states can be well stabilized in sufficiently large attractor networks.</p>
</blockquote>
<p>å¸å¼•å­åŠ¨åŠ›å­¦å¼•èµ·çš„å»å™ªå¯¹äºè®°å¿†ç»´æŒå°¤ä¸ºé‡è¦, å¦åˆ™, å™ªå£°å¼•èµ·çš„åå·®ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œç§¯ç´¯å’Œå¢é•¿.</p>
<p>ç¦»æ•£å¸å¼•å­é€šè¿‡å°†æ‰°åŠ¨çŠ¶æ€æ˜ å°„å›ç‚¹å¸å¼•å­æ¥ä¸æ–­æŠ¹å»æ‰€æœ‰å™ªå£°, äº§ç”Ÿé›¶æ¼‚ç§». å¯¹äºä½œä¸ºè®°å¿†çŠ¶æ€çš„è¿ç»­å¸å¼•å­, æ‰€æœ‰æ­£äº¤äºæµå½¢çš„å™ªå£°éƒ½è¢«çº æ­£; å› æ­¤, å™ªå£°æ•ˆåº”å‡€å‡å°‘äº† $\sim\sqrt{K/N}\ll 1$ çš„å› å­ (å‚è€ƒæ–‡çŒ® 45ã€55).</p>
<p>ç„¶è€Œ, å¸å¼•å­æµå½¢ä¸Šçš„æ‰€æœ‰çŠ¶æ€éƒ½æ˜¯ä¸­æ€§ç¨³å®šçš„, å› æ­¤çŠ¶æ€å¯ä»¥æ²¿ç€å¸å¼•å­æ¼‚ç§». å› æ­¤, æ²¿ç€ $K$ å¸å¼•å­ç»´åº¦çš„å™ªå£°åˆ†é‡ä¸ä¼šè¢«å†…éƒ¨çº æ­£, å¹¶å¯¼è‡´è¿œç¦»åˆå§‹çŠ¶æ€çš„ç´¯ç§¯æ¼‚ç§», å…¶æ–¹å·®ä¸ $KT/N$ æˆæ­£æ¯”, å…¶ä¸­ $T$ æ˜¯ç»è¿‡çš„æ—¶é—´. å› æ­¤, é€šè¿‡ $1/N$ æ–¹å·®çš„é™ä½, å³ä½¿æ˜¯è¿ç»­çš„è®°å¿†çŠ¶æ€ä¹Ÿå¯ä»¥åœ¨è¶³å¤Ÿå¤§çš„å¸å¼•å­ç½‘ç»œä¸­å¾—åˆ°å¾ˆå¥½çš„ç¨³å®š.</p>
<blockquote>
<p>Although content-addressable long-term memory and error reduction can be instantiated through feedforward computations involving only a few steps in place of attractor dynamics, recurrent attractor dynamics are indispensable for the generation of persistent activity states (and thus for short-term memory through persistent activity) and integration, as we discuss below.</p>
</blockquote>
<p>å°½ç®¡å†…å®¹å¯å¯»å€çš„é•¿æœŸè®°å¿†å’Œé”™è¯¯å‡å°‘å¯ä»¥é€šè¿‡æ¶‰åŠä»…å‡ ä¸ªæ­¥éª¤çš„å‰é¦ˆè®¡ç®—æ¥å®ç°, è€Œä¸æ˜¯å¸å¼•å­åŠ¨åŠ›å­¦, ä½†æ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·, é€’å½’å¸å¼•å­åŠ¨åŠ›å­¦å¯¹äºç”ŸæˆæŒç»­æ´»åŠ¨çŠ¶æ€ (å› æ­¤é€šè¿‡æŒç»­æ´»åŠ¨è¿›è¡ŒçŸ­æœŸè®°å¿†) å’Œç§¯åˆ†æ˜¯ä¸å¯æˆ–ç¼ºçš„.</p>
<h2 id="robust-classification">Robust classification<a hidden class="anchor" aria-hidden="true" href="#robust-classification">#</a></h2>
<blockquote>
<p>When there are finitely many separated attractors (each a discrete attractor or a continuous manifold), states that are not initially on one of the attractors will flow to one of the attractors. An input to the network can then be classified according to the attractor to which the network state flows after initialization by the input. We can now identify inputs based on  the attractors they flow to, a mechanism of classification. If the dynamics of the network further correctly assign corrupted versions of an input to the same attractor state as the uncorrupted input, this constitutes robust classification.</p>
<p>In other words, the dynamical basins of attraction of the network must align with the Voronoi regions of the attractor states (that is, corrupted inputs that are closest in distance to one of the uncorrupted inputs should flow to that inputâ€™s attractor through the dynamics and not another). This is approximately the case for attractor networks operating well below capacity, but typically deteriorates when attractor networks are pushed towards their capacity.</p>
</blockquote>
<p>å½“å­˜åœ¨æœ‰é™æ•°é‡çš„åˆ†ç¦»å¸å¼•å­ (æ¯ä¸ªéƒ½æ˜¯ç¦»æ•£å¸å¼•å­æˆ–è¿ç»­æµå½¢) æ—¶, æœ€åˆä¸åœ¨å…¶ä¸­ä¸€ä¸ªå¸å¼•å­ä¸Šçš„çŠ¶æ€å°†æµå‘å…¶ä¸­ä¸€ä¸ªå¸å¼•å­. ç„¶å, å¯ä»¥æ ¹æ®ç½‘ç»œçŠ¶æ€åœ¨è¾“å…¥åˆå§‹åŒ–åæµå‘çš„å¸å¼•å­å¯¹ç½‘ç»œçš„è¾“å…¥è¿›è¡Œåˆ†ç±». æˆ‘ä»¬ç°åœ¨å¯ä»¥æ ¹æ®å®ƒä»¬æµå‘çš„å¸å¼•å­æ¥è¯†åˆ«è¾“å…¥, è¿™æ˜¯ä¸€ç§åˆ†ç±»æœºåˆ¶. å¦‚æœç½‘ç»œçš„åŠ¨åŠ›å­¦è¿›ä¸€æ­¥å°†è¾“å…¥çš„æŸåç‰ˆæœ¬æ­£ç¡®åœ°åˆ†é…ç»™ä¸æœªæŸåè¾“å…¥ç›¸åŒçš„å¸å¼•å­çŠ¶æ€, è¿™å°±æ„æˆäº†ç¨³å¥åˆ†ç±».</p>
<p>æ¢å¥è¯è¯´, ç½‘ç»œçš„åŠ¨åŠ›è°·åœ°å¿…é¡»ä¸å¸å¼•å­çŠ¶æ€çš„ Voronoi åŒºåŸŸå¯¹é½ (å³, ä¸æœªæŸåè¾“å…¥è·ç¦»æœ€è¿‘çš„æŸåè¾“å…¥åº”è¯¥é€šè¿‡åŠ¨åŠ›å­¦æµå‘è¯¥è¾“å…¥çš„å¸å¼•å­, è€Œä¸æ˜¯å¦ä¸€ä¸ª).  å¯¹äºåœ¨å®¹é‡ä»¥ä¸‹è‰¯å¥½è¿è¡Œçš„å¸å¼•å­ç½‘ç»œæ¥è¯´, è¿™å¤§è‡´æ˜¯æ­£ç¡®çš„, ä½†å½“å¸å¼•å­ç½‘ç»œè¢«æ¨å‘å…¶å®¹é‡æ—¶é€šå¸¸ä¼šæ¶åŒ–.</p>
<h2 id="integration">Integration<a hidden class="anchor" aria-hidden="true" href="#integration">#</a></h2>
<blockquote>
<p>Single neurons integrate their inputs, but usually can only do this over the timescales associated with their membrane capacitances, typically 10-100 ms. Continuous-attractor dynamics can enable neural circuits to integrate over much longer timescales (in the order of about 1-100 s).</p>
</blockquote>
<p>å•ä¸ªç¥ç»å…ƒç§¯åˆ†å®ƒä»¬çš„è¾“å…¥, ä½†é€šå¸¸åªèƒ½åœ¨ä¸å…¶è†œç”µå®¹ç›¸å…³çš„æ—¶é—´å°ºåº¦ä¸Šè¿›è¡Œç§¯åˆ†, é€šå¸¸ä¸º 10-100 æ¯«ç§’. è¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦å¯ä»¥ä½¿ç¥ç»å›è·¯åœ¨æ›´é•¿çš„æ—¶é—´å°ºåº¦ä¸Šè¿›è¡Œç§¯åˆ† (å¤§çº¦ 1-100 ç§’).</p>
<blockquote>
<p>A pattern-forming continuous-attractor network requires an additional mechanism to gain the functionality of an integrator: a way to shift the internal state along the attractor in response to an input that encodes changes in the external variable (Fig. 2d, left).</p>
<p>Conceptually, the simplest way to build a shift mechanism is by a <strong>copy-and-offset construction</strong>: construct multiple copies or subpopulations of the attractor network, each with slightly offset (asymmetric) weights in the sense that active neurons centre their excitation or point of maximal disinhibition slightly offset from themselves on the neural sheet (for example, see that the network in Fig. 1g is a slightly asymmetric version of the network in Fig. 1c). The states in each such network will then form a <strong>limit-cycle attractor</strong>, with patterns of activity flowing in the direction of the asymmetry in each copy.</p>
<p>If opposing copies are coupled together, the pattern is stabilized through a push-pull balance. A velocity input whose components project differentially to the copies will break the push-pull balance, driving the pattern along the flow direction of the more active copy (Fig. 1g).</p>
<p>Thus, the total direction and magnitude of the shift of the pattern, corresponding to movement along the attractor manifold, represents the time integral of the velocity input to the network. This common principle unifies the mechanisms across diverse integrator models.</p>
</blockquote>
<p>å½¢æˆæ¨¡å¼çš„è¿ç»­å¸å¼•å­ç½‘ç»œéœ€è¦ä¸€ä¸ªé¢å¤–çš„æœºåˆ¶æ¥ä½¿è·å¾—ç§¯åˆ†å™¨çš„åŠŸèƒ½: ä¸€ç§ç¼–ç  å¤–éƒ¨å˜é‡ å˜åŒ–é‡çš„è¾“å…¥, æ²¿å¸å¼•å­å¹³ç§»å†…éƒ¨çŠ¶æ€çš„æ–¹æ³• (å›¾ 2d, å·¦).</p>
<p>ä»æ¦‚å¿µä¸Šè®², æ„å»ºå¹³ç§»æœºåˆ¶çš„æœ€ç®€å•æ–¹æ³•æ˜¯é€šè¿‡ <strong>å¤åˆ¶å’Œåç§»ç»“æ„</strong>: æ„å»ºå¤šä¸ªå¸å¼•å­ç½‘ç»œçš„å¤‡ä»½æˆ–å­ç¾¤ä½“, æ¯ä¸ªå¤‡ä»½å…·æœ‰ç•¥å¾®åç§» (éå¯¹ç§°) æƒé‡, è¿™æ„å‘³ç€æ´»è·ƒç¥ç»å…ƒå°†å…¶å…´å¥‹æˆ–æœ€å¤§å»æŠ‘åˆ¶ç‚¹ç¨å¾®åç¦»ç¥ç»ç‰‡ä¸Šçš„è‡ªèº«ä¸­å¿ƒ (ä¾‹å¦‚, å‚è§å›¾ 1g ä¸­çš„ç½‘ç»œæ˜¯å›¾ 1c ä¸­ç½‘ç»œçš„ç•¥å¾®éå¯¹ç§°ç‰ˆæœ¬).  ç„¶å, æ¯ä¸ªè¿™æ ·çš„ç½‘ç»œä¸­çš„çŠ¶æ€å°†å½¢æˆ <strong>æé™ç¯å¸å¼•å­</strong>, æ¿€æ´»æ¨¡å¼æ²¿ç€æ¯ä¸ªå‰¯æœ¬ä¸­éå¯¹ç§°æ€§çš„æ–¹å‘æµåŠ¨.</p>
<p>å¦‚æœå°†ç›¸åçš„å‰¯æœ¬è€¦åˆåœ¨ä¸€èµ·, é€šè¿‡æ¨-æ‹‰å¹³è¡¡ç¨³å®šæ¨¡å¼. å…¶åˆ†é‡å·®å¼‚æ€§æŠ•å½±åˆ°å‰¯æœ¬çš„é€Ÿåº¦è¾“å…¥å°†æ‰“ç ´æ¨æ‹‰å¹³è¡¡, æ²¿ç€æ›´æ´»è·ƒå‰¯æœ¬çš„æµåŠ¨æ–¹å‘é©±åŠ¨æ¨¡å¼ (å›¾ 1g).</p>
<p>å› æ­¤, æ¨¡å¼çš„æ€»æ–¹å‘å’Œå¹…åº¦åç§», å¯¹åº”äºæ²¿å¸å¼•å­æµå½¢çš„è¿åŠ¨, è¡¨ç¤ºç½‘ç»œå¯¹é€Ÿåº¦è¾“å…¥çš„æ—¶é—´ç§¯åˆ†. è¿™ä¸€å…±åŒåŸç†ç»Ÿä¸€äº†å„ç§ç§¯åˆ†å™¨æ¨¡å‹ä¸­çš„æœºåˆ¶.</p>
<blockquote>
<p>æ•°å­¦ä¸Š</p>
<p>$$
x(t) = x(t_{0}) + \int_{t_{0}}^{t}\dot{x}(\tau)\mathrm{d}\tau
$$</p>
<p>$x$: bump ä½ç½®; $\dot{x}$: bump ç§»åŠ¨é€Ÿåº¦.</p>
</blockquote>
<h2 id="decision-making">Decision-making<a hidden class="anchor" aria-hidden="true" href="#decision-making">#</a></h2>
<blockquote>
<p>If, instead of a velocity signal, the input to an integrator network consisted of temporally varying positive and negative evidence in support of each of two options (Fig. 2d, right) (or in the case of multiple options, evidence vectors instead of velocity vectors), the network would integrate those inputs and thus perform <strong>evidence accumulation</strong>.</p>
</blockquote>
<p>å¦‚æœ, ç§¯åˆ†å™¨ç½‘ç»œçš„è¾“å…¥ä¸æ˜¯é€Ÿåº¦ä¿¡å·, è€Œæ˜¯æ”¯æŒä¸¤ä¸ªé€‰é¡¹ä¸­æ¯ä¸ªé€‰é¡¹çš„æ—¶é—´å˜åŒ–çš„æ­£è´Ÿè¯æ® (å›¾ 2d, å³)  (æˆ–è€…åœ¨å¤šé€‰æƒ…å†µä¸‹, æ˜¯è¯æ®å‘é‡è€Œä¸æ˜¯é€Ÿåº¦å‘é‡) , åˆ™ç½‘ç»œå°†ç§¯åˆ†è¿™äº›è¾“å…¥, ä»è€Œæ‰§è¡Œ <strong>è¯æ®ç§¯ç´¯</strong>.</p>
<blockquote>
<p>è®¾è¯æ®è¾“å…¥ä¸º $e(t) = e_{A}(t) - e_{B}(t)$, åˆ™è¯æ®ç§¯ç´¯ä¸º</p>
<p>$$
s(t) = s(0) + \int_{0}^{t}e(\tau)\mathrm{d}\tau,\quad \text{or }\mathbf{s}(t) = \mathbf{s}(0) + \int_{0}^{t}\mathbf{e}(\tau)\mathrm{d}\tau
$$</p>
</blockquote>
<blockquote>
<p>Decision-making can be viewed as a selection process applied to an integrator that is based on a readout that detects when the integrator state has accumulated enough evidence and moved past a decision threshold.</p>
<p>The selection process can be external to the integrator, in the form of a readout circuit that detects such threshold crossings and outputs the decision.</p>
<p>Alternatively, the selection process can be built into the dynamics of the integrator itself, in the form of a more complex attractor landscape, in which the states move along a continuous attractor but, at some point, the continuous attractor gives way to a pair of discrete attractors, towards which the states flow (Fig. 2e).</p>
<p>Neural WTA models implement such a <strong>hybrid analogue-discrete computation</strong>. The parameters of WTA networks determine the balance between integration dynamics and competitive dynamics, and thus how well the network integrates later evidence: when the network is tuned to be a perfect integrator, its response to inputs is gradual, and small amounts of evidence cause (reversible) flow along the continuous-attractor manifold. In cases in which competition dominates, the response to evidence is a fast flow towards one of the discrete attractors; beyond a point, the flow is nearly irreversible, leading to rapid decision-making and the discounting of later evidence.</p>
</blockquote>
<p>å¯ä»¥å°†å†³ç­–è§†ä¸ºåº”ç”¨äºç§¯åˆ†å™¨çš„é€‰æ‹©è¿‡ç¨‹, åŸºäºæ£€æµ‹ç§¯åˆ†å™¨çŠ¶æ€ä½•æ—¶ç§¯ç´¯è¶³å¤Ÿè¯æ®å¹¶è¶…è¿‡å†³ç­–é˜ˆå€¼çš„è¯»å‡º.</p>
<p>é€‰æ‹©è¿‡ç¨‹å¯ä»¥æ˜¯ç§¯åˆ†å™¨å¤–éƒ¨çš„, ä»¥è¯»å‡ºå›è·¯çš„å½¢å¼æ£€æµ‹æ­¤ç±»é˜ˆå€¼äº¤å‰å¹¶è¾“å‡ºå†³ç­–.</p>
<p>æˆ–è€…, é€‰æ‹©è¿‡ç¨‹å¯ä»¥å†…ç½®äºç§¯åˆ†å™¨æœ¬èº«çš„åŠ¨åŠ›å­¦ä¸­, ä»¥æ›´å¤æ‚çš„å¸å¼•å­æ™¯è§‚çš„å½¢å¼, å…¶ä¸­çŠ¶æ€æ²¿ç€è¿ç»­å¸å¼•å­ç§»åŠ¨, ä½†åœ¨æŸäº›ç‚¹ä¸Š, è¿ç»­å¸å¼•å­è®©ä½äºä¸€å¯¹ç¦»æ•£å¸å¼•å­, çŠ¶æ€æœå‘è¿™äº›å¸å¼•å­æµåŠ¨ (å›¾ 2e).</p>
<p>ç¥ç» WTA æ¨¡å‹å®ç°äº†è¿™ç§ <strong>æ··åˆæ¨¡æ‹Ÿ-ç¦»æ•£è®¡ç®—</strong>. WTA ç½‘ç»œçš„å‚æ•°å†³å®šäº†ç§¯åˆ†åŠ¨åŠ›å­¦å’Œç«äº‰åŠ¨åŠ›å­¦ä¹‹é—´çš„å¹³è¡¡, ä»è€Œå†³å®šäº†ç½‘ç»œæ•´åˆåç»­è¯æ®çš„èƒ½åŠ›: å½“ç½‘ç»œè¢«è°ƒåˆ¶ä¸ºå®Œç¾ç§¯åˆ†å™¨æ—¶, å…¶å¯¹è¾“å…¥çš„å“åº”æ˜¯æ¸è¿›çš„, å°‘é‡è¯æ®ä¼šå¯¼è‡´æ²¿è¿ç»­å¸å¼•å­æµå½¢çš„ (å¯é€†) æµåŠ¨. åœ¨ç«äº‰å ä¸»å¯¼åœ°ä½çš„æƒ…å†µä¸‹, å¯¹è¯æ®çš„å“åº”æ˜¯å¿«é€Ÿæµå‘å…¶ä¸­ä¸€ä¸ªç¦»æ•£å¸å¼•å­; è¶…è¿‡æŸä¸€ç‚¹å, æµåŠ¨å‡ ä¹æ˜¯ä¸å¯é€†çš„, å¯¼è‡´å¿«é€Ÿå†³ç­–å’Œå¯¹åç»­è¯æ®çš„æŠ˜æ‰£.</p>
<blockquote>
<p>Neural WTA networks can leverage specific neural non-linearities to accurately and rapidly (in $\sim \log{(N)}$ time) make the best decision among $N$ alternatives, even if the presented data are noisy (fluctuating over time around their means) and even if the number of options varies over orders of magnitude.</p>
</blockquote>
<p>ç¥ç» WTA ç½‘ç»œå¯ä»¥åˆ©ç”¨ç‰¹å®šçš„ç¥ç»éçº¿æ€§, ä»¥å‡†ç¡®ä¸”å¿«é€Ÿ (åœ¨ $\sim \log{(N)}$ æ—¶é—´å†…) åœ¨ $N$ ä¸ªå¤‡é€‰æ–¹æ¡ˆä¸­åšå‡ºæœ€ä½³å†³ç­–, å³ä½¿æ‰€å‘ˆç°çš„æ•°æ®æ˜¯å˜ˆæ‚çš„ (å›´ç»•å…¶å‡å€¼éšæ—¶é—´æ³¢åŠ¨) , å³ä½¿é€‰é¡¹æ•°é‡å˜åŒ–äº†å‡ ä¸ªæ•°é‡çº§.</p>
<h2 id="sequence-generation">Sequence generation<a hidden class="anchor" aria-hidden="true" href="#sequence-generation">#</a></h2>
<blockquote>
<p>Attractor dynamics can be important for stabilizing another longtimescale behaviour: <strong>the generation of sequences</strong>. Robust sequences can be constructed as low-dimensional limit-cycle attractors, in which high-dimensional perturbations are corrected while along the attractor, there is a systematic, periodic or quasiperiodic flow of states.</p>
<p>The attractor property that affords ongoing de-noising is important for preventing <strong>spatial dispersion</strong> and <strong>temporal dissipation</strong> of the activity packet during sequence generation.</p>
</blockquote>
<p>å¸å¼•å­åŠ¨åŠ›å­¦å¯¹äºç¨³å®šå¦ä¸€ç§é•¿æœŸè¡Œä¸ºä¹Ÿå¾ˆé‡è¦: <strong>åºåˆ—ç”Ÿæˆ</strong>. ç¨³å¥çš„åºåˆ—å¯ä»¥æ„å»ºä¸ºä½ç»´æé™ç¯å¸å¼•å­, åœ¨è¿™ç§å¸å¼•å­ä¸­, é«˜ç»´æ‰°åŠ¨è¢«çº æ­£; æ²¿ç€å¸å¼•å­(æµå½¢), çŠ¶æ€å­˜åœ¨ç³»ç»Ÿçš„ä¸”(å‡†)å‘¨æœŸæ€§çš„æµåŠ¨.</p>
<p>ä¿æŒå»å™ªçš„å¸å¼•å­å±æ€§å¯¹äºé˜²æ­¢åºåˆ—ç”Ÿæˆè¿‡ç¨‹ä¸­æ´»åŠ¨åŒ…çš„ <strong>ç©ºé—´é¢‘æ•£</strong> å’Œ <strong>æ—¶é—´è€—æ•£</strong> éå¸¸é‡è¦.</p>
<blockquote>
<p>Similar to the case for stationary attractor manifolds, the small components of noise along the limit-cycle attractors are not correctable and lead to a gradual accumulation of drift, which for sequence generation is manifest as <strong>timing variability</strong>: the <strong>standard deviation</strong> in the time of reaching the $T$th state in the sequence is predicted to grow as $\sqrt{T}$ for unbiased random drift along the attractor.</p>
</blockquote>
<p>ä¸é™æ­¢å¸å¼•å­æµå½¢çš„æƒ…å†µç±»ä¼¼, å™ªå£°æ²¿æé™ç¯å¸å¼•å­çš„å°åˆ†é‡æ˜¯ä¸å¯çº æ­£çš„, å¹¶å¯¼è‡´é€æ¸ç§¯ç´¯çš„æ¼‚ç§», å¯¹äºåºåˆ—ç”Ÿæˆæ¥è¯´, è¿™è¡¨ç°ä¸º <strong>æ—¶é—´å˜åŒ–æ€§</strong>: é¢„è®¡è¾¾åˆ°åºåˆ—ä¸­ç¬¬ $T$ ä¸ªçŠ¶æ€çš„æ—¶é—´çš„ <strong>æ ‡å‡†åå·®</strong> å°†éšç€æ²¿å¸å¼•å­çš„æ— åç½®éšæœºæ¼‚ç§»è€Œå¢é•¿ä¸º $\sqrt{T}$.</p>
<h1 id="evidence-of-attractors-in-the-brain">Evidence of attractors in the brain<a hidden class="anchor" aria-hidden="true" href="#evidence-of-attractors-in-the-brain">#</a></h1>
<h2 id="criteria-for-attractor-dynamics">Criteria for attractor dynamics<a hidden class="anchor" aria-hidden="true" href="#criteria-for-attractor-dynamics">#</a></h2>
<blockquote>
<p>The fundamental predictions of attractor models centre on the statespace dynamics of the circuit, as initially explicitly discussed and tested in refs.9,15,77,78.</p>
<p>First, a systemâ€™s states should be found localized at or around a low-dimensional set of states that correspond to the attractors in the state space.</p>
<p>Second, a systemâ€™s state should flow quickly back to the low-dimensional state after perturbation.</p>
<p>Third, the set of attractor states â€” quantified either by <em>direct characterization of the full state space</em> or by <em>the relationships between cells</em> â€” should be invariant, persisting over time and after removal of tuned input, across conditions, across behavioural states and even when there are induced variations in the mapping from internal states to external inputs.</p>
<p>Fourth, integrator networks should further exhibit the property of <strong>isometry</strong>, whereby lengths of coding space along a dimension are allocated to equal displacements along a dimension of the external variable.</p>
<p>Additional predictions of attractor dynamics models, that are not as fundamental in the sense that they are not theoretically necessary or sufficient but are nevertheless of high importance because they are highly supportive of the mechanisms of attractor dynamics, are anatomical and structural correlates: the existence of low-dimensional physical structures and directly visible symmetries in connectivity between cells.</p>
</blockquote>
<p>å¸å¼•å­æ¨¡å‹çš„åŸºæœ¬é¢„æµ‹é›†ä¸­åœ¨å›è·¯çš„çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ä¸Š, æœ€åˆåœ¨å‚è€ƒæ–‡çŒ® 9ã€15ã€77ã€78 ä¸­æ˜ç¡®è®¨è®ºå’Œæµ‹è¯•.</p>
<p>é¦–å…ˆ, ç³»ç»Ÿçš„çŠ¶æ€åº”å®šä½åœ¨ä¸çŠ¶æ€ç©ºé—´ä¸­çš„å¸å¼•å­å¯¹åº”çš„ä½ç»´çŠ¶æ€é›†ä¸Šæˆ–é™„è¿‘.</p>
<p>å…¶æ¬¡, ç³»ç»Ÿçš„çŠ¶æ€åœ¨æ‰°åŠ¨ååº”è¿…é€Ÿæµå›ä½ç»´çŠ¶æ€.</p>
<p>ç¬¬ä¸‰, å¸å¼•å­çŠ¶æ€é›†â€”â€”é€šè¿‡ <em>å¯¹å®Œæ•´çŠ¶æ€ç©ºé—´çš„ç›´æ¥è¡¨å¾</em> æˆ–é€šè¿‡ <em>ç»†èƒä¹‹é—´çš„å…³ç³»</em> è¿›è¡Œé‡åŒ–â€”â€”åº”æ˜¯ä¸å˜çš„, åœ¨æ—¶é—´ä¸ŠæŒç»­å­˜åœ¨, å¹¶ä¸”åœ¨å»é™¤è°ƒåˆ¶è¾“å…¥å, åœ¨ä¸åŒæ¡ä»¶ä¸‹ã€ä¸åŒè¡Œä¸ºçŠ¶æ€ä¸‹, ç”šè‡³åœ¨å†…éƒ¨çŠ¶æ€åˆ°å¤–éƒ¨è¾“å…¥çš„æ˜ å°„å‘ç”Ÿè¯±å¯¼å˜åŒ–æ—¶ä¹Ÿæ˜¯å¦‚æ­¤.</p>
<p>ç¬¬å››, ç§¯åˆ†å™¨ç½‘ç»œè¿˜åº”è¡¨ç°å‡º <strong>ç­‰è·æ€§</strong> çš„ç‰¹æ€§, å³ç¼–ç ç©ºé—´æ²¿æŸä¸€ç»´åº¦çš„é•¿åº¦åˆ†é…ç»™å¤–éƒ¨å˜é‡æŸä¸€ç»´åº¦ä¸Šçš„ç›¸ç­‰ä½ç§».</p>
<blockquote>
<p>æ¯”å¦‚, ç½‘ç»œçŠ¶æ€æ²¿ç¯ç§»åŠ¨ 10 åº¦å¯¹åº”çœŸå®å¤´æœå‘ç§»åŠ¨ 10 åº¦. ç½‘ç»œæ´»åŠ¨ç©ºé—´ä¸ç‰©ç†ç©ºé—´å­˜åœ¨ç­‰æ¯”ä¾‹æ˜ å°„.</p>
</blockquote>
<p>æ­¤å¤–ï¼Œå¸å¼•å­åŠ¨åŠ›å­¦æ¨¡å‹è¿˜æœ‰ä¸€äº›é™„åŠ é¢„æµ‹â€”â€”è¿™äº›é¢„æµ‹åœ¨ç†è®ºä¸Šå¹¶éå¿…è¦æˆ–å……åˆ†ï¼Œä½†ä»ç„¶éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä»¬ä¸ºå¸å¼•å­åŠ¨åŠ›æœºåˆ¶æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒâ€”â€”å³è§£å‰–å’Œç»“æ„æ–¹é¢çš„å¯¹åº”ç‰¹å¾ï¼šåŒ…æ‹¬ä½ç»´ç‰©ç†ç»“æ„çš„å­˜åœ¨ï¼Œä»¥åŠç»†èƒä¹‹é—´è¿æ¥ä¸­å¯ç›´æ¥è§‚å¯Ÿåˆ°çš„å¯¹ç§°æ€§ã€‚</p>
<blockquote>
<p>As we have seen, attractor networks dynamics need not be used by the brain in an <strong>autonomous</strong> setting: inputs that drive attractor networks can be an important part of their function, for instance in integration and <strong>evidence accumulation</strong>.</p>
<p>Nevertheless, because attractor systems are characterized by their internally generated or <strong>autonomous dynamics</strong>, putative attractor networks are best tested in conditions that minimize external cues that are time-varying or tuned to provide localized inputs along the putative attractor â€” that is, in an effectively autonomous setting.</p>
</blockquote>
<p>æ­£å¦‚æˆ‘ä»¬å·²ç»çœ‹åˆ°çš„ï¼Œå¸å¼•å­ç½‘ç»œçš„åŠ¨åŠ›å­¦å¹¶ä¸å¿…é¡»åœ¨ <strong>å®Œå…¨è‡ªä¸»</strong> çš„æƒ…å†µä¸‹è¢«å¤§è„‘ä½¿ç”¨ï¼šé©±åŠ¨å¸å¼•å­ç½‘ç»œçš„å¤–éƒ¨è¾“å…¥ä¹Ÿæ˜¯å…¶åŠŸèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¾‹å¦‚åœ¨ç§¯åˆ†å’Œ <strong>è¯æ®ç´¯ç§¯</strong> ä¸­ã€‚</p>
<blockquote>
<p>å¤§è„‘ä½¿ç”¨å¸å¼•å­æ—¶, æœ‰æ—¶ä»¤å…¶è‡ªä¸»ç»´æŒ, æœ‰æ—¶ä»¤å…¶æ¥æ”¶å¤–éƒ¨è¾“å…¥ä»è€Œç§¯åˆ†.</p>
</blockquote>
<p>ç„¶è€Œï¼Œç”±äºå¸å¼•å­ç³»ç»Ÿçš„ä¸€ä¸ªå®šä¹‰ç‰¹å¾æ˜¯å®ƒä»¬ç”±å†…éƒ¨äº§ç”Ÿçš„ã€å³ <strong>è‡ªä¸»åŠ¨åŠ›å­¦</strong>ï¼Œå› æ­¤ï¼Œåœ¨æµ‹è¯•å€™é€‰çš„å¸å¼•å­ç½‘ç»œæ—¶ï¼Œæœ€å¥½æ˜¯åœ¨è¿™æ ·çš„æ¡ä»¶: ä½¿ç”¨é‚£äº›èƒ½å¤Ÿæœ€å°åŒ–å«æ—¶çš„å¤–éƒ¨æç¤ºï¼Œæˆ–è€…æœ€å°åŒ–é‚£äº›è°ƒåˆ¶ä¸ºæ²¿ç€æ‰€è€ƒå¯Ÿçš„å¸å¼•å­ç»´åº¦æä¾›å±€éƒ¨åŒ–è¾“å…¥çš„æç¤ºâ€”â€”ä¹Ÿå°±æ˜¯ï¼Œåœ¨ä¸€ä¸ªâ€œæœ‰æ•ˆè‡ªä¸»â€çš„ç¯å¢ƒä¸‹è¿›è¡Œæµ‹è¯•ã€‚</p>
<blockquote>
<p>Innovations in recording methods that have made it possible to record multiple neurons simultaneously in animals performing naturalistic behaviours have enabled crucial tests of these state-space predictions of attractor models described above. The newest methods provide activity data from thousands of neurons in a circuit, enabling characterization of the low-dimensional state-space dynamics of whole circuits.</p>
</blockquote>
<p>è®°å½•æ–¹æ³•çš„åˆ›æ–°ä½¿å¾—åœ¨åŠ¨ç‰©è¿›è¡Œè‡ªç„¶è¡Œä¸ºæ—¶èƒ½å¤ŸåŒæ—¶è®°å½•å¤šä¸ªç¥ç»å…ƒæ´»åŠ¨ï¼Œè¿™ä¸ºéªŒè¯ä¸Šè¿°å¸å¼•å­æ¨¡å‹å¯¹çŠ¶æ€ç©ºé—´çš„é¢„æµ‹æä¾›äº†å…³é”®æµ‹è¯•æ‰‹æ®µã€‚æœ€æ–°æŠ€æœ¯å¯è·å–ç¥ç»å›è·¯ä¸­æ•°åƒä¸ªç¥ç»å…ƒçš„æ´»åŠ¨æ•°æ®ï¼Œä»è€Œèƒ½å¤Ÿè¡¨å¾æ•´ä¸ªç¥ç»å›è·¯çš„ä½ç»´çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ç‰¹æ€§ã€‚</p>
<blockquote>
<p>When the attractor manifolds have three or fewer dimensions, one can directly visualize them by projecting or <strong>embedding</strong> the highdimensional state spaces into dimension $\leq 3$. This can be done using methods such as <strong>principle components analysis</strong>, multidimensional scaling, tensor factorization or other linear methods for projection; or Isomap, locally linear embedding, $t$-distributed stochastic neighbour embedding, variational autoencoders, latent factor analysis via dynamical systems and nonlinear tensor factorization, among others, for nonlinear embedding.</p>
<p>These methods can also be useful when manifolds have dimension $\geq 3$ but are topologically simple. For topologically non-trivial structures (such as rings and tori), especially those of dimension $\geq 3$, topological data analysis methods become important.</p>
</blockquote>
<p>å½“å¸å¼•å­æµå½¢çš„ç»´åº¦ä¸º 3 æˆ–æ›´å°‘æ—¶, å¯ä»¥é€šè¿‡å°†é«˜ç»´çŠ¶æ€ç©ºé—´æŠ•å½±æˆ– <strong>åµŒå…¥</strong> åˆ°ç»´åº¦ $\leq 3$ æ¥ç›´æ¥å¯è§†åŒ–å®ƒä»¬. è¿™å¯ä»¥ä½¿ç”¨ä¸»æˆåˆ†åˆ†æã€å¤šç»´å°ºåº¦åˆ†æã€å¼ é‡åˆ†è§£æˆ–å…¶ä»–çº¿æ€§æŠ•å½±æ–¹æ³•æ¥å®Œæˆ; æˆ–è€…ä½¿ç”¨ Isomapã€å±€éƒ¨çº¿æ€§åµŒå…¥ã€$t$ åˆ†å¸ƒéšæœºé‚»åŸŸåµŒå…¥ã€å˜åˆ†è‡ªç¼–ç å™¨ã€é€šè¿‡åŠ¨åŠ›ç³»ç»Ÿçš„æ½œåœ¨å› å­åˆ†æå’Œéçº¿æ€§å¼ é‡åˆ†è§£ç­‰è¿›è¡Œéçº¿æ€§åµŒå…¥.</p>
<p>è¿™äº›æ–¹æ³•åœ¨æµå½¢çš„ç»´åº¦ $\geq 3$ ä½†æ‹“æ‰‘ç»“æ„ç®€å•æ—¶ä¹Ÿå¾ˆæœ‰ç”¨. å¯¹äºæ‹“æ‰‘éå¹³å‡¡ç»“æ„ (å¦‚ç¯å’Œç¯é¢) , å°¤å…¶æ˜¯é‚£äº›ç»´åº¦ $\geq 3$ çš„ç»“æ„, æ‹“æ‰‘æ•°æ®åˆ†ææ–¹æ³•å˜å¾—é‡è¦.</p>
<blockquote>
<p>Testing the first, second and third predictions of attractor models described above requires examination of the state-space structure of the population, rather than the more conventional characterization of relationships (tuning curves) between cell activity and input or output variables.</p>
<p>The most direct way to examine state-space structure is to record enough cells simultaneously that it is possible to characterize the full state-space manifold. However, the existence, stability and invariance of low-dimensional state-space structures (the first three predictions) can be inferred indirectly from smaller samples of simultaneously recorded cells, for example by characterizing invariant structure in pairwise cell-cell relationships, as has been successfully done in several studies.</p>
</blockquote>
<p>æµ‹è¯•ä¸Šè¿°å¸å¼•å­æ¨¡å‹çš„ç¬¬ä¸€ä¸ªã€ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªé¢„æµ‹éœ€è¦æ£€æŸ¥é›†ç¾¤çš„çŠ¶æ€ç©ºé—´ç»“æ„, è€Œä¸æ˜¯æ›´ä¼ ç»Ÿçš„è¡¨å¾ç»†èƒæ´»åŠ¨ä¸è¾“å…¥æˆ–è¾“å‡ºå˜é‡ä¹‹é—´å…³ç³» (è°ƒåˆ¶æ›²çº¿).</p>
<p>æ£€æŸ¥çŠ¶æ€ç©ºé—´ç»“æ„çš„æœ€ç›´æ¥æ–¹æ³•æ˜¯åŒæ—¶è®°å½•è¶³å¤Ÿå¤šçš„ç»†èƒ, ä»¥ä¾¿èƒ½å¤Ÿè¡¨å¾å®Œæ•´çš„çŠ¶æ€ç©ºé—´æµå½¢. ç„¶è€Œ, ä½ç»´çŠ¶æ€ç©ºé—´ç»“æ„çš„å­˜åœ¨ã€ç¨³å®šæ€§å’Œä¸å˜æ€§ (å‰ä¸‰ä¸ªé¢„æµ‹) å¯ä»¥ä»åŒæ­¥è®°å½•çš„è¾ƒå°æ ·æœ¬ä¸­é—´æ¥æ¨æ–­å‡ºæ¥, ä¾‹å¦‚é€šè¿‡è¡¨å¾æˆå¯¹ç»†èƒ-ç»†èƒå…³ç³»ä¸­çš„ä¸å˜ç»“æ„, æ­£å¦‚å‡ é¡¹ç ”ç©¶ä¸­æˆåŠŸå®Œæˆçš„é‚£æ ·.</p>
<blockquote>
<p>The existence and stability of low-dimensional state-space structures are necessary but not sufficient for identification of recurrent attractor dynamics in a target network.</p>
<p>First, if the behaviours, circuit fluctuations and inputs to the network are themselves low-dimensional, then any observed low-dimensionality of the circuit states may be ascribed to those inputs and reveals little about intrinsic constraints imposed by the circuit.</p>
<p>Second, even if inputs and behaviours are highdimensional, a low-dimensional feedforward projection into the target network would generate low-dimensional states, and high-dimensional perturbations to the circuit would not persist.</p>
<p>The essential, defining prediction of attractor dynamics is that of invariance: because the states are internally generated and stabilized by strong recurrent connectivity, the population states and cell-cell relationships should be invariant when probed across time and across various input conditions, including when tuned input is removed and across waking and sleep. In simple terms, the stable low-dimensional states should be invariant across a broad range of conditions.</p>
</blockquote>
<p>ä½ç»´çŠ¶æ€ç©ºé—´ç»“æ„çš„å­˜åœ¨å’Œç¨³å®šæ€§æ˜¯è¯†åˆ«ç›®æ ‡ç½‘ç»œä¸­é€’å½’å¸å¼•å­åŠ¨åŠ›å­¦çš„å¿…è¦ä½†ä¸å……åˆ†æ¡ä»¶.</p>
<p>é¦–å…ˆ, å¦‚æœç½‘ç»œçš„è¡Œä¸ºã€å›è·¯æ³¢åŠ¨å’Œè¾“å…¥æœ¬èº«æ˜¯ä½ç»´çš„, é‚£ä¹ˆå›è·¯çŠ¶æ€çš„ä»»ä½•è§‚å¯Ÿåˆ°çš„ä½ç»´æ€§éƒ½å¯ä»¥å½’å› äºè¿™äº›è¾“å…¥, å¹¶ä¸”å‡ ä¹æ²¡æœ‰æ­ç¤ºå›è·¯æ–½åŠ çš„å†…åœ¨çº¦æŸ.</p>
<p>å…¶æ¬¡, å³ä½¿è¾“å…¥å’Œè¡Œä¸ºæ˜¯é«˜ç»´çš„, è¿›å…¥ç›®æ ‡ç½‘ç»œçš„ä½ç»´å‰é¦ˆæŠ•å½±ä¹Ÿä¼šç”Ÿæˆä½ç»´çŠ¶æ€, å¹¶ä¸”å¯¹å›è·¯çš„é«˜ç»´æ‰°åŠ¨ä¸ä¼šæŒç»­å­˜åœ¨.</p>
<p>å¸å¼•å­åŠ¨åŠ›å­¦çš„åŸºæœ¬å®šä¹‰é¢„æµ‹æ˜¯ä¸å˜æ€§: ç”±äºçŠ¶æ€æ˜¯ç”±å¼ºé€’å½’è¿æ¥å†…éƒ¨ç”Ÿæˆå’Œç¨³å®šçš„, å› æ­¤å½“è·¨æ—¶é—´å’Œå„ç§è¾“å…¥æ¡ä»¶è¿›è¡Œæ¢æµ‹æ—¶, ç¾¤ä½“çŠ¶æ€å’Œç»†èƒ-ç»†èƒå…³ç³»åº”ä¿æŒä¸å˜, åŒ…æ‹¬åœ¨å»é™¤è°ƒåˆ¶è¾“å…¥ä»¥åŠåœ¨æ¸…é†’å’Œç¡çœ æœŸé—´. ç®€å•æ¥è¯´, ç¨³å®šçš„ä½ç»´çŠ¶æ€åº”åœ¨å¹¿æ³›æ¡ä»¶ä¸‹ä¿æŒä¸å˜.</p>
<blockquote>
<p>Next is the question of <strong>circuit localization</strong>: does a circuit exhibiting the key signatures of attractor dynamics give rise to these dynamics, or are they a readout of some other region?</p>
<p>Localization need not be a primary goal of establishing attractor dynamics: an important problem is to simply characterize whether the brain solves certain problems through attractor dynamics, regardless of which local circuits create these dynamics.</p>
<p>Nevertheless, the persistence of activity states in attractors can lend a helping hand to localization efforts. If a region gives rise to or is upstream (but not downstream) of the attractor dynamics, perturbations that alter its state along the set of attractors should persist after the perturbing drive is removed.</p>
</blockquote>
<p>ä¸‹ä¸€ä¸ªé—®é¢˜æ˜¯ <strong>å›è·¯å®šä½</strong>: è¡¨ç°å‡ºå¸å¼•å­åŠ¨åŠ›å­¦å…³é”®ç‰¹å¾çš„å›è·¯, æ˜¯(è‡ªè¡Œ)äº§ç”Ÿäº†è¿™äº›åŠ¨åŠ›å­¦, è¿˜æ˜¯å®ƒä»¬æ˜¯åœ¨è¯»å–æŸä¸ªå…¶ä»–è„‘åŒº(çš„å¸å¼•å­çŠ¶æ€)ï¼Ÿ</p>
<blockquote>
<p>è¿™å¯¹äºç†è§£å¤§è„‘åŠŸèƒ½çš„åˆ†å·¥éå¸¸é‡è¦.</p>
</blockquote>
<p>å®šä½ä¸å¿…æ˜¯å»ºç«‹å¸å¼•å­åŠ¨åŠ›å­¦çš„é¦–è¦ç›®æ ‡: ä¸€ä¸ªé‡è¦çš„é—®é¢˜ä»…ä»…æ˜¯å¼„æ¸…å¤§è„‘æ˜¯å¦é€šè¿‡å¸å¼•å­åŠ¨åŠ›å­¦æ¥è§£å†³æŸäº›é—®é¢˜ï¼Œè€Œä¸ç®¡ç©¶ç«Ÿæ˜¯å“ªä¸€ä¸ªå±€éƒ¨ç¥ç»å›è·¯ç”Ÿæˆäº†è¿™äº›åŠ¨åŠ›å­¦.</p>
<p>ç„¶è€Œï¼Œå¸å¼•å­ä¸­æ´»åŠ¨çŠ¶æ€çš„æŒç»­æ€§å¯ä»¥å¸®åŠ©è¿›è¡Œå›è·¯å®šä½ã€‚å¦‚æœä¸€ä¸ªè„‘åŒºæœ¬èº«ç”Ÿæˆ (æˆ–å¤„äºå¸å¼•å­åŠ¨åŠ›å­¦çš„ä¸Šæ¸¸ï¼Œè€Œä¸æ˜¯ä¸‹æ¸¸) ï¼Œé‚£ä¹ˆå¯¹å…¶æ–½åŠ èƒ½å¤Ÿæ”¹å˜å…¶åœ¨å¸å¼•å­é›†åˆä¸Šä½ç½®çš„æ‰°åŠ¨åï¼Œåœ¨æ‰°åŠ¨è¾“å…¥ç§»é™¤ä¹‹åï¼Œè¿™äº›çŠ¶æ€å˜åŒ–ä»åº”æŒç»­å­˜åœ¨ã€‚</p>
<blockquote>
<p>As we describe next, theoretically motivated analyses of population activity data have firmly established that low-dimensional attractor dynamics are ubiquitous in the brain, across levels in the brainâ€™s hierarchy and across species.</p>
</blockquote>
<p>æ­£å¦‚æˆ‘ä»¬æ¥ä¸‹æ¥æè¿°çš„é‚£æ ·, å¯¹ç¾¤ä½“æ´»åŠ¨æ•°æ®çš„ç†è®ºåŠ¨æœºåˆ†æå·²ç»ç‰¢å›ºåœ°ç¡®ç«‹äº†ä½ç»´å¸å¼•å­åŠ¨åŠ›å­¦åœ¨å¤§è„‘ä¸­æ— å¤„ä¸åœ¨, è·¨è¶Šå¤§è„‘å±‚æ¬¡ç»“æ„çš„å„ä¸ªå±‚æ¬¡å’Œç‰©ç§.</p>
<h2 id="discrete-attractors-1">Discrete attractors<a hidden class="anchor" aria-hidden="true" href="#discrete-attractors-1">#</a></h2>
<h3 id="up-and-down-states">Up and down states.<a hidden class="anchor" aria-hidden="true" href="#up-and-down-states">#</a></h3>
<blockquote>
<p>The simplest example of non-trivial discrete attractor dynamics (that is, beyond a single point attractor) is <strong>bistability</strong>.</p>
<p>Bistable dynamics are a feature of <strong>cortical activity</strong> in the form of up and down states, in which the subthreshold membrane potential of neurons switches between a <strong>hyperpolarized</strong> state and a <strong>relatively depolarized</strong> one, with long persistence (in the order of hundreds of milliseconds to seconds) per state (Fig. 3a).</p>
<p>The two states are relatively invariant over time, as seen in the relatively sharply peaked histograms (Fig. 3a), and despite presumed internal noise in the system the peaks are well separated, suggesting relatively rapid corrective dynamics towards the two states.</p>
<p>There is little evidence of a strong contribution from cellular bistability in supporting these states, suggesting that it is a network-driven phenomenon involving <strong>self-excitation</strong> and <strong>global inhibition</strong>. Transitions are believed to be driven through adaptation (from up to down) and by stochastic as well as external coordinating events (from down to up).</p>
<p>Although these states and switches can occur in the cortex without input from the thalamus and striatum, they tend to be synchronous across the cortex and striatum. Thus, the origin of up and down states may be highly distributed.</p>
</blockquote>
<p>éå¹³å‡¡ç¦»æ•£å¸å¼•å­åŠ¨åŠ›å­¦ (å³è¶…å‡ºå•ç‚¹å¸å¼•å­) çš„æœ€ç®€å•ä¾‹å­æ˜¯ <strong>åŒç¨³æ€</strong>.</p>
<p>åŒç¨³æ€åŠ¨åŠ›å­¦æ˜¯ <strong>çš®å±‚æ´»åŠ¨</strong> çš„ä¸€ä¸ªç‰¹å¾, è¡¨ç°ä¸ºä¸Šå‡å’Œä¸‹é™çŠ¶æ€, å…¶ä¸­ç¥ç»å…ƒçš„äºšé˜ˆå€¼è†œç”µä½åœ¨ <strong>è¶…æåŒ–</strong> çŠ¶æ€å’Œ <strong>ç›¸å¯¹å»æåŒ–</strong> çŠ¶æ€ä¹‹é—´åˆ‡æ¢, æ¯ä¸ªçŠ¶æ€æŒç»­æ—¶é—´è¾ƒé•¿ (å¤§çº¦æ•°ç™¾æ¯«ç§’åˆ°æ•°ç§’)  (å›¾ 3a).</p>
<blockquote>
<p>down: è¶…æåŒ–, æ¥è¿‘é™æ¯çŠ¶æ€; up: å»æåŒ–, æ¥è¿‘æ”¾ç”µé˜ˆå€¼, ç½‘ç»œæ´»åŠ¨å¼º.</p>
</blockquote>
<p>è¿™ä¸¤ä¸ªçŠ¶æ€åœ¨æ—¶é—´ä¸Šç›¸å¯¹ç¨³å®šï¼Œä»æ´»åŠ¨åˆ†å¸ƒç›´æ–¹å›¾ä¸­çš„å°–é”å³°å¯ä»¥çœ‹å‡ºæ¥ (å›¾3a) ã€‚å°½ç®¡ç³»ç»Ÿå†…éƒ¨å­˜åœ¨å™ªå£°ï¼Œè¿™ä¸¤ä¸ªå³°ä»ç„¶æ˜æ˜¾åˆ†ç¦»ï¼Œè¿™è¡¨æ˜ç³»ç»Ÿå…·æœ‰æœå‘è¿™ä¸¤ä¸ªçŠ¶æ€çš„å¿«é€Ÿçº æ­£åŠ¨åŠ›å­¦ã€‚</p>
<p>ç›®å‰å‡ ä¹æ²¡æœ‰è¯æ®è¡¨æ˜è¿™äº›çŠ¶æ€æ˜¯ç”±(å•ä¸ª)ç»†èƒå±‚é¢çš„åŒç¨³æ€äº§ç”Ÿçš„ï¼Œç›¸åï¼Œå®ƒä»¬ä¼¼ä¹æ˜¯ä¸€ä¸ªåŒ…å« <strong>è‡ªæ¿€æ´»</strong> å’Œ <strong>å…¨å±€æŠ‘åˆ¶</strong> çš„ç½‘ç»œé©±åŠ¨ç°è±¡ã€‚çŠ¶æ€è½¬æ¢ä¸€èˆ¬è®¤ä¸ºæ˜¯é€šè¿‡é€‚åº”æœºåˆ¶ (ä»ä¸Šåˆ°ä¸‹) ä»¥åŠéšæœºäº‹ä»¶å’Œå¤–éƒ¨åè°ƒä¿¡å· (ä»ä¸‹åˆ°ä¸Š) è§¦å‘çš„ã€‚</p>
<p>è™½ç„¶ä¸Š/ä¸‹çŠ¶æ€åœ¨æ²¡æœ‰æ¥è‡ªä¸˜è„‘æˆ–çº¹çŠ¶ä½“çš„è¾“å…¥æ—¶ä¹Ÿä¼šåœ¨çš®å±‚ä¸­å‘ç”Ÿï¼Œä½†å®ƒä»¬åœ¨çš®å±‚å’Œçº¹çŠ¶ä½“ä¹‹é—´å¾€å¾€æ˜¯åŒæ­¥çš„ã€‚å› æ­¤ï¼Œä¸Š/ä¸‹çŠ¶æ€çš„èµ·æºå¯èƒ½æ˜¯é«˜åº¦åˆ†å¸ƒå¼çš„.</p>
<h3 id="perceptual-bistability">Perceptual bistability<a hidden class="anchor" aria-hidden="true" href="#perceptual-bistability">#</a></h3>
<blockquote>
<p>Visual and auditory percepts including <strong>binocular rivalry</strong>, the Necker cube and some <strong>auditory illusions</strong> offer clear examples of bistability in neural processing, suggesting the operation of a dynamical system with two attractors.</p>
<p>In these illusions, the brain (at the level of perceptual reports) selects one possible interpretation of an ambiguous input, often switching between possibilities. Although the phenomenon has long been known and studied, no localized bistable attractor circuit has been identified as the basis of perceptual bistability.</p>
<p>Indeed, some percepts may involve top-down activation and modulation of activity across many brain areas, suggesting once again a <strong>widely distributed</strong> circuit for bistability.</p>
</blockquote>
<p>è§†è§‰å’Œå¬è§‰çŸ¥è§‰, åŒ…æ‹¬ <strong>åŒçœ¼ç«äº‰</strong>ã€Necker ç«‹æ–¹ä½“å’Œä¸€äº› <strong>å¹»å¬é”™è§‰</strong>, æä¾›äº†ç¥ç»å¤„ç†ä¸­çš„åŒç¨³æ€çš„æ¸…æ™°ä¾‹å­, è¡¨æ˜å…·æœ‰ä¸¤ä¸ªå¸å¼•å­çš„åŠ¨åŠ›ç³»ç»Ÿçš„è¿è¡Œ.</p>
<blockquote>
<p>åŒçœ¼ç«äº‰: ä¸¤åªçœ¼è¾“å…¥ä¸åŒå›¾åƒ, ä¸€æ¬¡åªèƒ½æ„è¯†åˆ°å…¶ä¸­ä¸€å¹…</p>
<p>Necker ç«‹æ–¹ä½“: ä¸€ç§äºŒç»´å›¾åƒ, å¯è¢«çŸ¥è§‰ä¸ºä¸¤ç§ä¸åŒçš„ä¸‰ç»´ç«‹æ–¹ä½“è§†è§’</p>
<p><img loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/e/e7/Necker_cube.svg" alt=""  /></p>
<p><img loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/2/21/Spinning_Dancer.gif" alt=""  /></p>
</blockquote>
<p>åœ¨è¿™äº›é”™è§‰ä¸­, å¤§è„‘ (åœ¨æ„ŸçŸ¥æŠ¥å‘Šçš„å±‚é¢ä¸Š) é€‰æ‹©å¯¹æ¨¡ç³Šè¾“å…¥çš„ä¸€ç§å¯èƒ½è§£é‡Š, é€šå¸¸åœ¨å¯èƒ½æ€§ä¹‹é—´åˆ‡æ¢. å°½ç®¡è¿™ä¸€ç°è±¡æ—©å·²ä¸ºäººæ‰€çŸ¥å¹¶è¢«ç ”ç©¶, ä½†å°šæœªç¡®å®šä»»ä½•å±€éƒ¨åŒç¨³æ€å¸å¼•å­å›è·¯ä½œä¸ºæ„ŸçŸ¥åŒç¨³æ€çš„åŸºç¡€.</p>
<p>äº‹å®ä¸Š, ä¸€äº›çŸ¥è§‰å¯èƒ½æ¶‰åŠå¯¹è·¨è„‘åŒºçš„è‡ªä¸Šè€Œä¸‹çš„æ¿€æ´»å’Œæ´»åŠ¨è°ƒåˆ¶, å†æ¬¡æš—ç¤ºåŒç¨³æ€å¯èƒ½ç”±ä¸€ä¸ª <strong>é«˜åº¦åˆ†å¸ƒå¼</strong> çš„å›è·¯äº§ç”Ÿ.</p>
<h3 id="bistability-in-a-premotor-area">Bistability in a premotor area<a hidden class="anchor" aria-hidden="true" href="#bistability-in-a-premotor-area">#</a></h3>
<blockquote>
<p>Recent studies identify and localize discrete attractor dynamics in a mouse premotor area, the anterior lateral motor cortex (ALM).</p>
<p>In a cued two-alternative delayed response task, ALM neurons exhibit persistent activity over a 1-s delay period.</p>
<p>During the post-cue delay period, activity evolves towards one of two states that guide the response (Fig. 3b), fulfilling the first prediction of attractor dynamics. The delay-period terminal states are similar for cues from different sensory modalities, partially meeting the prediction of invariance.</p>
<p>ALM perturbations during the delay are either erased (corrected) by the circuit (Fig. 3b, top) or drive a jump to the opposite state (Fig. 3b, bottom), which results in the animal making the wrong action, suggesting bistable switching dynamics similar to the mechanism shown in either Fig. 1b or Fig. 2e.</p>
</blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶åœ¨å°é¼ çš„å‰è¿åŠ¨åŒºåŸŸâ€”â€”å‰å¤–ä¾§è¿åŠ¨çš®å±‚ (ALM) ä¸­è¯†åˆ«å‡ºå¹¶å®šä½äº†ç¦»æ•£å¸å¼•å­åŠ¨åŠ›å­¦ã€‚</p>
<p>åœ¨ä¸€ä¸ªç”±æç¤ºå¼•å¯¼çš„äºŒé€‰ä¸€å»¶è¿Ÿååº”ä»»åŠ¡ä¸­ï¼ŒALM ç¥ç»å…ƒåœ¨çº¦ 1 ç§’çš„å»¶è¿ŸæœŸå†…è¡¨ç°å‡ºæŒç»­æ´»åŠ¨ã€‚</p>
<p>åœ¨æç¤ºä¹‹åçš„å»¶è¿Ÿé˜¶æ®µï¼Œæ´»åŠ¨ä¼šå‘ä¸¤ä¸ªçŠ¶æ€ä¹‹ä¸€æ¼”åŒ–ï¼Œè€Œè¿™ä¸¤ä¸ªçŠ¶æ€å°†å¼•å¯¼åŠ¨ç‰©çš„è¡Œä¸ºååº” (å›¾ 3b) ï¼Œæ»¡è¶³äº†å¸å¼•å­åŠ¨åŠ›å­¦çš„ç¬¬ä¸€ä¸ªé¢„æµ‹ã€‚</p>
<blockquote>
<p>å¼ºæŒç»­æ´»åŠ¨; é€æ¸åˆ†åŒ–ä¸ºå·¦/å³å¸å¼•å­; çŠ¶æ€ä¸€æ—¦æ¥è¿‘å¸å¼•å­å°±è¢«å¸å¼•ç¨³å®š.</p>
</blockquote>
<p>å¯¹äºæ¥è‡ªä¸åŒæ„Ÿå®˜æ¨¡æ€çš„çº¿ç´¢ï¼Œå»¶è¿ŸæœŸçš„ç»ˆæœ«çŠ¶æ€å½¼æ­¤ç›¸ä¼¼ï¼Œä»è€Œéƒ¨åˆ†æ»¡è¶³äº†å¸å¼•å­åŠ¨åŠ›å­¦å…³äºâ€œä¸å˜æ€§â€çš„é¢„æµ‹ã€‚</p>
<p>åœ¨å»¶è¿ŸæœŸé—´å¯¹ ALM è¿›è¡Œæ‰°åŠ¨æ—¶ï¼Œå…¶å½±å“è¦ä¹ˆè¢«å›è·¯æŠ¹é™¤ (çº æ­£)  (å›¾ 3b ä¸Š) ï¼Œè¦ä¹ˆå°†çŠ¶æ€æ¨å‘å¦ä¸€ä¸ªå¸å¼•å­çŠ¶æ€ (å›¾ 3b ä¸‹) ï¼Œå¯¼è‡´åŠ¨ç‰©åšå‡ºé”™è¯¯åŠ¨ä½œï¼Œè¿™è¡¨æ˜ ALM ä¸­å­˜åœ¨ç±»ä¼¼å›¾ 1b æˆ–å›¾ 2e æ‰€ç¤ºæœºåˆ¶çš„åŒç¨³æ€åˆ‡æ¢åŠ¨åŠ›å­¦ã€‚</p>
<blockquote>
<p>Given the long training time required for the task and the resulting tailoring of the ALM dynamics to the specific task structure â€” bistability for a two-choice task â€” it is likely that this system acquires its dynamics through slow plasticity and, thus, that the networkâ€™s recurrent structure is malleable in adult animals. New results showing the existence of small (on the scale of about $100 \mu\text{m}$) clusters of locally recurrent neurons in the ALM that can maintain persistent responses to microstimulation may provide experimental evidence of the theoretically posited mixed modular networks (below) that are hypothesized to support robust and high-capacity memory states.</p>
</blockquote>
<p>é‰´äºè¯¥ä»»åŠ¡æ‰€éœ€çš„é•¿æ—¶é—´è®­ç»ƒä»¥åŠ ALM åŠ¨åŠ›å­¦å¯¹ç‰¹å®šä»»åŠ¡ç»“æ„çš„è°ƒæ•´â€”â€”åŒç¨³æ€ç”¨äºä¸¤ç§é€‰æ‹©ä»»åŠ¡â€”â€”è¯¥ç³»ç»Ÿå¾ˆå¯èƒ½é€šè¿‡ç¼“æ…¢çš„å¯å¡‘æ€§è·å¾—å…¶åŠ¨åŠ›å­¦, å› æ­¤ç½‘ç»œçš„é€’å½’ç»“æ„åœ¨æˆå¹´åŠ¨ç‰©ä¸­æ˜¯å¯å¡‘çš„. æ–°çš„ç»“æœæ˜¾ç¤º, åœ¨ ALM ä¸­å­˜åœ¨å°è§„æ¨¡ (çº¦ $100 \mu\text{m}$ è§„æ¨¡) çš„å±€éƒ¨é€’å½’ç¥ç»å…ƒé›†ç¾¤, è¿™äº›é›†ç¾¤å¯ä»¥ç»´æŒå¯¹å¾®åˆºæ¿€çš„æŒç»­ååº”, å¯èƒ½ä¸ºç†è®ºä¸Šå‡è®¾çš„æ··åˆæ¨¡å—åŒ–ç½‘ç»œæä¾›äº†å®éªŒè¯æ®, è¿™äº›ç½‘ç»œè¢«å‡è®¾æ”¯æŒç¨³å¥ä¸”é«˜å®¹é‡çš„è®°å¿†çŠ¶æ€.</p>
<h3 id="discrete-multistability">Discrete multistability<a hidden class="anchor" aria-hidden="true" href="#discrete-multistability">#</a></h3>
<blockquote>
<p>Hopfield networks and WTA networks (which can be viewed as a special type of Hopfield network, with bistable switch networks as a special type of WTA network) are models of multistability beyond bistability.</p>
</blockquote>
<p>Hopfield ç½‘ç»œå’Œ WTA ç½‘ç»œ (å¯ä»¥å°†å…¶è§†ä¸º Hopfield ç½‘ç»œçš„ä¸€ç§ç‰¹æ®Šç±»å‹, åŒç¨³æ€å¼€å…³ç½‘ç»œä½œä¸º WTA ç½‘ç»œçš„ä¸€ç§ç‰¹æ®Šç±»å‹) æ˜¯è¶…è¶ŠåŒç¨³æ€çš„å¤šç¨³æ€æ¨¡å‹.</p>
<blockquote>
<p>At present, the evidence for discrete multistability as a circuitlevel brain process is less direct and less exhaustive than that for continuous-attractor networks (described below).</p>
<p>However, there are many likely candidate systems and brain regions with dynamics that are suggestive of and consistent with discrete multistability, at least of the special case of WTA attractor dynamics â€” including in the <strong>mammalian hippocampus</strong> and <strong>auditory cortex</strong>, and in the fly and mammalian olfactory system.</p>
<p>In particular, many of these circuits exhibit <strong>global inhibition</strong> that clearly narrows and refines activity in the circuit (Fig. 3c, left), and also show evidence of selective recurrent excitation that leads to multiple distinct and stably correlated input responses in distinct subpopulations of cells (Fig. 3c, middle and right).</p>
<p>In our view, it is likely that these circuits exhibit multiple discrete attractor states, but quantitative testing of the first three predictions of attractor dynamics and direct demonstration of these states as stable and invariant remain an important future direction for characterizing these circuits.</p>
</blockquote>
<p>ç›®å‰, ä½œä¸ºå›è·¯çº§å¤§è„‘è¿‡ç¨‹çš„ç¦»æ•£å¤šç¨³æ€çš„è¯æ®ä¸å¦‚è¿ç»­å¸å¼•å­ç½‘ç»œ (ä¸‹é¢æè¿°çš„) ç›´æ¥å’Œè¯¦å°½.</p>
<p>ç„¶è€Œ, æœ‰è®¸å¤šå¯èƒ½çš„å€™é€‰ç³»ç»Ÿå’Œå¤§è„‘åŒºåŸŸ, å…¶åŠ¨åŠ›å­¦æš—ç¤ºå¹¶ä¸ç¦»æ•£å¤šç¨³æ€ä¸€è‡´, è‡³å°‘æ˜¯ WTA å¸å¼•å­åŠ¨åŠ›å­¦çš„ç‰¹æ®Šæƒ…å†µâ€”â€”åŒ…æ‹¬ <strong>å“ºä¹³åŠ¨ç‰©æµ·é©¬ä½“</strong> å’Œ <strong>å¬è§‰çš®å±‚</strong>, ä»¥åŠæœè‡å’Œå“ºä¹³åŠ¨ç‰©çš„å—…è§‰ç³»ç»Ÿ.</p>
<p>ç‰¹åˆ«æ˜¯, è¿™äº›å›è·¯ä¸­çš„è®¸å¤šéƒ½è¡¨ç°å‡º <strong>å…¨å±€æŠ‘åˆ¶</strong>, æ˜æ˜¾ç¼©å°å’Œç»†åŒ–äº†å›è·¯ä¸­çš„æ´»åŠ¨ (å›¾ 3c, å·¦) , å¹¶ä¸”è¿˜æ˜¾ç¤ºå‡ºé€‰æ‹©æ€§é€’å½’å…´å¥‹çš„è¯æ®, å¯¼è‡´ç»†èƒä¸åŒå­ç¾¤ä¸­å¤šä¸ªä¸åŒä¸”ç¨³å®šç›¸å…³çš„è¾“å…¥å“åº” (å›¾ 3c, ä¸­é—´å’Œå³).</p>
<p>åœ¨æˆ‘ä»¬çœ‹æ¥, è¿™äº›å›è·¯å¾ˆå¯èƒ½è¡¨ç°å‡ºå¤šä¸ªç¦»æ•£å¸å¼•å­çŠ¶æ€, ä½†å¯¹å¸å¼•å­åŠ¨åŠ›å­¦å‰ä¸‰ä¸ªé¢„æµ‹çš„å®šé‡æµ‹è¯•ä»¥åŠå°†è¿™äº›çŠ¶æ€ç›´æ¥è¯æ˜ä¸ºç¨³å®šä¸”ä¸å˜ä»ç„¶æ˜¯è¡¨å¾è¿™äº›å›è·¯çš„é‡è¦æœªæ¥æ–¹å‘.</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/ZBXCIJrkLdqViu7.png" alt=""  /></p>
<p><strong>a</strong>, Multi-unit activity (MUA) and single-unit activity ($V_{m}$) during cortical up states and down states show signatures of bistability (clusters and histograms at bottom).</p>
</blockquote>
<p>a, çš®å±‚ &ldquo;ä¸Šæ€&rdquo; å’Œ &ldquo;ä¸‹æ€&rdquo; æœŸé—´çš„å¤šå•å…ƒæ´»åŠ¨ (MUA) å’Œå•å…ƒæ´»åŠ¨(è†œç”µä½) ($V_{m}$) æ˜¾ç¤ºå‡ºåŒç¨³æ€çš„ç‰¹å¾ (åº•éƒ¨çš„ç°‡å’Œç›´æ–¹å›¾).</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/nVNkDAQuoEpmwd9.png" alt=""  /></p>
<p><strong>b</strong>, Delay-period dynamics in rodent premotor area (anterolateral motor cortex (ALM)) during a binary decision task (blue and red correspond to correct and incorrect direction choices, respectively). Before the animal makes a motor report of its decision (at the â€˜goâ€™ cue delivery), ALM activity seems to converge to one of two discrete end points (blue and red curves and histograms, top). Perturbations (optogenetic inhibition, denoted by pale blue) are either robustly erased (top; dashed lines show the unperturbed trajectory, and solid line shows a return to the unperturbed trajectory) or flip the dynamics so that the end points are reversed (bottom) and the animal reports the incorrect decision.</p>
</blockquote>
<p>b, åœ¨å•®é½¿åŠ¨ç‰©é¢å‰è¿åŠ¨åŒº (anterolateral motor cortexï¼ŒALM) æ‰§è¡ŒäºŒé€‰å†³ç­–ä»»åŠ¡çš„ç­‰å¾…æœŸ (delay-period) åŠ¨åŠ›å­¦ (è“è‰²å’Œçº¢è‰²åˆ†åˆ«å¯¹åº”æ­£ç¡®å’Œé”™è¯¯çš„æ–¹å‘é€‰æ‹©) ã€‚åœ¨åŠ¨ç‰©ä½œå‡ºè¿åŠ¨æ€§æŠ¥å‘Š (åœ¨ â€œgoâ€ æç¤ºä¹‹å‰) ï¼ŒALM æ´»åŠ¨çœ‹èµ·æ¥ä¼šæ”¶æ•›åˆ°ä¸¤ä¸ªç¦»æ•£ç»ˆç‚¹ä¹‹ä¸€ (é¡¶å›¾çš„è“è‰²å’Œçº¢è‰²æ›²çº¿ä¸ç›´æ–¹å›¾) ã€‚å¯¹ç½‘ç»œçš„æ‰°åŠ¨ (ç”¨å…‰é—ä¼ å­¦æŠ‘åˆ¶ï¼Œå›¾ä¸­ä»¥æ·¡è“è‰²è¡¨ç¤º) è¦ä¹ˆè¢«ç½‘ç»œç¨³å¥åœ°æŠ¹å» (é¡¶éƒ¨ï¼›è™šçº¿è¡¨ç¤ºæœªæ‰°åŠ¨æ—¶çš„è½¨è¿¹ï¼Œå®çº¿è¡¨ç¤ºæ‰°åŠ¨åè¿”å›åˆ°æœªæ‰°åŠ¨è½¨è¿¹) ï¼Œè¦ä¹ˆä½¿åŠ¨åŠ›å­¦ç¿»è½¬ï¼Œä½¿ç»ˆç‚¹äº’æ¢ (åº•éƒ¨) ï¼Œä»è€ŒåŠ¨ç‰©æŠ¥å‘Šé”™è¯¯çš„å†³ç­–ã€‚</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/jYckHCWSdB7wthM.png" alt=""  /></p>
<p><strong>c</strong>, Evidence of all-to-all inhibition and competitive winner-takes-all (WTA) recurrent dynamics in the fly olfactory system. Kenyon cells (KCs) activate anterior paired lateral (APL) inhibitory neurons, which in turn globally inhibit KCs. KC responses to odours, when input from the APL neurons is intact, are sparse: top-left image shows calcium fluorescence responses of KCs to odorant isoamyl acetate. KC responses are also decorrelated across odours (left). Blocking either KC drive to APL neurons or APL inhibition of KCs results in dense and correlated odour responses (middle, right).</p>
</blockquote>
<p>c, åœ¨æœè‡å—…è§‰ç³»ç»Ÿä¸­ï¼Œæœ‰è¯æ®æ”¯æŒ â€œå…¨è¿é€šæŠ‘åˆ¶ + ç«äº‰æ€§èƒœè€…é€šåƒ (WTA) â€ çš„å›é¦ˆåŠ¨åŠ›å­¦ã€‚Kenyon ç»†èƒ (KCs) æ¿€æ´»å‰å¯¹ä¾§å¯¹ç§°çš„å‰è…¹ä¾§æŠ‘åˆ¶æ€§ç¥ç»å…ƒ (anterior paired lateralï¼ŒAPL) ï¼ŒAPL åè¿‡æ¥å¯¹æ‰€æœ‰ KCs æ–½åŠ å…¨å±€æŠ‘åˆ¶ã€‚å½“ APL çš„æŠ‘åˆ¶é€šè·¯å®Œæ•´æ—¶ï¼ŒKCs å¯¹æ°”å‘³çš„å“åº”æ˜¯ç¨€ç–çš„ï¼šå·¦ä¸Šå›¾æ˜¾ç¤º KCs å¯¹æ°”å‘³å¼‚æˆŠé†‡ (isoamyl acetate) çš„é’™è§å…‰å“åº”ã€‚KCs å¯¹ä¸åŒæ°”å‘³ä¹‹é—´çš„å“åº”ä¹Ÿè¢«å»ç›¸å…³ (å·¦) ã€‚é˜»æ–­ KC é©±åŠ¨ APL æˆ–é˜»æ–­ APL å¯¹ KCs çš„æŠ‘åˆ¶éƒ½ä¼šå¯¼è‡´å¯¹æ°”å‘³çš„å“åº”å˜å¾—å¯†é›†ä¸”é«˜åº¦ç›¸å…³ (ä¸­ã€å³å›¾) ã€‚</p>
</blockquote>
<blockquote>
<p>è‹¥ç³»ç»Ÿæ˜¯çœŸåŒç¨³æ€ï¼Œæ•°æ®ä¼šèšæˆä¸¤ç°‡ (ä¸Šæ€ç°‡ã€ä¸‹æ€ç°‡) ï¼Œå¯¹åº”ä¸‹å›¾ä¸­ â€œèšç±»â€ å’Œ â€œç›´æ–¹å›¾â€ çš„åŒå³°å½¢æ€ã€‚</p>
</blockquote>
<h2 id="continuous-attractors-1">Continuous attractors<a hidden class="anchor" aria-hidden="true" href="#continuous-attractors-1">#</a></h2>
<h3 id="the-oculomotor-integrator">The oculomotor integrator<a hidden class="anchor" aria-hidden="true" href="#the-oculomotor-integrator">#</a></h3>
<blockquote>
<p>The <strong>oculomotor integrator</strong>, together with the <strong>head-direction circuit</strong>, was one of the first systems in neuroscience to be studied theoretically and experimentally as a continuous-attractor network â€” specifically as a line attractor (Fig. 1e). This network, which is presynaptic to the motor neurons that control horizontal eye position, is highly conserved across vertebrates, from fish to primates.</p>
<p>It integrates pulse-like saccadic eye movement-command signals to generate step-like stable muscle tension command signals (Fig. 4a) that persist autonomously at graded activity levels after removal of the movement cue and even in the dark in the absence of visual feedback (Fig. 4b; third prediction), and thus enable stable gaze fixation at various degrees of <strong>eccentricity</strong>. Saccadic inputs knock the system slightly off the linear response states, but the neural responses rapidly decay back towards the persistent firing states (in line with the second prediction). Remarkably, the same system also integrates smooth head-velocity signals to permit gaze stabilization during head movement.</p>
</blockquote>
<p><strong>çœ¼åŠ¨ç§¯åˆ†å™¨</strong> ä¸ <strong>å¤´æœå‘å›è·¯</strong> ä¸€èµ·, æ˜¯ç¥ç»ç§‘å­¦ä¸­æœ€æ—©ä½œä¸ºè¿ç»­å¸å¼•å­ç½‘ç»œè¿›è¡Œç†è®ºå’Œå®éªŒç ”ç©¶çš„ç³»ç»Ÿä¹‹ä¸€â€”â€”å°¤å…¶æ˜¯çº¿æ€§å¸å¼•å­ (å›¾ 1e).  è¯¥ç½‘ç»œä½äºæ§åˆ¶æ°´å¹³çœ¼ä½çš„è¿åŠ¨ç¥ç»å…ƒçš„å‰çªè§¦å¤„, åœ¨ä»é±¼ç±»åˆ°çµé•¿ç±»åŠ¨ç‰©çš„è„Šæ¤åŠ¨ç‰©ä¸­é«˜åº¦ä¿å®ˆ.</p>
<p>å®ƒç§¯åˆ†è„‰å†²çŠ¶çš„æ‰«è§†çœ¼åŠ¨æŒ‡ä»¤ä¿¡å·, ä»¥ç”Ÿæˆé˜¶æ¢¯çŠ¶ç¨³å®šçš„è‚Œè‚‰å¼ åŠ›æŒ‡ä»¤ä¿¡å· (å›¾ 4a) , åœ¨å»é™¤è¿åŠ¨æç¤ºåç”šè‡³åœ¨é»‘æš—ä¸­æ²¡æœ‰è§†è§‰åé¦ˆçš„æƒ…å†µä¸‹ä»¥åˆ†çº§æ´»åŠ¨æ°´å¹³è‡ªä¸»æŒç»­å­˜åœ¨ (å›¾ 4b; ç¬¬ä¸‰ä¸ªé¢„æµ‹) , ä»è€Œå®ç°å„ç§ <strong>åå¿ƒåº¦</strong> çš„ç¨³å®šå‡è§†å›ºå®š. æ‰«è§†è¾“å…¥ä¼šä½¿ç³»ç»Ÿç•¥å¾®åç¦»çº¿æ€§å“åº”çŠ¶æ€, ä½†ç¥ç»å“åº”ä¼šè¿…é€Ÿè¡°å‡å›æŒç»­å‘å°„çŠ¶æ€ (ç¬¦åˆç¬¬äºŒä¸ªé¢„æµ‹).  å€¼å¾—æ³¨æ„çš„æ˜¯, åŒä¸€ç³»ç»Ÿè¿˜ç§¯åˆ†å¹³æ»‘çš„å¤´éƒ¨é€Ÿåº¦ä¿¡å·, ä»¥å…è®¸åœ¨å¤´éƒ¨è¿åŠ¨æœŸé—´ç¨³å®šå‡è§†.</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/CJznfR8E5QUZ7Ke.png" alt=""  /></p>
<p><strong>a</strong>, In the goldfish, the positions of the ipsilateral and contralateral eyes ($E_{\text{ipsi}}$ and $E_{\text{contra}}$, respectively) can be maintained for a stable horizontal gaze during inter-saccadic fixation at different angular positions (top two traces). This is supported by stable steps in firing rate by oculomotor integrator neurons (bottom two traces show extracellularly recorded firing rate and voltage (V)), which integrate transient (in the order of about 100 ms) saccadic command bursts.</p>
</blockquote>
<p><strong>a</strong>, åœ¨é‡‘é±¼ä¸­, åŒä¾§çœ¼ç›çš„ä½ç½® (åˆ†åˆ«ä¸ºåŒä¾§çœ¼ $E_{\text{ipsi}}$ å’Œå¯¹ä¾§çœ¼ $E_{\text{contra}}$) å¯ä»¥åœ¨ä¸åŒè§’åº¦ä½ç½®çš„æ‰«è§†é—´å›ºå®šæœŸé—´ä¿æŒç¨³å®šçš„æ°´å¹³å‡è§† (é¡¶éƒ¨ä¸¤æ¡æ›²çº¿).  è¿™å¾—ç›Šäºçœ¼åŠ¨ç§¯åˆ†å™¨ç¥ç»å…ƒçš„ç¨³å®šé˜¶è·ƒå‘å°„ç‡ (åº•éƒ¨ä¸¤æ¡æ›²çº¿æ˜¾ç¤ºç»†èƒå¤–è®°å½•çš„å‘å°„ç‡å’Œç”µå‹ (V)) , å®ƒä»¬ç§¯åˆ†ç¬æ—¶ (å¤§çº¦ 100 æ¯«ç§’) æ‰«è§†æŒ‡ä»¤çˆ†å‘.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/BvtJ4ET7IHbnfDx.png" alt=""  /></p>
<p><strong>b</strong>, Oculomotor neurons drive eye position with linearly ramping tuning curves (bottom). Their responses are the same in the light and the dark (top), and thus do not depend on visual input for gaze stabilization on the timescale of seconds.</p>
</blockquote>
<p><strong>b</strong>, çœ¼åŠ¨ç¥ç»å…ƒé€šè¿‡çº¿æ€§æ–œå¡è°ƒåˆ¶æ›²çº¿é©±åŠ¨çœ¼ä½ (åº•éƒ¨).  å®ƒä»¬çš„å“åº”åœ¨å…‰ç…§å’Œé»‘æš—ä¸­æ˜¯ç›¸åŒçš„ (é¡¶éƒ¨) , å› æ­¤åœ¨æ•°ç§’çš„æ—¶é—´å°ºåº¦ä¸Šä¸ä¾èµ–äºè§†è§‰è¾“å…¥æ¥ç¨³å®šå‡è§†.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/HyNX7de1xFp9a6w.png" alt=""  /></p>
<p><strong>c</strong>, Transient current injection into individual oculomotor neurons results in only a transient (that is, not persistent)** decrease (left)** or <strong>increase (right)</strong> in firing rate, consistent with lack of a cellular origin for persistent intersaccadic firing.</p>
</blockquote>
<p><strong>c</strong>, å¯¹å•ä¸ªçœ¼åŠ¨ç¥ç»å…ƒè¿›è¡Œç¬æ—¶ç”µæµæ³¨å…¥ä»…å¯¼è‡´å‘å°„ç‡çš„ç¬æ—¶ (å³éæŒç»­)** é™ä½ (å·¦)**  æˆ– <strong>å¢åŠ  (å³)</strong> , è¿™ä¸æŒç»­æ‰«è§†é—´å‘å°„ç¼ºä¹ç»†èƒæ¥æºä¸€è‡´.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/cyk7wRISYDr2PBn.png" alt=""  /></p>
<p><strong>d</strong>, Injection of kainic acid into the oculomotor integrator produces leaky dynamics in horizontal eye position, consistent with network models. The leak is pronounced in the dark and is still present although reduced, presumably because of visual feedback, during illumination (triangles).</p>
</blockquote>
<p><strong>d</strong>, å‘çœ¼åŠ¨ç§¯åˆ†å™¨æ³¨å…¥ kainic acid ä¼šåœ¨æ°´å¹³çœ¼ä½ä¸­äº§ç”Ÿæ³„æ¼åŠ¨åŠ›å­¦, è¿™ä¸ç½‘ç»œæ¨¡å‹ä¸€è‡´. æ³„æ¼åœ¨é»‘æš—ä¸­å°¤ä¸ºæ˜æ˜¾, åœ¨å…‰ç…§æœŸé—´ (ç”¨ä¸‰è§’å½¢è¡¨ç¤º) è™½ç„¶æœ‰æ‰€å‡å°‘ä½†ä»ç„¶å­˜åœ¨, è¿™å¯èƒ½æ˜¯ç”±äºè§†è§‰åé¦ˆçš„ç¼˜æ•….</p>
</blockquote>
<blockquote>
<p>ç†æƒ³çš„ç§¯åˆ†å™¨åº”è¯¥ä½¿å¾—çœ¼ç›ä½ç½®ä¿æŒç¨³å®š. è€Œä¸å®Œç¾çš„ç§¯åˆ†å™¨ä¼šä½¿å¾—çœ¼ç›æŒ‡æ•°è¡°å‡å›ä¸­å¿ƒ:</p>
<p>$$
\dot{x} = -\frac{1}{\tau} x\Rightarrow x(t) = x(0) e^{-t/\tau}
$$</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/7ftRJ6EQ1x4TrBF.png" alt=""  /></p>
<p><strong>e</strong>, Visual training (here, from the motion of dots of light in a planetarium-like set-up) that mimics leaky or unstable eye positions in goldfish can mistune the oculomotor integrator, making it unstable or leaky, respectively. Arrows highlight fixations following saccades towards the mid position.</p>
</blockquote>
<p><strong>e</strong>, æ¨¡æ‹Ÿé‡‘é±¼ä¸­æ³„æ¼æˆ–ä¸ç¨³å®šçœ¼ä½çš„è§†è§‰è®­ç»ƒ (è¿™é‡Œæ˜¯é€šè¿‡åœ¨ç±»ä¼¼å¤©æ–‡é¦†çš„è®¾ç½®ä¸­ç‚¹å…‰æºçš„è¿åŠ¨) å¯ä»¥ä½¿çœ¼åŠ¨ç§¯åˆ†å™¨å¤±è°ƒ, åˆ†åˆ«ä½¿å…¶å˜å¾—ä¸ç¨³å®šæˆ–æ³„æ¼. ç®­å¤´çªå‡ºæ˜¾ç¤ºäº†æ‰«è§†å‘ä¸­é—´ä½ç½®åçš„å‡è§†.</p>
</blockquote>
<blockquote>
<p>Integration functionality is a network-level rather than single-cell process: single neurons do not generate persistent responses to transient current injections (Fig. 4c, inset), whereas decreasing network feedback through the use of synaptic blockers reduces the time constant of integration and results in a leaky integrator (Fig. 4c).</p>
<p>It is possible to reduce or increase network feedback through training with a virtual surround that generates an artificial retinal-slip percept (Fig. 4d), implying that the system is capable of error-driven fine-tuning to maintain a high degree of persistence144.</p>
<p>Finally, a recent electron microscopy reconstruction finds recurrent synaptic interconnectivity between integrator neurons, with excitatory connections between ipsilateral neurons and primarily inhibitory contralateral projections, in excellent agreement with line-attractor models of the oculomotor circuit (Fig. 1e).</p>
</blockquote>
<p>ç§¯åˆ†åŠŸèƒ½æ˜¯ç½‘ç»œçº§è€Œéå•ç»†èƒè¿‡ç¨‹: å•ä¸ªç¥ç»å…ƒå¯¹ç¬æ—¶ç”µæµè¾“å…¥ä¸ä¼šäº§ç”ŸæŒç»­å“åº” (å›¾ 4c, æ’å›¾) , è€Œé€šè¿‡ä½¿ç”¨çªè§¦é˜»æ–­å‰‚å‡å°‘ç½‘ç»œåé¦ˆä¼šé™ä½ç§¯åˆ†çš„æ—¶é—´å¸¸æ•°, å¹¶å¯¼è‡´æ³„æ¼ç§¯åˆ†å™¨ (å›¾ 4c).</p>
<p>é€šè¿‡ä½¿ç”¨äº§ç”Ÿäººå·¥è§†ç½‘è†œæ»‘åŠ¨çŸ¥è§‰çš„è™šæ‹Ÿç¯å¢ƒè¿›è¡Œè®­ç»ƒ, å¯ä»¥å‡å°‘æˆ–å¢åŠ ç½‘ç»œåé¦ˆ (å›¾ 4d) , è¿™æ„å‘³ç€è¯¥ç³»ç»Ÿèƒ½å¤Ÿè¿›è¡Œè¯¯å·®é©±åŠ¨çš„å¾®è°ƒä»¥ä¿æŒé«˜åº¦çš„æŒç»­æ€§.</p>
<p>æœ€å, æœ€è¿‘çš„ä¸€é¡¹ç”µå­æ˜¾å¾®é•œé‡å»ºå‘ç°äº†ç§¯åˆ†å™¨ç¥ç»å…ƒä¹‹é—´çš„é€’å½’çªè§¦äº’è¿, åŒä¾§ç¥ç»å…ƒä¹‹é—´å­˜åœ¨å…´å¥‹æ€§è¿æ¥, è€Œä¸»è¦æ˜¯å¯¹ä¾§æŠ•å°„æŠ‘åˆ¶, è¿™ä¸çœ¼åŠ¨å›è·¯çš„çº¿æ€§å¸å¼•å­æ¨¡å‹éå¸¸å»åˆ (å›¾ 1e).</p>
<h3 id="head-direction-cells">Head-direction cells<a hidden class="anchor" aria-hidden="true" href="#head-direction-cells">#</a></h3>
<blockquote>
<p>Some of the earliest experiments to suggest the existence of low-dimensional continuous-attractor dynamics were done in the <strong>rodent</strong> head-direction circuit (Fig. 5a,b).</p>
<p>The headdirection circuit in mammals maintains an updated internal compass estimate of the heading direction, relative to some arbitrary external reference, as animals move around. It does so by integrating internal rotational velocity estimates during navigation and incorporating information from external cues. The head-direction circuit is modelled as a ring-attractor network (Fig. 1c,g, left).</p>
<p>Before large population recordings became available, cell-cell correlations established that the network states remained invariant on a very lowdimensional manifold across environments (Fig. 5a), in line with the first and third predictions. The complete set of states of the several thousand-neuron mammalian head-direction network was shown to consist solely of a one-dimensional ring (Fig. 5b) (in line with the first prediction), revealing that the brain has completely factorized its navigational representations to dedicate a circuit only to head direction.</p>
<p>Furthermore, intervals in the state-space ring manifold map isometrically to intervals of head direction (in line with the fourth prediction), as evidenced by a close match between the isometrically parameterized internal ring states and the measured head direction (Fig. 5b, inset and right).</p>
</blockquote>
<p>æœ€æ—©çš„ä¸€äº›è¡¨æ˜å­˜åœ¨ä½ç»´è¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦çš„å®éªŒæ˜¯åœ¨ <strong>å•®é½¿åŠ¨ç‰©</strong> å¤´éƒ¨æ–¹å‘å›è·¯ä¸­å®Œæˆçš„ (å›¾ 5a, b).</p>
<p>å“ºä¹³åŠ¨ç‰©çš„å¤´éƒ¨æ–¹å‘å›è·¯åœ¨åŠ¨ç‰©å››å¤„ç§»åŠ¨æ—¶, ä¿æŒç›¸å¯¹äºæŸä¸ªä»»æ„å¤–éƒ¨å‚è€ƒçš„èˆªå‘æ–¹å‘çš„æ›´æ–°å†…éƒ¨æŒ‡å—é’ˆä¼°è®¡. å®ƒé€šè¿‡åœ¨å¯¼èˆªè¿‡ç¨‹ä¸­ç§¯åˆ†å†…éƒ¨æ—‹è½¬é€Ÿåº¦ä¼°è®¡å¹¶ç»“åˆæ¥è‡ªå¤–éƒ¨çº¿ç´¢çš„ä¿¡æ¯æ¥å®ç°è¿™ä¸€ç‚¹. å¤´éƒ¨æ–¹å‘å›è·¯è¢«å»ºæ¨¡ä¸ºç¯å½¢å¸å¼•å­ç½‘ç»œ (å›¾ 1c, g, å·¦).</p>
<p>åœ¨å¤§è§„æ¨¡é›†ç¾¤è®°å½•å®ç°å‰, ç»†èƒ-ç»†èƒç›¸å…³æ€§ç¡®å®šäº†ç½‘ç»œçŠ¶æ€åœ¨éå¸¸ä½ç»´æµå½¢ä¸Šåœ¨ä¸åŒç¯å¢ƒä¸­ä¿æŒä¸å˜ (å›¾ 5a) , ç¬¦åˆç¬¬ä¸€ä¸ªå’Œç¬¬ä¸‰ä¸ªé¢„æµ‹. å“ºä¹³åŠ¨ç‰©å¤´éƒ¨æ–¹å‘ç½‘ç»œçš„å‡ åƒä¸ªç¥ç»å…ƒçš„å®Œæ•´çŠ¶æ€é›†è¢«è¯æ˜ä»…ç”±ä¸€ç»´ç¯ç»„æˆ (å›¾ 5b)  (ç¬¦åˆç¬¬ä¸€ä¸ªé¢„æµ‹) , æ­ç¤ºäº†å¤§è„‘å·²ç»å®Œå…¨åˆ†è§£äº†å…¶å¯¼èˆªè¡¨ç¤º, ä»¥ä¸“é—¨ç”¨äºå¤´éƒ¨æ–¹å‘çš„å›è·¯.</p>
<p>æ­¤å¤–, çŠ¶æ€ç©ºé—´ç¯æµå½¢ä¸­çš„é—´éš”ä¸å¤´éƒ¨æ–¹å‘çš„é—´éš”ç­‰è·æ˜ å°„ (ç¬¦åˆç¬¬å››ä¸ªé¢„æµ‹) , è¿™å¯ä»¥é€šè¿‡å†…ç¯çŠ¶æ€ä¸æµ‹é‡çš„å¤´éƒ¨æ–¹å‘ä¹‹é—´çš„å¯†åˆ‡åŒ¹é…æ¥è¯æ˜ (å›¾ 5b, æ’å›¾å’Œå³ä¾§).</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/As3IblP8FEmGDM9.png" alt=""  /></p>
<p><strong>a</strong>, Activity of two cells in the rat head-direction circuit during free foraging in a two-dimensional circular arena with a globally orienting cue (top). When the cue is removed (bottom), the fields rotate, but the cells maintain their tuning shapes and relative tuning angles (pale curves show the cellsâ€™ activity from the top plot, but globally rotated).</p>
</blockquote>
<p><strong>a</strong>, å¤§é¼ å¤´éƒ¨æ–¹å‘å›è·¯ä¸­ä¸¤ä¸ªç»†èƒåœ¨å¸¦æœ‰å…¨å±€å®šå‘æç¤ºçš„äºŒç»´åœ†å°ä¸­è‡ªç”±è§…é£Ÿæ—¶çš„æ´»åŠ¨ (é¡¶éƒ¨).  å½“æç¤ºè¢«ç§»é™¤æ—¶ (åº•éƒ¨) , ç»†èƒçš„è°ƒåˆ¶åœºä¼šæ—‹è½¬, ä½†ç»†èƒä¿æŒå…¶è°ƒåˆ¶å½¢çŠ¶å’Œç›¸å¯¹è°ƒåˆ¶è§’åº¦ (æ·¡è‰²æ›²çº¿æ˜¾ç¤ºé¡¶éƒ¨å›¾ä¸­çš„ç»†èƒæ´»åŠ¨, ä½†è¿›è¡Œäº†å…¨å±€æ—‹è½¬).</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/YOgUwJcbKQ7js35.png" alt=""  /></p>
<p><strong>b</strong>, The population-level states of the anterodorsal thalamus during free-foraging and other natural behaviour in a two-dimensional environment, shown through nonlinear embedding in two dimensions and independently validated by topological data analysis, are confined to a onedimensional ring (as in Fig. 1c). Inset: another view of the same ring manifold in three dimensions (left). The manifold is colourized based on a computational approach called SPUD (spline parameterization for unsupervised decoding): the manifold is fit by a spline of matching dimension and topology (middle), and the spline is parameterized isometrically; equal changes in parameter value for equal distances along the manifold (right). Parameter changes are indicated by colour.</p>
</blockquote>
<p><strong>b</strong>, åœ¨äºŒç»´ç¯å¢ƒä¸­è‡ªç”±è§…é£Ÿå’Œå…¶ä»–è‡ªç„¶è¡Œä¸ºæœŸé—´, å‰èƒŒä¸˜è„‘çš„é›†ç¾¤æ°´å¹³çŠ¶æ€é€šè¿‡äºŒç»´éçº¿æ€§åµŒå…¥æ˜¾ç¤ºï¼Œå¹¶é€šè¿‡æ‹“æ‰‘æ•°æ®åˆ†æç‹¬ç«‹éªŒè¯ï¼Œé™åˆ¶åœ¨ä¸€ç»´ç¯ä¸Š (å¦‚å›¾ 1c æ‰€ç¤º).  æ’å›¾: ä¸‰ç»´ä¸­åŒä¸€ç¯æµå½¢çš„å¦ä¸€ç§è§†å›¾ (å·¦).  æµå½¢åŸºäºä¸€ç§ç§°ä¸º SPUD (æ— ç›‘ç£è§£ç çš„æ ·æ¡å‚æ•°åŒ–) çš„è®¡ç®—æ–¹æ³•è¿›è¡Œç€è‰²: æµå½¢ç”±å…·æœ‰åŒ¹é…ç»´åº¦å’Œæ‹“æ‰‘çš„æ ·æ¡æ‹Ÿåˆ (ä¸­é—´) , å¹¶ä¸”æ ·æ¡æ˜¯ç­‰è·å‚æ•°åŒ–çš„; æµå½¢ä¸Šç›¸ç­‰è·ç¦»çš„å‚æ•°å€¼å˜åŒ–ç›¸ç­‰ (å³).  å‚æ•°å˜åŒ–ç”±é¢œè‰²è¡¨ç¤º.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/iUAJh5KZR7q6N1C.png" alt=""  /></p>
<p><strong>c</strong>, There is a close match between unsupervised isometric parametrization of the manifold from part b and the externally measured head direction of the rodent.</p>
</blockquote>
<p><strong>c</strong>, æ¥è‡ª b éƒ¨åˆ†çš„æµå½¢çš„æ— ç›‘ç£ç­‰è·å‚æ•°åŒ–ä¸å•®é½¿åŠ¨ç‰©å¤–éƒ¨æµ‹é‡çš„å¤´éƒ¨æ–¹å‘ä¹‹é—´å­˜åœ¨å¯†åˆ‡åŒ¹é….</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/oKupJH8qL1tMG7g.png" alt=""  /></p>
<p><strong>d,e</strong>, The same cells as in part b were recorded during rapid eye movement (REM) sleep (green): the states during REM sleep remain confined to a one-dimensional ring that precisely overlays the ring of waking states (blue, part e), and states off the ring exhibit large flows (black arrows) back towards the ring (part d).</p>
</blockquote>
<p><strong>d,e</strong>, ä¸ b éƒ¨åˆ†ç›¸åŒçš„ç»†èƒåœ¨å¿«é€Ÿçœ¼åŠ¨ (REM) ç¡çœ æœŸé—´è¢«è®°å½• (ç»¿è‰²) : REM ç¡çœ æœŸé—´çš„çŠ¶æ€ä»ç„¶é™åˆ¶åœ¨ä¸€ä¸ªä¸€ç»´ç¯ä¸Šï¼Œè¯¥ç¯ç²¾ç¡®åœ°è¦†ç›–äº†æ¸…é†’çŠ¶æ€çš„ç¯ (è“è‰²ï¼Œe éƒ¨åˆ†) , è€Œç¯å¤–çš„çŠ¶æ€è¡¨ç°å‡ºå¤§å¹…åº¦çš„æµåŠ¨ (é»‘è‰²ç®­å¤´) å›åˆ°ç¯ä¸Š (d éƒ¨åˆ†).</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/OQtJGwHU4XLaP3u.png" alt=""  /></p>
<p><strong>f</strong>, Calcium imaging of activity in the physically ring-shaped Drosophila ellipsoid body reveals a localized bump of excitation that follows the movement of a cue in the flyâ€™s visual field.</p>
</blockquote>
<p><strong>f</strong>, å¯¹ç‰©ç†ç¯å½¢æœè‡æ¤­åœ†ä½“ä¸­æ´»åŠ¨çš„é’™æˆåƒæ­ç¤ºäº†ä¸€ä¸ªå±€éƒ¨çš„å…´å¥‹å³°, å®ƒè·Ÿéšæœè‡è§†è§‰åœºä¸­çº¿ç´¢çš„è¿åŠ¨.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/lFi9KVWQcartGHS.png" alt=""  /></p>
<p><strong>g</strong>, A combination of electrophysiology and electron microscopy imaging of the central complex in flies has provided detailed layout and connectivity data for comparison with predicted connectivity in ring attractor models.</p>
</blockquote>
<p><strong>g</strong>, å¯¹æœè‡ä¸­å¤®å¤åˆä½“çš„ç”µç”Ÿç†å­¦å’Œç”µå­æ˜¾å¾®é•œæˆåƒçš„ç»“åˆæä¾›äº†è¯¦ç»†çš„å¸ƒå±€å’Œè¿æ¥æ•°æ®, ä»¥ä¸ç¯å½¢å¸å¼•å­æ¨¡å‹ä¸­é¢„æµ‹çš„è¿æ¥è¿›è¡Œæ¯”è¾ƒ.</p>
</blockquote>
<blockquote>
<p>After natural perturbations away from the ring attractor, the activity of the head-direction circuit flowed back to it (Fig. 5d), meeting the second prediction, and the ring manifold was invariant across waking and rapid eye movement (REM) sleep (Fig. 5e), meeting the third prediction.</p>
<p>These findings explicitly validate the most fundamental predictions of ring attractor models and continuous attractor-based integrators, providing (together with the grid cell system; see below) the most direct and compelling evidence of continuous-attractor dynamics in the brain.</p>
</blockquote>
<p>åœ¨è¿œç¦»ç¯å½¢å¸å¼•å­è¿›è¡Œè‡ªç„¶æ‰°åŠ¨å, å¤´éƒ¨æ–¹å‘å›è·¯çš„æ´»åŠ¨æµå›åˆ°å®ƒ (å›¾ 5d) , æ»¡è¶³äº†ç¬¬äºŒä¸ªé¢„æµ‹, å¹¶ä¸”ç¯æµå½¢åœ¨æ¸…é†’å’Œå¿«é€Ÿçœ¼åŠ¨ (REM) ç¡çœ æœŸé—´æ˜¯ä¸å˜çš„ (å›¾ 5e) , æ»¡è¶³äº†ç¬¬ä¸‰ä¸ªé¢„æµ‹.</p>
<p>è¿™äº›å‘ç°æ˜ç¡®éªŒè¯äº†ç¯å½¢å¸å¼•å­æ¨¡å‹å’ŒåŸºäºè¿ç»­å¸å¼•å­çš„ç§¯åˆ†å™¨çš„æœ€åŸºæœ¬é¢„æµ‹, æä¾›äº† (ä¸ç½‘æ ¼ç»†èƒç³»ç»Ÿä¸€èµ·; è§ä¸‹æ–‡) å¤§è„‘ä¸­è¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦çš„æœ€ç›´æ¥å’Œä»¤äººä¿¡æœçš„è¯æ®.</p>
<blockquote>
<p>In a striking example of <strong>convergent evolution</strong>, <strong>Drosophila</strong> compute head-direction estimates using apparently very similar dynamics to <strong>mammals</strong>. The fly neural compass circuit is topographically organized such that the neuropil forms a physical ring-shaped structure in the ellipsoid body, with a local moving activity peak that tracks head direction as the fly turns (Fig. 5f).</p>
<p>Other notable advantages of the fly circuit in the effort to characterize its mechanisms are that the number of neurons is small and their morphology and connectivity have been fully traced (Fig. 5g). This detailed view of the circuit permits quantitative, not just qualitative, comparisons with ring-attractor models.</p>
</blockquote>
<p>åœ¨ <strong>è¶‹åŒè¿›åŒ–</strong> çš„æ˜¾è‘—ä¾‹è¯ä¸­ï¼Œ<strong>æœè‡</strong> ä¸ <strong>å“ºä¹³åŠ¨ç‰©</strong> é‡‡ç”¨æä¸ºç›¸ä¼¼çš„åŠ¨åŠ›å­¦æœºåˆ¶æ¥è®¡ç®—å¤´éƒ¨æ–¹å‘ä¼°è®¡å€¼ã€‚æœè‡ç¥ç»æŒ‡å—é’ˆå›è·¯å…·æœ‰æ‹“æ‰‘ç»“æ„ç‰¹å¾ï¼šç¥ç»èƒ¶è´¨åœ¨æ¤­åœ†ä½“ä¸­å½¢æˆç‰©ç†ç¯çŠ¶ç»“æ„ï¼Œå…¶å±€éƒ¨æ´»åŠ¨å³°å€¼éšæœè‡è½¬å‘è€Œå®æ—¶è¿½è¸ªå¤´éƒ¨æ–¹å‘ (å›¾5f) ã€‚</p>
<p>åœ¨åŠªåŠ›è¡¨å¾å…¶æœºåˆ¶æ–¹é¢, æœè‡å›è·¯çš„å…¶ä»–æ˜¾è‘—ä¼˜åŠ¿æ˜¯ç¥ç»å…ƒæ•°é‡å°‘ä¸”å…¶å½¢æ€å’Œè¿æ¥æ€§å·²è¢«å®Œå…¨è¿½è¸ª (å›¾ 5g).  å¯¹å›è·¯çš„è¿™ç§è¯¦ç»†äº†è§£å…è®¸å¯¹ç¯å½¢å¸å¼•å­æ¨¡å‹è¿›è¡Œå®šé‡è€Œä¸ä»…ä»…æ˜¯å®šæ€§çš„æ¯”è¾ƒ.</p>
<blockquote>
<p>The combined activity and connectivity data reveal that the fly head-direction system quite literally implements the copy-and-offset double-ring network architecture that has been proposed for velocity integration. However, the dimensionality of the fly head-direction circuit and its full state-space dynamics remain to be characterized.</p>
<p>Notably, although the circuit is organized physically as a ring network, recent evidence suggests that the insect head-direction circuit may be involved in performing two-dimensional path integration as well. Thus, unlike the anterodorsal thalamic nucleus network in mammals, the insect head-direction circuit may not be confined to a one-dimensional ring of attractor states that fully factorizes out the representation of head direction in its representation of spatial variables.</p>
</blockquote>
<p>ç»“åˆæ´»åŠ¨å’Œè¿æ¥æ•°æ®è¡¨æ˜, æœè‡å¤´éƒ¨æ–¹å‘ç³»ç»Ÿå®é™…ä¸Šå®ç°äº†ä¸ºé€Ÿåº¦ç§¯åˆ†æå‡ºçš„å¤åˆ¶å’Œåç§»åŒç¯ç½‘ç»œæ¶æ„. ç„¶è€Œ, æœè‡å¤´éƒ¨æ–¹å‘å›è·¯çš„ç»´åº¦åŠå…¶å®Œæ•´çš„çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ä»æœ‰å¾…è¡¨å¾.</p>
<p>å€¼å¾—æ³¨æ„çš„æ˜¯, å°½ç®¡è¯¥å›è·¯åœ¨ç‰©ç†ä¸Šç»„ç»‡ä¸ºç¯å½¢ç½‘ç»œ, ä½†æœ€è¿‘çš„è¯æ®è¡¨æ˜æ˜†è™«å¤´éƒ¨æ–¹å‘å›è·¯å¯èƒ½ä¹Ÿå‚ä¸æ‰§è¡ŒäºŒç»´è·¯å¾„ç§¯åˆ†. å› æ­¤, ä¸å“ºä¹³åŠ¨ç‰©çš„å‰èƒŒä¸˜è„‘æ ¸ç½‘ç»œä¸åŒ, æ˜†è™«å¤´éƒ¨æ–¹å‘å›è·¯å¯èƒ½ä¸é™äºä¸€ç»´ç¯å½¢å¸å¼•å­çŠ¶æ€, è¿™äº›çŠ¶æ€åœ¨å…¶ç©ºé—´å˜é‡è¡¨ç¤ºä¸­å®Œå…¨åˆ†è§£äº†å¤´éƒ¨æ–¹å‘çš„è¡¨ç¤º.</p>
<blockquote>
<p>Finally, the head-direction system of both insects and mammals can be re-anchored and reset based on tuned external cues, and this can change the orientation tuning curves of cells and moment by moment firing rates of cells in a way that remains consistent with the third prediction for attractor dynamics.</p>
</blockquote>
<p>æœ€å, æ˜†è™«å’Œå“ºä¹³åŠ¨ç‰©çš„å¤´éƒ¨æ–¹å‘ç³»ç»Ÿéƒ½å¯ä»¥åŸºäºè°ƒåˆ¶çš„å¤–éƒ¨çº¿ç´¢é‡æ–°é”šå®šå’Œé‡ç½®, è¿™å¯ä»¥ä»¥ä¸€ç§ä¸å¸å¼•å­åŠ¨åŠ›å­¦ç¬¬ä¸‰ä¸ªé¢„æµ‹ä¿æŒä¸€è‡´çš„æ–¹å¼æ”¹å˜ç»†èƒçš„æ–¹å‘è°ƒåˆ¶æ›²çº¿å’Œç»†èƒçš„ç¬æ—¶å‘å°„ç‡.</p>
<h3 id="grid-cells">Grid cells<a hidden class="anchor" aria-hidden="true" href="#grid-cells">#</a></h3>
<blockquote>
<p>A grid cell encodes spatial location through a periodic <strong>triangular-lattice</strong> discharge pattern that tiles explored two-dimensional spaces. Grid cell phases update during movement in the light and in the dark to reflect the animalâ€™s current position, as a two-dimensional phase.</p>
<p>Continuous-attractor models of grid cells are based on collective Turing pattern formation, explain their velocity integration function and predict that grid cells should exist in large sets with identical spatial periodicity and orientation, but tile all possible twodimensional phases.</p>
<p>As with the first general prediction of continuousattractor models, they specifically predict that the population states of such a set of cells should be confined to merely two dimensions along a torus-shaped manifold that remains unchanged across environments and behavioural states15 (Fig. 1d, rightmost column).</p>
</blockquote>
<p>ç½‘æ ¼ç»†èƒé€šè¿‡å‘¨æœŸæ€§çš„ <strong>ä¸‰è§’æ™¶æ ¼</strong> æ”¾ç”µæ¨¡å¼ç¼–ç ç©ºé—´ä½ç½®, è¯¥æ¨¡å¼å¹³é“ºäº†æ¢ç´¢çš„äºŒç»´ç©ºé—´. ç½‘æ ¼ç»†èƒåœ¨æœ‰å…‰å’Œé»‘æš—ä¸­è¿åŠ¨æ—¶æ›´æ–°å…¶ç›¸ä½, å³åŠ¨ç‰©çš„å½“å‰ä½ç½®æ˜ å°„ä¸ºäºŒç»´ç›¸ä½.</p>
<p>ç½‘æ ¼ç»†èƒçš„è¿ç»­å¸å¼•å­æ¨¡å‹åŸºäºé›†ä½“ Turing æ¨¡å¼å½¢æˆ, è§£é‡Šäº†å®ƒä»¬çš„é€Ÿåº¦ç§¯åˆ†åŠŸèƒ½, å¹¶é¢„æµ‹ç½‘æ ¼ç»†èƒåº”è¯¥å­˜åœ¨äºå…·æœ‰ç›¸åŒç©ºé—´å‘¨æœŸæ€§å’Œæ–¹å‘çš„å¤§é›†åˆä¸­, ä½†å¹³é“ºæ‰€æœ‰å¯èƒ½çš„äºŒç»´ç›¸ä½.</p>
<p>ä¸è¿ç»­å¸å¼•å­æ¨¡å‹çš„ç¬¬ä¸€ä¸ªä¸€èˆ¬é¢„æµ‹ä¸€æ ·, å®ƒä»¬ç‰¹åˆ«é¢„æµ‹: è¿™æ ·ä¸€ç»„ç»†èƒçš„ç¾¤ä½“çŠ¶æ€åº”ä»…é™äºæ²¿ç€ç¯é¢å½¢æµå½¢çš„ä¸¤ä¸ªç»´åº¦, è¯¥æµå½¢åœ¨ä¸åŒç¯å¢ƒå’Œè¡Œä¸ºçŠ¶æ€ä¸‹ä¿æŒä¸å˜ (å›¾ 1d, æœ€å³åˆ—).</p>
<blockquote>
<p>Analyses of simultaneously recorded grid cells with similar periods revealed that their periods and orientations are identical down to estimation noise (thus defining a discrete population, subsequently called a â€˜moduleâ€™) and that they tile all possible two-dimensional phases, strongly suggesting a two-dimensional torus in line with the first prediction.</p>
<p>Moreover, the relative firing phases and grid parameter ratios of co-modular cells are tightly conserved even as the spatial tuning of cells varies across time and environments (Fig. 6a), with the dimensionality of the spatial environment (Fig. 6b) and with large environmental rescaling-driven deformations of grid tuning, confirming the prediction of invariance. In addition, the detailed cell-cell relationships seen in waking exploration that define the low-dimensional response of a grid module are conserved across overnight sleep in grid cells but not in place cells (Fig. 6c), establishing that the low-dimensional states are autonomously generated.</p>
<p>In line with all of the fundamental predictions of continuous-attractor dynamics, these findings established that each grid moduleâ€™s response is very low-dimensional; is invariant across environments, time and behavioural states; and is internally stabilized and autonomously generated.</p>
<p>Most recently, these findings were confirmed by large-scale recordings of grid cells that made it possible to directly characterize the grid cell population response by applying the <strong>topological analyses of statespace structure</strong> pioneered earlier to grid cells (Fig. 6e), directly illustrating the low-dimensional, toroidal and invariant state-space structure of grid cell modules.</p>
</blockquote>
<p>å¯¹åŒæ­¥è®°å½•çš„å…·æœ‰ç›¸ä¼¼å‘¨æœŸçš„ç½‘æ ¼ç»†èƒçš„åˆ†æè¡¨æ˜, å®ƒä»¬çš„å‘¨æœŸå’Œæ–¹å‘åœ¨ä¼°è®¡å™ªå£°èŒƒå›´å†…æ˜¯ç›¸åŒçš„ (ä»è€Œå®šä¹‰äº†ä¸€ä¸ªç¦»æ•£ç¾¤ä½“, éšåç§°ä¸º &ldquo;æ¨¡å—&rdquo; ) , å¹¶ä¸”å®ƒä»¬å¹³é“ºäº†æ‰€æœ‰å¯èƒ½çš„äºŒç»´ç›¸ä½, è¿™å¼ºçƒˆæš—ç¤ºäº†ä¸ç¬¬ä¸€ä¸ªé¢„æµ‹ä¸€è‡´çš„äºŒç»´ç¯é¢.</p>
<p>æ­¤å¤–, å³ä½¿éšç€æ—¶é—´å’Œç¯å¢ƒ (å›¾ 6a) ä¸­ç»†èƒç©ºé—´è°ƒåˆ¶çš„å˜åŒ–, ä»¥åŠç»†èƒç½‘æ ¼è°ƒåˆ¶çš„å¤§è§„æ¨¡ç¯å¢ƒé‡ç¼©æ”¾é©±åŠ¨å˜å½¢, å…±æ¨¡ç»†èƒçš„ç›¸å¯¹å‘å°„ç›¸ä½å’Œç½‘æ ¼å‚æ•°æ¯”ç‡ä¹Ÿå¾—åˆ°äº†ç´§å¯†ä¿å­˜, ç¡®è®¤äº†ä¸å˜æ€§çš„é¢„æµ‹. æ­¤å¤–, åœ¨ç½‘æ ¼ç»†èƒä¸­è¿‡å¤œç¡çœ æœŸé—´, å®šä¹‰ç½‘æ ¼æ¨¡å—ä½ç»´å“åº”çš„æ¸…é†’æ¢ç´¢ä¸­è§‚å¯Ÿåˆ°çš„è¯¦ç»†ç»†èƒ-ç»†èƒå…³ç³»å¾—ä»¥ä¿å­˜, è€Œåœ¨ä½ç½®ç»†èƒä¸­åˆ™æ²¡æœ‰ (å›¾ 6c) , ç¡®ç«‹äº†ä½ç»´çŠ¶æ€æ˜¯è‡ªä¸»ç”Ÿæˆçš„.</p>
<p>ç¬¦åˆè¿ç»­å¸å¼•å­åŠ¨åŠ›å­¦çš„æ‰€æœ‰åŸºæœ¬é¢„æµ‹, è¿™äº›å‘ç°ç¡®ç«‹äº†æ¯ä¸ªç½‘æ ¼æ¨¡å—å“åº”</p>
<ol>
<li>æ˜¯éå¸¸ä½ç»´çš„;</li>
<li>åœ¨ç¯å¢ƒã€æ—¶é—´å’Œè¡Œä¸ºçŠ¶æ€ä¸‹æ˜¯ä¸å˜çš„;</li>
<li>æ˜¯å†…éƒ¨ç¨³å®šå’Œè‡ªä¸»ç”Ÿæˆçš„.</li>
</ol>
<p>æœ€è¿‘, é€šè¿‡å¯¹ç½‘æ ¼ç»†èƒçš„å¤§å°ºåº¦è®°å½•è¯å®äº†è¿™äº›å‘ç°, è¿™ä½¿å¾—é€šè¿‡åº”ç”¨æ—©å…ˆåœ¨ç½‘æ ¼ç»†èƒä¸Šå¼€åˆ›çš„ <strong>çŠ¶æ€ç©ºé—´ç»“æ„æ‹“æ‰‘åˆ†æ</strong> æ¥ç›´æ¥è¡¨å¾ç½‘æ ¼ç»†èƒç¾¤ä½“å“åº”æˆä¸ºå¯èƒ½ (å›¾ 6e) , ç›´æ¥è¯´æ˜äº†ç½‘æ ¼ç»†èƒæ¨¡å—çš„ä½ç»´ã€ç¯é¢å’Œä¸å˜çŠ¶æ€ç©ºé—´ç»“æ„.</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/dOYKDiZfCu5Pncz.png" alt=""  /></p>
<p><strong>a</strong>, The spatial tuning periods and orientations of grid cells reconfigure substantially in novel environments (left: firing patterns of an example pair of grid cells in a familiar and a novel environment), but cellâ€“cell relationships remain the same, as seen from the tight covariance of changes across cells (right), implying an internally generated low-dimensional structure. Each colour corresponds to a variable that describes the lattice of the spatial tuning curve of the cell, as shown in the schematic.</p>
</blockquote>
<p><strong>a</strong>, ç½‘æ ¼ç»†èƒåœ¨æ–°ç¯å¢ƒä¸­çš„ç©ºé—´è°ƒåˆ¶å‘¨æœŸå’Œæ–¹å‘å‘ç”Ÿäº†å®è´¨æ€§é‡æ„ (å·¦å›¾: ä¾‹å­ä¸­ä¸€å¯¹ç½‘æ ¼ç»†èƒåœ¨ç†Ÿæ‚‰å’Œæ–°ç¯å¢ƒä¸­çš„å‘å°„æ¨¡å¼) , ä½†ç»†èƒ-ç»†èƒå…³ç³»ä¿æŒä¸å˜, ä»ç»†èƒé—´å˜åŒ–çš„ç´§å¯†åæ–¹å·®ä¸­å¯ä»¥çœ‹å‡º (å³å›¾) , è¿™æ„å‘³ç€ä¸€ä¸ªå†…éƒ¨ç”Ÿæˆçš„ä½ç»´ç»“æ„. æ¯ç§é¢œè‰²å¯¹åº”æè¿°ç»†èƒç©ºé—´è°ƒåˆ¶æ›²çº¿æ™¶æ ¼çš„å˜é‡, å¦‚ç¤ºæ„å›¾æ‰€ç¤º.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/a3RdhUSVzFBoT9l.png" alt=""  /></p>
<p><strong>b</strong>, The non-periodic responses of two example co-modular cells (dark blue) on a one-dimensional linear track do not look like simple offsets of one another, raising the question of whether cellâ€“cell relationships have reconfigured and the grid cell dynamics are not low-dimensional and invariant. However, the responses of the cells can be predicted (light blue) as parallel slices through the two-dimensional grid (bottom), and their two-dimensional relative phase offset is predicted by the separation of the one-dimensional response slices, showing that the cell relationships and two-dimensional circuit dynamics are preserved across diverse conditions.</p>
</blockquote>
<p><strong>b</strong>, ä¸¤ä¸ªä¾‹å­ä¸­å…±æ¨¡ç»†èƒ (æ·±è“è‰²) åœ¨ä¸€ç»´çº¿æ€§è½¨é“ä¸Šçš„éå‘¨æœŸæ€§å“åº”çœ‹èµ·æ¥ä¸åƒå½¼æ­¤çš„ç®€å•åç§», è¿™å¼•å‘äº†ç»†èƒ-ç»†èƒå…³ç³»æ˜¯å¦å·²é‡æ–°é…ç½®ä»¥åŠç½‘æ ¼ç»†èƒåŠ¨åŠ›å­¦æ˜¯å¦ä¸æ˜¯ä½ç»´å’Œä¸å˜çš„é—®é¢˜. ç„¶è€Œ, è¿™äº›ç»†èƒçš„å“åº”å¯ä»¥é¢„æµ‹ (æµ…è“è‰²) ä¸ºäºŒç»´ç½‘æ ¼çš„å¹³è¡Œåˆ‡ç‰‡ (åº•éƒ¨) , å®ƒä»¬çš„äºŒç»´ç›¸å¯¹ç›¸ä½åç§»ç”±ä¸€ç»´å“åº”åˆ‡ç‰‡çš„åˆ†ç¦»é¢„æµ‹, æ˜¾ç¤ºç»†èƒå…³ç³»å’ŒäºŒç»´å›è·¯åŠ¨åŠ›å­¦åœ¨å„ç§æ¡ä»¶ä¸‹å¾—ä»¥ä¿å­˜.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/jSJhz67b3n2stKo.png" alt=""  /></p>
<p><strong>c</strong>, Pairwise correlations between grid cells in the medial entorhinal cortex (MEC) measured during navigation are  preserved across overnight rapid eye movement (REM) sleep and non-REM (NREM) sleep, whereas those of place cells in hippocampal area CA1 are not.</p>
</blockquote>
<p><strong>c</strong>, åœ¨å¯¼èˆªæœŸé—´æµ‹é‡çš„å†…å—…çš®å±‚ (MEC) ä¸­ç½‘æ ¼ç»†èƒä¹‹é—´çš„æˆå¯¹ç›¸å…³æ€§åœ¨è¿‡å¤œå¿«é€Ÿçœ¼åŠ¨ (REM) ç¡çœ å’Œé REM (NREM) ç¡çœ æœŸé—´å¾—ä»¥ä¿å­˜, è€Œæµ·é©¬åŒº CA1 ä¸­ä½ç½®ç»†èƒçš„ç›¸å…³æ€§åˆ™æ²¡æœ‰.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/i5dTpcuhI7mUAzH.png" alt=""  /></p>
<p><strong>d</strong>, Grid cells are anatomically arranged according to their relative spatial firing phases. Left: cell positions in a field of view of the MEC coloured according to the phase of their spatial tuning curves. The relative cortical positions of same-phase cells make a triangular lattice pattern (middle), with a grid-like autocorrelation pattern (right).</p>
</blockquote>
<p><strong>d</strong>, ç½‘æ ¼ç»†èƒæ ¹æ®å…¶ç›¸å¯¹ç©ºé—´å‘å°„ç›¸ä½è¿›è¡Œè§£å‰–æ’åˆ—. å·¦å›¾: MEC è§†é‡ä¸­ç»†èƒçš„ä½ç½®æ ¹æ®å…¶ç©ºé—´è°ƒåˆ¶æ›²çº¿çš„ç›¸ä½è¿›è¡Œç€è‰². åŒç›¸ä½ç»†èƒçš„ç›¸å¯¹çš®å±‚ä½ç½®å½¢æˆä¸‰è§’å½¢æ™¶æ ¼æ¨¡å¼ (ä¸­é—´) , å…·æœ‰ç½‘æ ¼çŠ¶è‡ªç›¸å…³æ¨¡å¼ (å³ä¾§).</p>
</blockquote>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/27/XMkWJ9zHNjorZsU.png" alt=""  /></p>
<p><strong>e</strong>, The population-level states of grid cells from one module (each dot represents the population state at one point in time) during free foraging in a two-dimensional environment are shown through nonlinear dimensionality reduction and confirmed by topological data analysis to lie on a two-dimensional torus (left) as predicted by models15. As the animal follows a spatial trajectory (right), the state moves along the torus manifold (left). Manifold colouring is a gradient along the first principal component of the data.</p>
</blockquote>
<p><strong>e</strong>, åœ¨äºŒç»´ç¯å¢ƒä¸­è‡ªç”±è§…é£ŸæœŸé—´, æ¥è‡ªä¸€ä¸ªæ¨¡å—çš„ç½‘æ ¼ç»†èƒçš„ç¾¤ä½“æ°´å¹³çŠ¶æ€ (æ¯ä¸ªç‚¹è¡¨ç¤ºæŸä¸€æ—¶é—´ç‚¹çš„ç¾¤ä½“çŠ¶æ€) é€šè¿‡éçº¿æ€§é™ç»´æ˜¾ç¤º, å¹¶é€šè¿‡æ‹“æ‰‘æ•°æ®åˆ†æç¡®è®¤ä½äºäºŒç»´ç¯é¢ä¸Š (å·¦ä¾§) , æ­£å¦‚æ¨¡å‹æ‰€é¢„æµ‹çš„é‚£æ ·. å½“åŠ¨ç‰©æ²¿ç€ç©ºé—´è½¨è¿¹ç§»åŠ¨æ—¶ (å³ä¾§) , çŠ¶æ€æ²¿ç€ç¯é¢æµå½¢ç§»åŠ¨ (å·¦ä¾§).  æµå½¢ç€è‰²æ˜¯æ•°æ®ç¬¬ä¸€ä¸»æˆåˆ†çš„æ¸å˜.</p>
</blockquote>
<blockquote>
<p>A corollary is that the grid cell response is not derived from upstream place cells, which remap across environments and during sleep (Fig. 6c): as shown in ref.101, this finding renders models in which the place cell response is primary to grid cells inconsistent with the data. Another corollary of the population states of grid cells remaining strictly preserved, even when their spatial tuning curves in two-dimensional and three-dimensional environments are altered so they do not form equilateral triangular grids, is that these variations must result from changes in how the invariant internal states are mapped to external states. Such changes may arise from, for example, alterations in velocity estimation that stretch the grid or from external cues that shift the phase of the grid cell network, rather than because of alterations in the internal grid network dynamics.</p>
</blockquote>
<p>ä¸€ä¸ªæ¨è®ºæ˜¯, ç½‘æ ¼ç»†èƒçš„å“åº”ä¸æ˜¯æ¥è‡ªä¸Šæ¸¸çš„ä½ç½®ç»†èƒ, è¿™äº›ç»†èƒåœ¨ç¯å¢ƒå’Œç¡çœ æœŸé—´ä¼šé‡æ–°æ˜ å°„ (å›¾ 6c) : æ­£å¦‚å‚è€ƒæ–‡çŒ® 101 æ‰€ç¤º, è¿™ä¸€å‘ç°ä½¿å¾—ä½ç½®ç»†èƒå“åº”å¯¹ç½‘æ ¼ç»†èƒèµ·ä¸»è¦ä½œç”¨çš„æ¨¡å‹ä¸æ•°æ®ä¸ä¸€è‡´. ç½‘æ ¼ç»†èƒçš„ç¾¤ä½“çŠ¶æ€ä¸¥æ ¼ä¿æŒä¸å˜, å³ä½¿å®ƒä»¬åœ¨äºŒç»´å’Œä¸‰ç»´ç¯å¢ƒä¸­çš„ç©ºé—´è°ƒåˆ¶æ›²çº¿è¢«æ”¹å˜, ä»¥è‡³äºå®ƒä»¬ä¸å½¢æˆç­‰è¾¹ä¸‰è§’å½¢ç½‘æ ¼, å…¶å¦ä¸€ä¸ªæ¨è®ºæ˜¯, è¿™äº›å˜åŒ–å¿…é¡»æ˜¯ç”±äºä¸å˜çš„å†…éƒ¨çŠ¶æ€å¦‚ä½•æ˜ å°„åˆ°å¤–éƒ¨çŠ¶æ€çš„å˜åŒ–æ‰€è‡´. è¿™äº›å˜åŒ–å¯èƒ½æºè‡ªä¾‹å¦‚é€Ÿåº¦ä¼°è®¡çš„æ”¹å˜, ä»è€Œæ‹‰ä¼¸ç½‘æ ¼, æˆ–æ¥è‡ªå¤–éƒ¨çº¿ç´¢, ä»è€Œç§»åŠ¨ç½‘æ ¼ç»†èƒç½‘ç»œçš„ç›¸ä½, è€Œä¸æ˜¯ç”±äºå†…éƒ¨ç½‘æ ¼ç½‘ç»œåŠ¨åŠ›å­¦çš„æ”¹å˜.</p>
<blockquote>
<p>Despite having periodic representations, and thus each only representing position as an ambiguous two-dimensional phase, collectively grid cells form a discrete set of modules with distinct but similar periodicities164. This allows grid cells to unambiguously represent position over a scale that grows exponentially in the number of grid modules.</p>
</blockquote>
<p>å°½ç®¡å…·æœ‰å‘¨æœŸæ€§è¡¨ç¤º, å› æ­¤æ¯ä¸ªä»…å°†ä½ç½®è¡¨ç¤ºä¸ºæ¨¡ç³Šçš„äºŒç»´ç›¸ä½, ä½†ç½‘æ ¼ç»†èƒé›†ä½“å½¢æˆäº†ä¸€ç»„å…·æœ‰ä¸åŒä½†ç›¸ä¼¼å‘¨æœŸæ€§çš„ç¦»æ•£æ¨¡å—. è¿™å…è®¸ç½‘æ ¼ç»†èƒä»¥éšç€ç½‘æ ¼æ¨¡å—æ•°é‡å‘ˆæŒ‡æ•°å¢é•¿çš„æ¯”ä¾‹æ˜ç¡®åœ°è¡¨ç¤ºä½ç½®.</p>
<blockquote>
<p>In sum, the head-direction cell and grid cell systems show that the same pattern formation principle â€” based on local excitation or disinhibition, with broader inhibition â€” that is pivotal for morphogenesis in plants and animals is also fundamental to the genesis of stationary continuous-attractor states for computation and representation in the brain.</p>
</blockquote>
<p>æ€»ä¹‹, å¤´éƒ¨æ–¹å‘ç»†èƒå’Œç½‘æ ¼ç»†èƒç³»ç»Ÿè¡¨æ˜, åŒæ ·çš„æ¨¡å¼å½¢æˆåŸç†â€”â€”åŸºäºå±€éƒ¨å…´å¥‹æˆ–å»æŠ‘åˆ¶, ä¼´éšç€æ›´å¹¿æ³›çš„æŠ‘åˆ¶â€”â€”å¯¹äºæ¤ç‰©å’ŒåŠ¨ç‰©çš„å½¢æ€å‘ç”Ÿè‡³å…³é‡è¦, å¯¹äºå¤§è„‘ä¸­è®¡ç®—å’Œè¡¨ç¤ºçš„é™æ€è¿ç»­å¸å¼•å­çŠ¶æ€çš„äº§ç”Ÿä¹Ÿæ˜¯åŸºæœ¬çš„.</p>
<h3 id="graded-working-memory-networks">Graded working memory networks<a hidden class="anchor" aria-hidden="true" href="#graded-working-memory-networks">#</a></h3>
<blockquote>
<p>In monkeys trained to saccade to a remembered cued location (selected from a set arranged in a circle), cells in the <strong>prefrontal cortex</strong> and <strong>posterior parietal cortex</strong> exhibit persistent activity across the delay period that is selective for the direction of the cue, consistent with the first and third predictions of attractor dynamics.</p>
<p>The delay period activity in the <strong>prefrontal cortex</strong> is a bump that moves apparently randomly along a onedimensional manifold with the characteristics of a diffusion process. Thus, the variance in bump location grows linearly with time during the delay, as predicted by continuous-attractor models, but the bump profile remains largely invariant (first and second predictions).</p>
<p>Bump movement predicts subsequent behavioural errors, suggesting that these states are repositories or read-outs of the memory.</p>
</blockquote>
<p>åœ¨æ¥å—è®­ç»ƒä»¥æ‰«è§†è®°å¿†æç¤ºä½ç½® (ä»æ’åˆ—æˆåœ†å½¢çš„ä¸€ç»„ä¸­é€‰æ‹©) çš„çŒ´å­ä¸­, <strong>å‰é¢å¶çš®å±‚</strong> å’Œ <strong>åé¡¶å¶çš®å±‚</strong> çš„ç»†èƒåœ¨å»¶è¿ŸæœŸé—´è¡¨ç°å‡ºå¯¹æç¤ºæ–¹å‘é€‰æ‹©æ€§çš„æŒç»­æ´»åŠ¨, è¿™ä¸å¸å¼•å­åŠ¨åŠ›å­¦çš„ç¬¬ä¸€ä¸ªå’Œç¬¬ä¸‰ä¸ªé¢„æµ‹ä¸€è‡´.</p>
<p><strong>å‰é¢å¶çš®å±‚</strong> ä¸­çš„å»¶è¿ŸæœŸæ´»åŠ¨æ˜¯ä¸€ä¸ªæ²¿ç€ä¸€ç»´æµå½¢éšæœºç§»åŠ¨çš„å³°, å…¶ç‰¹å¾ç±»ä¼¼äºæ‰©æ•£è¿‡ç¨‹. å› æ­¤, åœ¨å»¶è¿ŸæœŸé—´, å³°ä½ç½®çš„æ–¹å·®éšç€æ—¶é—´çº¿æ€§å¢é•¿, æ­£å¦‚è¿ç»­å¸å¼•å­æ¨¡å‹æ‰€é¢„æµ‹çš„é‚£æ ·, ä½†å³°å€¼è½®å»“åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¿æŒä¸å˜ (ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªé¢„æµ‹).</p>
<p>å³°å€¼ç§»åŠ¨é¢„æµ‹éšåçš„è¡Œä¸ºé”™è¯¯, è¡¨æ˜è¿™äº›çŠ¶æ€æ˜¯è®°å¿†çš„å­˜å‚¨åº“æˆ–è¯»å‡º.</p>
<blockquote>
<p>The need for extensive training and the resulting tailoring of the attractor states to this specific but not naturally encountered multi-cue task suggests that this attractor forms through learning in a flexible system. We might therefore also expect a loss of the neural correlation structure if the animal is subsequently trained on other tasks, unlike with the grid and head-direction cell networks.</p>
</blockquote>
<p>éœ€è¦å¤§é‡è®­ç»ƒä»¥åŠå¯¹å¸å¼•å­çŠ¶æ€çš„è°ƒæ•´ä»¥é€‚åº”è¿™ä¸€ç‰¹å®šä½†éè‡ªç„¶é‡åˆ°çš„å¤šæç¤ºä»»åŠ¡, è¡¨æ˜è¯¥å¸å¼•å­æ˜¯é€šè¿‡çµæ´»ç³»ç»Ÿä¸­çš„å­¦ä¹ å½¢æˆçš„. å› æ­¤, å¦‚æœåŠ¨ç‰©éšåæ¥å—å…¶ä»–ä»»åŠ¡çš„è®­ç»ƒ, æˆ‘ä»¬ä¹Ÿå¯èƒ½æœŸæœ›ç¥ç»ç›¸å…³ç»“æ„çš„ä¸¢å¤±, è¿™ä¸ç½‘æ ¼å’Œå¤´éƒ¨æ–¹å‘ç»†èƒç½‘ç»œä¸åŒ.</p>
<h2 id="limit-cycle-attractors">Limit-cycle attractors<a hidden class="anchor" aria-hidden="true" href="#limit-cycle-attractors">#</a></h2>
<blockquote>
<p>The CNS and <strong>peripheral nervous system</strong> contain numerous instances of periodic dynamics, from the spiking of single neurons to circadian rhythms and sleep-cycle generation, to rhythmic activity in motor circuits.</p>
<p>The amplitude of a linear oscillator is set by the initial condition (for example, the height at which a pendulum is released), whereas <u>limit-cycle oscillators have an invariant intrinsic amplitude</u>. Thus, oscillations that decay or whose long-term amplitude or frequency changes after transient perturbation are not limit cycles.</p>
</blockquote>
<p>ä¸­æ¢ç¥ç»ç³»ç»Ÿå’Œ <strong>å¤–å‘¨ç¥ç»ç³»ç»Ÿ</strong> åŒ…å«è®¸å¤šå‘¨æœŸæ€§åŠ¨åŠ›å­¦çš„å®ä¾‹, ä»å•ä¸ªç¥ç»å…ƒçš„å°–å³°å‘å°„åˆ°æ˜¼å¤œèŠ‚å¾‹å’Œç¡çœ å‘¨æœŸç”Ÿæˆ, å†åˆ°è¿åŠ¨å›è·¯ä¸­çš„èŠ‚å¾‹æ´»åŠ¨.</p>
<p>çº¿æ€§æŒ¯è¡å™¨çš„æŒ¯å¹…ç”±åˆå§‹æ¡ä»¶è®¾å®š (ä¾‹å¦‚, å•æ‘†é‡Šæ”¾æ—¶çš„é«˜åº¦) , è€Œ<u>æé™ç¯æŒ¯è¡å™¨å…·æœ‰ä¸å˜çš„å†…ç¦€æŒ¯å¹…</u>. å› æ­¤, åœ¨ç¬æ—¶æ‰°åŠ¨åè¡°å‡æˆ–å…¶é•¿æœŸæŒ¯å¹…æˆ–é¢‘ç‡å‘ç”Ÿå˜åŒ–çš„æŒ¯è¡ä¸æ˜¯æé™ç¯.</p>
<blockquote>
<p>Many of the oscillations noted above maintain their amplitude over time and, given their robustness, are probably generated through attractor dynamics.</p>
<p>Experimentally well-characterized examples of sustained periodic dynamics are <strong>central pattern generators</strong> in spinal motor circuits that drive swimming, crawling, walking, breathing and digestion; these differ in specifics across species but have common principles of mechanism and operation, including high robustness.</p>
<p>Central pattern generator circuits typically integrate external feedback, but can operate in isolation without external drive189. However, driven (non-autonomous) systems could exhibit limit cycles that are attributable to their inputs rather than to intrinsic attractor dynamics.</p>
</blockquote>
<p>è®¸å¤šä¸Šè¿°æŒ¯è¡éšç€æ—¶é—´çš„æ¨ç§»ä¿æŒå…¶æŒ¯å¹…, å¹¶ä¸”é‰´äºå…¶ç¨³å¥æ€§, å¯èƒ½æ˜¯é€šè¿‡å¸å¼•å­åŠ¨åŠ›å­¦äº§ç”Ÿçš„.</p>
<p>å®éªŒä¸Šè¡¨å¾è‰¯å¥½çš„æŒç»­å‘¨æœŸåŠ¨åŠ›å­¦çš„ä¾‹å­æ˜¯è„Šé«“è¿åŠ¨å›è·¯ä¸­çš„ <strong>ä¸­å¤®æ¨¡å¼å‘ç”Ÿå™¨</strong>, å®ƒä»¬é©±åŠ¨æ¸¸æ³³ã€çˆ¬è¡Œã€è¡Œèµ°ã€å‘¼å¸å’Œæ¶ˆåŒ–; è¿™äº›åœ¨ä¸åŒç‰©ç§ä¸­å…·ä½“æƒ…å†µä¸åŒ, ä½†åœ¨æœºåˆ¶å’Œæ“ä½œåŸç†ä¸Šå…·æœ‰å…±åŒç‚¹, åŒ…æ‹¬é«˜åº¦çš„ç¨³å¥æ€§.</p>
<p>ä¸­å¤®æ¨¡å¼å‘ç”Ÿå™¨å›è·¯é€šå¸¸ç§¯åˆ†å¤–éƒ¨åé¦ˆ, ä½†ä¹Ÿå¯ä»¥åœ¨æ²¡æœ‰å¤–éƒ¨é©±åŠ¨çš„æƒ…å†µä¸‹ç‹¬ç«‹è¿è¡Œ. ç„¶è€Œ, å—é©±åŠ¨ (éè‡ªä¸») ç³»ç»Ÿå¯èƒ½è¡¨ç°å‡ºæé™ç¯, å…¶å½’å› äº(å¤–éƒ¨)è¾“å…¥è€Œä¸æ˜¯å†…ç¦€çš„å¸å¼•å­åŠ¨åŠ›å­¦.</p>
<blockquote>
<p>Given the sizeable literature on these topics, we refer the reader to some excellent papers and reviews.</p>
</blockquote>
<p>é‰´äºè¿™äº›ä¸»é¢˜çš„åºå¤§æ–‡çŒ®, æˆ‘ä»¬å»ºè®®è¯»è€…å‚è€ƒä¸€äº›ä¼˜ç§€çš„è®ºæ–‡å’Œç»¼è¿°æ–‡ç« .</p>
<h1 id="departures-from-attractor-dynamics">Departures from attractor dynamics<a hidden class="anchor" aria-hidden="true" href="#departures-from-attractor-dynamics">#</a></h1>
<blockquote>
<p>Not all circuits hypothesized to exhibit low-dimensional attractor dynamics seem under further experimentation to do so, or currently lack sufficient evidence to establish such dynamics in the circuit. We discuss three such examples.</p>
</blockquote>
<p>å¹¶éæ‰€æœ‰å‡è®¾è¡¨ç°å‡ºä½ç»´å¸å¼•å­åŠ¨åŠ›å­¦çš„å›è·¯åœ¨è¿›ä¸€æ­¥å®éªŒä¸­ä¼¼ä¹éƒ½è¿™æ ·åš, æˆ–è€…ç›®å‰ç¼ºä¹è¶³å¤Ÿçš„è¯æ®æ¥ç¡®ç«‹å›è·¯ä¸­çš„è¿™ç§åŠ¨åŠ›å­¦. æˆ‘ä»¬è®¨è®ºä¸‰ä¸ªè¿™æ ·çš„ä¾‹å­.</p>
<h2 id="orientation-tuning-in-visual-cortex">Orientation tuning in visual cortex<a hidden class="anchor" aria-hidden="true" href="#orientation-tuning-in-visual-cortex">#</a></h2>
<blockquote>
<p>The circuit of simple cells in the <strong>primary visual cortex</strong> (V1) satisfies some key properties of attractor networks: V1 and V2 cells exhibit orientation-tuned responses to real and illusory edges, and in V1 the activity of neurons with similar orientation tuning is correlated during spontaneous activity.</p>
<p>However, changing the state of an attractor requires strong inputs and is slow, inconsistent with the need for perceptual systems to respond sensitively and rapidly.</p>
<p>Moreover, the responses to illusory edges in V1 tend to occur at longer latency than responses to real edges, suggestive of top-down inputs rather than within-V1 dynamics. These observations lend weight to the possibility that responses might be dominated by feedforward drive, potentially with non-normal amplification processes. Quantitative characterizations of response speed will be important to draw clear conclusions about V1 circuit dynamics.</p>
</blockquote>
<p><strong>åˆçº§è§†è§‰çš®å±‚</strong> (V1) ä¸­ç®€å•ç»†èƒçš„å›è·¯æ»¡è¶³å¸å¼•å­ç½‘ç»œçš„ä¸€äº›å…³é”®å±æ€§: V1 å’Œ V2 ç»†èƒå¯¹çœŸå®å’Œé”™è§‰è¾¹ç¼˜è¡¨ç°å‡ºæ–¹å‘è°ƒåˆ¶å“åº”, å¹¶ä¸”åœ¨ V1 ä¸­, å…·æœ‰ç›¸ä¼¼æ–¹å‘è°ƒåˆ¶çš„ç¥ç»å…ƒåœ¨è‡ªå‘æ´»åŠ¨æœŸé—´çš„æ´»åŠ¨æ˜¯ç›¸å…³çš„.</p>
<p>ç„¶è€Œ, å¸å¼•å­çŠ¶æ€æ”¹å˜éœ€è¦å¼ºè¾“å…¥å¹¶ä¸”å¾ˆæ…¢, è¿™ä¸æ„ŸçŸ¥ç³»ç»Ÿéœ€è¦æ•æ„Ÿä¸”å¿«é€Ÿå“åº”çš„éœ€æ±‚ä¸ä¸€è‡´.</p>
<p>æ­¤å¤–, V1 ä¸­å¯¹é”™è§‰è¾¹ç¼˜çš„å“åº”å¾€å¾€æ¯”å¯¹çœŸå®è¾¹ç¼˜çš„å“åº”å‘ç”Ÿå¾—æ›´æ™š, è¿™è¡¨æ˜æ˜¯è‡ªä¸Šè€Œä¸‹çš„è¾“å…¥è€Œä¸æ˜¯ V1 å†…éƒ¨åŠ¨åŠ›å­¦. è¿™äº›è§‚å¯Ÿç»“æœæ”¯æŒè¿™æ ·ä¸€ç§å¯èƒ½æ€§, å³å“åº”å¯èƒ½ä»¥å‰é¦ˆé©±åŠ¨ä¸ºä¸», å¯èƒ½ä¼´éšç€éæ­£è§„æ”¾å¤§è¿‡ç¨‹. å¯¹å“åº”é€Ÿåº¦çš„å®šé‡è¡¨å¾å¯¹äºå¾—å‡ºå…³äº V1 å›è·¯åŠ¨åŠ›å­¦çš„æ˜ç¡®ç»“è®ºå°†éå¸¸é‡è¦.</p>
<h2 id="place-cells">Place cells<a hidden class="anchor" aria-hidden="true" href="#place-cells">#</a></h2>
<blockquote>
<p>Place cells form stable representations of space that can persist in the dark and shortly after the animal has fallen asleep. In any particular environment, the population response lies on a lowdimensional manifold in state space.</p>
<p>Accordingly, the place cell circuit has been modelled as a continuous-attractor network with one or multiple overlapping maps, whereby each map is a different assignment of cells to spatial locations.</p>
<p>However, the storage of multiple high-resolution maps in a homogeneous attractor network severely limits capacity. Cell-cell correlations are not preserved across environments, as implied by the phenomenon of remapping. Similar to V1 neurons, place cells might be better described as deriving their tuning by forming conjunctions between multiple feedforward inputs, including those from grid cells and cells that encode external cues such as borders, landmarks and reward sites.</p>
<p>At the same time, place cells exhibit sequential activation of previous trajectories during activity hippocampal replay. This sequential activation is hypothesized to be generated by recurrent connections in hippocampal area CA3, suggesting that recurrent and feedforward dynamics may collaborate in the generation of place cell states; more  recent models are beginning to capture this interplay. Closing the book on the question of autonomous low-dimensional dynamics in what, in our view, is the far more complex response of place cells than grid cells requires more detailed experimentation, analysis and modelling.</p>
</blockquote>
<p>ä½ç½®ç»†èƒå½¢æˆç©ºé—´çš„ç¨³å®šè¡¨ç¤º, è¿™äº›è¡¨ç¤ºå¯ä»¥åœ¨é»‘æš—ä¸­æŒç»­å­˜åœ¨, å¹¶ä¸”åœ¨åŠ¨ç‰©å…¥ç¡åä¸ä¹…ä¹Ÿèƒ½æŒç»­å­˜åœ¨. åœ¨ä»»ä½•ç‰¹å®šç¯å¢ƒä¸­, ç¾¤ä½“å“åº”ä½äºçŠ¶æ€ç©ºé—´ä¸­çš„ä½ç»´æµå½¢ä¸Š.</p>
<p>å› æ­¤, ä½ç½®ç»†èƒå›è·¯è¢«å»ºæ¨¡ä¸ºå…·æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªé‡å åœ°å›¾çš„è¿ç»­å¸å¼•å­ç½‘ç»œ, å…¶ä¸­æ¯ä¸ªåœ°å›¾æ˜¯ç»†èƒä»¬åˆ°ç©ºé—´ä½ç½®çš„ä¸åŒåˆ†é….</p>
<p>ç„¶è€Œ, åœ¨å‡åŒ€å¸å¼•å­ç½‘ç»œä¸­å­˜å‚¨å¤šä¸ªé«˜åˆ†è¾¨ç‡åœ°å›¾æ—¶, å®¹é‡è¢«ä¸¥é‡é™åˆ¶. ç»†èƒ-ç»†èƒç›¸å…³æ€§åœ¨ä¸åŒç¯å¢ƒä¸­æ²¡æœ‰å¾—åˆ°ä¿å­˜, è¿™ä¸é‡æ–°æ˜ å°„ç°è±¡æ‰€æš—ç¤ºçš„ä¸€è‡´. ç±»ä¼¼äº V1 ç¥ç»å…ƒ, ä½ç½®ç»†èƒå¯èƒ½æ›´å¥½åœ°æè¿°ä¸ºé€šè¿‡å½¢æˆå¤šä¸ªå‰é¦ˆè¾“å…¥çš„ç»“åˆæ¥è·å¾—å…¶è°ƒåˆ¶, åŒ…æ‹¬æ¥è‡ªç½‘æ ¼ç»†èƒå’Œç¼–ç å¤–éƒ¨çº¿ç´¢ (å¦‚è¾¹ç•Œã€åœ°æ ‡å’Œå¥–åŠ±åœ°ç‚¹) çš„ç»†èƒçš„è¾“å…¥.</p>
<p>ä¸æ­¤åŒæ—¶, ä½ç½®ç»†èƒåœ¨æµ·é©¬é‡æ”¾æœŸé—´è¡¨ç°å‡ºå…ˆå‰è½¨è¿¹çš„åºåˆ—æ¿€æ´». è¿™ç§åºåˆ—æ¿€æ´»è¢«å‡è®¾æ˜¯ç”±æµ·é©¬ä½“ CA3 åŒºåŸŸä¸­çš„é€’å½’è¿æ¥äº§ç”Ÿçš„, è¿™è¡¨æ˜é€’å½’å’Œå‰é¦ˆåŠ¨åŠ›å­¦å¯èƒ½åœ¨ä½ç½®ç»†èƒçŠ¶æ€çš„ç”Ÿæˆä¸­ååŒå·¥ä½œ; æœ€è¿‘çš„æ¨¡å‹å¼€å§‹æ•æ‰è¿™ç§ç›¸äº’ä½œç”¨. è¦è§£å†³æˆ‘ä»¬è®¤ä¸ºä½ç½®ç»†èƒçš„å“åº”æ¯”ç½‘æ ¼ç»†èƒå¤æ‚å¾—å¤šçš„é—®é¢˜ä¸­è‡ªä¸»ä½ç»´åŠ¨åŠ›å­¦çš„é—®é¢˜, éœ€è¦æ›´è¯¦ç»†çš„å®éªŒã€åˆ†æå’Œå»ºæ¨¡.</p>
<h2 id="motor-cortical-trajectories">Motor cortical trajectories<a hidden class="anchor" aria-hidden="true" href="#motor-cortical-trajectories">#</a></h2>
<blockquote>
<p>Finally, recordings of <strong>motor cortical activity</strong> during stereotyped arm movements in primates reveal the existence of stable low-dimensional trajectories, similar to the trajectories in state space that were originally characterized in olfactory circuit responses to different odours.</p>
<p>Limit cycles and other low-dimensional attractors have been hypothesized to have a key role in cortical movement generation. The behaviours typically performed during these neural recordings are themselves restricted to be stereotyped and low-dimensional, and thus it remains unclear whether activity would remain equally low-dimensional across richer behaviours (for example, over the set of all possible arm movements).</p>
<p>Recent evidence from perturbation experiments suggests that neural trajectories in the motor cortex during skilled movements are driven by input from the thalamus, and thus that the circuits for motor pattern generation in the CNS might be distributed across multiple brain regions.</p>
<p>Characterizing the intrinsic dimensionality of motor cortical activity, and determining whether the command to make more-complex motions involves multiple upstream or distributed primitive attractors, remain important open questions for both clinical brain-machine interfaces and neuroscience.</p>
</blockquote>
<p>æœ€å, åœ¨çµé•¿ç±»åŠ¨ç‰©è¿›è¡Œåˆ»æ¿è‡‚éƒ¨è¿åŠ¨æœŸé—´è®°å½•çš„ <strong>è¿åŠ¨çš®å±‚æ´»åŠ¨</strong> æ­ç¤ºäº†ç¨³å®šçš„ä½ç»´è½¨è¿¹çš„å­˜åœ¨, ç±»ä¼¼äºæœ€åˆåœ¨å¯¹ä¸åŒæ°”å‘³çš„å—…è§‰å›è·¯å“åº”ä¸­è¡¨å¾çš„çŠ¶æ€ç©ºé—´ä¸­çš„è½¨è¿¹.</p>
<p>æé™ç¯å’Œå…¶ä»–ä½ç»´å¸å¼•å­è¢«å‡è®¾åœ¨çš®å±‚è¿åŠ¨ç”Ÿæˆä¸­èµ·å…³é”®ä½œç”¨. è¿™äº›ç¥ç»è®°å½•æœŸé—´é€šå¸¸æ‰§è¡Œçš„è¡Œä¸ºæœ¬èº«è¢«é™åˆ¶ä¸ºåˆ»æ¿ä¸”ä½ç»´, å› æ­¤å°šä¸æ¸…æ¥šåœ¨æ›´ä¸°å¯Œçš„è¡Œä¸º (ä¾‹å¦‚, åœ¨æ‰€æœ‰å¯èƒ½çš„æ‰‹è‡‚è¿åŠ¨é›†åˆä¸Š) ä¸­æ´»åŠ¨æ˜¯å¦ä»ç„¶ä¿æŒåŒæ ·çš„ä½ç»´.</p>
<p>æ¥è‡ªæ‰°åŠ¨å®éªŒçš„æœ€æ–°è¯æ®è¡¨æ˜, ç†Ÿç»ƒè¿åŠ¨æœŸé—´è¿åŠ¨çš®å±‚ä¸­çš„ç¥ç»è½¨è¿¹æ˜¯ç”±ä¸˜è„‘è¾“å…¥é©±åŠ¨çš„, å› æ­¤ CNS ä¸­è¿åŠ¨æ¨¡å¼ç”Ÿæˆçš„å›è·¯å¯èƒ½åˆ†å¸ƒåœ¨å¤šä¸ªå¤§è„‘åŒºåŸŸ.</p>
<p>è¡¨å¾è¿åŠ¨çš®å±‚æ´»åŠ¨çš„å†…åœ¨ç»´åº¦, å¹¶ç¡®å®šå‘å‡ºæ›´å¤æ‚è¿åŠ¨å‘½ä»¤æ˜¯å¦æ¶‰åŠå¤šä¸ªä¸Šæ¸¸æˆ–åˆ†å¸ƒå¼åŸå§‹å¸å¼•å­, ä»ç„¶æ˜¯ä¸´åºŠè„‘-æœºæ¥å£å’Œç¥ç»ç§‘å­¦çš„é‡è¦æœªè§£ä¹‹è°œ.</p>
<h1 id="flexibility-despite-rigidity">Flexibility despite rigidity<a hidden class="anchor" aria-hidden="true" href="#flexibility-despite-rigidity">#</a></h1>
<blockquote>
<p>The attractor networks we have described in this Review are typically rigid across time and conditions. However, recent experimental and theoretical work has suggested that low-dimensional and rigid attractor states could be reused and recombined to create versatile and efficient systems for representation and computation in new situations.</p>
</blockquote>
<p>æˆ‘ä»¬åœ¨æœ¬ç»¼è¿°ä¸­æè¿°çš„å¸å¼•å­ç½‘ç»œé€šå¸¸åœ¨æ—¶é—´å’Œæ¡ä»¶ä¸Šæ˜¯åˆšæ€§çš„. ç„¶è€Œ, æœ€è¿‘çš„å®éªŒå’Œç†è®ºå·¥ä½œè¡¨æ˜, ä½ç»´ä¸”åˆšæ€§çš„å¸å¼•å­çŠ¶æ€å¯ä»¥è¢«é‡å¤ä½¿ç”¨å’Œé‡æ–°ç»„åˆ, ä»¥åœ¨æ–°æƒ…å†µä¸‹åˆ›å»ºç”¨äºè¡¨ç¤ºå’Œè®¡ç®—çš„å¤šåŠŸèƒ½ä¸”é«˜æ•ˆçš„ç³»ç»Ÿ.</p>
<blockquote>
<p>Building a representation (Fig. 2a) could proceed by painstakingly constructing a large set of associative feedforward correspondences, equivalent to a look-up table.</p>
<p>By contrast, an attractor that is an integrator requires only two feedforward correspondences: an anchoring process that identifies one external state to one internal one, and then an association of external movement-based velocities with the internal shift mechanism in the integrator (Fig. 2f).</p>
<p>Thus, continuous attractors that are also integrators could enable, for example, the rapid construction and even inference of states visited for the first time through a new trajectory, and could be reused to represent multiple variables.</p>
<p>Indeed, the brain seems to (re)use grid cells and place cells when navigating in space and in non-spatial domains; recent work shows how the dimensionality of the represented variable could be greater than the individual attractor networks.</p>
</blockquote>
<p>æ„å»ºè¡¨ç¤º (å›¾ 2a) å¯ä»¥é€šè¿‡è¾›è‹¦åœ°æ„å»ºå¤§é‡å…³è”çš„å‰é¦ˆå¯¹åº”å…³ç³»æ¥è¿›è¡Œ, è¿™ç›¸å½“äºä¸€ä¸ªæŸ¥æ‰¾è¡¨.</p>
<p>ç›¸æ¯”ä¹‹ä¸‹, ä½œä¸ºç§¯åˆ†å™¨çš„å¸å¼•å­åªéœ€è¦ä¸¤ä¸ªå‰é¦ˆå¯¹åº”å…³ç³»: ä¸€ä¸ªé”šå®šè¿‡ç¨‹, å°†ä¸€ä¸ªå¤–éƒ¨çŠ¶æ€è¯†åˆ«ä¸ºä¸€ä¸ªå†…éƒ¨çŠ¶æ€, ç„¶åå°†åŸºäºå¤–éƒ¨è¿åŠ¨çš„é€Ÿåº¦ä¸ç§¯åˆ†å™¨ä¸­çš„å†…éƒ¨ç§»ä½æœºåˆ¶ç›¸å…³è” (å›¾ 2f).</p>
<p>å› æ­¤, æ—¢æ˜¯è¿ç»­å¸å¼•å­åˆæ˜¯ç§¯åˆ†å™¨çš„å¸å¼•å­å¯ä»¥å®ç°, ä¾‹å¦‚, é€šè¿‡æ–°çš„è½¨è¿¹å¿«é€Ÿæ„å»ºç”šè‡³æ¨æ–­é¦–æ¬¡è®¿é—®çš„çŠ¶æ€, å¹¶ä¸”å¯ä»¥é‡å¤ä½¿ç”¨ä»¥è¡¨ç¤ºå¤šä¸ªå˜é‡.</p>
<p>äº‹å®ä¸Š, å¤§è„‘ä¼¼ä¹çš„ç¡®åœ¨ç©ºé—´å’Œéç©ºé—´é¢†åŸŸå¯¼èˆªæ—¶ (é‡æ–°) ä½¿ç”¨ç½‘æ ¼ç»†èƒå’Œä½ç½®ç»†èƒ; æœ€è¿‘çš„å·¥ä½œæ˜¾ç¤º, æ‰€è¡¨ç¤ºå˜é‡çš„ç»´åº¦å¯èƒ½å¤§äºå•ä¸ªå¸å¼•å­ç½‘ç»œ.</p>
<blockquote>
<p>A further line of work has posited that networks composed of modular subnetworks, each an attractor network, enable a given number of neurons to represent an exponentially larger number of representational or memory states through combinations of states than fully connected, Hopfield-like networks can.</p>
<p>Although the combinatorial states expressed by the set of attractor networks are not themselves attractors, it is possible to couple together these subnetworks to generate an exponential number of attractor states such that they each have a reasonably sized basin and are thus robust (Fig. 2). The states in these networks cannot have arbitrary form and content; they are defined by the rigid states of each module.</p>
<p>Thus, a crucial question is how they could be leveraged for memory. Such high-capacity sets of attractor states have been shown to provide possible models for high-capacity and robust action selection, robust classification62 and smoothly decaying associative memory. Moreover, the principles described in this paragraph can be combined in a â€˜mixed modular coding schemeâ€™ to represent and store inputs of any dimensionality relative to the individual attractor networks, so long as it is lower than the summed attractor dimension across networks, without needing to reconfigure the recurrent network (Fig. 2h). Much of the potential for alternative uses, configurations or combinations of attractor networks remains unexplored and is ripe for further study.</p>
</blockquote>
<p>è¿›ä¸€æ­¥çš„å·¥ä½œè·¯çº¿å‡è®¾, ç”±æ¨¡å—åŒ–å­ç½‘ç»œç»„æˆçš„ç½‘ç»œ, æ¯ä¸ªéƒ½æ˜¯ä¸€ä¸ªå¸å¼•å­ç½‘ç»œ, ä½¿å¾—ç»™å®šæ•°é‡çš„ç¥ç»å…ƒèƒ½å¤Ÿé€šè¿‡çŠ¶æ€ç»„åˆè¡¨ç¤ºæ¯”å®Œå…¨è¿æ¥çš„ç±»ä¼¼ Hopfield ç½‘ç»œæ›´å¤šçš„è¡¨ç¤ºæˆ–è®°å¿†çŠ¶æ€.</p>
<p>å°½ç®¡ç”±ä¸€ç»„å¸å¼•å­ç½‘ç»œè¡¨è¾¾çš„ç»„åˆçŠ¶æ€æœ¬èº«ä¸æ˜¯å¸å¼•å­, ä½†å¯ä»¥å°†è¿™äº›å­ç½‘ç»œè€¦åˆåœ¨ä¸€èµ·ä»¥ç”ŸæˆæŒ‡æ•°æ•°é‡çš„å¸å¼•å­çŠ¶æ€, ä½¿å®ƒä»¬æ¯ä¸ªéƒ½æœ‰ä¸€ä¸ªåˆç†å¤§å°çš„è°·åº•, å› æ­¤æ˜¯ç¨³å¥çš„ (å›¾ 2).  è¿™äº›ç½‘ç»œä¸­çš„çŠ¶æ€ä¸èƒ½å…·æœ‰ä»»æ„å½¢å¼å’Œå†…å®¹; å®ƒä»¬ç”±æ¯ä¸ªæ¨¡å—çš„åˆšæ€§çŠ¶æ€å®šä¹‰.</p>
<p>å› æ­¤, ä¸€ä¸ªå…³é”®é—®é¢˜æ˜¯å¦‚ä½•åˆ©ç”¨å®ƒä»¬è¿›è¡Œè®°å¿†. å·²ç»è¯æ˜, è¿™äº›é«˜å®¹é‡çš„å¸å¼•å­çŠ¶æ€é›†ä¸ºé«˜å®¹é‡å’Œç¨³å¥çš„åŠ¨ä½œé€‰æ‹©ã€ç¨³å¥åˆ†ç±»å’Œå¹³æ»‘è¡°å‡çš„å…³è”è®°å¿†æä¾›äº†å¯èƒ½çš„æ¨¡å‹. æ­¤å¤–, æœ¬æ®µä¸­æè¿°çš„åŸç†å¯ä»¥ç»“åˆåœ¨ &ldquo;æ··åˆæ¨¡å—ç¼–ç æ–¹æ¡ˆ&rdquo; ä¸­, ä»¥ç›¸å¯¹äºå•ä¸ªå¸å¼•å­ç½‘ç»œè¡¨ç¤ºå’Œå­˜å‚¨ä»»ä½•ç»´åº¦çš„è¾“å…¥, åªè¦å®ƒä½äºè·¨ç½‘ç»œçš„æ€»å¸å¼•å­ç»´åº¦, è€Œæ— éœ€é‡æ–°é…ç½®é€’å½’ç½‘ç»œ (å›¾ 2h).  å¯¹å¸å¼•å­ç½‘ç»œçš„æ›¿ä»£ç”¨é€”ã€é…ç½®æˆ–ç»„åˆçš„æ½œåŠ›ä»æœ‰å¾ˆå¤§ä¸€éƒ¨åˆ†æœªè¢«æ¢ç´¢, å€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶.</p>
<h1 id="looking-ahead">Looking ahead<a hidden class="anchor" aria-hidden="true" href="#looking-ahead">#</a></h1>
<blockquote>
<p>The theory of attractor dynamics in the brain has provided a powerful and unifying conceptual framework for understanding integration, representation, memory, error correction and efficient learning and inference in the brain. The experimental effort to study candidate attractor circuits and test their predictions has been a fertile field of research, and population-wide physiology techniques have led to breath-taking direct visualizations of attractor dynamics at work in the brain.</p>
</blockquote>
<p>å¸å¼•å­åŠ¨åŠ›å­¦ç†è®ºä¸ºç†è§£å¤§è„‘ä¸­çš„ç§¯åˆ†ã€è¡¨ç¤ºã€è®°å¿†ã€è¯¯å·®æ ¡æ­£ä»¥åŠé«˜æ•ˆå­¦ä¹ å’Œæ¨ç†æä¾›äº†ä¸€ä¸ªå¼ºå¤§è€Œç»Ÿä¸€çš„æ¦‚å¿µæ¡†æ¶. ç ”ç©¶å€™é€‰å¸å¼•å­å›è·¯å¹¶æµ‹è¯•å…¶é¢„æµ‹çš„å®éªŒå·¥ä½œä¸€ç›´æ˜¯ä¸€ä¸ªå¯Œæœ‰æˆæœçš„ç ”ç©¶é¢†åŸŸ, ç¾¤ä½“èŒƒå›´çš„ç”Ÿç†æŠ€æœ¯å·²ç»å¯¼è‡´äº†å¯¹å¤§è„‘ä¸­å¸å¼•å­åŠ¨åŠ›å­¦å·¥ä½œçš„æƒŠäººç›´æ¥å¯è§†åŒ–.</p>
<blockquote>
<p>The theory is also proving to be a powerful tool in interpreting how artificial neural networks (ANNs) solve complex tasks. ANNs trained to robustly solve memory, integration and decision-making tasks in domains as diverse as spatial navigation, vision and language develop attractor dynamics, suggesting that attractor networks not only are able to solve such problems but also might be necessary when the computing elements are memoryless neurons. Furthermore, equipping ANNs with preconfigured attractor networks can help produce faster,  more data-efficient and generalizable learning. Because ANNs can be trained on complex tasks and then fully examined after learning, they will potentially more readily contribute to the next chapter in our understanding of how continuous-attractor networks can interact and combine with other mechanisms to enable the brain to solve rich problems associated with intelligence.</p>
</blockquote>
<p>è¯¥ç†è®ºä¹Ÿè¢«è¯æ˜æ˜¯è§£é‡Šäººå·¥ç¥ç»ç½‘ç»œ (ANN) å¦‚ä½•è§£å†³å¤æ‚ä»»åŠ¡çš„å¼ºå¤§å·¥å…·. åœ¨ç©ºé—´å¯¼èˆªã€è§†è§‰å’Œè¯­è¨€ç­‰å¤šç§é¢†åŸŸä¸­, ç»è¿‡è®­ç»ƒä»¥ç¨³å¥åœ°è§£å†³è®°å¿†ã€ç§¯åˆ†å’Œå†³ç­–ä»»åŠ¡çš„ ANN å‘å±•å‡ºå¸å¼•å­åŠ¨åŠ›å­¦, è¿™è¡¨æ˜å¸å¼•å­ç½‘ç»œä¸ä»…èƒ½å¤Ÿè§£å†³æ­¤ç±»é—®é¢˜, è€Œä¸”å½“è®¡ç®—å…ƒç´ æ˜¯æ— è®°å¿†ç¥ç»å…ƒæ—¶å¯èƒ½æ˜¯å¿…è¦çš„. æ­¤å¤–, ä¸º ANN é…å¤‡é¢„é…ç½®çš„å¸å¼•å­ç½‘ç»œå¯ä»¥å¸®åŠ©äº§ç”Ÿæ›´å¿«ã€æ›´é«˜æ•ˆå’Œæ›´å…·æ³›åŒ–èƒ½åŠ›çš„å­¦ä¹ . ç”±äº ANN å¯ä»¥åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒ, ç„¶ååœ¨å­¦ä¹ åè¿›è¡Œå…¨é¢æ£€æŸ¥, å®ƒä»¬æœ‰å¯èƒ½æ›´å®¹æ˜“ä¸ºæˆ‘ä»¬ç†è§£è¿ç»­å¸å¼•å­ç½‘ç»œå¦‚ä½•ä¸å…¶ä»–æœºåˆ¶ç›¸äº’ä½œç”¨å’Œç»“åˆä»¥ä½¿å¤§è„‘èƒ½å¤Ÿè§£å†³ä¸æ™ºèƒ½ç›¸å…³çš„ä¸°å¯Œé—®é¢˜çš„ä¸‹ä¸€ç« åšå‡ºè´¡çŒ®.</p>
<blockquote>
<p>Notable mechanistic questions about attractor networks also remain open. One avenue may involve moving away from the high firing-rate asynchronous spiking regimens to better understand whether low firing-rate synchronous spiking networks might support attractor dynamics â€” and thus permit a combination of fast timescale dynamics such as spike synchronization and oscillatory phase dynamics. For continuous attractors, understanding how the brain deals with the problem of fine-tuning in linear networks or the imposition and maintenance of a continuous symmetry across neurons remains unknown and is ripe for resolution.</p>
</blockquote>
<p>å…³äºå¸å¼•å­ç½‘ç»œçš„æ˜¾è‘—æœºåˆ¶é—®é¢˜ä»ç„¶æ‚¬è€Œæœªå†³. ä¸€ä¸ªé€”å¾„å¯èƒ½æ¶‰åŠè¿œç¦»é«˜å‘å°„ç‡å¼‚æ­¥å°–å³°å‘å°„æ–¹æ¡ˆ, ä»¥æ›´å¥½åœ°ç†è§£ä½å‘å°„ç‡åŒæ­¥å°–å³°å‘å°„ç½‘ç»œæ˜¯å¦å¯èƒ½æ”¯æŒå¸å¼•å­åŠ¨åŠ›å­¦â€”â€”ä»è€Œå…è®¸å¿«é€Ÿæ—¶é—´å°ºåº¦åŠ¨åŠ›å­¦ (å¦‚å°–å³°åŒæ­¥å’ŒæŒ¯è¡ç›¸ä½åŠ¨åŠ›å­¦) çš„ç»„åˆ. å¯¹äºè¿ç»­å¸å¼•å­, äº†è§£å¤§è„‘å¦‚ä½•å¤„ç†çº¿æ€§ç½‘ç»œä¸­çš„å¾®è°ƒé—®é¢˜æˆ–åœ¨ç¥ç»å…ƒä¹‹é—´å¼ºåŠ å’Œç»´æŒè¿ç»­å¯¹ç§°æ€§ä»ç„¶æœªçŸ¥, å¹¶ä¸”æœ‰å¾…è§£å†³.</p>
<blockquote>
<p>A few models of the development of continuous attractors show how they could emerge simply through unsupervised associative plasticity, whereas others are based on combining feedback of known or plausible error signals with neural activity in relatively simple learning rules. The rest of such models train networks on a high-level goal through error backpropagation, combined with several other constraints on architecture or the form the solutions should take. As recent work suggests, however, training ANNs to solve tasks is not a panacea for understanding the brainâ€™s solutions. All models of attractor network development are incomplete for different reasons: the unsupervised models require uniform exploration of the input variable space and suppression of recurrent weights during their training, whereas backpropagation models do not offer an account of how loss functions, learning and additional constraints might be generated and implemented in biological systems.</p>
</blockquote>
<p>ä¸€äº›è¿ç»­å¸å¼•å­çš„å‘å±•æ¨¡å‹è¡¨æ˜, å®ƒä»¬å¯ä»¥é€šè¿‡æ— ç›‘ç£å…³è”å¯å¡‘æ€§ç®€å•åœ°å‡ºç°, è€Œå…¶ä»–æ¨¡å‹åˆ™åŸºäºå°†å·²çŸ¥æˆ–åˆç†çš„è¯¯å·®ä¿¡å·åé¦ˆä¸ç›¸å¯¹ç®€å•çš„å­¦ä¹ è§„åˆ™ä¸­çš„ç¥ç»æ´»åŠ¨ç›¸ç»“åˆ. å…¶ä½™æ­¤ç±»æ¨¡å‹é€šè¿‡è¯¯å·®åå‘ä¼ æ’­å¯¹ç½‘ç»œè¿›è¡Œè®­ç»ƒ, ä»¥å®ç°é«˜çº§ç›®æ ‡, å¹¶ç»“åˆå¯¹æ¶æ„æˆ–è§£å†³æ–¹æ¡ˆåº”é‡‡å–çš„å½¢å¼çš„å…¶ä»–å‡ ä¸ªçº¦æŸ. ç„¶è€Œ, æ­£å¦‚æœ€è¿‘çš„å·¥ä½œæ‰€è¡¨æ˜çš„é‚£æ ·, è®­ç»ƒ ANN ä»¥è§£å†³ä»»åŠ¡å¹¶ä¸æ˜¯ç†è§£å¤§è„‘è§£å†³æ–¹æ¡ˆçš„çµä¸¹å¦™è¯. æ‰€æœ‰å¸å¼•å­ç½‘ç»œå‘å±•çš„æ¨¡å‹ç”±äºä¸åŒçš„åŸå› éƒ½æ˜¯ä¸å®Œæ•´çš„: æ— ç›‘ç£æ¨¡å‹éœ€è¦å¯¹è¾“å…¥å˜é‡ç©ºé—´è¿›è¡Œå‡åŒ€æ¢ç´¢å¹¶åœ¨è®­ç»ƒæœŸé—´æŠ‘åˆ¶é€’å½’æƒé‡, è€Œåå‘ä¼ æ’­æ¨¡å‹åˆ™æ²¡æœ‰è§£é‡Šå¦‚ä½•åœ¨ç”Ÿç‰©ç³»ç»Ÿä¸­ç”Ÿæˆå’Œå®ç°æŸå¤±å‡½æ•°ã€å­¦ä¹ å’Œå…¶ä»–çº¦æŸ.</p>
<blockquote>
<p>There is much left to do in the field and an exciting vista ahead. On the experimental side, tools for high-resolution population-level neural recordings and perturbation across multiple brain areas enable us to peer further and deeper than ever. On the theory side, future developments will help us conceptualize how such circuits could help underwrite intelligent computation through the formation, interaction and reuse of multiple low-dimensional attractors or attractor-like structures.</p>
</blockquote>
<p>è¯¥é¢†åŸŸè¿˜æœ‰å¾ˆå¤šå·¥ä½œè¦åš, å‰æ™¯ä»¤äººå…´å¥‹. åœ¨å®éªŒæ–¹é¢, ç”¨äºé«˜åˆ†è¾¨ç‡ç¾¤ä½“æ°´å¹³ç¥ç»è®°å½•å’Œè·¨å¤šä¸ªå¤§è„‘åŒºåŸŸæ‰°åŠ¨çš„å·¥å…·ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´æ·±å…¥åœ°çª¥è§†. åœ¨ç†è®ºæ–¹é¢, æœªæ¥çš„å‘å±•å°†å¸®åŠ©æˆ‘ä»¬æ¦‚å¿µåŒ–è¿™äº›å›è·¯å¦‚ä½•é€šè¿‡å½¢æˆã€ç›¸äº’ä½œç”¨å’Œé‡å¤ä½¿ç”¨å¤šä¸ªä½ç»´å¸å¼•å­æˆ–ç±»ä¼¼å¸å¼•å­ç»“æ„æ¥æ”¯æŒæ™ºèƒ½è®¡ç®—.</p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/read/reference/theory-of-orientation-tuning-in-visual-cortex/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>Theory of Orientation Tuning in Visual Cortex</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/cs/mannual/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>æ™®æƒ ç®—åŠ›å¹³å°åŸ¹è®­</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>


<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        
        
        
        
        
        
        
        
        
        

        
        
        
        
        
        
        
        
        
        
        

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  
</body>







<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
