<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Neural networks and physical systems with emergent collective computational abilities | æ— å¤„æƒ¹å°˜åŸƒ</title>
<meta name="keywords" content="">
<meta name="description" content="Hopfield model">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/reference/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/reference/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>







<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="Neural networks and physical systems with emergent collective computational abilities" />
<meta property="og:description" content="Hopfield model" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/reference/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities/" />
<meta property="og:image" content="https://s2.loli.net/2025/10/11/24AbJ6gClaTxZB1.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-10-11T00:18:23+08:00" />
<meta property="article:modified_time" content="2025-10-11T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/10/11/24AbJ6gClaTxZB1.png" />
<meta name="twitter:title" content="Neural networks and physical systems with emergent collective computational abilities"/>
<meta name="twitter:description" content="Hopfield model"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ“• é˜…è¯»",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "ğŸ“• æ–‡çŒ®",
          "item": "https://Muatyz.github.io/posts/read/reference/"
        }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Neural networks and physical systems with emergent collective computational abilities",
      "item": "https://Muatyz.github.io/posts/read/reference/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Neural networks and physical systems with emergent collective computational abilities",
  "name": "Neural networks and physical systems with emergent collective computational abilities",
  "description": "Hopfield model",
  "keywords": [
    ""
  ],
  "articleBody": "Abstract Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems -having a large number of simple equivalent components (or neurons). The physical meaning ofcontent-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details ofthe modeling or the failure of individual devices.\nå¯¹ç”Ÿç‰©æœ‰æœºä½“æˆ–è®¡ç®—æœºæ„é€ æœ‰ç”¨çš„è®¡ç®—ç‰¹æ€§ï¼Œå¯ä»¥ä½œä¸ºæ‹¥æœ‰å¤§é‡ç®€å•ç­‰æ•ˆç»„ä»¶ï¼ˆæˆ–ç¥ç»å…ƒï¼‰çš„ç³»ç»Ÿçš„é›†ä½“ç‰¹æ€§è€Œå‡ºç°ã€‚å†…å®¹å¯å¯»å€è®°å¿†çš„ç‰©ç†æ„ä¹‰å¯ä»¥ç”¨ç³»ç»ŸçŠ¶æ€çš„é€‚å½“ç›¸ç©ºé—´æµæ¥æè¿°ã€‚æœ¬æ–‡ç»™å‡ºäº†è¿™æ ·ä¸€ä¸ªç³»ç»Ÿæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºç¥ç»ç”Ÿç‰©å­¦çš„æŸäº›æ–¹é¢ï¼Œä½†å¾ˆå®¹æ˜“é€‚ç”¨äºé›†æˆç”µè·¯ã€‚è¿™ä¸ªæ¨¡å‹çš„é›†ä½“ç‰¹æ€§äº§ç”Ÿäº†ä¸€ä¸ªå†…å®¹å¯å¯»å€å­˜å‚¨å™¨ï¼Œå®ƒèƒ½æ­£ç¡®åœ°ä»ä»»ä½•è¶³å¤Ÿå¤§å°çš„å­éƒ¨åˆ†äº§ç”Ÿæ•´ä¸ªå­˜å‚¨å™¨ã€‚ç³»ç»ŸçŠ¶æ€çš„æ—¶é—´æ¼”åŒ–ç®—æ³•åŸºäºå¼‚æ­¥å¹¶è¡Œå¤„ç†ã€‚å…¶ä»–å‡ºç°çš„é›†ä½“ç‰¹æ€§åŒ…æ‹¬ä¸€å®šçš„æ¦‚æ‹¬èƒ½åŠ›ã€ç†Ÿæ‚‰åº¦è¯†åˆ«èƒ½åŠ›ã€åˆ†ç±»èƒ½åŠ›ã€çº é”™èƒ½åŠ›å’Œæ—¶åºä¿æŒèƒ½åŠ›ã€‚é›†ä½“ç‰¹æ€§å¯¹å»ºæ¨¡ç»†èŠ‚æˆ–å•ä¸ªè®¾å¤‡æ•…éšœçš„æ•æ„Ÿåº¦å¾ˆä½ã€‚\nGiven the dynamical electrochemical properties ofneurons and their interconnections (synapses), we readily understand schemes that use a few neurons to obtain elementary useful biological behavior. Our understanding of such simple circuits in electronics allows us to plan larger and more complex circuits which are essential to large computers. Because evolution has no such plan, it becomes relevant to ask whether the ability of large collections of neurons to perform â€œcomputationalâ€ tasks may in part be a spontaneous collective consequence of having a large number of interacting simple neurons.\né‰´äºç¥ç»å…ƒåŠå…¶ç›¸äº’è¿æ¥ï¼ˆçªè§¦ï¼‰çš„åŠ¨æ€ç”µåŒ–å­¦ç‰¹æ€§ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“ç†è§£ä½¿ç”¨å°‘é‡ç¥ç»å…ƒæ¥è·å¾—åŸºæœ¬æœ‰ç”¨çš„ç”Ÿç‰©è¡Œä¸ºçš„æ–¹æ¡ˆã€‚æˆ‘ä»¬å¯¹ç”µå­å­¦ä¸­è¿™ç§ç®€å•ç”µè·¯çš„ç†è§£ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®¾è®¡æ›´å¤§æ›´å¤æ‚çš„ç”µè·¯ï¼Œè¿™å¯¹äºå¤§å‹è®¡ç®—æœºæ¥è¯´æ˜¯å¿…ä¸å¯å°‘çš„ã€‚ç”±äºè¿›åŒ–æ²¡æœ‰è¿™æ ·çš„è®¡åˆ’ï¼Œå› æ­¤æœ‰å¿…è¦é—®ä¸€ä¸‹ï¼Œå¤§é‡ç¥ç»å…ƒé›†åˆæ‰§è¡Œâ€œè®¡ç®—â€ä»»åŠ¡çš„èƒ½åŠ›æ˜¯å¦åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯æ‹¥æœ‰å¤§é‡ç›¸äº’ä½œç”¨çš„ç®€å•ç¥ç»å…ƒçš„è‡ªå‘é›†ä½“ç»“æœã€‚\nIn physical systems made from a large number of simple elements, interactions among large numbers of elementary components yield collective phenomena such as the stable magnetic orientations and domains in a magnetic system or the vortex patterns in fluid flow. Do analogous collective phenomena in a system of simple interacting neurons have useful â€œcomputationalâ€ correlates? For example, are the stability of memories, the construction of categories of generalization, or time-sequential memory also emergent properties and collective in origin? This paper examines a new modeling ofthis old and fundamental question and shows that important computational properties spontaneously arise.\nåœ¨ç”±å¤§é‡ç®€å•å…ƒç´ ç»„æˆçš„ç‰©ç†ç³»ç»Ÿä¸­ï¼Œå¤§é‡åŸºæœ¬ç»„ä»¶ä¹‹é—´çš„ç›¸äº’ä½œç”¨äº§ç”Ÿäº†é›†ä½“ç°è±¡ï¼Œä¾‹å¦‚ç£ç³»ç»Ÿä¸­çš„ç¨³å®šç£å–å‘å’Œç•´ï¼Œæˆ–æµä½“æµåŠ¨ä¸­çš„æ¶¡æ—‹æ¨¡å¼ã€‚ç®€å•ç›¸äº’ä½œç”¨ç¥ç»å…ƒç³»ç»Ÿä¸­æ˜¯å¦å­˜åœ¨ç±»ä¼¼çš„é›†ä½“ç°è±¡ï¼Œå¹¶å…·æœ‰æœ‰ç”¨çš„â€œè®¡ç®—â€ç›¸å…³æ€§ï¼Ÿä¾‹å¦‚ï¼Œè®°å¿†çš„ç¨³å®šæ€§ã€æ¦‚æ‹¬ç±»åˆ«çš„æ„å»ºæˆ–æ—¶é—´åºåˆ—è®°å¿†æ˜¯å¦ä¹Ÿæ˜¯æ¶Œç°å±æ€§ä¸”å…·æœ‰é›†ä½“èµ·æºï¼Ÿæœ¬æ–‡è€ƒå¯Ÿäº†è¿™ä¸€å¤è€è€ŒåŸºæœ¬é—®é¢˜çš„æ–°å»ºæ¨¡ï¼Œå¹¶è¡¨æ˜é‡è¦çš„è®¡ç®—å±æ€§ä¼šè‡ªå‘å‡ºç°ã€‚\nAll modeling is based on details, and the details of neuroanatomy and neural function are both myriad and incompletely known. In many physical systems, the nature of the emergent collective properties is insensitive to the details inserted in the model (e.g., collisions are essential to generate sound waves, but any reasonable interatomic force law will yield appropriate collisions). In the same spirit, I will seek collective properties that are robust against change in the model details.\næ‰€æœ‰å»ºæ¨¡éƒ½æ˜¯åŸºäºç»†èŠ‚çš„ï¼Œè€Œç¥ç»è§£å‰–å­¦å’Œç¥ç»åŠŸèƒ½çš„ç»†èŠ‚æ—¢ç¹å¤šåˆä¸å®Œå…¨ä¸ºäººæ‰€çŸ¥ã€‚åœ¨è®¸å¤šç‰©ç†ç³»ç»Ÿä¸­ï¼Œæ¶Œç°çš„é›†ä½“å±æ€§çš„æ€§è´¨å¯¹æ¨¡å‹ä¸­æ’å…¥çš„ç»†èŠ‚ä¸æ•æ„Ÿï¼ˆä¾‹å¦‚ï¼Œç¢°æ’å¯¹äºäº§ç”Ÿå£°æ³¢æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œä½†ä»»ä½•åˆç†çš„åŸå­é—´åŠ›å®šå¾‹éƒ½èƒ½äº§ç”Ÿé€‚å½“çš„ç¢°æ’ï¼‰ã€‚æœ¬ç€åŒæ ·çš„ç²¾ç¥ï¼Œæˆ‘å°†å¯»æ‰¾å¯¹æ¨¡å‹ç»†èŠ‚å˜åŒ–å…·æœ‰é²æ£’æ€§çš„é›†ä½“å±æ€§ã€‚\nThe model could be readily implemented by integrated circuit hardware. The conclusions suggest the design of a delocalized content-addressable memory or categorizer using extensive asynchronous parallel processing.\nè¯¥æ¨¡å‹å¯ä»¥é€šè¿‡é›†æˆç”µè·¯ç¡¬ä»¶è½»æ¾å®ç°ã€‚ç»“è®ºè¡¨æ˜ï¼Œä½¿ç”¨å¹¿æ³›çš„å¼‚æ­¥å¹¶è¡Œå¤„ç†è®¾è®¡ä¸€ä¸ªåˆ†æ•£çš„å†…å®¹å¯å¯»å€å­˜å‚¨å™¨æˆ–åˆ†ç±»å™¨æ˜¯å¯è¡Œçš„ã€‚\nThe general content-addressable memory of a physical system Suppose that an item stored in memory is â€œH. A. Kramers \u0026 G. H. Wannier Phys. Rev. 60, 252 (1941).â€ A general contentaddressable memory would be capable of retrieving this entire memory item on the basis of sufficient partial information. The input \"\u0026 Wannier, (1941)\" might suffice. An ideal memory could deal with errors and retrieve this reference even from the input â€œVannier, (1941)â€. In computers, only relatively simple forms of content-addressable memory have been made in hardware. Sophisticated ideas like error correction in accessing information are usually introduced as software.\nå‡è®¾å­˜å‚¨åœ¨å†…å­˜ä¸­çš„ä¸€é¡¹æ˜¯ â€œH. A. Kramers \u0026 G. H. Wannier Phys. Rev. 60, 252 (1941).â€ ä¸€ä¸ªé€šç”¨çš„å†…å®¹å¯å¯»å€å­˜å‚¨å™¨èƒ½å¤ŸåŸºäºè¶³å¤Ÿçš„éƒ¨åˆ†ä¿¡æ¯æ£€ç´¢å‡ºæ•´ä¸ªå­˜å‚¨é¡¹ã€‚è¾“å…¥ \"\u0026 Wannier, (1941)\" å¯èƒ½å°±è¶³å¤Ÿäº†ã€‚ä¸€ä¸ªç†æƒ³çš„å­˜å‚¨å™¨å¯ä»¥å¤„ç†é”™è¯¯ï¼Œç”šè‡³å¯ä»¥ä»è¾“å…¥ â€œVannier, (1941)â€ ä¸­æ£€ç´¢å‡ºè¿™ä¸ªå‚è€ƒã€‚åœ¨è®¡ç®—æœºä¸­ï¼Œåªæœ‰ç›¸å¯¹ç®€å•å½¢å¼çš„å†…å®¹å¯å¯»å€å­˜å‚¨å™¨è¢«ç¡¬ä»¶å®ç°ã€‚åƒè®¿é—®ä¿¡æ¯ä¸­çš„çº é”™è¿™æ ·å¤æ‚çš„æƒ³æ³•é€šå¸¸ä½œä¸ºè½¯ä»¶å¼•å…¥ã€‚\nThere are classes of physical systems whose spontaneous behavior can be used as a form of general (and error-correcting) content-addressable memory. Consider the time evolution of a physical system that can be described by a set of general coordinates. A point in state space then represents the instantaneous condition of the system. This state space may be either continuous or discrete (as in the case of $N$ Ising spins).\nå­˜åœ¨ä¸€äº›ç‰©ç†ç³»ç»Ÿï¼Œå…¶è‡ªå‘è¡Œä¸ºå¯ä»¥ç”¨ä½œä¸€ç§é€šç”¨ï¼ˆå’Œçº é”™ï¼‰å†…å®¹å¯å¯»å€å­˜å‚¨å™¨ã€‚è€ƒè™‘ä¸€ä¸ªå¯ä»¥ç”¨ä¸€ç»„å¹¿ä¹‰åæ ‡æè¿°çš„ç‰©ç†ç³»ç»Ÿçš„æ—¶é—´æ¼”åŒ–ã€‚çŠ¶æ€ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ä»£è¡¨ç³»ç»Ÿçš„ç¬æ—¶çŠ¶æ€ã€‚è¿™ä¸ªçŠ¶æ€ç©ºé—´å¯ä»¥æ˜¯è¿ç»­çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¦»æ•£çš„ï¼ˆå¦‚ $N$ ä¸ª Ising è‡ªæ—‹çš„æƒ…å†µï¼‰ã€‚\nThe equations ofmotion ofthe system describe a flow in state space. Various classes offlow patterns are possible, but the systems of use for memory particularly include those that flow toward locally stable points from anywhere within regions around those points. A particle with frictional damping moving in a potential well with two minima exemplifies such a dynamics.\nç³»ç»Ÿçš„è¿åŠ¨æ–¹ç¨‹æè¿°äº†çŠ¶æ€ç©ºé—´ä¸­çš„æµåŠ¨ã€‚å„ç§ç±»å‹çš„æµåŠ¨æ¨¡å¼éƒ½æ˜¯å¯èƒ½çš„ï¼Œä½†ç”¨äºå­˜å‚¨çš„ç³»ç»Ÿç‰¹åˆ«åŒ…æ‹¬é‚£äº›ä»è¿™äº›ç‚¹å‘¨å›´çš„ä»»ä½•åœ°æ–¹æµå‘å±€éƒ¨ç¨³å®šç‚¹çš„ç³»ç»Ÿã€‚åœ¨å…·æœ‰ä¸¤ä¸ªæå°å€¼çš„åŠ¿é˜±ä¸­è¿åŠ¨å¹¶å…·æœ‰æ‘©æ“¦é˜»å°¼çš„ç²’å­å°±æ˜¯è¿™ç§åŠ¨åŠ›å­¦çš„ä¸€ä¸ªä¾‹å­ã€‚\nIf the flow is not completely deterministic, the description is more complicated. In the two-well problems above, if the frictional force is characterized by atemperature, it must also produce a random driving force. The limit points become small limiting regions, and the stability becomes not absolute. But as long as the stochastic effects are small, the essence of local stable points remains.\nå¦‚æœæµåŠ¨ä¸æ˜¯å®Œå…¨ç¡®å®šæ€§çš„ï¼Œé‚£ä¹ˆæè¿°å°±ä¼šæ›´å¤æ‚ã€‚åœ¨ä¸Šè¿°åŒé˜±é—®é¢˜ä¸­ï¼Œå¦‚æœæ‘©æ“¦åŠ›ç”±æ¸©åº¦è¡¨å¾ï¼Œå®ƒè¿˜å¿…é¡»äº§ç”Ÿä¸€ä¸ªéšæœºé©±åŠ¨åŠ›ã€‚æé™ç‚¹å˜æˆäº†å°çš„æé™åŒºåŸŸï¼Œç¨³å®šæ€§ä¹Ÿä¸æ˜¯ç»å¯¹çš„ã€‚ä½†åªè¦éšæœºæ•ˆåº”å¾ˆå°ï¼Œå±€éƒ¨ç¨³å®šç‚¹çš„æœ¬è´¨ä»ç„¶å­˜åœ¨ã€‚\nConsider a physical system described by many coordinates $X_1\\cdots X_N$, the components of a state vector $X$. Let the system have locally stable limit points $X_a, X_b,\\cdots$. Then, if the system is started sufficiently near any $X_a$, as at $X = X_a + \\Delta$, it will proceed in time until $X\\approx X_a$. We can regard the information stored in the system as the vectors $X_a, X_b, \\cdots$. The starting point $X = X_a + \\Delta$ represents a partial knowledge of the item $X_a$, and the system then generates the total information $X_{a}$.\nè€ƒè™‘ä¸€ä¸ªç”±è®¸å¤šåæ ‡ $X_1\\cdots X_N$ æè¿°çš„ç‰©ç†ç³»ç»Ÿï¼Œè¿™äº›åæ ‡æ˜¯çŠ¶æ€å‘é‡ $X$ çš„åˆ†é‡ã€‚è®©ç³»ç»Ÿå…·æœ‰å±€éƒ¨ç¨³å®šçš„æé™ç‚¹ $X_a, X_b,\\cdots$ã€‚é‚£ä¹ˆï¼Œå¦‚æœç³»ç»Ÿä»è¶³å¤Ÿæ¥è¿‘ä»»ä½• $X_a$ çš„åœ°æ–¹å¼€å§‹ï¼Œä¾‹å¦‚åœ¨ $X = X_a + \\Delta$ å¤„ï¼Œå®ƒå°†éšç€æ—¶é—´çš„æ¨ç§»ç›´åˆ° $X\\approx X_a$ã€‚æˆ‘ä»¬å¯ä»¥å°†å­˜å‚¨åœ¨ç³»ç»Ÿä¸­çš„ä¿¡æ¯è§†ä¸ºå‘é‡ $X_a, X_b, \\cdots$ã€‚èµ·å§‹ç‚¹ $X = X_a + \\Delta$ ä»£è¡¨å¯¹é¡¹ $X_a$ çš„éƒ¨åˆ†äº†è§£ï¼Œç„¶åç³»ç»Ÿç”Ÿæˆå®Œæ•´çš„ä¿¡æ¯ $X_{a}$ã€‚\nAny physical system whose dynamics in phase space is dominated by a substantial number of locally stable states to which it is attracted can therefore be regarded as a general contentaddressable memory. The physical system will be a potentially useful memory if, in addition, any prescribed set of states can readily be made the stable states of the system.\nå› æ­¤ï¼Œä»»ä½•åœ¨ç›¸ç©ºé—´ä¸­ç”±å¤§é‡å±€éƒ¨ç¨³å®šçŠ¶æ€ä¸»å¯¼å…¶åŠ¨åŠ›å­¦å¹¶è¢«å¸å¼•åˆ°è¿™äº›çŠ¶æ€çš„ç‰©ç†ç³»ç»Ÿéƒ½å¯ä»¥è¢«è§†ä¸ºä¸€ç§é€šç”¨çš„å†…å®¹å¯å¯»å€å­˜å‚¨å™¨ã€‚å¦‚æœè¿˜å¯ä»¥è½»æ¾åœ°å°†ä»»ä½•é¢„å®šçš„ä¸€ç»„çŠ¶æ€è®¾ä¸ºç³»ç»Ÿçš„ç¨³å®šçŠ¶æ€ï¼Œé‚£ä¹ˆè¯¥ç‰©ç†ç³»ç»Ÿå°†æˆä¸ºä¸€ä¸ªæ½œåœ¨æœ‰ç”¨çš„å­˜å‚¨å™¨ã€‚\nThe model system The processing devices will be called neurons. Each neuron $i$ has two states like those of McCullough and Pitts: $V_i = 0$(â€œnot firingâ€) and $V_i = 1$ (â€œfiring at maximum rateâ€). When neuron $i$ has a connection made to it from neuron $j$, the strength of connection is defined as $T_{ij}$. (Nonconnected neurons have $T_{ij} = 0$.) The instantaneous state of the system is specified by listing the $N$ values of $V_i$, so it is represented by a binary word of $N$ bits.\nå¤„ç†è®¾å¤‡å°†è¢«ç§°ä¸ºç¥ç»å…ƒã€‚æ¯ä¸ªç¥ç»å…ƒ $i$ æœ‰ä¸¤ä¸ªçŠ¶æ€ï¼Œç±»ä¼¼äº McCullough å’Œ Pitts çš„çŠ¶æ€ï¼š$V_i = 0$ï¼ˆâ€œä¸å‘ç«â€ï¼‰å’Œ $V_i = 1$ï¼ˆâ€œä»¥æœ€å¤§é€Ÿç‡å‘ç«â€ï¼‰ã€‚å½“ç¥ç»å…ƒ $i$ ä¸ç¥ç»å…ƒ $j$ å»ºç«‹è¿æ¥æ—¶ï¼Œè¿æ¥çš„å¼ºåº¦å®šä¹‰ä¸º $T_{ij}$ã€‚ï¼ˆæœªè¿æ¥çš„ç¥ç»å…ƒå…·æœ‰ $T_{ij} = 0$ã€‚ï¼‰ç³»ç»Ÿçš„ç¬æ—¶çŠ¶æ€é€šè¿‡åˆ—å‡º $N$ ä¸ª $V_i$ çš„å€¼æ¥æŒ‡å®šï¼Œå› æ­¤å®ƒè¡¨ç¤ºä¸ºä¸€ä¸ªç”± $N$ ä½ç»„æˆçš„äºŒè¿›åˆ¶å­—ã€‚\nThe state changes in time according to the following algorithm. For each neuron $i$ there is a fixed threshold $U_i$. Each neuron $i$ readjusts its state randomly in time but with a mean attempt rate $W$, setting\n$$ \\begin{equation*} \\begin{aligned} \\begin{array}{c}V_{i}\\rightarrow 1\\\\V_{i}\\rightarrow 0\\end{array}\\quad\\text{if}\\quad\\sum_{j\\neq i}T_{ij}V_{j}\\begin{array}{c} \u003eU_{i}\\\\ ",
  "wordCount" : "12367",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/10/11/24AbJ6gClaTxZB1.png","datePublished": "2025-10-11T00:18:23+08:00",
  "dateModified": "2025-10-11T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/reference/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "æ— å¤„æƒ¹å°˜åŸƒ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿— (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿—</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/">ğŸ“• é˜…è¯»</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/">ğŸ“• æ–‡çŒ®</a></div>
            <h1 class="post-title">
                Neural networks and physical systems with emergent collective computational abilities
            </h1>
            <div class="post-description">
                Hopfield model
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-10-11
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>12367å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>25åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/10/11/24AbJ6gClaTxZB1.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                <li>
                    <a href="#the-general-content-addressable-memory-of-a-physical-system" aria-label="The general content-addressable memory of a physical system">The general content-addressable memory of a physical system</a></li>
                <li>
                    <a href="#the-model-system" aria-label="The model system">The model system</a></li>
                <li>
                    <a href="#the-information-storage-algorithm" aria-label="The information storage algorithm">The information storage algorithm</a></li>
                <li>
                    <a href="#the-biological-interpretation-of-the-model" aria-label="The biological interpretation of the model">The biological interpretation of the model</a></li>
                <li>
                    <a href="#studies-of-the-collective-behaviors-of-the-model" aria-label="Studies of the collective behaviors of the model">Studies of the collective behaviors of the model</a></li>
                <li>
                    <a href="#discussion" aria-label="Discussion">Discussion</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><h1 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h1>
<blockquote>
<p>Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems -having a large number of simple equivalent components (or neurons). The physical meaning ofcontent-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details ofthe modeling or the failure of individual devices.</p>
</blockquote>
<p>å¯¹ç”Ÿç‰©æœ‰æœºä½“æˆ–è®¡ç®—æœºæ„é€ æœ‰ç”¨çš„è®¡ç®—ç‰¹æ€§ï¼Œå¯ä»¥ä½œä¸ºæ‹¥æœ‰å¤§é‡ç®€å•ç­‰æ•ˆç»„ä»¶ï¼ˆæˆ–ç¥ç»å…ƒï¼‰çš„ç³»ç»Ÿçš„é›†ä½“ç‰¹æ€§è€Œå‡ºç°ã€‚å†…å®¹å¯å¯»å€è®°å¿†çš„ç‰©ç†æ„ä¹‰å¯ä»¥ç”¨ç³»ç»ŸçŠ¶æ€çš„é€‚å½“ç›¸ç©ºé—´æµæ¥æè¿°ã€‚æœ¬æ–‡ç»™å‡ºäº†è¿™æ ·ä¸€ä¸ªç³»ç»Ÿæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºç¥ç»ç”Ÿç‰©å­¦çš„æŸäº›æ–¹é¢ï¼Œä½†å¾ˆå®¹æ˜“é€‚ç”¨äºé›†æˆç”µè·¯ã€‚è¿™ä¸ªæ¨¡å‹çš„é›†ä½“ç‰¹æ€§äº§ç”Ÿäº†ä¸€ä¸ªå†…å®¹å¯å¯»å€å­˜å‚¨å™¨ï¼Œå®ƒèƒ½æ­£ç¡®åœ°ä»ä»»ä½•è¶³å¤Ÿå¤§å°çš„å­éƒ¨åˆ†äº§ç”Ÿæ•´ä¸ªå­˜å‚¨å™¨ã€‚ç³»ç»ŸçŠ¶æ€çš„æ—¶é—´æ¼”åŒ–ç®—æ³•åŸºäºå¼‚æ­¥å¹¶è¡Œå¤„ç†ã€‚å…¶ä»–å‡ºç°çš„é›†ä½“ç‰¹æ€§åŒ…æ‹¬ä¸€å®šçš„æ¦‚æ‹¬èƒ½åŠ›ã€ç†Ÿæ‚‰åº¦è¯†åˆ«èƒ½åŠ›ã€åˆ†ç±»èƒ½åŠ›ã€çº é”™èƒ½åŠ›å’Œæ—¶åºä¿æŒèƒ½åŠ›ã€‚é›†ä½“ç‰¹æ€§å¯¹å»ºæ¨¡ç»†èŠ‚æˆ–å•ä¸ªè®¾å¤‡æ•…éšœçš„æ•æ„Ÿåº¦å¾ˆä½ã€‚</p>
<hr>
<blockquote>
<p>Given the dynamical electrochemical properties ofneurons and their interconnections (synapses), we readily understand schemes that use a few neurons to obtain elementary useful biological behavior. Our understanding of such simple circuits in electronics allows us to plan larger and more complex circuits which are essential to large computers. Because evolution has no such plan, it becomes relevant to ask whether the ability of large collections of neurons to perform &ldquo;computational&rdquo; tasks may in part be a spontaneous collective consequence of having a large number of interacting simple neurons.</p>
</blockquote>
<p>é‰´äºç¥ç»å…ƒåŠå…¶ç›¸äº’è¿æ¥ï¼ˆçªè§¦ï¼‰çš„åŠ¨æ€ç”µåŒ–å­¦ç‰¹æ€§ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“ç†è§£ä½¿ç”¨å°‘é‡ç¥ç»å…ƒæ¥è·å¾—åŸºæœ¬æœ‰ç”¨çš„ç”Ÿç‰©è¡Œä¸ºçš„æ–¹æ¡ˆã€‚æˆ‘ä»¬å¯¹ç”µå­å­¦ä¸­è¿™ç§ç®€å•ç”µè·¯çš„ç†è§£ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®¾è®¡æ›´å¤§æ›´å¤æ‚çš„ç”µè·¯ï¼Œè¿™å¯¹äºå¤§å‹è®¡ç®—æœºæ¥è¯´æ˜¯å¿…ä¸å¯å°‘çš„ã€‚ç”±äºè¿›åŒ–æ²¡æœ‰è¿™æ ·çš„è®¡åˆ’ï¼Œå› æ­¤æœ‰å¿…è¦é—®ä¸€ä¸‹ï¼Œå¤§é‡ç¥ç»å…ƒé›†åˆæ‰§è¡Œâ€œè®¡ç®—â€ä»»åŠ¡çš„èƒ½åŠ›æ˜¯å¦åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯æ‹¥æœ‰å¤§é‡ç›¸äº’ä½œç”¨çš„ç®€å•ç¥ç»å…ƒçš„è‡ªå‘é›†ä½“ç»“æœã€‚</p>
<blockquote>
<p>In physical systems made from a large number of simple elements, interactions among large numbers of elementary components yield collective phenomena such as the stable magnetic orientations and domains in a magnetic system or the vortex  patterns in fluid flow. Do analogous collective phenomena in a system of simple interacting neurons have useful &ldquo;computational&rdquo; correlates? For example, are the stability of memories,  the construction of categories of generalization, or time-sequential memory also emergent properties and collective in origin? This paper examines a new modeling ofthis old and fundamental question and shows that important computational properties spontaneously arise.</p>
</blockquote>
<p>åœ¨ç”±å¤§é‡ç®€å•å…ƒç´ ç»„æˆçš„ç‰©ç†ç³»ç»Ÿä¸­ï¼Œå¤§é‡åŸºæœ¬ç»„ä»¶ä¹‹é—´çš„ç›¸äº’ä½œç”¨äº§ç”Ÿäº†é›†ä½“ç°è±¡ï¼Œä¾‹å¦‚ç£ç³»ç»Ÿä¸­çš„ç¨³å®šç£å–å‘å’Œç•´ï¼Œæˆ–æµä½“æµåŠ¨ä¸­çš„æ¶¡æ—‹æ¨¡å¼ã€‚ç®€å•ç›¸äº’ä½œç”¨ç¥ç»å…ƒç³»ç»Ÿä¸­æ˜¯å¦å­˜åœ¨ç±»ä¼¼çš„é›†ä½“ç°è±¡ï¼Œå¹¶å…·æœ‰æœ‰ç”¨çš„â€œè®¡ç®—â€ç›¸å…³æ€§ï¼Ÿä¾‹å¦‚ï¼Œè®°å¿†çš„ç¨³å®šæ€§ã€æ¦‚æ‹¬ç±»åˆ«çš„æ„å»ºæˆ–æ—¶é—´åºåˆ—è®°å¿†æ˜¯å¦ä¹Ÿæ˜¯æ¶Œç°å±æ€§ä¸”å…·æœ‰é›†ä½“èµ·æºï¼Ÿæœ¬æ–‡è€ƒå¯Ÿäº†è¿™ä¸€å¤è€è€ŒåŸºæœ¬é—®é¢˜çš„æ–°å»ºæ¨¡ï¼Œå¹¶è¡¨æ˜é‡è¦çš„è®¡ç®—å±æ€§ä¼šè‡ªå‘å‡ºç°ã€‚</p>
<blockquote>
<p>All modeling is based on details, and the details of neuroanatomy and neural function are both myriad and incompletely known. In many physical systems, the nature of the emergent collective properties is insensitive to the details inserted in the model (e.g., collisions are essential to generate sound  waves, but any reasonable interatomic force law will yield appropriate collisions). In the same spirit, I will seek collective properties that are robust against change in the model details.</p>
</blockquote>
<p>æ‰€æœ‰å»ºæ¨¡éƒ½æ˜¯åŸºäºç»†èŠ‚çš„ï¼Œè€Œç¥ç»è§£å‰–å­¦å’Œç¥ç»åŠŸèƒ½çš„ç»†èŠ‚æ—¢ç¹å¤šåˆä¸å®Œå…¨ä¸ºäººæ‰€çŸ¥ã€‚åœ¨è®¸å¤šç‰©ç†ç³»ç»Ÿä¸­ï¼Œæ¶Œç°çš„é›†ä½“å±æ€§çš„æ€§è´¨å¯¹æ¨¡å‹ä¸­æ’å…¥çš„ç»†èŠ‚ä¸æ•æ„Ÿï¼ˆä¾‹å¦‚ï¼Œç¢°æ’å¯¹äºäº§ç”Ÿå£°æ³¢æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œä½†ä»»ä½•åˆç†çš„åŸå­é—´åŠ›å®šå¾‹éƒ½èƒ½äº§ç”Ÿé€‚å½“çš„ç¢°æ’ï¼‰ã€‚æœ¬ç€åŒæ ·çš„ç²¾ç¥ï¼Œæˆ‘å°†å¯»æ‰¾å¯¹æ¨¡å‹ç»†èŠ‚å˜åŒ–å…·æœ‰é²æ£’æ€§çš„é›†ä½“å±æ€§ã€‚</p>
<blockquote>
<p>The model could be readily implemented by integrated circuit hardware. The conclusions suggest the design of a delocalized content-addressable memory or categorizer using extensive asynchronous parallel processing.</p>
</blockquote>
<p>è¯¥æ¨¡å‹å¯ä»¥é€šè¿‡é›†æˆç”µè·¯ç¡¬ä»¶è½»æ¾å®ç°ã€‚ç»“è®ºè¡¨æ˜ï¼Œä½¿ç”¨å¹¿æ³›çš„å¼‚æ­¥å¹¶è¡Œå¤„ç†è®¾è®¡ä¸€ä¸ªåˆ†æ•£çš„å†…å®¹å¯å¯»å€å­˜å‚¨å™¨æˆ–åˆ†ç±»å™¨æ˜¯å¯è¡Œçš„ã€‚</p>
<h1 id="the-general-content-addressable-memory-of-a-physical-system">The general content-addressable memory of a physical system<a hidden class="anchor" aria-hidden="true" href="#the-general-content-addressable-memory-of-a-physical-system">#</a></h1>
<blockquote>
<p>Suppose that an item stored in memory is <em>&ldquo;H. A. Kramers &amp; G. H. Wannier Phys. Rev. 60, 252 (1941).&rdquo;</em> A general contentaddressable memory would be capable of retrieving this entire memory item on the basis of sufficient partial information. The input <em>&quot;&amp; Wannier, (1941)&quot;</em> might suffice. An ideal memory could deal with errors and retrieve this reference even from the input <em>&ldquo;Vannier, (1941)&rdquo;</em>. In computers, only relatively simple forms of content-addressable memory have been made in hardware. Sophisticated ideas like error correction in accessing information are usually introduced as software.</p>
</blockquote>
<p>å‡è®¾å­˜å‚¨åœ¨å†…å­˜ä¸­çš„ä¸€é¡¹æ˜¯ <em>&ldquo;H. A. Kramers &amp; G. H. Wannier Phys. Rev. 60, 252 (1941).&rdquo;</em> ä¸€ä¸ªé€šç”¨çš„å†…å®¹å¯å¯»å€å­˜å‚¨å™¨èƒ½å¤ŸåŸºäºè¶³å¤Ÿçš„éƒ¨åˆ†ä¿¡æ¯æ£€ç´¢å‡ºæ•´ä¸ªå­˜å‚¨é¡¹ã€‚è¾“å…¥ <em>&quot;&amp; Wannier, (1941)&quot;</em> å¯èƒ½å°±è¶³å¤Ÿäº†ã€‚ä¸€ä¸ªç†æƒ³çš„å­˜å‚¨å™¨å¯ä»¥å¤„ç†é”™è¯¯ï¼Œç”šè‡³å¯ä»¥ä»è¾“å…¥ <em>&ldquo;Vannier, (1941)&rdquo;</em> ä¸­æ£€ç´¢å‡ºè¿™ä¸ªå‚è€ƒã€‚åœ¨è®¡ç®—æœºä¸­ï¼Œåªæœ‰ç›¸å¯¹ç®€å•å½¢å¼çš„å†…å®¹å¯å¯»å€å­˜å‚¨å™¨è¢«ç¡¬ä»¶å®ç°ã€‚åƒè®¿é—®ä¿¡æ¯ä¸­çš„çº é”™è¿™æ ·å¤æ‚çš„æƒ³æ³•é€šå¸¸ä½œä¸ºè½¯ä»¶å¼•å…¥ã€‚</p>
<blockquote>
<p>There are classes of physical systems whose spontaneous behavior can be used as a form of general (and error-correcting) content-addressable memory. Consider the time evolution of a physical system that can be described by a set of general coordinates. A point in state space then represents the instantaneous condition of the system. This state space may be either continuous or discrete (as in the case of $N$ Ising spins).</p>
</blockquote>
<p>å­˜åœ¨ä¸€äº›ç‰©ç†ç³»ç»Ÿï¼Œå…¶è‡ªå‘è¡Œä¸ºå¯ä»¥ç”¨ä½œä¸€ç§é€šç”¨ï¼ˆå’Œçº é”™ï¼‰å†…å®¹å¯å¯»å€å­˜å‚¨å™¨ã€‚è€ƒè™‘ä¸€ä¸ªå¯ä»¥ç”¨ä¸€ç»„å¹¿ä¹‰åæ ‡æè¿°çš„ç‰©ç†ç³»ç»Ÿçš„æ—¶é—´æ¼”åŒ–ã€‚çŠ¶æ€ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ä»£è¡¨ç³»ç»Ÿçš„ç¬æ—¶çŠ¶æ€ã€‚è¿™ä¸ªçŠ¶æ€ç©ºé—´å¯ä»¥æ˜¯è¿ç»­çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¦»æ•£çš„ï¼ˆå¦‚ $N$ ä¸ª Ising è‡ªæ—‹çš„æƒ…å†µï¼‰ã€‚</p>
<blockquote>
<p>The equations ofmotion ofthe system describe a flow in state space. Various classes offlow patterns are possible, but the systems of use for memory particularly include those that flow toward locally stable points from anywhere within regions around those points. A particle with frictional damping moving in a potential well with two minima exemplifies such a dynamics.</p>
</blockquote>
<p>ç³»ç»Ÿçš„è¿åŠ¨æ–¹ç¨‹æè¿°äº†çŠ¶æ€ç©ºé—´ä¸­çš„æµåŠ¨ã€‚å„ç§ç±»å‹çš„æµåŠ¨æ¨¡å¼éƒ½æ˜¯å¯èƒ½çš„ï¼Œä½†ç”¨äºå­˜å‚¨çš„ç³»ç»Ÿç‰¹åˆ«åŒ…æ‹¬é‚£äº›ä»è¿™äº›ç‚¹å‘¨å›´çš„ä»»ä½•åœ°æ–¹æµå‘å±€éƒ¨ç¨³å®šç‚¹çš„ç³»ç»Ÿã€‚åœ¨å…·æœ‰ä¸¤ä¸ªæå°å€¼çš„åŠ¿é˜±ä¸­è¿åŠ¨å¹¶å…·æœ‰æ‘©æ“¦é˜»å°¼çš„ç²’å­å°±æ˜¯è¿™ç§åŠ¨åŠ›å­¦çš„ä¸€ä¸ªä¾‹å­ã€‚</p>
<blockquote>
<p>If the flow is not completely deterministic, the description is more complicated. In the two-well problems above, if the frictional force is characterized by atemperature, it must also produce a random driving force. The limit points become small  limiting regions, and the stability becomes not absolute. But as long as the stochastic effects are small, the essence of local stable points remains.</p>
</blockquote>
<p>å¦‚æœæµåŠ¨ä¸æ˜¯å®Œå…¨ç¡®å®šæ€§çš„ï¼Œé‚£ä¹ˆæè¿°å°±ä¼šæ›´å¤æ‚ã€‚åœ¨ä¸Šè¿°åŒé˜±é—®é¢˜ä¸­ï¼Œå¦‚æœæ‘©æ“¦åŠ›ç”±æ¸©åº¦è¡¨å¾ï¼Œå®ƒè¿˜å¿…é¡»äº§ç”Ÿä¸€ä¸ªéšæœºé©±åŠ¨åŠ›ã€‚æé™ç‚¹å˜æˆäº†å°çš„æé™åŒºåŸŸï¼Œç¨³å®šæ€§ä¹Ÿä¸æ˜¯ç»å¯¹çš„ã€‚ä½†åªè¦éšæœºæ•ˆåº”å¾ˆå°ï¼Œå±€éƒ¨ç¨³å®šç‚¹çš„æœ¬è´¨ä»ç„¶å­˜åœ¨ã€‚</p>
<blockquote>
<p>Consider a physical system described by many coordinates $X_1\cdots X_N$, the components of a state vector $X$. Let the system  have locally stable limit points $X_a, X_b,\cdots$. Then, if the system is started sufficiently near any $X_a$, as at $X = X_a + \Delta$, it will  proceed in time until $X\approx X_a$. We can regard the information  stored in the system as the vectors $X_a, X_b, \cdots$. The starting point $X = X_a + \Delta$ represents a partial knowledge of the item $X_a$, and the system then generates the total information $X_{a}$.</p>
</blockquote>
<p>è€ƒè™‘ä¸€ä¸ªç”±è®¸å¤šåæ ‡ $X_1\cdots X_N$ æè¿°çš„ç‰©ç†ç³»ç»Ÿï¼Œè¿™äº›åæ ‡æ˜¯çŠ¶æ€å‘é‡ $X$ çš„åˆ†é‡ã€‚è®©ç³»ç»Ÿå…·æœ‰å±€éƒ¨ç¨³å®šçš„æé™ç‚¹ $X_a, X_b,\cdots$ã€‚é‚£ä¹ˆï¼Œå¦‚æœç³»ç»Ÿä»è¶³å¤Ÿæ¥è¿‘ä»»ä½• $X_a$ çš„åœ°æ–¹å¼€å§‹ï¼Œä¾‹å¦‚åœ¨ $X = X_a + \Delta$ å¤„ï¼Œå®ƒå°†éšç€æ—¶é—´çš„æ¨ç§»ç›´åˆ° $X\approx X_a$ã€‚æˆ‘ä»¬å¯ä»¥å°†å­˜å‚¨åœ¨ç³»ç»Ÿä¸­çš„ä¿¡æ¯è§†ä¸ºå‘é‡ $X_a, X_b, \cdots$ã€‚èµ·å§‹ç‚¹ $X = X_a + \Delta$ ä»£è¡¨å¯¹é¡¹ $X_a$ çš„éƒ¨åˆ†äº†è§£ï¼Œç„¶åç³»ç»Ÿç”Ÿæˆå®Œæ•´çš„ä¿¡æ¯ $X_{a}$ã€‚</p>
<blockquote>
<p>Any physical system whose dynamics in phase space is dominated by a substantial number of locally stable states to which it is attracted can therefore be regarded as a general contentaddressable memory. The physical system will be a potentially  useful memory if, in addition, any prescribed set of states can readily be made the stable states of the system.</p>
</blockquote>
<p>å› æ­¤ï¼Œä»»ä½•åœ¨ç›¸ç©ºé—´ä¸­ç”±å¤§é‡å±€éƒ¨ç¨³å®šçŠ¶æ€ä¸»å¯¼å…¶åŠ¨åŠ›å­¦å¹¶è¢«å¸å¼•åˆ°è¿™äº›çŠ¶æ€çš„ç‰©ç†ç³»ç»Ÿéƒ½å¯ä»¥è¢«è§†ä¸ºä¸€ç§é€šç”¨çš„å†…å®¹å¯å¯»å€å­˜å‚¨å™¨ã€‚å¦‚æœè¿˜å¯ä»¥è½»æ¾åœ°å°†ä»»ä½•é¢„å®šçš„ä¸€ç»„çŠ¶æ€è®¾ä¸ºç³»ç»Ÿçš„ç¨³å®šçŠ¶æ€ï¼Œé‚£ä¹ˆè¯¥ç‰©ç†ç³»ç»Ÿå°†æˆä¸ºä¸€ä¸ªæ½œåœ¨æœ‰ç”¨çš„å­˜å‚¨å™¨ã€‚</p>
<h1 id="the-model-system">The model system<a hidden class="anchor" aria-hidden="true" href="#the-model-system">#</a></h1>
<blockquote>
<p>The processing devices will be called neurons. Each neuron $i$ has two states like those of McCullough and Pitts: $V_i = 0$(&ldquo;not firing&rdquo;) and $V_i = 1$ (&ldquo;firing at maximum rate&rdquo;). When neuron $i$ has a connection made to it from neuron $j$, the strength  of connection is defined as $T_{ij}$. (Nonconnected neurons have $T_{ij} = 0$.) The instantaneous state of the system is specified by listing the $N$ values of $V_i$, so it is represented by a binary word of $N$ bits.</p>
</blockquote>
<p>å¤„ç†è®¾å¤‡å°†è¢«ç§°ä¸ºç¥ç»å…ƒã€‚æ¯ä¸ªç¥ç»å…ƒ $i$ æœ‰ä¸¤ä¸ªçŠ¶æ€ï¼Œç±»ä¼¼äº McCullough å’Œ Pitts çš„çŠ¶æ€ï¼š$V_i = 0$ï¼ˆâ€œä¸å‘ç«â€ï¼‰å’Œ $V_i = 1$ï¼ˆâ€œä»¥æœ€å¤§é€Ÿç‡å‘ç«â€ï¼‰ã€‚å½“ç¥ç»å…ƒ $i$ ä¸ç¥ç»å…ƒ $j$ å»ºç«‹è¿æ¥æ—¶ï¼Œè¿æ¥çš„å¼ºåº¦å®šä¹‰ä¸º $T_{ij}$ã€‚ï¼ˆæœªè¿æ¥çš„ç¥ç»å…ƒå…·æœ‰ $T_{ij} = 0$ã€‚ï¼‰ç³»ç»Ÿçš„ç¬æ—¶çŠ¶æ€é€šè¿‡åˆ—å‡º $N$ ä¸ª $V_i$ çš„å€¼æ¥æŒ‡å®šï¼Œå› æ­¤å®ƒè¡¨ç¤ºä¸ºä¸€ä¸ªç”± $N$ ä½ç»„æˆçš„äºŒè¿›åˆ¶å­—ã€‚</p>
<blockquote>
<p>The state changes in time according to the following algorithm. For each neuron $i$ there is a fixed threshold $U_i$. Each neuron $i$ readjusts its state randomly in time but with a mean attempt rate $W$, setting</p>
<p>$$
\begin{equation*}
\begin{aligned}
\begin{array}{c}V_{i}\rightarrow 1\\V_{i}\rightarrow 0\end{array}\quad\text{if}\quad\sum_{j\neq i}T_{ij}V_{j}\begin{array}{c} &gt;U_{i}\\ &lt;U_{i}\end{array}
\end{aligned}
\end{equation*}
$$</p>
<p>Thus, each neuron randomly and asynchronously evaluates whether it is above or below threshold and readjusts accordingly. (Unless otherwise stated, we choose $U_i = 0$.)</p>
</blockquote>
<p>ç¥ç»å…ƒ $i$ çš„çŠ¶æ€æ ¹æ®ä»¥ä¸‹ç®—æ³•éšæ—¶é—´å˜åŒ–ã€‚å¯¹äºæ¯ä¸ªç¥ç»å…ƒ $i$ï¼Œéƒ½æœ‰ä¸€ä¸ªå›ºå®šçš„é˜ˆå€¼ $U_i$ã€‚æ¯ä¸ªç¥ç»å…ƒ $i$ ä»¥å¹³å‡å°è¯•é€Ÿç‡ $W$ éšæœºåœ°è°ƒæ•´å…¶çŠ¶æ€ï¼Œè®¾ç½®</p>
<p>$$
\begin{equation*}
\begin{aligned}
\begin{array}{c}V_{i}\rightarrow 1\\V_{i}\rightarrow 0\end{array}\quad\text{if}\quad\sum_{j\neq i}T_{ij}V_{j}\begin{array}{c} &gt;U_{i}\\ &lt;U_{i}\end{array}
\end{aligned}
\end{equation*}
$$</p>
<p>å› æ­¤ï¼Œæ¯ä¸ªç¥ç»å…ƒéšæœºä¸”å¼‚æ­¥åœ°è¯„ä¼°å®ƒæ˜¯é«˜äºè¿˜æ˜¯ä½äºé˜ˆå€¼ï¼Œå¹¶ç›¸åº”åœ°è¿›è¡Œè°ƒæ•´ã€‚ï¼ˆé™¤éå¦æœ‰è¯´æ˜ï¼Œæˆ‘ä»¬é€‰æ‹© $U_i = 0$ã€‚ï¼‰</p>
<blockquote>
<p>Although this model has superficial similarities to the Perceptron the essential differences are responsible for the new results. First, Perceptrons were modeled chiefly with neural connections in a &ldquo;forward&rdquo; direction $A\rightarrow B \rightarrow C \rightarrow D$. The analysis of networks with strong backward coupling $A\rightleftarrows B\rightleftarrows C\rightleftarrows A$ proved intractable. All our interesting results arise  as consequences of the strong back-coupling. Second, Perceptron studies usually made a random net ofneurons deal directly with a real physical world and did not ask the questions essential to finding the more abstract emergent computational properties. Finally, Perceptron modeling required synchronous neurons like a conventional digital computer. There is no evidence for such global synchrony and, given the delays of nerve signal propagation, there would be no way to use global synchrony effectively. Chiefly computational properties which can exist in spite of asynchrony have interesting implications in biology.</p>
</blockquote>
<p>è™½ç„¶è¿™ä¸ªæ¨¡å‹åœ¨è¡¨é¢ä¸Šä¸æ„ŸçŸ¥å™¨æœ‰ç›¸ä¼¼ä¹‹å¤„ï¼Œä½†æœ¬è´¨ä¸Šçš„å·®å¼‚å¯¼è‡´äº†æ–°çš„ç»“æœã€‚é¦–å…ˆï¼Œæ„ŸçŸ¥å™¨ä¸»è¦ä»¥â€œå‰å‘â€æ–¹å‘çš„ç¥ç»è¿æ¥è¿›è¡Œå»ºæ¨¡ $A\rightarrow B \rightarrow C \rightarrow D$ã€‚å…·æœ‰å¼ºåå‘è€¦åˆ $A\rightleftarrows B\rightleftarrows C\rightleftarrows A$ çš„ç½‘ç»œåˆ†æè¢«è¯æ˜æ˜¯éš¾ä»¥å¤„ç†çš„ã€‚æˆ‘ä»¬æ‰€æœ‰æœ‰è¶£çš„ç»“æœéƒ½æ˜¯å¼ºåå‘è€¦åˆçš„ç»“æœã€‚å…¶æ¬¡ï¼Œæ„ŸçŸ¥å™¨ç ”ç©¶é€šå¸¸è®©ä¸€ä¸ªéšæœºç¥ç»å…ƒç½‘ç»œç›´æ¥å¤„ç†ä¸€ä¸ªçœŸå®çš„ç‰©ç†ä¸–ç•Œï¼Œå¹¶æ²¡æœ‰æå‡ºå‘ç°æ›´æŠ½è±¡çš„æ¶Œç°è®¡ç®—å±æ€§æ‰€å¿…éœ€çš„é—®é¢˜ã€‚æœ€åï¼Œæ„ŸçŸ¥å™¨å»ºæ¨¡éœ€è¦åƒä¼ ç»Ÿæ•°å­—è®¡ç®—æœºä¸€æ ·åŒæ­¥çš„ç¥ç»å…ƒã€‚æ²¡æœ‰è¯æ®è¡¨æ˜å­˜åœ¨è¿™ç§å…¨å±€åŒæ­¥ï¼Œå¹¶ä¸”è€ƒè™‘åˆ°ç¥ç»ä¿¡å·ä¼ æ’­çš„å»¶è¿Ÿï¼Œä¹Ÿæ²¡æœ‰åŠæ³•æœ‰æ•ˆåœ°åˆ©ç”¨å…¨å±€åŒæ­¥ã€‚ä¸»è¦æ˜¯é‚£äº›å°½ç®¡å­˜åœ¨å¼‚æ­¥æ€§ä½†ä»ç„¶å¯ä»¥å­˜åœ¨çš„è®¡ç®—å±æ€§åœ¨ç”Ÿç‰©å­¦ä¸­å…·æœ‰æœ‰è¶£çš„æ„ä¹‰ã€‚</p>
<h1 id="the-information-storage-algorithm">The information storage algorithm<a hidden class="anchor" aria-hidden="true" href="#the-information-storage-algorithm">#</a></h1>
<blockquote>
<p>Suppose we wish to store the set of states $V^{s}, s = 1\cdots n$. We use the storage prescription</p>
<p>$$
T_{ij} = \sum_{s}(2V_{i}^{s}-1)(2V_{j}^{s}-1)
$$</p>
<p>but with $T_{ii} = 0$. From this definition</p>
<p>$$
\sum_{j}T_{ij}V_{j}^{s^{\prime}} = \sum_{s}(2V_{i}^{s}-1)\left[\sum_{j}V_{j}^{s^{\prime}}(2V_{j}^{s}-1)\right] \equiv H_{j}^{s^{\prime}}.
$$</p>
<p>The mean value of the bracketed term in Eq. 3 is $0$ unless $s = s^{\prime}$, for which the mean is $N/2$. This pseudoorthogonality yields</p>
<p>$$
\sum_{j}T_{ij}V_{j}^{s^{\prime}}\equiv \langle H_{i}^{s^{\prime}}\rangle \approx (2V_{i}^{s^{\prime}}-1)N/2.
$$</p>
<p>and is positive if $V_{i}^{s^{\prime}} = 1$ and negative if $V_{i}^{s^{\prime}} = 0$. Except for the noise coming from the $s\neq s^{\prime}$ terms, the stored state would always be stable under our processing algorithm.</p>
</blockquote>
<p>å‡è®¾æˆ‘ä»¬å¸Œæœ›å­˜å‚¨çŠ¶æ€é›† $V^{s}, s = 1\cdots n$ã€‚æˆ‘ä»¬ä½¿ç”¨å­˜å‚¨å¤„æ–¹</p>
<p>$$
T_{ij} = \sum_{s}(2V_{i}^{s}-1)(2V_{j}^{s}-1)
$$</p>
<p>ä½† $T_{ii} = 0$ã€‚æ ¹æ®è¿™ä¸ªå®šä¹‰</p>
<p>$$
\sum_{j}T_{ij}V_{j}^{s^{\prime}} = \sum_{s}(2V_{i}^{s}-1)\left[\sum_{j}V_{j}^{s^{\prime}}(2V_{j}^{s}-1)\right] \equiv H_{j}^{s^{\prime}}.
$$</p>
<p>æ‹¬å·ä¸­é¡¹çš„å¹³å‡å€¼åœ¨ $s \neq s^{\prime}$ æ—¶ä¸º $0$ï¼Œè€Œåœ¨ $s = s^{\prime}$ æ—¶å¹³å‡å€¼ä¸º $N/2$ã€‚è¿™ç§ä¼ªæ­£äº¤æ€§äº§ç”Ÿäº†</p>
<p>$$
\sum_{j}T_{ij}V_{j}^{s^{\prime}}\equiv \langle H_{i}^{s^{\prime}}\rangle \approx (2V_{i}^{s^{\prime}}-1)N/2.
$$</p>
<p>å¹¶ä¸”å½“ $V_{i}^{s^{\prime}} = 1$ æ—¶ä¸ºæ­£ï¼Œå½“ $V_{i}^{s^{\prime}} = 0$ æ—¶ä¸ºè´Ÿã€‚é™¤äº†æ¥è‡ª $s\neq s^{\prime}$ é¡¹çš„å™ªå£°å¤–ï¼Œå­˜å‚¨çš„çŠ¶æ€åœ¨æˆ‘ä»¬çš„å¤„ç†ç®—æ³•ä¸‹æ€»æ˜¯ç¨³å®šçš„ã€‚</p>
<blockquote>
<p>Such matrices $T_{ij}$ have been used in theories of linear associative nets to produce an output pattern from a paired input stimulus, $S_1 \rightarrow O_1$. A second association $S_2 \rightarrow O_2$ can be simultaneously stored in the same network. But the confusing stimulus $0.6 S_1 + 0.4 O_2$ will produce a generally meaningless mixed output $0.6 O_1 + 0.4 O_2$. Our model, in contrast, will use its strong nonlinearity to make choices, produce categories, and regenerate information and, with high probability, will generate the output $O_1$ from such a  confusing mixed stimulus.</p>
</blockquote>
<p>è¿™æ ·çš„çŸ©é˜µ $T_{ij}$ å·²è¢«ç”¨äºçº¿æ€§è”æƒ³ç½‘ç»œçš„ç†è®ºä¸­ï¼Œä»¥ä»é…å¯¹çš„è¾“å…¥åˆºæ¿€ä¸­äº§ç”Ÿè¾“å‡ºæ¨¡å¼ï¼Œ$S_1 \rightarrow O_1$ã€‚ç¬¬äºŒä¸ªå…³è” $S_2 \rightarrow O_2$ å¯ä»¥åŒæ—¶å­˜å‚¨åœ¨åŒä¸€ä¸ªç½‘ç»œä¸­ã€‚ä½†æ··æ·†çš„åˆºæ¿€ $0.6 S_1 + 0.4 O_2$ é€šå¸¸ä¼šäº§ç”Ÿä¸€ä¸ªæ¯«æ— æ„ä¹‰çš„æ··åˆè¾“å‡º $0.6 O_1 + 0.4 O_2$ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°†åˆ©ç”¨å…¶å¼ºéçº¿æ€§æ¥åšå‡ºé€‰æ‹©ã€äº§ç”Ÿç±»åˆ«å’Œå†ç”Ÿä¿¡æ¯ï¼Œå¹¶ä¸”å¾ˆå¯èƒ½ä¼šä»è¿™æ ·ä¸€ä¸ªæ··æ·†çš„æ··åˆåˆºæ¿€ä¸­ç”Ÿæˆè¾“å‡º $O_1$ã€‚</p>
<blockquote>
<p>A linear associative net must be connected in a complex way with an external nonlinear logic processor in order to yield true computation. Complex circuitry is easy to plan but more difficult to discuss in evolutionary terms. In contrast, our model obtains its emergent computational properties from simple properties of many cells rather than circuitry.</p>
</blockquote>
<p>çº¿æ€§è”æƒ³ç½‘ç»œå¿…é¡»ä¸å¤–éƒ¨éçº¿æ€§é€»è¾‘å¤„ç†å™¨ä»¥å¤æ‚çš„æ–¹å¼è¿æ¥ï¼Œæ‰èƒ½äº§ç”ŸçœŸæ­£çš„è®¡ç®—ã€‚å¤æ‚çš„ç”µè·¯è®¾è®¡å¾ˆå®¹æ˜“ï¼Œä½†åœ¨è¿›åŒ–è®ºæ–¹é¢æ›´éš¾è®¨è®ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»è®¸å¤šç»†èƒçš„ç®€å•å±æ€§ä¸­è·å¾—å…¶æ¶Œç°çš„è®¡ç®—å±æ€§ï¼Œè€Œä¸æ˜¯ç”µè·¯ã€‚</p>
<h1 id="the-biological-interpretation-of-the-model">The biological interpretation of the model<a hidden class="anchor" aria-hidden="true" href="#the-biological-interpretation-of-the-model">#</a></h1>
<blockquote>
<p>Most neurons are capable of generating a train of action potentials-propagating pulses of electrochemical activity-when the  average potential across their membrane is held well above its  normal resting value. The mean rate at which action potentials are generated is a smooth function of the mean membrane potential, having the general form shown in Fig. 1.</p>
</blockquote>
<p>å½“ç¥ç»å…ƒè†œä¸Šçš„å¹³å‡ç”µä½è¿œé«˜äºå…¶æ­£å¸¸é™æ¯å€¼æ—¶ï¼Œå¤§å¤šæ•°ç¥ç»å…ƒéƒ½èƒ½äº§ç”Ÿä¸€è¿ä¸²åŠ¨ä½œç”µä½&ndash;ç”µåŒ–å­¦æ´»åŠ¨çš„ä¼ æ’­è„‰å†²ã€‚åŠ¨ä½œç”µä½äº§ç”Ÿçš„å¹³å‡é€Ÿç‡æ˜¯å¹³å‡è†œç”µä½çš„å¹³æ»‘å‡½æ•°ï¼Œå…¶ä¸€èˆ¬å½¢å¼å¦‚å›¾ 1 æ‰€ç¤ºã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/10/12/ysvzm6M4QnGReKO.png" alt=""  /></p>
<p>FIG. 1. Firing rate versus membrane voltage for a typical neuron (solid line), dropping to 0 for large negative potentials and saturating for positive potentials. The broken lines show approximations used in modeling.</p>
</blockquote>
<p>å›¾1æ˜¾ç¤ºäº†å…¸å‹ç¥ç»å…ƒçš„æ”¾ç”µé¢‘ç‡ä¸è†œç”µå‹çš„å…³ç³»ï¼ˆå®çº¿ï¼‰ï¼Œåœ¨å¤§è´Ÿç”µä½ä¸‹é™ä¸º0ï¼Œåœ¨æ­£ç”µä½ä¸‹é¥±å’Œã€‚è™šçº¿æ˜¾ç¤ºäº†å»ºæ¨¡ä¸­ä½¿ç”¨çš„è¿‘ä¼¼ã€‚</p>
</blockquote>
<blockquote>
<p>The biological information sent to other neurons often lies in a short-time average of the firing rate. When this is so, one can neglect the details of individual action potentials and  regard Fig. 1 as a smooth input-output relationship. [Parallel pathways carrying the same information would enhance the ability of the system to extract a short-term average firing rate.]</p>
</blockquote>
<p>å‘é€åˆ°å…¶ä»–ç¥ç»å…ƒçš„ç”Ÿç‰©ä¿¡æ¯é€šå¸¸å­˜åœ¨äºæ”¾ç”µç‡çš„çŸ­æ—¶é—´å¹³å‡å€¼ä¸­ã€‚å½“æƒ…å†µå¦‚æ­¤æ—¶ï¼Œå¯ä»¥å¿½ç•¥å•ä¸ªåŠ¨ä½œç”µä½çš„ç»†èŠ‚ï¼Œå¹¶å°†å›¾ 1 è§†ä¸ºä¸€ä¸ªå¹³æ»‘çš„è¾“å…¥è¾“å‡ºå…³ç³»ã€‚[æºå¸¦ç›¸åŒä¿¡æ¯çš„å¹¶è¡Œè·¯å¾„å°†å¢å¼ºç³»ç»Ÿæå–çŸ­æœŸå¹³å‡æ”¾ç”µé¢‘ç‡çš„èƒ½åŠ›ã€‚]</p>
<blockquote>
<p>A study of emergent collective effects and spontaneous computation must necessarily focus on the nonlinearity of the input-output relationship. The essence of computation is nonlinear logical operations. The particle interactions that produce true collective effects in particle dynamics come from a nonlinear dependence of forces on positions of the particles. Whereas linear associative networks have emphasized the linear central region of Fig. 1, we will replace the input-output relationship by the dot-dash step. Those neurons whose operation is dominantly linear merely provide a pathway of communication between nonlinear neurons. Thus, we consider a network of &ldquo;on or off&rdquo; neurons, granting that some of the interconnections may be by way of neurons operating in the linear regime.</p>
</blockquote>
<p>å¯¹çªå‘é›†ä½“æ•ˆåº”å’Œè‡ªå‘è®¡ç®—çš„ç ”ç©¶ï¼Œå¿…é¡»å…³æ³¨è¾“å…¥-è¾“å‡ºå…³ç³»çš„éçº¿æ€§ã€‚è®¡ç®—çš„æœ¬è´¨æ˜¯éçº¿æ€§é€»è¾‘è¿ç®—ã€‚ç²’å­åŠ¨åŠ›å­¦ä¸­äº§ç”ŸçœŸæ­£é›†ä½“æ•ˆåº”çš„ç²’å­ç›¸äº’ä½œç”¨æ¥è‡ªäºåŠ›å¯¹ç²’å­ä½ç½®çš„éçº¿æ€§ä¾èµ–ã€‚çº¿æ€§å…³è”ç½‘ç»œå¼ºè°ƒçš„æ˜¯å›¾ 1 ä¸­çš„çº¿æ€§ä¸­å¿ƒåŒºåŸŸï¼Œè€Œæˆ‘ä»¬å°†ç”¨ç‚¹-è™šçº¿æ­¥éª¤å–ä»£è¾“å…¥-è¾“å‡ºå…³ç³»ã€‚é‚£äº›ä»¥çº¿æ€§æ“ä½œä¸ºä¸»çš„ç¥ç»å…ƒåªæ˜¯æä¾›äº†éçº¿æ€§ç¥ç»å…ƒä¹‹é—´çš„äº¤æµé€”å¾„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘çš„æ˜¯ä¸€ä¸ªç”± &ldquo;å¼€æˆ–å…³&rdquo; ç¥ç»å…ƒç»„æˆçš„ç½‘ç»œï¼Œå½“ç„¶å…¶ä¸­çš„ä¸€äº›ç›¸äº’è¿æ¥å¯èƒ½æ˜¯é€šè¿‡åœ¨çº¿æ€§æœºåˆ¶ä¸‹å·¥ä½œçš„ç¥ç»å…ƒå®ç°çš„ã€‚</p>
<blockquote>
<p>Delays in synaptic transmission (of partially stochastic character) and in the transmission of impulses along axons and dendrites produce a delay between the input of a neuron and the generation of an effective output. All such delays have been modeled by a single parameter, the stochastic mean processing time $1/W$.</p>
</blockquote>
<p>çªè§¦ä¼ é€’ï¼ˆå…·æœ‰éƒ¨åˆ†éšæœºç‰¹æ€§ï¼‰ä»¥åŠæ²¿è½´çªå’Œæ ‘çªä¼ é€’å†²åŠ¨çš„å»¶è¿Ÿä¼šå¯¼è‡´ç¥ç»å…ƒè¾“å…¥ä¸äº§ç”Ÿæœ‰æ•ˆè¾“å‡ºä¹‹é—´çš„å»¶è¿Ÿã€‚æ‰€æœ‰è¿™äº›å»¶è¿Ÿéƒ½è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªå‚æ•°ï¼Œå³éšæœºå¹³å‡å¤„ç†æ—¶é—´ $1/W$ã€‚</p>
<blockquote>
<p>The input to a particular neuron arises from the current leaks of the synapses to that neuron, which influence the cell mean potential. The synapses are activated by arriving action potentials. The input signal to a cell $i$ can be taken to be</p>
<p>$$
\sum_{j}T_{ij}V_{j}
$$</p>
<p>where $T_{ij}$ represents the effectiveness of a synapse. Fig. 1 thus becomes an input-output relationship for a neuron.</p>
</blockquote>
<p>ç‰¹å®šç¥ç»å…ƒçš„è¾“å…¥æ¥è‡ªäºçªè§¦çš„ç”µæµæ³„æ¼ï¼Œè¿™äº›æ³„æ¼å½±å“ç»†èƒçš„å¹³å‡ç”µä½ã€‚çªè§¦ç”±åˆ°è¾¾çš„åŠ¨ä½œç”µä½æ¿€æ´»ã€‚å¯¹ç»†èƒ $i$ çš„è¾“å…¥ä¿¡å·å¯ä»¥è¡¨ç¤ºä¸º</p>
<p>$$
\sum_{j}T_{ij}V_{j}
$$</p>
<p>å…¶ä¸­ $T_{ij}$ ä»£è¡¨çªè§¦çš„æœ‰æ•ˆæ€§ã€‚å› æ­¤ï¼Œå›¾ 1 æˆä¸ºç¥ç»å…ƒçš„è¾“å…¥-è¾“å‡ºå…³ç³»ã€‚</p>
<blockquote>
<p>Little, Shaw, and Roney have developed ideas on the collective functioning ofneural nets based on &ldquo;on/off&rdquo; neurons and synchronous processing. However, in their model the relative timing of action potential spikes was central and resulted in reverberating action potential trains. Our model and theirs have limited formal similarity, although there may be connections at a deeper level.</p>
</blockquote>
<p>Littleã€Shaw å’Œ Roney åŸºäºâ€œå¼€/å…³â€ç¥ç»å…ƒå’ŒåŒæ­¥å¤„ç†ï¼Œå‘å±•äº†å…³äºç¥ç»ç½‘ç»œé›†ä½“åŠŸèƒ½çš„æ€æƒ³ã€‚ç„¶è€Œï¼Œåœ¨ä»–ä»¬çš„æ¨¡å‹ä¸­ï¼ŒåŠ¨ä½œç”µä½å³°å€¼çš„ç›¸å¯¹æ—¶æœºæ˜¯æ ¸å¿ƒï¼Œå¹¶å¯¼è‡´äº†å›å“çš„åŠ¨ä½œç”µä½åºåˆ—ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸ä»–ä»¬çš„æ¨¡å‹åœ¨å½¢å¼ä¸Šæœ‰ä¸€å®šçš„ç›¸ä¼¼æ€§ï¼Œå°½ç®¡åœ¨æ›´æ·±å±‚æ¬¡ä¸Šå¯èƒ½å­˜åœ¨è”ç³»ã€‚</p>
<blockquote>
<p>Most modeling of neural learning networks has been based on synapses of a general type described by Hebb and Eccles. The essential ingredient is the modification of $T_{ij}$ by correlations like</p>
<p>$$
\Delta T_{ij} = [V_{i}(t)V_{j}(t)]_{\text{average}}
$$</p>
<p>where the average is some appropriate calculation over past  history. Decay in time and effects of $[V_{i}(t)]_{\text{avg}}$ or $[V_{j}(t)]_{\text{avg}}$ are also allowed. Model networks with such synapses can construct the associative $T_{ij}$ of Eq. 2. We will therefore initially  assume that such a $T_{ij}$ has been produced by previous experience (or inheritance). The Hebbian property need not reside in single synapses; small groups of cells which produce such a net effect would suffice.</p>
</blockquote>
<p>å¤§å¤šæ•°ç¥ç»å­¦ä¹ ç½‘ç»œçš„å»ºæ¨¡éƒ½æ˜¯åŸºäº Hebb å’Œ Eccles æè¿°çš„ä¸€èˆ¬ç±»å‹çš„çªè§¦ã€‚åŸºæœ¬æˆåˆ†æ˜¯é€šè¿‡ç±»ä¼¼ä»¥ä¸‹çš„ç›¸å…³æ€§æ¥ä¿®æ”¹ $T_{ij}$</p>
<p>$$
\Delta T_{ij} = [V_{i}(t)V_{j}(t)]_{\text{average}}
$$</p>
<p>å…¶ä¸­å¹³å‡å€¼æ˜¯å¯¹è¿‡å»å†å²è¿›è¡Œçš„ä¸€äº›é€‚å½“è®¡ç®—ã€‚æ—¶é—´è¡°å‡å’Œ $[V_{i}(t)]_{\text{avg}}$ æˆ– $[V_{j}(t)]_{\text{avg}}$ çš„å½±å“ä¹Ÿæ˜¯å…è®¸çš„ã€‚å…·æœ‰è¿™ç§çªè§¦çš„æ¨¡å‹ç½‘ç»œå¯ä»¥æ„å»ºæ–¹ç¨‹ 2 ä¸­çš„å…³è” $T_{ij}$ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æœ€åˆå‡è®¾è¿™æ ·çš„ $T_{ij}$ æ˜¯ç”±å…ˆå‰çš„ç»éªŒï¼ˆæˆ–é—ä¼ ï¼‰äº§ç”Ÿçš„ã€‚Hebbian å±æ€§ä¸å¿…å­˜åœ¨äºå•ä¸ªçªè§¦ä¸­ï¼Œäº§ç”Ÿè¿™ç§å‡€æ•ˆåº”çš„å°ç»„ç»†èƒå°±è¶³å¤Ÿäº†ã€‚</p>
<blockquote>
<p>The network of cells we describe performs an abstract calculation and, for applications, the inputs should be appropriately coded. In visual processing, for example, feature extraction should previously have been done. The present modeling might then be related to how an entity or Gestalt is remembered or categorized on the basis of inputs representing a collection of its features.</p>
</blockquote>
<p>æˆ‘ä»¬æè¿°çš„ç»†èƒç½‘ç»œæ‰§è¡ŒæŠ½è±¡è®¡ç®—ï¼Œå¯¹äºåº”ç”¨æ¥è¯´ï¼Œè¾“å…¥åº”è¯¥è¢«é€‚å½“åœ°ç¼–ç ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰å¤„ç†ä¸­ï¼Œåº”è¯¥å…ˆè¿›è¡Œç‰¹å¾æå–ã€‚ç„¶åï¼Œå½“å‰çš„å»ºæ¨¡å¯èƒ½ä¸å¦‚ä½•åŸºäºä»£è¡¨å…¶ç‰¹å¾é›†åˆçš„è¾“å…¥æ¥è®°å¿†æˆ–åˆ†ç±»ä¸€ä¸ªå®ä½“æˆ–æ ¼å¼å¡”æœ‰å…³ã€‚</p>
<h1 id="studies-of-the-collective-behaviors-of-the-model">Studies of the collective behaviors of the model<a hidden class="anchor" aria-hidden="true" href="#studies-of-the-collective-behaviors-of-the-model">#</a></h1>
<blockquote>
<p>The model has stable limit points. Consider the special case $T_{ij}  = T_{ji}$, and define</p>
<p>$$
E = -\frac{1}{2}\sum_{i,j\neq i}T_{ij}V_{i}V_{j}
$$</p>
<p>$\Delta E$ due to $\Delta V_{i}$ is given by</p>
<p>$$
\Delta E = -\Delta V_{i}\sum_{j\neq i^{\prime}}T_{ij}V_{j}
$$</p>
<p>Thus, the algorithm for altering $V_{i}$ causes $E$ to be a monotonically decreasing function. State changes will continue until a least (local) $E$ is reached. This case is isomorphic with an Ising model. $T_{ij}$ provides the role ofthe exchange coupling, and there  is also an external local field at each site. When $T_{ij}$ is symmetric but has a random character (the spin glass) there are known to be many (locally) stable states.</p>
</blockquote>
<p>è¯¥æ¨¡å‹å…·æœ‰ç¨³å®šçš„æé™ç‚¹ã€‚è€ƒè™‘ç‰¹æ®Šæƒ…å†µ $T_{ij}  = T_{ji}$ï¼Œå¹¶å®šä¹‰</p>
<p>$$
E = -\frac{1}{2}\sum_{i,j\neq i}T_{ij}V_{i}V_{j}
$$</p>
<p>ç”±äº $\Delta V_{i}$ å¼•èµ·çš„ $\Delta E$ ç”±ä¸‹å¼ç»™å‡º</p>
<p>$$
\Delta E = -\Delta V_{i}\sum_{j\neq i^{\prime}}T_{ij}V_{j}
$$</p>
<p>å› æ­¤ï¼Œæ”¹å˜ $V_{i}$ çš„ç®—æ³•ä½¿å¾— $E$ æˆä¸ºå•è°ƒé€’å‡å‡½æ•°ã€‚çŠ¶æ€å˜åŒ–å°†æŒç»­ç›´åˆ°è¾¾åˆ°æœ€å°ï¼ˆå±€éƒ¨ï¼‰$E$ã€‚è¿™ç§æƒ…å†µä¸ Ising æ¨¡å‹åŒæ„ã€‚$T_{ij}$ æä¾›äº†äº¤æ¢è€¦åˆçš„ä½œç”¨ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªä½ç½®è¿˜æœ‰ä¸€ä¸ªå¤–éƒ¨å±€éƒ¨åœºã€‚å½“ $T_{ij}$ æ˜¯å¯¹ç§°çš„ä½†å…·æœ‰éšæœºç‰¹æ€§ï¼ˆè‡ªæ—‹ç»ç’ƒï¼‰æ—¶ï¼Œå·²çŸ¥å­˜åœ¨è®¸å¤šï¼ˆå±€éƒ¨ï¼‰ç¨³å®šçŠ¶æ€ã€‚</p>
<blockquote>
<p>Monte Carlo calculations were made on systems of $N = 30$ and $N = 100$, to examine the effect of removing the $T_{ij} = T_{ji}$ restriction. Each element of $T_{ij}$ was chosen as a random number between $-1$ and $1$. The neural architecture of typical cortical regions and also of simple ganglia of invertebrates suggests the importance of $100-10,000$ cells with intense mutual interconnections in elementary processing, so our scale of $N$ is slightly small.</p>
</blockquote>
<p>è¿›è¡Œäº† $N = 30$ å’Œ $N = 100$ ç³»ç»Ÿçš„è’™ç‰¹å¡æ´›è®¡ç®—ï¼Œä»¥æ£€æŸ¥å»é™¤ $T_{ij} = T_{ji}$ é™åˆ¶çš„å½±å“ã€‚$T_{ij}$ çš„æ¯ä¸ªå…ƒç´ éƒ½è¢«é€‰æ‹©ä¸ºä»‹äº $-1$ å’Œ $1$ ä¹‹é—´çš„éšæœºæ•°ã€‚å…¸å‹çš®å±‚åŒºåŸŸä»¥åŠæ— è„Šæ¤åŠ¨ç‰©ç®€å•ç¥ç»èŠ‚çš„ç¥ç»ç»“æ„è¡¨æ˜ï¼Œåœ¨åŸºæœ¬å¤„ç†è¿‡ç¨‹ä¸­ï¼Œ$100-10,000$ ä¸ªç»†èƒå…·æœ‰å¼ºçƒˆçš„ç›¸äº’è¿æ¥ï¼Œå› æ­¤æˆ‘ä»¬çš„ $N$ è§„æ¨¡ç•¥å°ã€‚</p>
<blockquote>
<p>The dynamics algorithm was initiated from randomly chosen initial starting configurations. For $N = 30$ the system never displayed an ergodic wandering through state space. Within a  time of about $4/W$ it settled into limiting behaviors, the commonest being a stable state. When $50$ trials were examined for a particular such random matrix, all would result in one of two  or three end states. A few stable states thus collect the flow from most of the initial state space. A simple cycle also occurred occasionally-for example, $\cdots A\rightarrow B\rightarrow A\rightarrow B\cdots$.</p>
</blockquote>
<p>å¯¹äº $N = 30$ï¼Œç³»ç»Ÿä»æœªæ˜¾ç¤ºå‡ºåœ¨çŠ¶æ€ç©ºé—´ä¸­çš„éå†æ¸¸è¡ã€‚åœ¨å¤§çº¦ $4/W$ çš„æ—¶é—´å†…ï¼Œå®ƒä¼šç¨³å®šä¸‹æ¥ï¼Œæœ€å¸¸è§çš„æ˜¯ä¸€ä¸ªç¨³å®šçŠ¶æ€ã€‚å½“æ£€æŸ¥äº†æŸä¸ªç‰¹å®šéšæœºçŸ©é˜µçš„ $50$ æ¬¡è¯•éªŒæ—¶ï¼Œæ‰€æœ‰è¯•éªŒéƒ½å°†å¯¼è‡´ä¸¤ä¸ªæˆ–ä¸‰ä¸ªæœ€ç»ˆçŠ¶æ€ä¹‹ä¸€ã€‚å› æ­¤ï¼Œä¸€äº›ç®€å•çš„å¾ªç¯ä¹Ÿå¶å°”å‘ç”Ÿï¼Œä¾‹å¦‚ï¼Œ$\cdots A\rightarrow B\rightarrow A\rightarrow B\cdots$ã€‚</p>
<blockquote>
<p>The third behavior seen was chaotic wandering in a small region of state space. The Hamming distance between two binary states $A$ and $B$ is defined as the number of places in which the digits are different. The chaotic wandering occurred within a short Hamming distance of one particular state. Statistics were done on the probability $p_{i}$ of the occurrence of a state in a time of wandering around this minimum, and an entropic measure of the available states $M$ was taken</p>
<p>$$
\ln{M} = -\sum p_{i}\ln{p_{i}}.
$$</p>
<p>A value of $M = 25$ was found for $N = 30$. The flow in phase space produced by this model algorithm has the properties necessary for a physical content-addressable memory whether or not $T_{ij}$ is symmetric.</p>
</blockquote>
<p>ç¬¬ä¸‰ç§è¡Œä¸ºæ˜¯åœ¨çŠ¶æ€ç©ºé—´çš„å°åŒºåŸŸå†…çš„æ··æ²Œæ¸¸è¡ã€‚å®šä¹‰ä¸¤ä¸ªäºŒè¿›åˆ¶çŠ¶æ€ $A$ å’Œ $B$ ä¹‹é—´çš„ Hamming è·ç¦»ä¸ºæ•°å­—ä¸åŒçš„ä½ç½®æ•°ã€‚æ··æ²Œæ¸¸è¡å‘ç”Ÿåœ¨ç¦»æŸä¸ªç‰¹å®šçŠ¶æ€å¾ˆè¿‘çš„ Hamming è·ç¦»å†…ã€‚å¯¹åœ¨å›´ç»•è¯¥æå°å€¼æ¸¸è¡ä¸€æ®µæ—¶é—´å†…çŠ¶æ€å‡ºç°çš„æ¦‚ç‡ $p_{i}$ è¿›è¡Œäº†ç»Ÿè®¡ï¼Œå¹¶é‡‡ç”¨äº†å¯ç”¨çŠ¶æ€çš„ç†µåº¦é‡ $M$</p>
<p>$$
\ln{M} = -\sum p_{i}\ln{p_{i}}
$$</p>
<p>å¯¹äº $N = 30$ï¼Œå‘ç° $M = 25$ã€‚æ— è®º $T_{ij}$ æ˜¯å¦å¯¹ç§°ï¼Œè¯¥æ¨¡å‹ç®—æ³•åœ¨ç›¸ç©ºé—´ä¸­äº§ç”Ÿçš„æµåŠ¨éƒ½å…·æœ‰ä½œä¸ºç‰©ç†å†…å®¹å¯å¯»å€å­˜å‚¨å™¨æ‰€å¿…éœ€çš„å±æ€§ã€‚</p>
<blockquote>
<p>Simulations with $N = 100$ were much slower and not quantitatively pursued. They showed qualitative similarity to $N = 30$.</p>
</blockquote>
<p>$N = 100$ çš„æ¨¡æ‹Ÿé€Ÿåº¦è¾ƒæ…¢ï¼Œæœªè¿›è¡Œå®šé‡ç ”ç©¶ã€‚å®ƒä»¬åœ¨å®šæ€§ä¸Šä¸ $N = 30$ æ˜¾ç¤ºå‡ºç›¸ä¼¼æ€§ã€‚</p>
<blockquote>
<p>Why should stable limit points or regions persist when $T_{ij} \neq T_{ji}$? If the algorithm at some time changes $V_{i}$ from $0$ to $1$ or vice versa, the change of the energy defined in Eq. 7 can be split into two terms, one of which is always negative. The second is identical if $T_{ij}$ is symmetric and is &ldquo;stochastic&rdquo; with mean 0 if $T_{ij}$ and $T_{ji}$ are randomly chosen. The algorithm for $T_{ij} \neq T_{ji}$, therefore changes $E$ in a fashion similar to the way $E$ would change in time for a symmetric $T_{ij}$ but with an algorithm corresponding to a finite temperature.</p>
</blockquote>
<p>ä¸ºä»€ä¹ˆå½“ $T_{ij} \neq T_{ji}$ æ—¶ï¼Œç¨³å®šçš„æé™ç‚¹æˆ–åŒºåŸŸä»ç„¶å­˜åœ¨ï¼Ÿå¦‚æœç®—æ³•åœ¨æŸä¸ªæ—¶é—´å°† $V_{i}$ ä» $0$ æ”¹å˜ä¸º $1$ æˆ–åä¹‹äº¦ç„¶ï¼Œåˆ™æ–¹ç¨‹ 7 ä¸­å®šä¹‰çš„èƒ½é‡å˜åŒ–å¯ä»¥åˆ†ä¸ºä¸¤é¡¹ï¼Œå…¶ä¸­ä¸€é¡¹æ€»æ˜¯è´Ÿçš„ã€‚å¦‚æœ $T_{ij}$ æ˜¯å¯¹ç§°çš„ï¼Œç¬¬äºŒé¡¹æ˜¯ç›¸åŒçš„ï¼›å¦‚æœ $T_{ij}$ å’Œ $T_{ji}$ æ˜¯éšæœºé€‰æ‹©çš„ï¼Œåˆ™å…¶ â€œéšæœºâ€ ä¸”å¹³å‡å€¼ä¸º 0ã€‚å› æ­¤ï¼Œ$T_{ij} \neq T_{ji}$ çš„ç®—æ³•ä»¥ç±»ä¼¼äºå¯¹ç§° $T_{ij}$ éšæ—¶é—´å˜åŒ–çš„æ–¹å¼æ”¹å˜ $E$ï¼Œä½†ç®—æ³•å¯¹åº”äºæœ‰é™æ¸©åº¦ã€‚</p>
<blockquote>
<p>About $0.15 N$ states can be simultaneously remembered before error in recall is severe. Computer modeling of memory storage according to Eq. 2 was carried out for $N = 30$ and $N = 100$. $n$ random memory states were chosen and the corresponding $T_{ij}$ was generated. If a nervous system preprocessed signals for efficient storage, the preprocessed information would appear random (e.g., the coding sequences of DNA have a random character). The random memory vectors thus simulate efficiently encoded real information, as well as representing our ignorance. The system was started at each assigned nominal memory state, and the state was allowed to evolve until stationary.</p>
</blockquote>
<p>åœ¨å›å¿†é”™è¯¯ä¸¥é‡ä¹‹å‰ï¼Œå¤§çº¦ $0.15 N$ ä¸ªçŠ¶æ€å¯ä»¥åŒæ—¶è¢«è®°ä½ã€‚æ ¹æ®æ–¹ç¨‹ 2 è¿›è¡Œäº† $N = 30$ å’Œ $N = 100$ çš„å­˜å‚¨è®°å¿†çš„è®¡ç®—æœºå»ºæ¨¡ã€‚é€‰æ‹©äº† $n$ ä¸ªéšæœºè®°å¿†çŠ¶æ€ï¼Œå¹¶ç”Ÿæˆäº†ç›¸åº”çš„ $T_{ij}$ã€‚å¦‚æœç¥ç»ç³»ç»Ÿå¯¹ä¿¡å·è¿›è¡Œé¢„å¤„ç†ä»¥å®ç°é«˜æ•ˆå­˜å‚¨ï¼Œåˆ™é¢„å¤„ç†çš„ä¿¡æ¯å°†å‘ˆç°éšæœºæ€§ï¼ˆä¾‹å¦‚ï¼ŒDNA çš„ç¼–ç åºåˆ—å…·æœ‰éšæœºç‰¹æ€§ï¼‰ã€‚å› æ­¤ï¼Œéšæœºè®°å¿†å‘é‡æœ‰æ•ˆåœ°æ¨¡æ‹Ÿäº†ç»è¿‡é«˜æ•ˆç¼–ç çš„çœŸå®ä¿¡æ¯ï¼ŒåŒæ—¶ä¹Ÿä»£è¡¨äº†æˆ‘ä»¬çš„æ— çŸ¥ã€‚ç³»ç»Ÿä»æ¯ä¸ªæŒ‡å®šçš„åä¹‰è®°å¿†çŠ¶æ€å¼€å§‹ï¼Œå¹¶å…è®¸çŠ¶æ€æ¼”å˜ç›´åˆ°ç¨³å®šã€‚</p>
<blockquote>
<p>Typical results are shown in Fig. 2. The statistics are averages over both the states in a given matrix and different matrices. With $n = 5$, the assigned memory states are almost always stable (and exactly recallable). For $n = 15$, about half of the nominally remembered states evolved to stable states with less than $5$ errors, but the rest evolved to states quite different from the starting points.</p>
</blockquote>
<p>å…¸å‹ç»“æœå¦‚å›¾ 2 æ‰€ç¤ºã€‚ç»Ÿè®¡æ•°æ®æ˜¯å¯¹ç»™å®šçŸ©é˜µä¸­çš„çŠ¶æ€å’Œä¸åŒçŸ©é˜µçš„å¹³å‡å€¼ã€‚å¯¹äº $n = 5$ï¼ŒæŒ‡å®šçš„è®°å¿†çŠ¶æ€å‡ ä¹æ€»æ˜¯ç¨³å®šçš„ï¼ˆå¹¶ä¸”å¯ä»¥ç²¾ç¡®å›å¿†ï¼‰ã€‚å¯¹äº $n = 15$ï¼Œå¤§çº¦ä¸€åŠçš„åä¹‰è®°å¿†çŠ¶æ€æ¼”å˜ä¸ºé”™è¯¯å°‘äº $5$ çš„ç¨³å®šçŠ¶æ€ï¼Œä½†å…¶ä½™çš„æ¼”å˜ä¸ºä¸èµ·å§‹ç‚¹å®Œå…¨ä¸åŒçš„çŠ¶æ€ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/10/12/lZRyYgeCwi8mVGJ.png" alt=""  /></p>
<p>FIG. 2. The probability distribution of the occurrence of errors in the location of the stable states obtained from nominally assigned memories.</p>
</blockquote>
<blockquote>
<p>å›¾ 2 æ˜¾ç¤ºäº†ä»åä¹‰ä¸Šåˆ†é…çš„è®°å¿†ä¸­è·å¾—çš„ç¨³å®šçŠ¶æ€ä½ç½®é”™è¯¯å‘ç”Ÿçš„æ¦‚ç‡åˆ†å¸ƒã€‚</p>
</blockquote>
</blockquote>
<blockquote>
<p>These results can be understood from an analysis of the effect of the noise terms. In Eq. 3, $H_{i}^{s^{\prime}}$ is the &ldquo;effective field&rdquo; on neuron $i$ when the state of the system is $s^{\prime}$, one of the nominal memory states. The expectation value of this sum, Eq. 4, is $\pm N/2$ as appropriate. The $s\neq s^{\prime}$ summation in Eq. 2 contributes no mean, but has a rms noise of $[(n - 1)N/2]^{1/2}2\equiv \sigma$. For $nN$ large, this noise is approximately Gaussian and the probability of an error in a single particular bit of a particular memory will be</p>
<p>$$
P = \frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{N/2}^{\infty}e^{-x^{2}/2\sigma^{2}}dx.
$$</p>
<p>For the case $n = 10$, $N = 100$, $P = 0.0091$, the probability that a state had no errors in its $100$ bits should be about $e^{-0.91} \approx 0.40$. In the simulation of Fig. 2, the experimental number was $0.6$.</p>
</blockquote>
<p>è¿™äº›ç»“æœå¯ä»¥ä»å¯¹å™ªå£°é¡¹å½±å“çš„åˆ†æä¸­ç†è§£ã€‚åœ¨æ–¹ç¨‹ 3 ä¸­ï¼Œå½“ç³»ç»ŸçŠ¶æ€ä¸º $s^{\prime}$ï¼ˆåä¹‰è®°å¿†çŠ¶æ€ä¹‹ä¸€ï¼‰æ—¶ï¼Œ$H_{i}^{s^{\prime}}$ æ˜¯ç¥ç»å…ƒ $i$ ä¸Šçš„â€œæœ‰æ•ˆåœºâ€ã€‚è¯¥å’Œå¼çš„æœŸæœ›å€¼ï¼ˆæ–¹ç¨‹ 4ï¼‰ä¸º $\pm N/2$ã€‚æ–¹ç¨‹ 2 ä¸­çš„ $s\neq s^{\prime}$ æ±‚å’Œæ²¡æœ‰è´¡çŒ®å‡å€¼ï¼Œä½†å…·æœ‰å‡æ–¹æ ¹å™ªå£° $[(n - 1)N/2]^{1/2}2\equiv \sigma$ã€‚å¯¹äºå¤§çš„ $nN$ï¼Œè¿™ç§å™ªå£°è¿‘ä¼¼ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œç‰¹å®šè®°å¿†çš„å•ä¸ªä½å‘ç”Ÿé”™è¯¯çš„æ¦‚ç‡ä¸º</p>
<p>$$
P = \frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{N/2}^{\infty}e^{-x^{2}/2\sigma^{2}}dx.
$$</p>
<p>å¯¹äº $n = 10$ï¼Œ$N = 100$ çš„æƒ…å†µï¼Œ$P = 0.0091$ï¼Œä¸€ä¸ªçŠ¶æ€åœ¨å…¶ $100$ ä½ä¸­æ²¡æœ‰é”™è¯¯çš„æ¦‚ç‡åº”çº¦ä¸º $e^{-0.91} \approx 0.40$ã€‚åœ¨å›¾ 2 çš„æ¨¡æ‹Ÿä¸­ï¼Œå®éªŒå€¼ä¸º $0.6$ã€‚</p>
<blockquote>
<p>The theoretical scaling of $n$ with $N$ at fixed $P$ was demonstrated in the simulations going between $N = 30$ and $N = 100$. The experimental results of half the memories being well retained at $n = 0.15 N$ and the rest badly retained is expected to be true for all large $N$. The information storage at a given level of accuracy can be increased by a factor of $2$ by a judicious choice of individual neuron thresholds. This choice is equivalent to using variables $\mu_{i} = \pm 1$, $T_{ij} = \sum_{s}\mu_{i}^{s}\mu_{j}^{s}$, and a threshold level of $0$.</p>
</blockquote>
<p>åœ¨å›ºå®š $P$ ä¸‹ï¼Œ$n$ éš $N$ çš„ç†è®ºæ ‡åº¦åœ¨ $N = 30$ å’Œ $N = 100$ ä¹‹é—´çš„æ¨¡æ‹Ÿä¸­å¾—åˆ°äº†è¯æ˜ã€‚åœ¨ $n = 0.15 N$ æ—¶ï¼Œä¸€åŠçš„è®°å¿†è¢«å¾ˆå¥½åœ°ä¿ç•™ï¼Œå…¶ä½™çš„è®°å¿†è¢«ä¸¥é‡ä¿ç•™ï¼Œè¿™ä¸€å®éªŒç»“æœé¢„è®¡å¯¹æ‰€æœ‰å¤§çš„ $N$ éƒ½æ˜¯æ­£ç¡®çš„ã€‚é€šè¿‡æ˜æ™ºåœ°é€‰æ‹©å•ä¸ªç¥ç»å…ƒé˜ˆå€¼ï¼Œå¯ä»¥å°†ç»™å®šç²¾åº¦æ°´å¹³ä¸‹çš„ä¿¡æ¯å­˜å‚¨å¢åŠ ä¸€ä¸ªå› å­ $2$ã€‚è¿™ç§é€‰æ‹©ç­‰åŒäºä½¿ç”¨å˜é‡ $\mu_{i} = \pm 1$ï¼Œ$T_{ij} = \sum_{s}\mu_{i}^{s}\mu_{j}^{s}$ï¼Œä»¥åŠé˜ˆå€¼æ°´å¹³ä¸º $0$ã€‚</p>
<blockquote>
<p>Given some arbitrary starting state, what is the resulting final state (or statistically, states)? To study this, evolutions from randomly chosen initial states were tabulated for $N = 30$ and $n = 5$. From the (inessential) symmetry of the algorithm, if $(101110\cdots)$ is an assigned stable state, $(010001\cdots)$ is also stable. Therefore, the matrices had $10$ nominal stable states. Approximately 85% of the trials ended in assigned memories, and 10% ended in stable states of no obvious meaning. An ambiguous 5% landed in stable states very near assigned memories. There was a range of a factor of 20 of the likelihood of finding these 10 states.</p>
</blockquote>
<p>ç»™å®šä¸€äº›ä»»æ„çš„èµ·å§‹çŠ¶æ€ï¼Œæœ€ç»ˆçŠ¶æ€ï¼ˆæˆ–ç»Ÿè®¡ä¸Šï¼ŒçŠ¶æ€ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿä¸ºäº†ç ”ç©¶è¿™ä¸€ç‚¹ï¼Œå¯¹ $N = 30$ å’Œ $n = 5$ çš„éšæœºé€‰æ‹©åˆå§‹çŠ¶æ€çš„æ¼”å˜è¿›è¡Œäº†ç»Ÿè®¡ã€‚ç”±äºç®—æ³•çš„ï¼ˆéæœ¬è´¨ï¼‰å¯¹ç§°æ€§ï¼Œå¦‚æœ $(101110\cdots)$ æ˜¯ä¸€ä¸ªæŒ‡å®šçš„ç¨³å®šçŠ¶æ€ï¼Œé‚£ä¹ˆ $(010001\cdots)$ ä¹Ÿæ˜¯ç¨³å®šçš„ã€‚å› æ­¤ï¼ŒçŸ©é˜µæœ‰ $10$ ä¸ªåä¹‰ç¨³å®šçŠ¶æ€ã€‚å¤§çº¦ 85% çš„è¯•éªŒä»¥æŒ‡å®šçš„è®°å¿†ç»“æŸï¼Œ10% ä»¥æ²¡æœ‰æ˜æ˜¾æ„ä¹‰çš„ç¨³å®šçŠ¶æ€ç»“æŸã€‚å¤§çº¦ 5% çš„ä¸æ˜ç¡®çŠ¶æ€è½åœ¨éå¸¸æ¥è¿‘æŒ‡å®šè®°å¿†çš„ç¨³å®šçŠ¶æ€ä¸­ã€‚æ‰¾åˆ°è¿™ 10 ä¸ªçŠ¶æ€çš„å¯èƒ½æ€§å­˜åœ¨ä¸€ä¸ªå› ç´ ä¸º 20 çš„èŒƒå›´ã€‚</p>
<blockquote>
<p>The algorithm leads to memories near the starting state. For $N = 30$, $n = 5$, partially random starting states were generated by random modification of known memories. The probability that the final state was that closest to the initial state was studied as a function of the distance between the initial state and the nearest memory state. For distance $\leq 5$, the nearest state was reached more than 90% of the time. Beyond that distance, the probability fell off smoothly, dropping to a level of $0.2$ (2 times random chance) for a distance of 12.</p>
</blockquote>
<p>è¯¥ç®—æ³•å¯¼è‡´è®°å¿†æ¥è¿‘èµ·å§‹çŠ¶æ€ã€‚å¯¹äº $N = 30$ï¼Œ$n = 5$ï¼Œé€šè¿‡éšæœºä¿®æ”¹å·²çŸ¥è®°å¿†ç”Ÿæˆéƒ¨åˆ†éšæœºèµ·å§‹çŠ¶æ€ã€‚ç ”ç©¶äº†æœ€ç»ˆçŠ¶æ€æ˜¯æœ€æ¥è¿‘åˆå§‹çŠ¶æ€çš„æ¦‚ç‡ï¼Œä½œä¸ºåˆå§‹çŠ¶æ€ä¸æœ€è¿‘è®°å¿†çŠ¶æ€ä¹‹é—´è·ç¦»çš„å‡½æ•°ã€‚å¯¹äºè·ç¦» $\leq 5$ï¼Œæœ€æ¥è¿‘çš„çŠ¶æ€è¶…è¿‡ 90% çš„æ—¶é—´è¢«è¾¾åˆ°ã€‚è¶…è¿‡è¯¥è·ç¦»åï¼Œæ¦‚ç‡å¹³ç¨³ä¸‹é™ï¼Œåœ¨è·ç¦»ä¸º 12 æ—¶é™è‡³ $0.2$ï¼ˆæ˜¯éšæœºæœºä¼šçš„ä¸¤å€ï¼‰ã€‚</p>
<blockquote>
<p>The phase space flow is apparently dominated by attractors  which are the nominally assigned memories, each of which dominates a substantial region around it. The flow is not entirely deterministic, and the system responds to an ambiguous starting state by a statistical choice between the memory states it most resembles.</p>
</blockquote>
<p>ç›¸ç©ºé—´æµæ˜¾ç„¶ç”±å¸å¼•å­ä¸»å¯¼ï¼Œè¿™äº›å¸å¼•å­æ˜¯åä¹‰ä¸Šåˆ†é…çš„è®°å¿†ï¼Œæ¯ä¸ªè®°å¿†éƒ½ä¸»å¯¼ç€å…¶å‘¨å›´çš„ä¸€ä¸ªé‡è¦åŒºåŸŸã€‚æµåŠ¨å¹¶éå®Œå…¨ç¡®å®šæ€§ï¼Œç³»ç»Ÿé€šè¿‡åœ¨å®ƒæœ€åƒçš„è®°å¿†çŠ¶æ€ä¹‹é—´è¿›è¡Œç»Ÿè®¡é€‰æ‹©æ¥å“åº”æ¨¡ç³Šçš„èµ·å§‹çŠ¶æ€ã€‚</p>
<blockquote>
<p>Were it desired to use such a system in an $S_{i}$-based content addressable memory, the algorithm should be used and modified to hold the known bits of information while letting the others adjust.</p>
</blockquote>
<p>å¦‚æœå¸Œæœ›åœ¨åŸºäº $S_{i}$ çš„å†…å®¹å¯å¯»å€å­˜å‚¨å™¨ä¸­ä½¿ç”¨è¿™æ ·çš„ç³»ç»Ÿï¼Œåˆ™åº”ä½¿ç”¨è¯¥ç®—æ³•å¹¶è¿›è¡Œä¿®æ”¹ï¼Œä»¥ä¿æŒå·²çŸ¥çš„ä¿¡æ¯ä½ï¼ŒåŒæ—¶è®©å…¶ä»–ä½è¿›è¡Œè°ƒæ•´ã€‚</p>
<blockquote>
<p>The model was studied by using a &ldquo;clipped&rdquo; $T_{ij}$, replacing $T_{ij}$ in Eq. 3 by $\pm 1$, the algebraic sign of $T_{ij}$. The purposes were to  examine the necessity of a linear synapse supposition (by making a highly nonlinear one) and to examine the efficiency of storage. Only $N(N/2)$ bits of information can possibly be stored in this symmetric matrix. Experimentally, for $N = 100$, $n = 9$, the level of errors was similar to that for the ordinary algorithm at $n =  12$. The signal-to-noise ratio can be evaluated analytically for this clipped algorithm and is reduced by a factor of $(2/\pi)^{1/2}$ compared with the unclipped case. For a fixed error probability, the number of memories must be reduced by $2/\pi$.</p>
</blockquote>
<p>é€šè¿‡ä½¿ç”¨ â€œç®€åŒ–â€ $T_{ij}$ ç ”ç©¶äº†è¯¥æ¨¡å‹ï¼Œåœ¨æ–¹ç¨‹ 3 ä¸­å°† $T_{ij}$ æ›¿æ¢ä¸º $T_{ij}$ çš„ä»£æ•°ç¬¦å· $\pm 1$ã€‚ç›®çš„åœ¨äºé€šè¿‡åˆ¶ä½œä¸€ä¸ªé«˜åº¦éçº¿æ€§çš„çªè§¦æ¥æ£€æŸ¥çº¿æ€§çªè§¦å‡è®¾çš„å¿…è¦æ€§ï¼Œå¹¶æ£€æŸ¥å­˜å‚¨æ•ˆç‡ã€‚åœ¨è¿™ä¸ªå¯¹ç§°çŸ©é˜µä¸­ï¼Œæœ€å¤šåªèƒ½å­˜å‚¨ $N(N/2)$ ä½ä¿¡æ¯ã€‚å®éªŒä¸Šï¼Œå¯¹äº $N = 100$ï¼Œ$n = 9$ï¼Œé”™è¯¯æ°´å¹³ä¸æ™®é€šç®—æ³•åœ¨ $n = 12$ æ—¶çš„é”™è¯¯æ°´å¹³ç›¸ä¼¼ã€‚å¯ä»¥å¯¹è¿™ä¸ªè£å‰ªç®—æ³•è¿›è¡Œä¿¡å™ªæ¯”çš„è§£æè¯„ä¼°ï¼Œå¹¶ä¸”ä¸æœªè£å‰ªæƒ…å†µç›¸æ¯”ï¼Œä¿¡å™ªæ¯”é™ä½äº†ä¸€ä¸ªå› å­ $(2/\pi)^{1/2}$ã€‚å¯¹äºå›ºå®šçš„é”™è¯¯æ¦‚ç‡ï¼Œè®°å¿†æ•°é‡å¿…é¡»å‡å°‘ $2/\pi$ã€‚</p>
<blockquote>
<p>With the $\mu$ algorithm and the clipped $T_{ij}$, both analysis and modeling showed that the maximal information stored for $N = 100$ occurred at about $n = 13$. Some errors were present, and the Shannon information stored corresponded to about $N(N/8)$ bits.</p>
</blockquote>
<p>ä½¿ç”¨ $\mu$ ç®—æ³•å’Œè£å‰ªçš„ $T_{ij}$ï¼Œåˆ†æå’Œå»ºæ¨¡éƒ½æ˜¾ç¤ºï¼Œå¯¹äº $N = 100$ï¼Œå­˜å‚¨çš„æœ€å¤§ä¿¡æ¯é‡çº¦åœ¨ $n = 13$ æ—¶å‡ºç°ã€‚å­˜åœ¨ä¸€äº›é”™è¯¯ï¼Œå­˜å‚¨çš„ Shannon ä¿¡æ¯å¯¹åº”äºå¤§çº¦ $N(N/8)$ bitã€‚</p>
<blockquote>
<p>New memories can be continually added to $T_{ij}$. The addition of new memories beyond the capacity overloads the system and makes all memory states irretrievable unless there is a provision for forgetting old memories.</p>
</blockquote>
<p>å¯ä»¥ä¸æ–­åœ°å°†æ–°è®°å¿†æ·»åŠ åˆ° $T_{ij}$ ä¸­ã€‚è¶…å‡ºå®¹é‡çš„æ–°è®°å¿†çš„æ·»åŠ ä¼šä½¿ç³»ç»Ÿè¿‡è½½ï¼Œå¹¶ä¸”é™¤éæœ‰å¿˜è®°æ—§è®°å¿†çš„è§„å®šï¼Œå¦åˆ™æ‰€æœ‰è®°å¿†çŠ¶æ€éƒ½æ— æ³•æ£€ç´¢ã€‚</p>
<blockquote>
<p>The saturation of the possible size of $T_{ij}$ will itself cause forgetting. Let the possible values of $T_{ij}$ be $0, \pm 1, \pm 2, \pm 3$, and be freely incremented within this range. If $T_{ij} = 3$, a next increment of +1 would be ignored and a next increment of $-1$ would reduce $T_{ij}$ to $2$. When $T_{ij}$ is so constructed, only the recent memory states are retained, with a slightly increased noise level. Memories from the distant past are no longer stable. How far into the past are states remembered depends on the digitizing depth of $T_{ij}$, and $0, \cdots,\pm 3$ is an appropriate level for $N = 100$. Other schemes can be used to keep too many memories from being simultaneously written, but this particular one is attractive because it requires no delicate balances and is a consequence of natural hardware.</p>
</blockquote>
<p>$T_{ij}$ å¯èƒ½çš„å¤§å°é¥±å’Œæœ¬èº«ä¼šå¯¼è‡´é—å¿˜ã€‚è®© $T_{ij}$ çš„å¯èƒ½å€¼ä¸º $0, \pm 1, \pm 2, \pm 3$ï¼Œå¹¶åœ¨æ­¤èŒƒå›´å†…è‡ªç”±é€’å¢ã€‚å¦‚æœ $T_{ij} = 3$ï¼Œä¸‹ä¸€ä¸ª $+1$ çš„å¢é‡å°†è¢«å¿½ç•¥ï¼Œä¸‹ä¸€ä¸ª $-1$ çš„å¢é‡å°†ä½¿ $T_{ij}$ é™è‡³ $2$ã€‚å½“ $T_{ij}$ è¿™æ ·æ„é€ æ—¶ï¼Œåªä¿ç•™æœ€è¿‘çš„è®°å¿†çŠ¶æ€ï¼Œå™ªå£°æ°´å¹³ç•¥æœ‰å¢åŠ ã€‚æ¥è‡ªé¥è¿œè¿‡å»çš„è®°å¿†ä¸å†ç¨³å®šã€‚è®°å¿†çŠ¶æ€è¢«è®°ä½çš„æ—¶é—´å–å†³äº $T_{ij}$ çš„æ•°å­—åŒ–æ·±åº¦ï¼Œè€Œ $0, \cdots,\pm 3$ æ˜¯ $N = 100$ çš„é€‚å½“æ°´å¹³ã€‚å¯ä»¥ä½¿ç”¨å…¶ä»–æ–¹æ¡ˆæ¥é˜²æ­¢åŒæ—¶å†™å…¥è¿‡å¤šçš„è®°å¿†ï¼Œä½†è¿™ä¸ªç‰¹å®šçš„æ–¹æ¡ˆå¾ˆæœ‰å¸å¼•åŠ›ï¼Œå› ä¸ºå®ƒä¸éœ€è¦å¾®å¦™çš„å¹³è¡¡ï¼Œå¹¶ä¸”æ˜¯è‡ªç„¶ç¡¬ä»¶çš„ç»“æœã€‚</p>
<blockquote>
<p>Real neurons need not make synapses both of $i\rightarrow j$ and $j\rightarrow i$. Particular synapses are restricted to one sign of output. We  therefore asked whether $T_{ij} = T_{ji}$ is important. Simulations were carried out with only one $ij$ connection: if $T_{ij}\neq 0$, $T_{ji} = 0$. The probability of making errors increased, but the algorithm continued to generate stable minima. A Gaussian noise description of the error rate shows that the signal-to-noise ratio for given $n$ and $N$ should be decreased by the factor $1/\sqrt{2}$, and the simulations were consistent with such a factor. This same analysis shows that the system generally fails in a &ldquo;soft&rdquo; fashion, with signal-to-noise ratio and error rate increasing slowly as more synapses fail.</p>
</blockquote>
<p>çœŸå®çš„ç¥ç»å…ƒä¸éœ€è¦åŒæ—¶å¯¹ $i\rightarrow j$ å’Œ $j\rightarrow i$ è¿›è¡Œçªè§¦è¿æ¥ã€‚ç‰¹å®šçš„çªè§¦ä»…é™äºä¸€ç§è¾“å‡ºç¬¦å·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é—®æ˜¯å¦ $T_{ij} = T_{ji}$ å¾ˆé‡è¦ã€‚è¿›è¡Œäº†ä»…æœ‰ä¸€ä¸ª $ij$ è¿æ¥çš„æ¨¡æ‹Ÿï¼šå¦‚æœ $T_{ij}\neq 0$ï¼Œåˆ™ $T_{ji} = 0$ã€‚é”™è¯¯çš„æ¦‚ç‡å¢åŠ äº†ï¼Œä½†ç®—æ³•ç»§ç»­ç”Ÿæˆç¨³å®šçš„æå°å€¼ã€‚å¯¹é”™è¯¯ç‡çš„é«˜æ–¯å™ªå£°æè¿°è¡¨æ˜ï¼Œå¯¹äºç»™å®šçš„ $n$ å’Œ $N$ï¼Œä¿¡å™ªæ¯”åº”é™ä½å› å­ $1/\sqrt{2}$ï¼Œæ¨¡æ‹Ÿç»“æœä¸è¯¥å› å­ä¸€è‡´ã€‚ç›¸åŒçš„åˆ†æè¡¨æ˜ï¼Œç³»ç»Ÿé€šå¸¸ä»¥â€œè½¯â€æ–¹å¼å¤±è´¥ï¼Œéšç€æ›´å¤šçªè§¦å¤±æ•ˆï¼Œä¿¡å™ªæ¯”å’Œé”™è¯¯ç‡ç¼“æ…¢å¢åŠ ã€‚</p>
<blockquote>
<p>Memories too close to each other are confused and tend to merge. For $N = 100$, a pair ofrandom memories should be separated by $50\pm 5$ Hamming units. The case $N = 100$, $n = 8$,  was studied with seven random memories and the eighth made up a Hamming distance of only $30$, $20$, or $10$ from one of the other seven memories. At a distance of $30$, both similar memories were usually stable. At a distance of $20$, the minima were usually distinct but displaced. At a distance of $10$, the minima were often fused.</p>
</blockquote>
<p>è¿‡äºæ¥è¿‘çš„è®°å¿†ä¼šæ··æ·†å¹¶å€¾å‘äºåˆå¹¶ã€‚å¯¹äº $N = 100$ï¼Œä¸€å¯¹éšæœºè®°å¿†åº”ç›¸éš” $50\pm 5$ ä¸ª Hamming å•ä½ã€‚ç ”ç©¶äº† $N = 100$ï¼Œ$n = 8$ çš„æƒ…å†µï¼Œå…¶ä¸­ä¸ƒä¸ªéšæœºè®°å¿†ï¼Œç¬¬å…«ä¸ªä¸å…¶ä»–ä¸ƒä¸ªè®°å¿†ä¹‹ä¸€çš„ Hamming è·ç¦»ä»…ä¸º $30$ã€$20$ æˆ– $10$ã€‚åœ¨è·ç¦»ä¸º $30$ æ—¶ï¼Œä¸¤ä¸ªç›¸ä¼¼çš„è®°å¿†é€šå¸¸æ˜¯ç¨³å®šçš„ã€‚åœ¨è·ç¦»ä¸º $20$ æ—¶ï¼Œæå°å€¼é€šå¸¸æ˜¯ä¸åŒçš„ä½†è¢«ä½ç§»äº†ã€‚åœ¨è·ç¦»ä¸º $10$ æ—¶ï¼Œæå°å€¼ç»å¸¸èåˆåœ¨ä¸€èµ·ã€‚</p>
<blockquote>
<p>The algorithm categorizes initial states according to the similarity to memory states. With a threshold of $0$, the system behaves as a forced categorizer.</p>
</blockquote>
<p>è¯¥ç®—æ³•æ ¹æ®ä¸è®°å¿†çŠ¶æ€çš„ç›¸ä¼¼æ€§å¯¹åˆå§‹çŠ¶æ€è¿›è¡Œåˆ†ç±»ã€‚ä½¿ç”¨ $0$ çš„é˜ˆå€¼æ—¶ï¼Œç³»ç»Ÿè¡¨ç°ä¸ºå¼ºåˆ¶åˆ†ç±»å™¨ã€‚</p>
<blockquote>
<p>The state $00000\cdots$ is always stable. For a threshold of $0$, this  stable state is much higher in energy than the stored memory  states and very seldom occurs. Adding a uniform threshold in  the algorithm is equivalent to raising the effective energy of the  stored memories compared to the $0000$ state, and $0000$ also  becomes a likely stable state. The $0000$ state is then generated by any initial state that does not resemble adequately closely one of the assigned memories and represents positive recognition that the starting state is not familiar.</p>
</blockquote>
<p>çŠ¶æ€ $00000\cdots$ æ€»æ˜¯ç¨³å®šçš„ã€‚å¯¹äºé˜ˆå€¼ä¸º $0$ï¼Œè¿™ä¸ªç¨³å®šçŠ¶æ€çš„èƒ½é‡è¿œé«˜äºå­˜å‚¨çš„è®°å¿†çŠ¶æ€ï¼Œå¹¶ä¸”å¾ˆå°‘å‘ç”Ÿã€‚åœ¨ç®—æ³•ä¸­æ·»åŠ ä¸€ä¸ªç»Ÿä¸€çš„é˜ˆå€¼ç›¸å½“äºæé«˜äº†å­˜å‚¨è®°å¿†çš„æœ‰æ•ˆèƒ½é‡ä¸ $0000$ çŠ¶æ€ç›¸æ¯”ï¼Œå¹¶ä¸” $0000$ ä¹Ÿæˆä¸ºä¸€ä¸ªå¯èƒ½çš„ç¨³å®šçŠ¶æ€ã€‚ç„¶åï¼Œä»»ä½•ä¸å¤Ÿæ¥è¿‘åœ°ç±»ä¼¼äºæŒ‡å®šè®°å¿†ä¹‹ä¸€çš„åˆå§‹çŠ¶æ€éƒ½ä¼šç”Ÿæˆ $0000$ çŠ¶æ€ï¼Œå¹¶è¡¨ç¤ºå¯¹èµ·å§‹çŠ¶æ€ä¸ç†Ÿæ‚‰çš„ç§¯æè¯†åˆ«ã€‚</p>
<blockquote>
<p>Familiarity can be recognized by other means when the memory is drastically overloaded. We examined the case $N = 100$, $n = 500$, in which there is a memory overload of a factor of $25$. None of the memory states assigned were stable. The initial rate of processing of a starting state is defined as the number of neuron state readjustments that occur in a time $1/2W$. Familiar and unfamiliar states were distinguishable most of the time at this level of overload on the basis of the initial processing rate, which was faster for unfamiliar states. This kind of familiarity can only be read out of the system by a class of neurons or devices abstracting average properties of the processing group.</p>
</blockquote>
<p>å½“è®°å¿†è¢«å¤§å¹…åº¦è¶…è½½æ—¶ï¼Œå¯ä»¥é€šè¿‡å…¶ä»–æ–¹å¼è¯†åˆ«ç†Ÿæ‚‰åº¦ã€‚æˆ‘ä»¬æ£€æŸ¥äº† $N = 100$ï¼Œ$n = 500$ çš„æƒ…å†µï¼Œå…¶ä¸­è®°å¿†è¶…è½½äº† $25$ å€ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ²¡æœ‰åˆ†é…çš„è®°å¿†çŠ¶æ€æ˜¯ç¨³å®šçš„ã€‚èµ·å§‹çŠ¶æ€çš„åˆå§‹å¤„ç†é€Ÿç‡å®šä¹‰ä¸ºåœ¨æ—¶é—´ $1/2W$ å†…å‘ç”Ÿçš„ç¥ç»å…ƒçŠ¶æ€é‡æ–°è°ƒæ•´çš„æ•°é‡ã€‚åœ¨è¿™ç§è¶…è½½æ°´å¹³ä¸‹ï¼Œå¤§å¤šæ•°æ—¶å€™å¯ä»¥æ ¹æ®åˆå§‹å¤„ç†é€Ÿç‡åŒºåˆ†ç†Ÿæ‚‰å’Œä¸ç†Ÿæ‚‰çš„çŠ¶æ€ï¼Œä¸ç†Ÿæ‚‰çš„çŠ¶æ€å¤„ç†é€Ÿåº¦æ›´å¿«ã€‚è¿™ç§ç†Ÿæ‚‰åº¦åªèƒ½é€šè¿‡ä¸€ç±»ç¥ç»å…ƒæˆ–è®¾å¤‡ä»å¤„ç†ç»„ä¸­æŠ½è±¡å‡ºå¹³å‡å±æ€§æ¥è¯»å–ç³»ç»Ÿã€‚</p>
<blockquote>
<p>For the cases so far considered, the expectation value of $T_{ij}$ was 0 for $i\neq j$. A set of memories can be stored with average correlations, and $\bar{T}_{ij} = C_{ij} \neq 0$ because there is a consistent internal correlation in the memories. If now a partial new state $X$ is stored</p>
<p>$$
\Delta T_{ij} = (2X_{i}-1)(2X_{j}-1)\quad i,j\leq k&lt;N
$$</p>
<p>using only $k$ of the neurons rather than $N$, an attempt to reconstruct it will generate a stable point for all $N$ neurons. The values of $X_{k+1}\cdots X_{N}$ that result will be determined primarily from the sign of</p>
<p>$$
\sum_{j=1}^{k}c_{ij}x_{j}
$$</p>
<p>and $X$ is completed according to the mean correlations of the other memories. The most effective implementation of this capacity stores a large number of correlated matrices weakly followed by a normal storage of $X$.</p>
</blockquote>
<p>åˆ°ç›®å‰ä¸ºæ­¢è€ƒè™‘çš„æƒ…å†µä¸­ï¼Œ$i\neq j$ æ—¶ $T_{ij}$ çš„æœŸæœ›å€¼ä¸º 0ã€‚ä¸€ç»„è®°å¿†å¯ä»¥å­˜å‚¨å…·æœ‰å¹³å‡ç›¸å…³æ€§çš„å†…å®¹ï¼Œå¹¶ä¸” $\bar{T}_{ij} = C_{ij} \neq 0$ï¼Œå› ä¸ºè®°å¿†ä¸­å­˜åœ¨ä¸€è‡´çš„å†…éƒ¨ç›¸å…³æ€§ã€‚å¦‚æœç°åœ¨å­˜å‚¨ä¸€ä¸ªéƒ¨åˆ†æ–°çŠ¶æ€ $X$</p>
<p>$$
\Delta T_{ij} = (2X_{i}-1)(2X_{j}-1)\quad i,j\leq k&lt;N
$$</p>
<p>ä»…ä½¿ç”¨ $k$ ä¸ªç¥ç»å…ƒè€Œä¸æ˜¯ $N$ï¼Œå°è¯•é‡æ„å®ƒå°†ä¸ºæ‰€æœ‰ $N$ ä¸ªç¥ç»å…ƒç”Ÿæˆä¸€ä¸ªç¨³å®šç‚¹ã€‚ç»“æœçš„ $X_{k+1}\cdots X_{N}$ çš„å€¼å°†ä¸»è¦ç”±ä»¥ä¸‹çš„ç¬¦å·å†³å®š</p>
<p>$$
\sum_{j=1}^{k}c_{ij}x_{j}
$$</p>
<p>å¹¶ä¸” $X$ æ ¹æ®å…¶ä»–è®°å¿†çš„å¹³å‡ç›¸å…³æ€§è¿›è¡Œè¡¥å…¨ã€‚è¿™ç§å®¹é‡çš„æœ€æœ‰æ•ˆå®ç°æ˜¯å¼±è·Ÿè¸ªå¤§é‡ç›¸å…³çŸ©é˜µï¼Œç„¶åæ­£å¸¸å­˜å‚¨ $X$ã€‚</p>
<blockquote>
<p>A nonsymmetric $T_{ij}$ can lead to the possibility that a minimum  will be only metastable and will be replaced in time by another minimum. Additional nonsymmetric terms which could be easily generated by a minor modification of Hebb synapses</p>
<p>$$
\Delta T_{ij} = A\sum_{s}(2V_{i}^{s+1}-1)(2V_{j}^{s}-1)
$$</p>
<p>were added to $T_{ij}$. When $A$ was judiciously adjusted, the system would spend a while near $V_{s}$ and then leave and go to a point near $V_{s+1}$. But sequences longer than four states proved impossible to generate, and even these were not faithfully followed.</p>
</blockquote>
<p>éå¯¹ç§°çš„ $T_{ij}$ å¯èƒ½å¯¼è‡´ä¸€ä¸ªæå°å€¼ä»…æ˜¯äºšç¨³æ€ï¼Œå¹¶ä¸”éšç€æ—¶é—´çš„æ¨ç§»ä¼šè¢«å¦ä¸€ä¸ªæå°å€¼å–ä»£ã€‚é€šè¿‡å¯¹ Hebb çªè§¦è¿›è¡Œå°å¹…ä¿®æ”¹ï¼Œå¯ä»¥è½»æ¾ç”Ÿæˆçš„é¢å¤–éå¯¹ç§°é¡¹</p>
<p>$$
\Delta T_{ij} = A\sum_{s}(2V_{i}^{s+1}-1)(2V_{j}^{s}-1)
$$</p>
<p>è¢«æ·»åŠ åˆ° $T_{ij}$ ä¸­ã€‚å½“ $A$ è¢«æ˜æ™ºåœ°è°ƒæ•´æ—¶ï¼Œç³»ç»Ÿä¼šåœ¨ $V_{s}$ é™„è¿‘åœç•™ä¸€æ®µæ—¶é—´ï¼Œç„¶åç¦»å¼€å¹¶å‰å¾€ $V_{s+1}$ é™„è¿‘çš„ç‚¹ã€‚ä½†è¯æ˜æ— æ³•ç”Ÿæˆè¶…è¿‡å››ä¸ªçŠ¶æ€çš„åºåˆ—ï¼Œå³ä½¿æ˜¯è¿™äº›çŠ¶æ€ä¹Ÿæ²¡æœ‰è¢«å¿ å®åœ°è·Ÿéšã€‚</p>
<h1 id="discussion">Discussion<a hidden class="anchor" aria-hidden="true" href="#discussion">#</a></h1>
<blockquote>
<p>In the model network each &ldquo;neuron&rdquo; has elementary properties, and the network has little structure. Nonetheless, collective computational properties spontaneously arose. Memories are retained as stable entities or Gestalts and can be correctly recalled from any reasonably sized subpart. Ambiguities are resolved on a statistical basis. Some capacity for generalization is present, and time ordering of memories can also be encoded. These properties follow from the nature of the flow in phase space produced by the processing algorithm, which does not appear to be strongly dependent on precise details of the modeling. This robustness suggests that similar effects will obtain even when more neurobiological details are added.</p>
</blockquote>
<p>åœ¨æ¨¡å‹ç½‘ç»œä¸­ï¼Œæ¯ä¸ª â€œç¥ç»å…ƒâ€ éƒ½å…·æœ‰åŸºæœ¬å±æ€§ï¼Œå¹¶ä¸”ç½‘ç»œå‡ ä¹æ²¡æœ‰ç»“æ„ã€‚å°½ç®¡å¦‚æ­¤ï¼Œé›†ä½“è®¡ç®—å±æ€§è‡ªå‘åœ°å‡ºç°äº†ã€‚è®°å¿†ä½œä¸ºç¨³å®šçš„å®ä½“æˆ–æ ¼å¼å¡”è¢«ä¿ç•™ï¼Œå¹¶ä¸”å¯ä»¥ä»ä»»ä½•åˆç†å¤§å°çš„å­éƒ¨åˆ†æ­£ç¡®å›å¿†å‡ºæ¥ã€‚æ¨¡ç³Šæ€§ä»¥ç»Ÿè®¡ä¸ºåŸºç¡€å¾—åˆ°è§£å†³ã€‚å­˜åœ¨ä¸€å®šçš„æ³›åŒ–èƒ½åŠ›ï¼Œè®°å¿†çš„æ—¶é—´é¡ºåºä¹Ÿå¯ä»¥è¢«ç¼–ç ã€‚è¿™äº›å±æ€§æºäºå¤„ç†ç®—æ³•åœ¨ç›¸ç©ºé—´ä¸­äº§ç”Ÿçš„æµåŠ¨çš„æ€§è´¨ï¼Œè¿™ä¼¼ä¹å¹¶ä¸å¼ºçƒˆä¾èµ–äºå»ºæ¨¡çš„ç²¾ç¡®ç»†èŠ‚ã€‚è¿™ç§é²æ£’æ€§è¡¨æ˜ï¼Œå³ä½¿æ·»åŠ äº†æ›´å¤šç¥ç»ç”Ÿç‰©å­¦ç»†èŠ‚ï¼Œä¹Ÿä¼šè·å¾—ç±»ä¼¼çš„æ•ˆæœã€‚</p>
<blockquote>
<p>Much of the architecture of regions of the brains of higher animals must be made from a proliferation of simple local circuits with well-defined functions. The bridge between simple circuits and the complex computational properties of higher nervous systems may be the spontaneous emergence of new computational capabilities from the collective behavior of large numbers of simple processing elements.</p>
</blockquote>
<p>é«˜ç­‰åŠ¨ç‰©å¤§è„‘åŒºåŸŸçš„å¤§éƒ¨åˆ†ç»“æ„å¿…é¡»ç”±å¤§é‡å…·æœ‰æ˜ç¡®å®šä¹‰åŠŸèƒ½çš„ç®€å•å±€éƒ¨ç”µè·¯ç»„æˆã€‚ç®€å•ç”µè·¯ä¸é«˜çº§ç¥ç»ç³»ç»Ÿå¤æ‚è®¡ç®—å±æ€§ä¹‹é—´çš„æ¡¥æ¢å¯èƒ½æ˜¯å¤§é‡ç®€å•å¤„ç†å•å…ƒçš„é›†ä½“è¡Œä¸ºä¸­è‡ªå‘å‡ºç°çš„æ–°è®¡ç®—èƒ½åŠ›ã€‚</p>
<blockquote>
<p>Implementation of a similar model by using integrated circuits would lead to chips which are much less sensitive to element failure and soft-failure than are normal circuits. Such chips would be wasteful of gates but could be made many times larger than standard designs at a given yield. Their asynchronous parallel processing capability would provide rapid solutions to some special classes of computational problems.</p>
</blockquote>
<p>é€šè¿‡ä½¿ç”¨é›†æˆç”µè·¯å®ç°ç±»ä¼¼çš„æ¨¡å‹å°†å¯¼è‡´èŠ¯ç‰‡å¯¹å…ƒä»¶æ•…éšœå’Œè½¯æ•…éšœçš„æ•æ„Ÿæ€§è¿œä½äºæ™®é€šç”µè·¯ã€‚è¿™äº›èŠ¯ç‰‡åœ¨é—¨ç”µè·¯æ–¹é¢å¯èƒ½æ¯”è¾ƒæµªè´¹ï¼Œä½†åœ¨ç»™å®šçš„äº§é‡ä¸‹å¯ä»¥æ¯”æ ‡å‡†è®¾è®¡å¤§å¾ˆå¤šå€ã€‚å®ƒä»¬çš„å¼‚æ­¥å¹¶è¡Œå¤„ç†èƒ½åŠ›å°†ä¸ºæŸäº›ç‰¹æ®Šç±»åˆ«çš„è®¡ç®—é—®é¢˜æä¾›å¿«é€Ÿè§£å†³æ–¹æ¡ˆã€‚</p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/read/reference/neurons-with-graded-response-have-collective-computational-properties-like-those-of-two-state-neurons/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>Neurons with graded response have collective computational properties like those of two-state neurons</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/read/reference/symmetries-and-continuous-attractors-in-disordered-neural-circuits/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>Symmetries and Continuous Attractors in Disordered Neural Circuits</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  
</body>







<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
