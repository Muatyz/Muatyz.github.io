<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Some History | æ— å¤„æƒ¹å°˜åŸƒ</title>
<meta name="keywords" content="">
<meta name="description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>







<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="Some History" />
<meta property="og:description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-2/" />
<meta property="og:image" content="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-12T00:18:23+08:00" />
<meta property="article:modified_time" content="2025-11-12T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" />
<meta name="twitter:title" content="Some History"/>
<meta name="twitter:description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ“• é˜…è¯»",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "ğŸ“• æ–‡çŒ®",
          "item": "https://Muatyz.github.io/posts/read/reference/"
        },

        {
          "@type": "ListItem",
          "position":  4 ,
          "name": "ğŸ“• Statistical mechanics for networks of real neurons",
          "item": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/"
        }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "Some History",
      "item": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Some History",
  "name": "Some History",
  "description": "çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦",
  "keywords": [
    ""
  ],
  "articleBody": " Today, neural network models are known to many different communities: physicists and applied mathematicians, computer scientists and engineers, neurobiologists and cognitive scientists. Neural networks are at the heart of an ongoing revolution in artificial intelligence, and are making their way into many aspects of scientific data analysis, from cell biology to CERN. Here we provide a brief (and perhaps idiosyncratic) reminder of how some of these ideas developed.\nä»Šå¤©ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹ä¸ºè®¸å¤šä¸åŒçš„ç¾¤ä½“æ‰€çŸ¥ï¼šç‰©ç†å­¦å®¶å’Œåº”ç”¨æ•°å­¦å®¶ã€è®¡ç®—æœºç§‘å­¦å®¶å’Œå·¥ç¨‹å¸ˆã€ç¥ç»ç”Ÿç‰©å­¦å®¶å’Œè®¤çŸ¥ç§‘å­¦å®¶ã€‚ç¥ç»ç½‘ç»œæ˜¯äººå·¥æ™ºèƒ½æ­£åœ¨è¿›è¡Œçš„é©å‘½çš„æ ¸å¿ƒï¼Œå¹¶æ­£åœ¨è¿›å…¥ä»ç»†èƒç”Ÿç‰©å­¦åˆ° CERN çš„è®¸å¤šç§‘å­¦æ•°æ®åˆ†ææ–¹é¢ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç®€è¦ï¼ˆä¹Ÿè®¸æ˜¯ç‰¹ç«‹ç‹¬è¡Œåœ°ï¼‰å›é¡¾äº†ä¸€äº›è¿™äº›æƒ³æ³•æ˜¯å¦‚ä½•å‘å±•çš„ã€‚\nPrehistoric times The engagement of physicists with neurons and the brain has a long and fascinating history. Our modern understanding of electricity has its roots in the 1700s with observations on nerves and muscles. The understanding of optics and acoustics that emerged in the 1800s was continuous with the exploration of vision and hearing. This involved thinking not just about the optics of the eye or the mechanics of the inner ear, but about the inferences that our brains can derive from the data collected by these physical instruments.\nç‰©ç†å­¦å®¶ä¸ç¥ç»å…ƒå’Œå¤§è„‘çš„æ¥è§¦æœ‰ç€æ‚ ä¹…è€Œè¿·äººçš„å†å²ã€‚æˆ‘ä»¬å¯¹ç”µçš„ç°ä»£ç†è§£èµ·æºäº 1700 å¹´ä»£å¯¹ç¥ç»å’Œè‚Œè‚‰çš„è§‚å¯Ÿã€‚19 ä¸–çºªå‡ºç°çš„å…‰å­¦å’Œå£°å­¦çš„ç†è§£ä¸å¯¹è§†è§‰å’Œå¬è§‰çš„æ¢ç´¢æ˜¯è¿ç»­çš„ã€‚è¿™ä¸ä»…æ¶‰åŠçœ¼ç›çš„å…‰å­¦æˆ–å†…è€³çš„åŠ›å­¦ï¼Œè¿˜æ¶‰åŠæˆ‘ä»¬çš„å¤§è„‘å¯ä»¥ä»è¿™äº›ç‰©ç†ä»ªå™¨æ”¶é›†çš„æ•°æ®ä¸­æ¨æ–­å‡ºçš„æ¨è®ºã€‚\nThe idea that the brain is made out of discrete cells, connected by synapses, dates from late 1800s (Ram Ìon y Cajal, 1894). The electrical signals from individual nerve cells (neurons) were first recorded in the 1920s, starting with the cells in sense organs that provide the input to the brain (Adrian, 1928). Observing these small signals required instruments no less sensitive than those in contemporary physics laboratories. The crucial observation is that neurons communicate by generating discrete, identical pulses of voltage across their membranes; these pulses are called action potentials or, more colloquially, spikes.\nå¤§è„‘ç”±ç¦»æ•£çš„ç»†èƒé€šè¿‡çªè§¦è¿æ¥è€Œæˆçš„æƒ³æ³•å¯ä»¥è¿½æº¯åˆ° 19 ä¸–çºªæœ«ï¼ˆRam Ìon y Cajalï¼Œ1894 å¹´ï¼‰ã€‚ä¸ªåˆ«ç¥ç»ç»†èƒï¼ˆç¥ç»å…ƒï¼‰çš„ç”µä¿¡å·æœ€æ—©åœ¨ 1920 å¹´ä»£è¢«è®°å½•ä¸‹æ¥ï¼Œé¦–å…ˆæ˜¯æä¾›å¤§è„‘è¾“å…¥çš„æ„Ÿè§‰å™¨å®˜ä¸­çš„ç»†èƒï¼ˆAdrianï¼Œ1928 å¹´ï¼‰ã€‚è§‚å¯Ÿè¿™äº›å¾®å°ä¿¡å·éœ€è¦ä¸å½“ä»£ç‰©ç†å®éªŒå®¤ä¸­çš„ä»ªå™¨åŒæ ·æ•æ„Ÿã€‚å…³é”®çš„è§‚å¯Ÿæ˜¯ï¼Œç¥ç»å…ƒé€šè¿‡åœ¨å…¶è†œä¸Šäº§ç”Ÿç¦»æ•£çš„ã€ç›¸åŒçš„ç”µå‹è„‰å†²æ¥è¿›è¡Œé€šä¿¡ï¼›è¿™äº›è„‰å†²ç§°ä¸ºåŠ¨ä½œç”µä½ï¼Œæˆ–è€…æ›´é€šä¿—åœ°è¯´ï¼Œç§°ä¸ºå°–å³°ã€‚\nBy the 1950s there was a clear mathematical description of the dynamics underlying the generation and propagation of spikes (Hodgkin and Huxley, 1952). Perhaps surprisingly, the terms in these equations could be taken literally as representing the action of real physical componentsâ€”ion channel proteins that allow the flow of specific ions across the cell membrane, and which open and close (or â€œgateâ€) in response to the transmembrane voltage. The progress from macroscopic phenomenology to the dynamics of individual channels is a beautiful chapter in the interaction of physics and biology. The classic textbook account is Aidley (1998); Dayan and Abbott (2001) discuss phenomenological models for spiking activity; and a broader biological context is provided by Kandel et al. (2012). Rieke et al. (1997) describe the way in which sequences of spikes represent information about the sensory world, and Bialek (2012) connects channels and spikes to other problems in the physics of biological systems.\nåˆ°äº† 1950 å¹´ä»£ï¼Œäººä»¬å·²ç»å¯¹äº§ç”Ÿå’Œä¼ æ’­å°–å³°çš„åŠ¨åŠ›å­¦æœ‰äº†æ¸…æ™°çš„æ•°å­¦æè¿°ï¼ˆHodgkin å’Œ Huxleyï¼Œ1952 å¹´ï¼‰ã€‚ä¹Ÿè®¸ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™äº›æ–¹ç¨‹ä¸­çš„æœ¯è¯­å¯ä»¥è¢«å­—é¢ç†è§£ä¸ºä»£è¡¨çœŸå®ç‰©ç†ç»„ä»¶çš„ä½œç”¨â€”â€”å…è®¸ç‰¹å®šç¦»å­ç©¿è¿‡ç»†èƒè†œæµåŠ¨çš„ç¦»å­é€šé“è›‹ç™½ï¼Œè¿™äº›è›‹ç™½ä¼šæ ¹æ®è·¨è†œç”µå‹çš„å˜åŒ–è€Œæ‰“å¼€å’Œå…³é—­ï¼ˆæˆ–â€œé—¨æ§â€ï¼‰ã€‚ä»å®è§‚ç°è±¡å­¦åˆ°å•ä¸ªé€šé“åŠ¨åŠ›å­¦çš„è¿›å±•æ˜¯ç‰©ç†å­¦ä¸ç”Ÿç‰©å­¦ç›¸äº’ä½œç”¨ä¸­çš„ä¸€ä¸ªç¾ä¸½ç¯‡ç« ã€‚ç»å…¸çš„æ•™ç§‘ä¹¦æ˜¯ Aidleyï¼ˆ1998ï¼‰ï¼›Dayan å’Œ Abbottï¼ˆ2001ï¼‰è®¨è®ºäº†å°–å³°æ´»åŠ¨çš„ç°è±¡å­¦æ¨¡å‹ï¼›Kandel ç­‰äººï¼ˆ2012ï¼‰æä¾›äº†æ›´å¹¿æ³›çš„ç”Ÿç‰©å­¦èƒŒæ™¯ã€‚Rieke ç­‰äººï¼ˆ1997ï¼‰æè¿°äº†å°–å³°åºåˆ—å¦‚ä½•è¡¨ç¤ºæœ‰å…³æ„Ÿå®˜ä¸–ç•Œçš„ä¿¡æ¯ï¼ŒBialekï¼ˆ2012ï¼‰å°†é€šé“å’Œå°–å³°ä¸ç”Ÿç‰©ç³»ç»Ÿç‰©ç†å­¦ä¸­çš„å…¶ä»–é—®é¢˜è”ç³»èµ·æ¥ã€‚\nEven before the mechanisms were clear, people began to think about how the quasiâ€“digital character of spiking could be harnessed to do computations (McCulloch and Pitts, 1943). This work comes after the foundational work of Turing (1937) on universal computation, but before any practical modern computers. The goal of this work was to show that the basic facts known about neurons were sufficient to support computing essentially anything. On the one hand this is a very positive theoretical development: the brain could be a computer, in a deep sense. On the other hand it is disappointing, since if the brain is a universal computer there is not much more that one can say about the dynamics..\nå³ä½¿åœ¨æœºåˆ¶å°šä¸æ¸…æ¥šä¹‹å‰ï¼Œäººä»¬å°±å¼€å§‹æ€è€ƒå¦‚ä½•åˆ©ç”¨å°–å³°çš„å‡†æ•°å­—ç‰¹æ€§æ¥è¿›è¡Œè®¡ç®—ï¼ˆMcCulloch å’Œ Pittsï¼Œ1943 å¹´ï¼‰ã€‚è¿™é¡¹å·¥ä½œæ˜¯åœ¨ Turingï¼ˆ1937 å¹´ï¼‰å…³äºé€šç”¨è®¡ç®—çš„åŸºç¡€æ€§å·¥ä½œä¹‹åï¼Œä½†åœ¨ä»»ä½•å®ç”¨çš„ç°ä»£è®¡ç®—æœºä¹‹å‰ã€‚è¿™é¡¹å·¥ä½œçš„ç›®æ ‡æ˜¯è¡¨æ˜ï¼Œå·²çŸ¥çš„å…³äºç¥ç»å…ƒçš„åŸºæœ¬äº‹å®è¶³ä»¥æ”¯æŒè®¡ç®—æœ¬è´¨ä¸Šçš„ä»»ä½•ä¸œè¥¿ã€‚ä¸€æ–¹é¢ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ç§¯æçš„ç†è®ºå‘å±•ï¼šå¤§è„‘å¯ä»¥åœ¨æ·±å±‚æ„ä¹‰ä¸Šæˆä¸ºä¸€å°è®¡ç®—æœºã€‚å¦ä¸€æ–¹é¢ï¼Œè¿™ä»¤äººå¤±æœ›ï¼Œå› ä¸ºå¦‚æœå¤§è„‘æ˜¯ä¸€å°é€šç”¨è®¡ç®—æœºï¼Œé‚£ä¹ˆå…³äºåŠ¨åŠ›å­¦å°±æ²¡æœ‰å¤ªå¤šå¯è¯´çš„äº†ã€‚\nThe way in which computation emerges from neurons in this early work clearly involves interactions among large numbers of cells in a network. Although single neurons can have remarkably precise dynamics in relation to sensory inputs and motor outputs (Hires et al., 2015; Nemenman et al., 2008; Rieke et al., 1997; Srivastava et al., 2017), there are many indications that our perceptions and actions, thoughts and memories typically are connected to the activity in many hundreds, perhaps even hundreds of thousands of neurons. Relevant activity in these large networks must be coordinated or collective.\nåœ¨è¿™é¡¹æ—©æœŸå·¥ä½œä¸­ï¼Œè®¡ç®—æ˜¯å¦‚ä½•ä»ç¥ç»å…ƒä¸­æ¶Œç°å‡ºæ¥çš„ï¼Œæ˜¾ç„¶æ¶‰åŠç½‘ç»œä¸­å¤§é‡ç»†èƒä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚å°½ç®¡å•ä¸ªç¥ç»å…ƒåœ¨æ„Ÿå®˜è¾“å…¥å’Œè¿åŠ¨è¾“å‡ºæ–¹é¢å¯ä»¥å…·æœ‰éå¸¸ç²¾ç¡®çš„åŠ¨åŠ›å­¦ï¼ˆHires ç­‰äººï¼Œ2015 å¹´ï¼›Nemenman ç­‰äººï¼Œ2008 å¹´ï¼›Rieke ç­‰äººï¼Œ1997 å¹´ï¼›Srivastava ç­‰äººï¼Œ2017 å¹´ï¼‰ï¼Œä½†æœ‰è®¸å¤šè¿¹è±¡è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ„ŸçŸ¥å’Œè¡ŒåŠ¨ã€æ€æƒ³å’Œè®°å¿†é€šå¸¸ä¸æ•°ç™¾ç”šè‡³æ•°åä¸‡ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨æœ‰å…³ã€‚è¿™äº›å¤§å‹ç½‘ç»œä¸­çš„ç›¸å…³æ´»åŠ¨å¿…é¡»æ˜¯åè°ƒæˆ–é›†ä½“çš„ã€‚\nThe idea that collective neural activity in the brain might be described with statistical mechanics was very much influenced by observations on the electroencephalogram or EEG (Wiener, 1958). The EEG is a macroscopic measure of activity, traditionally done simply by placing electrodes on the scalp, and the existence of the EEG is prima facie evidence that the electrical activity of many, many neurons must be correlated. There is also the remarkable story of a demonstration by Adrian, in which he sat quietly with his eyes closed with electrodes attached to his head. The signals, sent to an oscilloscope, showed the characteristic â€œalpha rhythmâ€ that occurs in resting states, roughly an oscillation at âˆ¼ 10 Hz. When asked to add two numbers in his head, the rhythm disappeared, replaced by less easily described patterns of activity (Adrian and Matthews, 1934). This should dispel any lingering doubts that your mental life is related to the electrical activity of your brain.\nå¤§è„‘ä¸­é›†ä½“ç¥ç»æ´»åŠ¨å¯èƒ½ç”¨ç»Ÿè®¡åŠ›å­¦æ¥æè¿°çš„æƒ³æ³•ï¼Œæ·±å—å¯¹è„‘ç”µå›¾æˆ– EEG è§‚å¯Ÿçš„å½±å“ï¼ˆWienerï¼Œ1958 å¹´ï¼‰ã€‚EEG æ˜¯ä¸€ç§å®è§‚çš„æ´»åŠ¨æµ‹é‡ï¼Œä¼ ç»Ÿä¸Šåªæ˜¯é€šè¿‡åœ¨å¤´çš®ä¸Šæ”¾ç½®ç”µææ¥å®Œæˆï¼ŒEEG çš„å­˜åœ¨æ˜¯ç¬¬ä¸€æ‰‹è¯æ®ï¼Œè¡¨æ˜è®¸å¤šç¥ç»å…ƒçš„ç”µæ´»åŠ¨å¿…é¡»æ˜¯ç›¸å…³çš„ã€‚è¿˜æœ‰ä¸€ä¸ªä»¤äººæƒŠè®¶çš„æ•…äº‹ï¼Œè®²è¿°äº† Adrian çš„ä¸€ä¸ªæ¼”ç¤ºï¼Œä»–å®‰é™åœ°åç€ï¼Œé—­ç€çœ¼ç›ï¼Œå¤´ä¸Šè¿æ¥ç€ç”µæã€‚ä¿¡å·è¢«å‘é€åˆ°ç¤ºæ³¢å™¨ä¸Šï¼Œæ˜¾ç¤ºå‡ºåœ¨é™æ¯çŠ¶æ€ä¸‹å‘ç”Ÿçš„ç‰¹å¾æ€§â€œÎ±èŠ‚å¾‹â€ï¼Œå¤§çº¦æ˜¯ âˆ¼ 10 Hz çš„æŒ¯è¡ã€‚å½“è¢«è¦æ±‚åœ¨è„‘æµ·ä¸­åŠ ä¸¤ä¸ªæ•°å­—æ—¶ï¼Œè¿™ç§èŠ‚å¾‹æ¶ˆå¤±äº†ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ä¸é‚£ä¹ˆå®¹æ˜“æè¿°çš„æ´»åŠ¨æ¨¡å¼ï¼ˆAdrian å’Œ Matthewsï¼Œ1934 å¹´ï¼‰ã€‚è¿™åº”è¯¥æ¶ˆé™¤ä»»ä½•å…³äºä½ å¯¹å¿ƒæ™ºä¸å¤§è„‘ç”µæ´»åŠ¨ç›¸å…³çš„è´¨ç–‘ã€‚\nIn the simplest models for neural dynamics, we describe the state of each neuron i at time t by a binary or Ising variable $\\sigma_{i}(t)$; $\\sigma_{i}(t) = +1$ means that the neuron is active, and $\\sigma_{i}(t) = 0$ means that the neuron is silent.2 We imagine the dynamics proceeding in discrete time steps $\\Delta\\tau$ . Each neuron sums inputs from other neurons, weighted by the strength $J_{ij}$ of the synapse or connection from cell $j\\rightarrow i$, and neurons switch into the active state if the total input is above a threshold:\n$$ \\sigma_{i}(t+\\Delta\\tau) = \\Theta\\left[\\sum_{j}J_{ij}\\sigma_{j}(t)-\\theta_{i}\\right] $$\nThe nature of the dynamics is encoded in the matrix $J_{ij}$ of synaptic strengths. If we think about arbitrary matrices, then the dynamics can be arbitrarily complex; progress depends on simplifying assumptions. It is useful to organize our discussion around two extreme simplifications. But keep in mind as we follow these threads that many of the developments occurred in parallel, and that there was considerable crosstalk.\nåœ¨ç¥ç»åŠ¨åŠ›å­¦çš„æœ€ç®€å•æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡äºŒè¿›åˆ¶æˆ– Ising å˜é‡ $\\sigma_{i}(t)$ æ¥æè¿°æ¯ä¸ªç¥ç»å…ƒ i åœ¨æ—¶é—´ t çš„çŠ¶æ€ï¼›$\\sigma_{i}(t) = +1$ æ„å‘³ç€ç¥ç»å…ƒå¤„äºæ´»è·ƒçŠ¶æ€ï¼Œ$\\sigma_{i}(t) = 0$ æ„å‘³ç€ç¥ç»å…ƒå¤„äºé™é»˜çŠ¶æ€ã€‚æˆ‘ä»¬æƒ³è±¡åŠ¨åŠ›å­¦ä»¥ç¦»æ•£æ—¶é—´æ­¥é•¿ $\\Delta\\tau$ è¿›è¡Œã€‚æ¯ä¸ªç¥ç»å…ƒå¯¹æ¥è‡ªå…¶ä»–ç¥ç»å…ƒçš„è¾“å…¥è¿›è¡Œæ±‚å’Œï¼Œè¿™äº›è¾“å…¥ç”±ä»ç»†èƒ $j\\rightarrow i$ çš„çªè§¦æˆ–è¿æ¥çš„å¼ºåº¦ $J_{ij}$ åŠ æƒï¼Œå¦‚æœæ€»è¾“å…¥è¶…è¿‡é˜ˆå€¼ï¼Œç¥ç»å…ƒå°±ä¼šåˆ‡æ¢åˆ°æ´»è·ƒçŠ¶æ€ï¼š\n$$ \\sigma_{i}(t+\\Delta\\tau) = \\Theta\\left[\\sum_{j}J_{ij}\\sigma_{j}(t)-\\theta_{i}\\right] $$\nåŠ¨åŠ›å­¦çš„æ€§è´¨ç¼–ç åœ¨çªè§¦å¼ºåº¦çŸ©é˜µ $J_{ij}$ ä¸­ã€‚å¦‚æœæˆ‘ä»¬è€ƒè™‘ä»»æ„çŸ©é˜µï¼Œé‚£ä¹ˆåŠ¨åŠ›å­¦å¯ä»¥æ˜¯ä»»æ„å¤æ‚çš„ï¼›è¿›å±•å–å†³äºç®€åŒ–å‡è®¾ã€‚å›´ç»•ä¸¤ä¸ªæç«¯ç®€åŒ–æ¥ç»„ç»‡æˆ‘ä»¬çš„è®¨è®ºæ˜¯æœ‰ç”¨çš„ã€‚ä½†è¯·è®°ä½ï¼Œåœ¨æˆ‘ä»¬è·Ÿéšè¿™äº›çº¿ç´¢æ—¶ï¼Œè®¸å¤šå‘å±•æ˜¯å¹¶è¡Œå‘ç”Ÿçš„ï¼Œå¹¶ä¸”å­˜åœ¨ç›¸å½“å¤§çš„ä¸²æ‰°ã€‚\nFrom perceptrons to deep networks One popular simplification is to assume that $J_{ij}$ has a feedâ€“forward, layered structure. This is the â€œperceptronâ€ architecture (Block, 1962; Block et al., 1962; Rosenblatt, 1961), illustrated in Fig 1A, which is simpler to analyze precisely because there are no feedback loops. It is convenient to label the neurons also by the layer $l$ in which they reside, and to generalize from binary variables to continuous ones, so that\n$$ x_{i}^{(l+1)} = g\\left[\\sum_{j}W_{ij}^{(l+1)}x_{j}^{(l)} - \\theta_{i}^{(l+1)}\\right] $$\nwhere the propagation through layers replaces propagation through time and $g[\\cdot]$ is a monotonic nonlinear function. Thus each neuron computes a single projection of its possible inputs from the previous layer, and then outputs a nonlinear function of this projection.\nä¸€ç§æµè¡Œçš„ç®€åŒ–æ˜¯å‡è®¾ $J_{ij}$ å…·æœ‰å‰é¦ˆçš„åˆ†å±‚ç»“æ„ã€‚è¿™æ˜¯â€œæ„ŸçŸ¥å™¨â€æ¶æ„ï¼ˆBlockï¼Œ1962 å¹´ï¼›Block ç­‰äººï¼Œ1962 å¹´ï¼›Rosenblattï¼Œ1961 å¹´ï¼‰ï¼Œå¦‚å›¾ 1A æ‰€ç¤ºï¼Œæ­£å› ä¸ºæ²¡æœ‰åé¦ˆå›è·¯ï¼Œæ‰€ä»¥æ›´å®¹æ˜“åˆ†æã€‚æ–¹ä¾¿çš„æ˜¯ï¼Œè¿˜å¯ä»¥æŒ‰å®ƒä»¬æ‰€åœ¨çš„å±‚ $l$ æ¥æ ‡è®°ç¥ç»å…ƒï¼Œå¹¶å°†äºŒè¿›åˆ¶å˜é‡æ¨å¹¿ä¸ºè¿ç»­å˜é‡ï¼Œå› æ­¤\n$$ x_{i}^{(l+1)} = g\\left[\\sum_{j}W_{ij}^{(l+1)}x_{j}^{(l)} - \\theta_{i}^{(l+1)}\\right] $$\nå…¶ä¸­é€šè¿‡å±‚çš„ä¼ æ’­å–ä»£äº†é€šè¿‡æ—¶é—´çš„ä¼ æ’­ï¼Œ$g[\\cdot]$ æ˜¯ä¸€ä¸ªå•è°ƒéçº¿æ€§å‡½æ•°ã€‚å› æ­¤ï¼Œæ¯ä¸ªç¥ç»å…ƒè®¡ç®—æ¥è‡ªå‰ä¸€å±‚çš„å¯èƒ½è¾“å…¥çš„å•ä¸ªæŠ•å½±ï¼Œç„¶åè¾“å‡ºè¯¥æŠ•å½±çš„éçº¿æ€§å‡½æ•°ã€‚\nIn the limit that $g[\\cdot]$ becomes a step function we recover binary variables and neuron $i$ in layer $l + 1$, can be thought of a dividing the space of its inputs in half, with a hyperplane perpendicular to the vector\n$$ \\vec{V} = \\{V_{j}\\} = \\{W_{ij}^{(l+1)}\\} $$\nThus the elementary computation is a binary classification of inputs,\n$$ x\\rightarrow y = \\Theta(\\vec{V}\\cdot\\vec{x}-\\theta) $$\nWe could imagine having access to many examples of the input vector $\\vec{x}$ labelled by the correct classification $y$, and thereby learning the optimal vector $\\vec{V}$ . This picture of learning to classify was present already âˆ¼1960, although it would take the full power of modern statistical physics to say that we really understand it. Crucially, if we think of the the $\\{x_{i}\\}$ or $\\{\\sigma_{i}\\}$ as being the microscopic variables in the system and the $J_{ij}$ as being the interactions among these variables, then learning is statistical mechanics in the space of interactions (Gardner, 1988; Gardner and Derrida, 1988; Levin et al., 1990; Watkin et al., 1993).\nåœ¨ $g[\\cdot]$ å˜æˆé˜¶è·ƒå‡½æ•°çš„æé™ä¸‹ï¼Œæˆ‘ä»¬æ¢å¤äº†äºŒè¿›åˆ¶å˜é‡ï¼Œå±‚ $l + 1$ ä¸­çš„ç¥ç»å…ƒ $i$ å¯ä»¥è¢«è®¤ä¸ºæ˜¯å°†å…¶è¾“å…¥ç©ºé—´åˆ†æˆä¸¤åŠï¼Œå…·æœ‰å‚ç›´äºå‘é‡çš„è¶…å¹³é¢\n$$ \\vec{V} = \\{V_{j}\\} = \\{W_{ij}^{(l+1)}\\} $$\nå› æ­¤ï¼ŒåŸºæœ¬çš„è®¡ç®—æ˜¯å¯¹è¾“å…¥çš„äºŒå…ƒåˆ†ç±»ï¼Œ\n$$ x\\rightarrow y = \\Theta(\\vec{V}\\cdot\\vec{x}-\\theta) $$\næˆ‘ä»¬å¯ä»¥æƒ³è±¡è·å¾—è®¸å¤šè¾“å…¥å‘é‡ $\\vec{x}$ çš„ç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹ç”±æ­£ç¡®çš„åˆ†ç±» $y$ æ ‡è®°ï¼Œä»è€Œå­¦ä¹ æœ€ä½³å‘é‡ $\\vec{V}$ã€‚è¿™ç§å­¦ä¹ åˆ†ç±»çš„å›¾æ™¯å·²ç»å‡ºç°åœ¨å¤§çº¦ 1960 å¹´ï¼Œå°½ç®¡éœ€è¦ç°ä»£ç»Ÿè®¡ç‰©ç†å­¦çš„å…¨éƒ¨åŠ›é‡æ‰èƒ½è¯´æˆ‘ä»¬çœŸæ­£ç†è§£äº†å®ƒã€‚å…³é”®æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å°† $\\{x_{i}\\}$ æˆ– $\\{\\sigma_{i}\\}$ è§†ä¸ºç³»ç»Ÿä¸­çš„å¾®è§‚å˜é‡ï¼Œè€Œå°† $J_{ij}$ è§†ä¸ºè¿™äº›å˜é‡ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œé‚£ä¹ˆå­¦ä¹ å°±æ˜¯åœ¨ç›¸äº’ä½œç”¨ç©ºé—´ä¸­çš„ç»Ÿè®¡åŠ›å­¦ï¼ˆGardnerï¼Œ1988 å¹´ï¼›Gardner å’Œ Derridaï¼Œ1988 å¹´ï¼›Levin ç­‰äººï¼Œ1990 å¹´ï¼›Watkin ç­‰äººï¼Œ1993 å¹´ï¼‰ã€‚\nAlthough many of the computations done by the brain can be framed as classification problems, such as attaching names or words to images, very few can be solved by a single step of linear separation. Again this was clear at the start, but development of these ideas took decades. Enthusiasm was dampened by an emphasis on what two layer networks could not do (Minsky and Papert, 1969), but eventually it became clear that multilayer perceptrons are much more powerful (Lapedes and Farber, 1988; LeCun, 1987), and theorems were proven to show that these systems can approximate any function (Hornik et al., 1989). As with the simple perceptron, optimal weights $W$ can be learned by fitting to many examples of input/output pairs. Importantly this doesnâ€™t require access to the â€œcorrectâ€ answers at every layer; instead if we work with continuous variables then the goodness of fit across many layers can be differentiated using the chain rule, and errors propagated back through the network to adjust the weights (Rumelhart et al., 1986).\nè™½ç„¶å¤§è„‘æ‰§è¡Œçš„è®¸å¤šè®¡ç®—éƒ½å¯ä»¥è¢«æ¡†å®šä¸ºåˆ†ç±»é—®é¢˜ï¼Œä¾‹å¦‚å°†åç§°æˆ–å•è¯é™„åŠ åˆ°å›¾åƒä¸Šï¼Œä½†å¾ˆå°‘æœ‰è®¡ç®—å¯ä»¥é€šè¿‡å•ä¸€æ­¥éª¤çš„çº¿æ€§åˆ†ç¦»æ¥è§£å†³ã€‚è¿™ä¸€äº‹å®ä»ä¸€å¼€å§‹å°±å¾ˆæ¸…æ¥šï¼Œä½†è¿™äº›æƒ³æ³•çš„å‘å±•èŠ±è´¹äº†å‡ åå¹´æ—¶é—´ã€‚ç”±äºå¼ºè°ƒä¸¤å±‚ç½‘ç»œæ— æ³•å®Œæˆçš„ä»»åŠ¡ï¼ˆMinsky å’Œ Papertï¼Œ1969 å¹´ï¼‰ï¼Œçƒ­æƒ…æœ‰æ‰€å‡å¼±ï¼Œä½†æœ€ç»ˆäººä»¬æ„è¯†åˆ°å¤šå±‚æ„ŸçŸ¥å™¨æ›´åŠ å¼ºå¤§ï¼ˆLapedes å’Œ Farberï¼Œ1988 å¹´ï¼›LeCunï¼Œ1987 å¹´ï¼‰ï¼Œå¹¶ä¸”å·²ç»è¯æ˜äº†è¿™äº›ç³»ç»Ÿå¯ä»¥è¿‘ä¼¼ä»»ä½•å‡½æ•°çš„å®šç†ï¼ˆHornik ç­‰äººï¼Œ1989 å¹´ï¼‰ã€‚ä¸ç®€å•æ„ŸçŸ¥å™¨ä¸€æ ·ï¼Œå¯ä»¥é€šè¿‡æ‹Ÿåˆè®¸å¤šè¾“å…¥/è¾“å‡ºå¯¹çš„ç¤ºä¾‹æ¥å­¦ä¹ æœ€ä½³æƒé‡ $W$ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ä¸éœ€è¦è®¿é—®æ¯ä¸€å±‚çš„â€œæ­£ç¡®â€ç­”æ¡ˆï¼›ç›¸åï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨è¿ç»­å˜é‡ï¼Œé‚£ä¹ˆé€šè¿‡é“¾å¼æ³•åˆ™å¯ä»¥åŒºåˆ†å¤šå±‚ä¹‹é—´çš„æ‹Ÿåˆä¼˜åº¦ï¼Œå¹¶å°†è¯¯å·®åå‘ä¼ æ’­é€šè¿‡ç½‘ç»œä»¥è°ƒæ•´æƒé‡ï¼ˆRumelhart ç­‰äººï¼Œ1986 å¹´ï¼‰ã€‚\nFast forward from the late 1980s to the mid 2010s. The few layers of early perceptrons became the many layers of â€œdeep networks,â€ in the spirit of Fig 1B; comparing the two panels of Fig 1 emphasizes the continuity of ideas across the decades. Advances in computing power and storage made it possible not just to simulate these models efficiently, but to solve the problem of finding optimal synaptic weights by comparing against millions or even billions of examples. These explorations led to networks so large that the number of weights needed to specify the network vastly exceeded the number of examples. Contrary to well established intuitions these â€œover parameterizedâ€ models worked, generalizing to new examples rather than overâ€“fitting to the training data. Although we donâ€™t fully understand them, these developments have fueled a revolution in artificial intelligence (AI).\nä» 1980 å¹´ä»£åæœŸå¿«è¿›åˆ° 2010 å¹´ä»£ä¸­æœŸã€‚æ—©æœŸæ„ŸçŸ¥å™¨çš„å‡ å±‚å˜æˆäº†â€œæ·±åº¦ç½‘ç»œâ€çš„å¤šå±‚ï¼Œå¦‚å›¾ 1B æ‰€ç¤ºï¼›æ¯”è¾ƒå›¾ 1 çš„ä¸¤ä¸ªé¢æ¿å¼ºè°ƒäº†å‡ åå¹´æ¥æ€æƒ³çš„è¿ç»­æ€§ã€‚è®¡ç®—èƒ½åŠ›å’Œå­˜å‚¨çš„è¿›æ­¥ä¸ä»…ä½¿å¾—é«˜æ•ˆåœ°æ¨¡æ‹Ÿè¿™äº›æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œè€Œä¸”é€šè¿‡ä¸æ•°ç™¾ä¸‡ç”šè‡³æ•°åäº¿ä¸ªç¤ºä¾‹è¿›è¡Œæ¯”è¾ƒï¼Œè§£å†³äº†å¯»æ‰¾æœ€ä½³çªè§¦æƒé‡çš„é—®é¢˜ã€‚è¿™äº›æ¢ç´¢å¯¼è‡´äº†å¦‚æ­¤åºå¤§çš„ç½‘ç»œï¼Œä»¥è‡³äºæŒ‡å®šç½‘ç»œæ‰€éœ€çš„æƒé‡æ•°é‡è¿œè¿œè¶…è¿‡äº†ç¤ºä¾‹çš„æ•°é‡ã€‚ä¸æ—¢å®šçš„ç›´è§‰ç›¸åï¼Œè¿™äº›â€œè¿‡å‚æ•°åŒ–â€æ¨¡å‹æœ‰æ•ˆï¼Œå¯¹æ–°ç¤ºä¾‹è¿›è¡Œäº†æ³›åŒ–ï¼Œè€Œä¸æ˜¯å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œäº†è¿‡æ‹Ÿåˆã€‚å°½ç®¡æˆ‘ä»¬å¹¶ä¸å®Œå…¨ç†è§£å®ƒä»¬ï¼Œä½†è¿™äº›å‘å±•æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„é©å‘½ã€‚\nFIG. 1 Neural networks with a feed-forward architecture, or â€œperceptrons.â€ (A) An early version of the idea, from Block (1962). (B) A modern version, with additional hidden layers. The first steps in the modern AI revolution involved similar networks, with many hidden layers, that achieved human-level performance on image classification and other tasks (LeCun et al., 2015).\nå›¾ä¸€. ç¥ç»ç½‘ç»œå…·æœ‰å‰é¦ˆæ¶æ„ï¼Œæˆ–â€œæ„ŸçŸ¥å™¨â€ã€‚ï¼ˆAï¼‰è¯¥æƒ³æ³•çš„æ—©æœŸç‰ˆæœ¬ï¼Œæ¥è‡ª Blockï¼ˆ1962 å¹´ï¼‰ã€‚ï¼ˆBï¼‰ç°ä»£ç‰ˆæœ¬ï¼Œå…·æœ‰é¢å¤–çš„éšè—å±‚ã€‚ç°ä»£ AI é©å‘½çš„ç¬¬ä¸€æ­¥æ¶‰åŠç±»ä¼¼çš„ç½‘ç»œï¼Œå…·æœ‰è®¸å¤šéšè—å±‚ï¼Œåœ¨å›¾åƒåˆ†ç±»å’Œå…¶ä»–ä»»åŠ¡ä¸Šå®ç°äº†äººç±»æ°´å¹³çš„æ€§èƒ½ï¼ˆLeCun ç­‰äººï¼Œ2015 å¹´ï¼‰ã€‚\nSymmetric networks Feedâ€“forward networks have the property that if $J_{ij}$ is nonzero, then $J_{ji} = 0$. Hopfield (1982, 1984) considered the opposite simplification: if neuron $i$ is connected to neuron $j$, then neuron $j$ is connected to neuron $i$, and the strength of the connection is the same, so that $J_{ij} = J_{ji}$. In this case the dynamics in Eq (1) have a Lyapunov function: at each time step the â€œenergyâ€\n$$ E = -\\frac{1}{2}\\sum_{ij}\\sigma_{i}J_{ij}\\sigma_{j} + \\sum_{i}\\theta_{i}\\sigma_{i} $$\neither decreases or stays constant. The evolution of the network state stops at local minima of the energy $E$, and only at these local minima. We recognize this energy function as an Ising model with pairwise interactions among the spins (neurons). This very explicit connection of neural dynamics to statistical physics triggered an avalanche of work, and textbook accounts of these ideas appeared quickly (Amit, 1989; Hertz et al., 1991).\nå‰é¦ˆç½‘ç»œå…·æœ‰è¿™æ ·çš„å±æ€§ï¼šå¦‚æœ $J_{ij}$ éé›¶ï¼Œåˆ™ $J_{ji} = 0$ã€‚Hopfieldï¼ˆ1982 å¹´ï¼Œ1984 å¹´ï¼‰è€ƒè™‘äº†ç›¸åçš„ç®€åŒ–ï¼šå¦‚æœç¥ç»å…ƒ $i$ ä¸ç¥ç»å…ƒ $j$ ç›¸è¿ï¼Œé‚£ä¹ˆç¥ç»å…ƒ $j$ ä¹Ÿä¸ç¥ç»å…ƒ $i$ ç›¸è¿ï¼Œå¹¶ä¸”è¿æ¥çš„å¼ºåº¦ç›¸åŒï¼Œå› æ­¤ $J_{ij} = J_{ji}$ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ–¹ç¨‹ï¼ˆ1ï¼‰ä¸­çš„åŠ¨åŠ›å­¦å…·æœ‰ Lyapunov å‡½æ•°ï¼šåœ¨æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸­ï¼Œâ€œèƒ½é‡â€\n$$ E = -\\frac{1}{2}\\sum_{ij}\\sigma_{i}J_{ij}\\sigma_{j} + \\sum_{i}\\theta_{i}\\sigma_{i} $$\nè¦ä¹ˆå‡å°‘ï¼Œè¦ä¹ˆä¿æŒä¸å˜ã€‚ç½‘ç»œçŠ¶æ€çš„æ¼”å˜åœ¨èƒ½é‡ $E$ çš„å±€éƒ¨æå°å€¼å¤„åœæ­¢ï¼Œå¹¶ä¸”ä»…åœ¨è¿™äº›å±€éƒ¨æå°å€¼å¤„åœæ­¢ã€‚æˆ‘ä»¬å°†è¿™ä¸ªèƒ½é‡å‡½æ•°è¯†åˆ«ä¸ºå…·æœ‰è‡ªæ—‹ï¼ˆç¥ç»å…ƒï¼‰ä¹‹é—´æˆå¯¹ç›¸äº’ä½œç”¨çš„ Ising æ¨¡å‹ã€‚ç¥ç»åŠ¨åŠ›å­¦ä¸ç»Ÿè®¡ç‰©ç†å­¦çš„è¿™ç§éå¸¸æ˜ç¡®çš„è”ç³»å¼•å‘äº†ä¸€ç³»åˆ—å·¥ä½œï¼Œå¹¶ä¸”è¿™äº›æ€æƒ³çš„æ•™ç§‘ä¹¦æè¿°å¾ˆå¿«å°±å‡ºç°äº†ï¼ˆAmitï¼Œ1989 å¹´ï¼›Hertz ç­‰äººï¼Œ1991 å¹´ï¼‰ã€‚\nIt was useful in visualizing the dynamics of symmetric networks that they can be realized by simple circuit components, using amplifiers with saturating outputs in place of neurons, as in Fig 2. As with perceptrons one generalize to soft spins, now in continuous time; one version of these dynamics is\n$$ \\tau\\frac{\\mathrm{d}x_{i}}{\\mathrm{d}t} = -x_{i} + \\sum_{j}J_{ij}g(x_{j}) $$\nThese models have the same collective behaviors as Ising spins (Hopfield, 1984).\nåœ¨å¯è§†åŒ–å¯¹ç§°ç½‘ç»œçš„åŠ¨åŠ›å­¦æ—¶ï¼Œå®ƒä»¬å¯ä»¥é€šè¿‡ç®€å•çš„ç”µè·¯ç»„ä»¶å®ç°ï¼Œè¿™å¾ˆæœ‰ç”¨ï¼Œä½¿ç”¨å…·æœ‰é¥±å’Œè¾“å‡ºçš„æ”¾å¤§å™¨ä»£æ›¿ç¥ç»å…ƒï¼Œå¦‚å›¾ 2 æ‰€ç¤ºã€‚ä¸æ„ŸçŸ¥å™¨ä¸€æ ·ï¼Œå¯ä»¥æ¨å¹¿åˆ°è½¯è‡ªæ—‹ï¼Œç°åœ¨æ˜¯åœ¨è¿ç»­æ—¶é—´ä¸­ï¼›è¿™äº›åŠ¨åŠ›å­¦çš„ä¸€ä¸ªç‰ˆæœ¬æ˜¯\n$$ \\tau\\frac{\\mathrm{d}x_{i}}{\\mathrm{d}t} = -x_{i} + \\sum_{j}J_{ij}g(x_{j}) $$\nè¿™äº›æ¨¡å‹å…·æœ‰ä¸ Ising è‡ªæ—‹ç›¸åŒçš„é›†ä½“è¡Œä¸ºï¼ˆHopfieldï¼Œ1984 å¹´ï¼‰ã€‚\nFIG. 2 Equivalent circuit and dynamics in a symmetric network (Hopfield and Tank, 1986). (A) â€¦ (B) Schematic energy function for the circuit in (A); solid contours are above a mean level and dashed contours below, with X marking fixed points at the bottoms of energy valleys. (C) Corresponding dynamics, shown as a flow field. \\\nå›¾äºŒ. å¯¹ç§°ç½‘ç»œä¸­çš„ç­‰æ•ˆç”µè·¯å’ŒåŠ¨åŠ›å­¦ï¼ˆHopfield å’Œ Tankï¼Œ1986 å¹´ï¼‰ã€‚ï¼ˆAï¼‰â€¦ï¼ˆBï¼‰ï¼ˆAï¼‰ä¸­ç”µè·¯çš„ç¤ºæ„èƒ½é‡å‡½æ•°ï¼›å®çº¿è½®å»“é«˜äºå¹³å‡æ°´å¹³ï¼Œè™šçº¿è½®å»“ä½äºå¹³å‡æ°´å¹³ï¼ŒX æ ‡è®°èƒ½é‡è°·åº•çš„å›ºå®šç‚¹ã€‚ï¼ˆCï¼‰ç›¸åº”çš„åŠ¨åŠ›å­¦ï¼Œæ˜¾ç¤ºä¸ºæµåœºã€‚\nA crucial point is that one can â€œprogramâ€ symmetric networks to place local minima at desired states. Since the dynamics will flow spontaneously toward these minima and stop, we can think of this programming as storing memories in the network, which then can be recovered by initializing the state anywhere in the relevant basin of attraction. Taking the mapping of the Lyapunov function to an energy more seriously, this memory storage represents a sculpting of the energy landscape, which is a more general idea. As an example, we can think about the evolution of amino acid sequences in proteins sculpting the energy landscape for folding.\nä¸€ä¸ªå…³é”®ç‚¹æ˜¯ï¼Œå¯ä»¥â€œç¼–ç¨‹â€å¯¹ç§°ç½‘ç»œä»¥åœ¨æ‰€éœ€çŠ¶æ€ä¸‹æ”¾ç½®å±€éƒ¨æå°å€¼ã€‚ç”±äºåŠ¨åŠ›å­¦å°†è‡ªå‘åœ°æµå‘è¿™äº›æå°å€¼å¹¶åœæ­¢ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ç§ç¼–ç¨‹è§†ä¸ºåœ¨ç½‘ç»œä¸­å­˜å‚¨è®°å¿†ï¼Œç„¶åå¯ä»¥é€šè¿‡åœ¨ç›¸å…³å¸å¼•ç›†åœ°ä¸­çš„ä»»ä½•ä½ç½®åˆå§‹åŒ–çŠ¶æ€æ¥æ¢å¤è¿™äº›è®°å¿†ã€‚æ›´è®¤çœŸåœ°è€ƒè™‘ Lyapunov å‡½æ•°åˆ°èƒ½é‡çš„æ˜ å°„ï¼Œè¿™ç§è®°å¿†å­˜å‚¨ä»£è¡¨äº†èƒ½é‡æ™¯è§‚çš„é›•åˆ»ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´ä¸€èˆ¬çš„æƒ³æ³•ã€‚ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘è›‹ç™½è´¨ä¸­æ°¨åŸºé…¸åºåˆ—çš„æ¼”åŒ–ï¼Œé›•åˆ»æŠ˜å çš„èƒ½é‡æ™¯è§‚ã€‚\nTo illustrate the idea of memory storage, consider the case where the thresholds $\\theta_{i} = 0$. Suppose we can construct a matrix of synaptic weights such that\n$$ J_{ij} = J\\xi_{i}\\xi_{j} $$\nwhere the $\\xi_{i} = +1$ are again a set of (now fixed) binary or Ising variables. Then the energy function becomes\n$$ E = -\\frac{J}{2}\\sum_{ij}\\sigma_{i}\\xi_{i}\\xi_{j}\\sigma_{j} = -\\frac{J}2{}\\left(\\sum_{i}\\sigma_{i}\\xi_{i}\\right)^{2} = -\\frac{J}{2}\\left(\\vec{\\sigma}\\cdot\\vec{\\xi}\\right)^{2} $$\nBecause both $\\vec{\\sigma}$ and $\\vec{\\xi}$ are binary vectors the energy is minimized when these vectors are equal. If want to be a bit fancier we can transform $\\sigma_{i}\\rightarrow \\widetilde{\\sigma}_{i} = \\sigma_{i}\\xi_{i}$, and we then realize that Eq (8) is gauge equivalent to the mean-field ferromagnet.\nä¸ºäº†è¯´æ˜è®°å¿†å­˜å‚¨çš„æƒ³æ³•ï¼Œè€ƒè™‘é˜ˆå€¼ $\\theta_{i} = 0$ çš„æƒ…å†µã€‚å‡è®¾æˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªçªè§¦æƒé‡çŸ©é˜µï¼Œä½¿å¾—\n$$ J_{ij} = J\\xi_{i}\\xi_{j} $$\nå…¶ä¸­ $\\xi_{i} = +1$ å†æ¬¡æ˜¯ä¸€ç»„ï¼ˆç°åœ¨å›ºå®šçš„ï¼‰äºŒè¿›åˆ¶æˆ– Ising å˜é‡ã€‚é‚£ä¹ˆèƒ½é‡å‡½æ•°å˜ä¸º\n$$ E = -\\frac{J}{2}\\sum_{ij}\\sigma_{i}\\xi_{i}\\xi_{j}\\sigma_{j} = -\\frac{J}2{}\\left(\\sum_{i}\\sigma_{i}\\xi_{i}\\right)^{2} = -\\frac{J}{2}\\left(\\vec{\\sigma}\\cdot\\vec{\\xi}\\right)^{2} $$\nç”±äº $\\vec{\\sigma}$ å’Œ $\\vec{\\xi}$ éƒ½æ˜¯äºŒè¿›åˆ¶å‘é‡ï¼Œå½“è¿™äº›å‘é‡ç›¸ç­‰æ—¶èƒ½é‡æœ€å°åŒ–ã€‚å¦‚æœæƒ³è¦æ›´èŠ±å“¨ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œå˜æ¢ $\\sigma_{i}\\rightarrow \\widetilde{\\sigma}_{i} = \\sigma_{i}\\xi_{i}$ï¼Œç„¶åæˆ‘ä»¬æ„è¯†åˆ°æ–¹ç¨‹ï¼ˆ8ï¼‰åœ¨è§„èŒƒä¸Šç­‰ä»·äºå¹³å‡åœºé“ç£ä½“ã€‚\nCrucially, we can generalize this construction,\n$$ J_{ij} = J \\left(\\xi_{i}^{(1)}\\xi_{j}^{(1)} + \\xi_{i}^{(2)}\\xi_{j}^{(2)} + \\cdots + \\xi_{i}^{(K)}\\xi_{j}^{(K)}\\right). $$\nIf network has $N$ neurons, and the number of these terms $K\\ll N$ , then typically the vectors $\\vec{\\xi}^{(\\mu)}$ are orthogonal, and the energy function will have multiple minima at $\\vec{\\sigma} = \\vec{\\xi}^{(\\mu)}$: we have a model that stores $K$ memories.\nå…³é”®æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥æ¨å¹¿è¿™ç§æ„é€ ï¼Œ\n$$ J_{ij} = J \\left(\\xi_{i}^{(1)}\\xi_{j}^{(1)} + \\xi_{i}^{(2)}\\xi_{j}^{(2)} + \\cdots + \\xi_{i}^{(K)}\\xi_{j}^{(K)}\\right). $$\nå¦‚æœç½‘ç»œæœ‰ $N$ ä¸ªç¥ç»å…ƒï¼Œå¹¶ä¸”è¿™äº›é¡¹çš„æ•°é‡ $K\\ll N$ï¼Œé‚£ä¹ˆé€šå¸¸å‘é‡ $\\vec{\\xi}^{(\\mu)}$ æ˜¯æ­£äº¤çš„ï¼Œå¹¶ä¸”èƒ½é‡å‡½æ•°å°†åœ¨ $\\vec{\\sigma} = \\vec{\\xi}^{(\\mu)}$ å¤„å…·æœ‰å¤šä¸ªæå°å€¼ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªå­˜å‚¨ $K$ ä¸ªè®°å¿†çš„æ¨¡å‹ã€‚\nTo make this more rigorous letâ€™s imagine that the states of the network are not just the minima of the energy function, but are drawn from a Boltzmann distribution at some inverse temperature $\\beta$; it is plausible that this emerges from a noisy version of the dynamics in Eq (1). Then we have\n$$ \\begin{aligned} P(\\vec{\\sigma}) \u0026= \\frac{1}{Z}\\exp{{-\\beta E(\\vec{\\sigma})}} \\\\ E(\\vec{\\sigma}) \u0026= -\\frac{J_{0}}{N}\\sum_{ij=1}^{N}\\sum_{\\mu=1}^{K}\\sigma_{i}\\xi_{i}^{\\mu}\\xi_{j}^{\\mu}\\sigma_{j} \\end{aligned} $$\nwhere we use the usual normalization of interactions by a factor $N$ to insure a thermodynamic limit. Because the stored patterns are fixed, this is a statistical mechanics problem with quenched disorder, a special kind of meanfield spin glass. As a first try we can take the stored patterns to be random vectors, which might make sense if we are describing a region of the brain where the mapping between the features of what we remember and the identities of neurons is very abstract. We can measure the success of recalling memories by measuring the order parameters\n$$ m_{\\mu} = \\overline{\\langle\\vec{\\xi}^{\\mu}\\cdot\\vec{\\sigma}\\rangle} $$\nwhere $\\langle\\cdots\\rangle$ denotes an average over the â€œthermalâ€ fluctuations in the neural state $\\vec{\\sigma}$ and $\\overline{\\cdots}$ denotes an average over the random choice of the patterns $\\vec{\\xi}^{\\mu}$.\nä¸ºäº†ä½¿è¿™ä¸€ç‚¹æ›´åŠ ä¸¥æ ¼ï¼Œè®©æˆ‘ä»¬æƒ³è±¡ç½‘ç»œçš„çŠ¶æ€ä¸ä»…ä»…æ˜¯èƒ½é‡å‡½æ•°çš„æå°å€¼ï¼Œè€Œæ˜¯ä»æŸä¸ªé€†æ¸©åº¦ $\\beta$ çš„ Boltzmann åˆ†å¸ƒä¸­æŠ½å–çš„ï¼›è¿™å¯èƒ½æ˜¯ä»æ–¹ç¨‹ï¼ˆ1ï¼‰ä¸­çš„åŠ¨åŠ›å­¦çš„å™ªå£°ç‰ˆæœ¬ä¸­å‡ºç°çš„ã€‚ç„¶åæˆ‘ä»¬æœ‰\n$$ \\begin{aligned} P(\\vec{\\sigma}) \u0026= \\frac{1}{Z}\\exp{{-\\beta E(\\vec{\\sigma})}} \\\\ E(\\vec{\\sigma}) \u0026= -\\frac{J_{0}}{N}\\sum_{ij=1}^{N}\\sum_{\\mu=1}^{K}\\sigma_{i}\\xi_{i}^{\\mu}\\xi_{j}^{\\mu}\\sigma_{j} \\end{aligned} $$\nå…¶ä¸­æˆ‘ä»¬ä½¿ç”¨é€šè¿‡å› å­ $N$ çš„ç›¸äº’ä½œç”¨çš„é€šå¸¸å½’ä¸€åŒ–æ¥ç¡®ä¿çƒ­åŠ›å­¦æé™ã€‚ç”±äºå­˜å‚¨çš„æ¨¡å¼æ˜¯å›ºå®šçš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æ·¬ç«æ— åºçš„ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œä¸€ç§ç‰¹æ®Šç±»å‹çš„å¹³å‡åœºè‡ªæ—‹ç»ç’ƒã€‚ä½œä¸ºç¬¬ä¸€æ¬¡å°è¯•ï¼Œæˆ‘ä»¬å¯ä»¥å°†å­˜å‚¨çš„æ¨¡å¼è§†ä¸ºéšæœºå‘é‡ï¼Œå¦‚æœæˆ‘ä»¬æ­£åœ¨æè¿°å¤§è„‘çš„ä¸€ä¸ªåŒºåŸŸï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬è®°å¿†çš„ç‰¹å¾ä¸ç¥ç»å…ƒçš„èº«ä»½ä¹‹é—´çš„æ˜ å°„éå¸¸æŠ½è±¡ï¼Œè¿™å¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æµ‹é‡åºå‚é‡æ¥è¡¡é‡å›å¿†è®°å¿†çš„æˆåŠŸ\n$$ m_{\\mu} = \\overline{\\langle\\vec{\\xi}^{\\mu}\\cdot\\vec{\\sigma}\\rangle} $$\nå…¶ä¸­ $\\langle\\cdots\\rangle$ è¡¨ç¤ºå¯¹ç¥ç»çŠ¶æ€ $\\vec{\\sigma}$ ä¸­çš„â€œçƒ­â€æ³¢åŠ¨çš„å¹³å‡ï¼Œ$\\overline{\\cdots}$ è¡¨ç¤ºå¯¹æ¨¡å¼ $\\vec{\\xi}^{\\mu}$ çš„éšæœºé€‰æ‹©çš„å¹³å‡ã€‚\nShortly before the introduction of these models, there had been dramatic developments in the statistical mechanics of disordered systems, including the solution of the fully meanâ€“field Sherringtonâ€“Kirkpatrick spin glass model (Mezard et al., 1987). These tools could be applied to neural networks, resulting in a phase diagram mapping the order parameters $\\{m_{\\mu}\\}$ as function of the fictitious temperature and the storage density $\\alpha = K/N$ , all in the thermodynamic limit $N\\to\\infty$ (Amit et al., 1985, 1987). In the limit of zero temperature, below a critical $\\alpha_{c} = 0.138$ only one of the $m_{\\mu}$ will be nonzero, and it takes values close to one; this survives to finite temperatures. Thus there is a whole phase in which this model provides effective even if not quite perfect recall. By now we think of neural network models not as an application of statistical mechanics, but as a source of problems.\nåœ¨å¼•å…¥è¿™äº›æ¨¡å‹ä¹‹å‰ï¼Œå…³äºæ— åºç³»ç»Ÿçš„ç»Ÿè®¡åŠ›å­¦å·²ç»æœ‰äº†æˆå‰§æ€§çš„è¿›å±•ï¼ŒåŒ…æ‹¬å®Œå…¨å¹³å‡åœº Sherringtonâ€“Kirkpatrick è‡ªæ—‹ç»ç’ƒæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆï¼ˆMezard ç­‰äººï¼Œ1987 å¹´ï¼‰ã€‚è¿™äº›å·¥å…·å¯ä»¥åº”ç”¨äºç¥ç»ç½‘ç»œï¼Œå¯¼è‡´äº†ä¸€ä¸ªç›¸å›¾ï¼Œå°†åºå‚é‡ $\\{m_{\\mu}\\}$ æ˜ å°„ä¸ºè™šæ‹Ÿæ¸©åº¦å’Œå­˜å‚¨å¯†åº¦ $\\alpha = K/N$ çš„å‡½æ•°ï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨çƒ­åŠ›å­¦æé™ $N\\to\\infty$ ä¸­ï¼ˆAmit ç­‰äººï¼Œ1985 å¹´ï¼Œ1987 å¹´ï¼‰ã€‚åœ¨é›¶æ¸©åº¦çš„æé™ä¸‹ï¼Œä½äºä¸´ç•Œå€¼ $\\alpha_{c} = 0.138$ æ—¶ï¼Œåªæœ‰ä¸€ä¸ª $m_{\\mu}$ å°†æ˜¯éé›¶çš„ï¼Œå¹¶ä¸”å®ƒçš„å€¼æ¥è¿‘äºä¸€ï¼›è¿™åœ¨æœ‰é™æ¸©åº¦ä¸‹ä»ç„¶å­˜åœ¨ã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸ªæ¨¡å‹æä¾›æœ‰æ•ˆä½†ä¸å®Œå…¨å®Œç¾å›å¿†çš„æ•´ä¸ªé˜¶æ®µä¸­ã€‚åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œæˆ‘ä»¬è®¤ä¸ºç¥ç»ç½‘ç»œæ¨¡å‹ä¸ä»…ä»…æ˜¯ç»Ÿè®¡åŠ›å­¦çš„ä¸€ä¸ªåº”ç”¨ï¼Œè€Œæ˜¯é—®é¢˜çš„æ¥æºã€‚\nAn important feature of the dynamics is that it is â€œassociative.â€ Many initial states will relax to the same local minimum of the energy, which is equivalent to saying the same memory can be recalled from many different cues. In particular, we can imagine that the many bits represented by the state $\\{\\sigma_{i}\\}$ can be grouped into features, e.g. parts of the image of a face, the sound of the personâ€™s voice, $\\dots$ . Under many conditions if one set of features is given and the others randomized, the nearest local minimum will have all the features correctly aligned (Hopfield, 1982). The fact that our mind conjures an image in response to a sound or a fragrance had once seemed mysterious, and this provides a path to demystification, built on the idea that stored and recalled memories are collective states of the network.\nåŠ¨åŠ›å­¦çš„ä¸€ä¸ªé‡è¦ç‰¹å¾æ˜¯å®ƒæ˜¯â€œè”æƒ³çš„â€ã€‚è®¸å¤šåˆå§‹çŠ¶æ€å°†å¼›è±«åˆ°èƒ½é‡çš„åŒä¸€å±€éƒ¨æå°å€¼ï¼Œè¿™ç›¸å½“äºè¯´åŒä¸€è®°å¿†å¯ä»¥ä»è®¸å¤šä¸åŒçš„çº¿ç´¢ä¸­å›å¿†èµ·æ¥ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡ç”±çŠ¶æ€ $\\{\\sigma_{i}\\}$ è¡¨ç¤ºçš„è®¸å¤šä½å¯ä»¥åˆ†ç»„ä¸ºç‰¹å¾ï¼Œä¾‹å¦‚é¢éƒ¨å›¾åƒçš„éƒ¨åˆ†ã€è¯¥äººçš„å£°éŸ³ç­‰ã€‚åœ¨è®¸å¤šæ¡ä»¶ä¸‹ï¼Œå¦‚æœç»™å‡ºä¸€ç»„ç‰¹å¾å¹¶å°†å…¶ä»–ç‰¹å¾éšæœºåŒ–ï¼Œåˆ™æœ€è¿‘çš„å±€éƒ¨æå°å€¼å°†ä½¿æ‰€æœ‰ç‰¹å¾æ­£ç¡®å¯¹é½ï¼ˆHopfieldï¼Œ1982 å¹´ï¼‰ã€‚æˆ‘ä»¬çš„å¿ƒçµå¯¹å£°éŸ³æˆ–é¦™å‘³äº§ç”Ÿå›¾åƒçš„äº‹å®æ›¾ç»æ˜¾å¾—ç¥ç§˜ï¼Œè€Œè¿™ä¸ºå»ç¥ç§˜åŒ–æä¾›äº†ä¸€æ¡é€”å¾„ï¼Œå»ºç«‹åœ¨å­˜å‚¨å’Œå›å¿†çš„è®°å¿†æ˜¯ç½‘ç»œçš„é›†ä½“çŠ¶æ€çš„æƒ³æ³•ä¹‹ä¸Šã€‚\nThe synaptic matrix in Eq (9) has an important feature. Suppose that the network is currently in some state $\\vec{\\sigma}$ and we would like to add this state to the list of stored memoriesâ€”i.e. we would like the network to learn the current state. Following Eq (9) we should change the synaptic weights\n$$ J_{ij}\\rightarrow J_{ij} + J\\sigma_{i}\\sigma_{j} $$\nFirst we note that the connection between neurons $i$ and $j$ changes in a way that depends only on these two neurons. This locality of the learning rule is in a way remarkable, since we might have thought that sculpting the energy landscape would require more global manipulations. Second, the change in synaptic strength depends on the correlation between the preâ€“synaptic neuron $j$ and the postâ€“synaptic neuron $i$: if the cells are active together, the synapse should be strengthened. This simple rule sometimes is summarized by saying that neurons that â€œfire together wire together,â€ and there is considerable evidence that real synapses change in this way. Indeed, although this idea has its origins in classical discussions (Hebb, 1949; James, 1904), more direct measurements demonstrating that correlated activity leads to long lasting increases of synaptic strength came only in the decade before Hopfieldâ€™s work (Bliss and LÃ¸mo, 1973).\næ–¹ç¨‹ï¼ˆ9ï¼‰ä¸­çš„çªè§¦çŸ©é˜µå…·æœ‰ä¸€ä¸ªé‡è¦ç‰¹å¾ã€‚å‡è®¾ç½‘ç»œå½“å‰å¤„äºæŸä¸ªçŠ¶æ€ $\\vec{\\sigma}$ï¼Œæˆ‘ä»¬æƒ³å°†è¯¥çŠ¶æ€æ·»åŠ åˆ°å­˜å‚¨çš„è®°å¿†åˆ—è¡¨ä¸­â€”â€”å³æˆ‘ä»¬å¸Œæœ›ç½‘ç»œå­¦ä¹ å½“å‰çŠ¶æ€ã€‚æ ¹æ®æ–¹ç¨‹ï¼ˆ9ï¼‰ï¼Œæˆ‘ä»¬åº”è¯¥æ”¹å˜çªè§¦æƒé‡\n$$ J_{ij}\\rightarrow J_{ij} + J\\sigma_{i}\\sigma_{j} $$\né¦–å…ˆæˆ‘ä»¬æ³¨æ„åˆ°ï¼Œç¥ç»å…ƒ $i$ å’Œ $j$ ä¹‹é—´çš„è¿æ¥ä»¥ä»…å–å†³äºè¿™ä¸¤ä¸ªç¥ç»å…ƒçš„æ–¹å¼å‘ç”Ÿå˜åŒ–ã€‚è¿™ç§å­¦ä¹ è§„åˆ™çš„å±€éƒ¨æ€§åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯æ˜¾è‘—çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½è®¤ä¸ºé›•åˆ»èƒ½é‡æ™¯è§‚éœ€è¦æ›´å…¨å±€çš„æ“ä½œã€‚å…¶æ¬¡ï¼Œçªè§¦å¼ºåº¦çš„å˜åŒ–å–å†³äºçªè§¦å‰ç¥ç»å…ƒ $j$ å’Œçªè§¦åç¥ç»å…ƒ $i$ ä¹‹é—´çš„ç›¸å…³æ€§ï¼šå¦‚æœç»†èƒä¸€èµ·æ´»è·ƒï¼Œçªè§¦åº”è¯¥è¢«åŠ å¼ºã€‚è¿™æ¡ç®€å•çš„è§„åˆ™æœ‰æ—¶è¢«æ€»ç»“ä¸ºâ€œå…±åŒå‘å°„çš„ç¥ç»å…ƒå…±åŒè¿æ¥â€ï¼Œå¹¶ä¸”æœ‰ç›¸å½“å¤šçš„è¯æ®è¡¨æ˜çœŸå®çš„çªè§¦ä»¥è¿™ç§æ–¹å¼å‘ç”Ÿå˜åŒ–ã€‚äº‹å®ä¸Šï¼Œå°½ç®¡è¿™ä¸ªæƒ³æ³•èµ·æºäºç»å…¸è®¨è®ºï¼ˆHebbï¼Œ1949 å¹´ï¼›Jamesï¼Œ1904 å¹´ï¼‰ï¼Œä½†åœ¨ Hopfield çš„å·¥ä½œä¹‹å‰çš„åå¹´ä¸­ï¼Œæ‰æœ‰æ›´ç›´æ¥çš„æµ‹é‡è¡¨æ˜ç›¸å…³æ´»åŠ¨ä¼šå¯¼è‡´çªè§¦å¼ºåº¦çš„é•¿æœŸå¢åŠ ï¼ˆBliss å’Œ LÃ¸moï¼Œ1973 å¹´ï¼‰ã€‚\nIn the first examples, the goal of computation was to recover a stored pattern from partial information (associative memory). Beyond memory, Hopfield and Tank (1985) soon showed that one could construct networks that solve classical optimization problems, and that many biologically relevant problems could be cast in this form (Hopfield and Tank, 1986). At the same time, the idea of simulated annealing (Kirkpatrick et al., 1983) led people to take much more seriously the mapping between â€œcomputationalâ€ problems of optimization and the â€œphysicalâ€ problems of finding minimum energy states of manyâ€“body systems. This led, for example, to connections between statistical mechanics and computational complexity (Kirkpatrick and Selman, 1994; Monasson et al., 1999). From an engineering point of view, models for neural networks connected immediately to the possibility of using modern chip design methods to build analog, rather than digital circuits (Mead, 1989). Taken together, these simple symmetric models of neural networks formed a nexus among statistical physics, computer science, neurobiology, and engineering.\nåœ¨æœ€åˆçš„ä¾‹å­ä¸­ï¼Œè®¡ç®—çš„ç›®æ ‡æ˜¯ä»éƒ¨åˆ†ä¿¡æ¯ä¸­æ¢å¤å­˜å‚¨çš„æ¨¡å¼ï¼ˆè”æƒ³è®°å¿†ï¼‰ã€‚è¶…è¶Šè®°å¿†ï¼ŒHopfield å’Œ Tankï¼ˆ1985 å¹´ï¼‰å¾ˆå¿«è¡¨æ˜ï¼Œå¯ä»¥æ„å»ºè§£å†³ç»å…¸ä¼˜åŒ–é—®é¢˜çš„ç½‘ç»œï¼Œå¹¶ä¸”è®¸å¤šç”Ÿç‰©å­¦ç›¸å…³çš„é—®é¢˜å¯ä»¥ä»¥è¿™ç§å½¢å¼è¿›è¡ŒæŠ•å°„ï¼ˆHopfield å’Œ Tankï¼Œ1986 å¹´ï¼‰ã€‚ä¸æ­¤åŒæ—¶ï¼Œæ¨¡æ‹Ÿé€€ç«çš„æƒ³æ³•ï¼ˆKirkpatrick ç­‰äººï¼Œ1983 å¹´ï¼‰ä½¿äººä»¬æ›´åŠ è®¤çœŸåœ°å¯¹å¾…â€œè®¡ç®—â€ä¼˜åŒ–é—®é¢˜ä¸å¯»æ‰¾å¤šä½“ç³»ç»Ÿæœ€ä½èƒ½é‡çŠ¶æ€çš„â€œç‰©ç†â€é—®é¢˜ä¹‹é—´çš„æ˜ å°„ã€‚è¿™å¯¼è‡´äº†ç»Ÿè®¡åŠ›å­¦ä¸è®¡ç®—å¤æ‚æ€§ä¹‹é—´çš„è”ç³»ï¼ˆKirkpatrick å’Œ Selmanï¼Œ1994 å¹´ï¼›Monasson ç­‰äººï¼Œ1999 å¹´ï¼‰ã€‚ä»å·¥ç¨‹çš„è§’åº¦æ¥çœ‹ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹ç«‹å³ä¸ä½¿ç”¨ç°ä»£èŠ¯ç‰‡è®¾è®¡æ–¹æ³•æ„å»ºæ¨¡æ‹Ÿè€Œéæ•°å­—ç”µè·¯çš„å¯èƒ½æ€§ç›¸å…³è”ï¼ˆMeadï¼Œ1989 å¹´ï¼‰ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç®€å•çš„å¯¹ç§°ç¥ç»ç½‘ç»œæ¨¡å‹å½¢æˆäº†ç»Ÿè®¡ç‰©ç†å­¦ã€è®¡ç®—æœºç§‘å­¦ã€ç¥ç»ç”Ÿç‰©å­¦å’Œå·¥ç¨‹å­¦ä¹‹é—´çš„çº½å¸¦ã€‚\nPerspectives Our emphasis in this review is on networks of real neurons. But it would be foolish to ignore what is happening in the world of engineered, artificial networks, which proceeds at a terrifying pace, realizing many of the old dreams for artificial intelligence (AI). Not so long ago we would have emphasized the tremendous progress being made on problems such as image recognition or game playing, where deep networks achieved something that approximates human level performance. Today, popular discussion is focused on generative AI, with networks that produces text and images that have a striking realism. Our theoretical understanding of why these things work remains quite weak. There are engineering questions about what practical problems can be solved with confidence by such systems, and ethical questions about how humanity will interact with these machines. The successes of AI even have led to some to suggest that the physicistsâ€™ notions of understanding might themselves be superseded. In opposition to this, many physicists are hopeful that ideas from statistical mechanics will help us build a better understanding of modern AI (Carleo et al., 2019; Mehta et al., 2019; Roberts and Yaida, 2022).\næœ¬æ–‡ç»¼è¿°çš„é‡ç‚¹æ˜¯ç°å®ç¥ç»å…ƒç½‘ç»œã€‚ä½†å¿½è§†å·¥ç¨‹åŒ–çš„äººå·¥ç½‘ç»œé¢†åŸŸæ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…æ˜¯æ„šè ¢çš„ï¼Œè¿™ä¸€é¢†åŸŸä»¥æƒŠäººçš„é€Ÿåº¦å‘å±•ï¼Œå®ç°äº†å¯¹äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è®¸å¤šæ—§æ¢¦æƒ³ã€‚ä¸ä¹…å‰ï¼Œæˆ‘ä»¬ä¼šå¼ºè°ƒåœ¨å›¾åƒè¯†åˆ«æˆ–æ¸¸æˆç­‰é—®é¢˜ä¸Šå–å¾—çš„å·¨å¤§è¿›å±•ï¼Œåœ¨è¿™äº›é—®é¢˜ä¸Šï¼Œæ·±åº¦ç½‘ç»œå®ç°äº†æ¥è¿‘äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚ä»Šå¤©ï¼Œæµè¡Œçš„è®¨è®ºé›†ä¸­åœ¨ç”Ÿæˆå¼ AI ä¸Šï¼Œç½‘ç»œç”Ÿæˆçš„æ–‡æœ¬å’Œå›¾åƒå…·æœ‰æƒŠäººçš„çœŸå®æ„Ÿã€‚æˆ‘ä»¬å¯¹è¿™äº›äº‹ç‰©ä¸ºä½•æœ‰æ•ˆçš„ç†è®ºç†è§£ä»ç„¶ç›¸å½“è–„å¼±ã€‚å…³äºè¿™äº›ç³»ç»Ÿå¯ä»¥è‡ªä¿¡åœ°è§£å†³å“ªäº›å®é™…é—®é¢˜å­˜åœ¨å·¥ç¨‹é—®é¢˜ï¼Œä»¥åŠå…³äºäººç±»å°†å¦‚ä½•ä¸è¿™äº›æœºå™¨äº’åŠ¨çš„ä¼¦ç†é—®é¢˜ã€‚AI çš„æˆåŠŸç”šè‡³å¯¼è‡´ä¸€äº›äººå»ºè®®ç‰©ç†å­¦å®¶çš„ç†è§£æ¦‚å¿µæœ¬èº«å¯èƒ½ä¼šè¢«å–ä»£ã€‚ä¸æ­¤ç›¸åï¼Œè®¸å¤šç‰©ç†å­¦å®¶å¸Œæœ›ç»Ÿè®¡åŠ›å­¦çš„æ€æƒ³å°†å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£ç°ä»£ AIï¼ˆCarleo ç­‰äººï¼Œ2019 å¹´ï¼›Mehta ç­‰äººï¼Œ2019 å¹´ï¼›Roberts å’Œ Yaidaï¼Œ2022 å¹´ï¼‰ã€‚\nIn a different direction, many physicists have been interested in more explicitly dynamical models of neural networks (Vogels et al., 2005), as in Eq (6). Guided by the statistical physics of disordered systems, one can study networks in which the matrix of synaptic connections is drawn at random, perhaps from an ensemble that captures some established features of real connectivity patterns. These same ideas can be used for probabilistic models of binary neurons; notable developments include the development of a dynamical meanâ€“field theory for these systems (van Vreeswijk and Sompolinsky, 1998).\nåœ¨å¦ä¸€ä¸ªæ–¹å‘ä¸Šï¼Œè®¸å¤šç‰©ç†å­¦å®¶å¯¹ç¥ç»ç½‘ç»œçš„æ›´æ˜ç¡®çš„åŠ¨æ€æ¨¡å‹æ„Ÿå…´è¶£ï¼ˆVogels ç­‰äººï¼Œ2005 å¹´ï¼‰ï¼Œå¦‚æ–¹ç¨‹ï¼ˆ6ï¼‰æ‰€ç¤ºã€‚åœ¨æ— åºç³»ç»Ÿçš„ç»Ÿè®¡ç‰©ç†å­¦çš„æŒ‡å¯¼ä¸‹ï¼Œå¯ä»¥ç ”ç©¶çªè§¦è¿æ¥çŸ©é˜µæ˜¯éšæœºæŠ½å–çš„ç½‘ç»œï¼Œå¯èƒ½æ¥è‡ªæ•æ‰çœŸå®è¿æ¥æ¨¡å¼çš„ä¸€äº›å·²å»ºç«‹ç‰¹å¾çš„é›†åˆã€‚è¿™äº›ç›¸åŒçš„æ€æƒ³å¯ä»¥ç”¨äºäºŒè¿›åˆ¶ç¥ç»å…ƒçš„æ¦‚ç‡æ¨¡å‹ï¼›å€¼å¾—æ³¨æ„çš„å‘å±•åŒ…æ‹¬ä¸ºè¿™äº›ç³»ç»Ÿå¼€å‘åŠ¨æ€å¹³å‡åœºç†è®ºï¼ˆvan Vreeswijk å’Œ Sompolinskyï¼Œ1998 å¹´ï¼‰ã€‚\nAgainst the background of these theoretical developments, there has been a revolution in the experimental exploration of the brain, driven by techniques that combine methods from physics, chemistry and biology. We believe that this provides an unprecedented opportunity to connect statistical physics ideas to quantitative measurements on network dynamics in real brains. We turn first to an overview of the experimental state of the art.\nåœ¨è¿™äº›ç†è®ºå‘å±•çš„èƒŒæ™¯ä¸‹ï¼Œå¤§è„‘çš„å®éªŒæ¢ç´¢å‘ç”Ÿäº†é©å‘½ï¼Œè¿™å¾—ç›Šäºç»“åˆäº†ç‰©ç†å­¦ã€åŒ–å­¦å’Œç”Ÿç‰©å­¦æ–¹æ³•çš„æŠ€æœ¯ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™ä¸ºå°†ç»Ÿè®¡ç‰©ç†å­¦æ€æƒ³ä¸å¯¹çœŸå®å¤§è„‘ä¸­ç½‘ç»œåŠ¨åŠ›å­¦çš„å®šé‡æµ‹é‡è”ç³»èµ·æ¥æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šã€‚æˆ‘ä»¬é¦–å…ˆæ¥æ¦‚è¿°ä¸€ä¸‹å®éªŒçš„æœ€æ–°çŠ¶æ€ã€‚\n",
  "wordCount" : "10055",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png","datePublished": "2025-11-12T00:18:23+08:00",
  "dateModified": "2025-11-12T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "æ— å¤„æƒ¹å°˜åŸƒ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿— (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿—</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/">ğŸ“• é˜…è¯»</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/">ğŸ“• æ–‡çŒ®</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/">ğŸ“• Statistical mechanics for networks of real neurons</a></div>
            <h1 class="post-title">
                Some History
            </h1>
            <div class="post-description">
                çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-11-12
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>10055å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>21åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#prehistoric-times" aria-label="Prehistoric times">Prehistoric times</a></li>
                <li>
                    <a href="#from-perceptrons-to-deep-networks" aria-label="From perceptrons to deep networks">From perceptrons to deep networks</a></li>
                <li>
                    <a href="#symmetric-networks" aria-label="Symmetric networks">Symmetric networks</a></li>
                <li>
                    <a href="#perspectives" aria-label="Perspectives">Perspectives</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><blockquote>
<p>Today, neural network models are known to many different communities: physicists and applied mathematicians, computer scientists and engineers, neurobiologists and cognitive scientists. Neural networks are at the heart of an ongoing revolution in artificial intelligence, and are making their way into many aspects of scientific data analysis, from cell biology to CERN. Here we provide a brief (and perhaps idiosyncratic) reminder of how some of these ideas developed.</p>
</blockquote>
<p>ä»Šå¤©ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹ä¸ºè®¸å¤šä¸åŒçš„ç¾¤ä½“æ‰€çŸ¥ï¼šç‰©ç†å­¦å®¶å’Œåº”ç”¨æ•°å­¦å®¶ã€è®¡ç®—æœºç§‘å­¦å®¶å’Œå·¥ç¨‹å¸ˆã€ç¥ç»ç”Ÿç‰©å­¦å®¶å’Œè®¤çŸ¥ç§‘å­¦å®¶ã€‚ç¥ç»ç½‘ç»œæ˜¯äººå·¥æ™ºèƒ½æ­£åœ¨è¿›è¡Œçš„é©å‘½çš„æ ¸å¿ƒï¼Œå¹¶æ­£åœ¨è¿›å…¥ä»ç»†èƒç”Ÿç‰©å­¦åˆ° CERN çš„è®¸å¤šç§‘å­¦æ•°æ®åˆ†ææ–¹é¢ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç®€è¦ï¼ˆä¹Ÿè®¸æ˜¯ç‰¹ç«‹ç‹¬è¡Œåœ°ï¼‰å›é¡¾äº†ä¸€äº›è¿™äº›æƒ³æ³•æ˜¯å¦‚ä½•å‘å±•çš„ã€‚</p>
<h1 id="prehistoric-times">Prehistoric times<a hidden class="anchor" aria-hidden="true" href="#prehistoric-times">#</a></h1>
<blockquote>
<p>The engagement of physicists with neurons and the brain has a long and fascinating history. Our modern understanding of electricity has its roots in the 1700s with observations on nerves and muscles. The understanding of optics and acoustics that emerged in the 1800s was continuous with the exploration of vision and hearing. This involved thinking not just about the optics of the eye or the mechanics of the inner ear, but about the inferences that our brains can derive from the data collected by these physical instruments.</p>
</blockquote>
<p>ç‰©ç†å­¦å®¶ä¸ç¥ç»å…ƒå’Œå¤§è„‘çš„æ¥è§¦æœ‰ç€æ‚ ä¹…è€Œè¿·äººçš„å†å²ã€‚æˆ‘ä»¬å¯¹ç”µçš„ç°ä»£ç†è§£èµ·æºäº 1700 å¹´ä»£å¯¹ç¥ç»å’Œè‚Œè‚‰çš„è§‚å¯Ÿã€‚19 ä¸–çºªå‡ºç°çš„å…‰å­¦å’Œå£°å­¦çš„ç†è§£ä¸å¯¹è§†è§‰å’Œå¬è§‰çš„æ¢ç´¢æ˜¯è¿ç»­çš„ã€‚è¿™ä¸ä»…æ¶‰åŠçœ¼ç›çš„å…‰å­¦æˆ–å†…è€³çš„åŠ›å­¦ï¼Œè¿˜æ¶‰åŠæˆ‘ä»¬çš„å¤§è„‘å¯ä»¥ä»è¿™äº›ç‰©ç†ä»ªå™¨æ”¶é›†çš„æ•°æ®ä¸­æ¨æ–­å‡ºçš„æ¨è®ºã€‚</p>
<blockquote>
<p>The idea that the brain is made out of discrete cells, connected by synapses, dates from late 1800s (Ram Ìon y Cajal, 1894). The electrical signals from individual nerve cells (neurons) were first recorded in the 1920s, starting with the cells in sense organs that provide the input to the brain (Adrian, 1928). Observing these small signals required instruments no less sensitive than those in contemporary physics laboratories. The crucial observation is that neurons communicate by generating discrete, identical pulses of voltage across their membranes; these pulses are called action potentials or, more colloquially, spikes.</p>
</blockquote>
<p>å¤§è„‘ç”±ç¦»æ•£çš„ç»†èƒé€šè¿‡çªè§¦è¿æ¥è€Œæˆçš„æƒ³æ³•å¯ä»¥è¿½æº¯åˆ° 19 ä¸–çºªæœ«ï¼ˆRam Ìon y Cajalï¼Œ1894 å¹´ï¼‰ã€‚ä¸ªåˆ«ç¥ç»ç»†èƒï¼ˆç¥ç»å…ƒï¼‰çš„ç”µä¿¡å·æœ€æ—©åœ¨ 1920 å¹´ä»£è¢«è®°å½•ä¸‹æ¥ï¼Œé¦–å…ˆæ˜¯æä¾›å¤§è„‘è¾“å…¥çš„æ„Ÿè§‰å™¨å®˜ä¸­çš„ç»†èƒï¼ˆAdrianï¼Œ1928 å¹´ï¼‰ã€‚è§‚å¯Ÿè¿™äº›å¾®å°ä¿¡å·éœ€è¦ä¸å½“ä»£ç‰©ç†å®éªŒå®¤ä¸­çš„ä»ªå™¨åŒæ ·æ•æ„Ÿã€‚å…³é”®çš„è§‚å¯Ÿæ˜¯ï¼Œç¥ç»å…ƒé€šè¿‡åœ¨å…¶è†œä¸Šäº§ç”Ÿç¦»æ•£çš„ã€ç›¸åŒçš„ç”µå‹è„‰å†²æ¥è¿›è¡Œé€šä¿¡ï¼›è¿™äº›è„‰å†²ç§°ä¸ºåŠ¨ä½œç”µä½ï¼Œæˆ–è€…æ›´é€šä¿—åœ°è¯´ï¼Œç§°ä¸ºå°–å³°ã€‚</p>
<blockquote>
<p>By the 1950s there was a clear mathematical description of the dynamics underlying the generation and propagation of spikes (Hodgkin and Huxley, 1952). Perhaps surprisingly, the terms in these equations could be taken literally as representing the action of real physical componentsâ€”ion channel proteins that allow the flow of specific ions across the cell membrane, and which open and close (or â€œgateâ€) in response to the transmembrane voltage. The progress from macroscopic phenomenology to the dynamics of individual channels is a beautiful chapter in the interaction of physics and biology. The classic textbook account is Aidley (1998); Dayan and Abbott (2001) discuss phenomenological models for spiking activity; and a broader biological context is provided by Kandel et al. (2012). Rieke et al. (1997) describe the way in which sequences of spikes represent information about the sensory world, and Bialek (2012) connects channels and spikes to other problems in the physics of biological systems.</p>
</blockquote>
<p>åˆ°äº† 1950 å¹´ä»£ï¼Œäººä»¬å·²ç»å¯¹äº§ç”Ÿå’Œä¼ æ’­å°–å³°çš„åŠ¨åŠ›å­¦æœ‰äº†æ¸…æ™°çš„æ•°å­¦æè¿°ï¼ˆHodgkin å’Œ Huxleyï¼Œ1952 å¹´ï¼‰ã€‚ä¹Ÿè®¸ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™äº›æ–¹ç¨‹ä¸­çš„æœ¯è¯­å¯ä»¥è¢«å­—é¢ç†è§£ä¸ºä»£è¡¨çœŸå®ç‰©ç†ç»„ä»¶çš„ä½œç”¨â€”â€”å…è®¸ç‰¹å®šç¦»å­ç©¿è¿‡ç»†èƒè†œæµåŠ¨çš„ç¦»å­é€šé“è›‹ç™½ï¼Œè¿™äº›è›‹ç™½ä¼šæ ¹æ®è·¨è†œç”µå‹çš„å˜åŒ–è€Œæ‰“å¼€å’Œå…³é—­ï¼ˆæˆ–â€œé—¨æ§â€ï¼‰ã€‚ä»å®è§‚ç°è±¡å­¦åˆ°å•ä¸ªé€šé“åŠ¨åŠ›å­¦çš„è¿›å±•æ˜¯ç‰©ç†å­¦ä¸ç”Ÿç‰©å­¦ç›¸äº’ä½œç”¨ä¸­çš„ä¸€ä¸ªç¾ä¸½ç¯‡ç« ã€‚ç»å…¸çš„æ•™ç§‘ä¹¦æ˜¯ Aidleyï¼ˆ1998ï¼‰ï¼›Dayan å’Œ Abbottï¼ˆ2001ï¼‰è®¨è®ºäº†å°–å³°æ´»åŠ¨çš„ç°è±¡å­¦æ¨¡å‹ï¼›Kandel ç­‰äººï¼ˆ2012ï¼‰æä¾›äº†æ›´å¹¿æ³›çš„ç”Ÿç‰©å­¦èƒŒæ™¯ã€‚Rieke ç­‰äººï¼ˆ1997ï¼‰æè¿°äº†å°–å³°åºåˆ—å¦‚ä½•è¡¨ç¤ºæœ‰å…³æ„Ÿå®˜ä¸–ç•Œçš„ä¿¡æ¯ï¼ŒBialekï¼ˆ2012ï¼‰å°†é€šé“å’Œå°–å³°ä¸ç”Ÿç‰©ç³»ç»Ÿç‰©ç†å­¦ä¸­çš„å…¶ä»–é—®é¢˜è”ç³»èµ·æ¥ã€‚</p>
<blockquote>
<p>Even before the mechanisms were clear, people began to think about how the quasiâ€“digital character of spiking could be harnessed to do computations (McCulloch and Pitts, 1943). This work comes after the foundational work of Turing (1937) on universal computation, but before any practical modern computers. The goal of this work was to show that the basic facts known about neurons were sufficient to support computing essentially anything. On the one hand this is a very positive theoretical development: the brain could be a computer, in a deep sense. On the other hand it is disappointing, since if the brain is a universal computer there is not much more that one can say about the dynamics..</p>
</blockquote>
<p>å³ä½¿åœ¨æœºåˆ¶å°šä¸æ¸…æ¥šä¹‹å‰ï¼Œäººä»¬å°±å¼€å§‹æ€è€ƒå¦‚ä½•åˆ©ç”¨å°–å³°çš„å‡†æ•°å­—ç‰¹æ€§æ¥è¿›è¡Œè®¡ç®—ï¼ˆMcCulloch å’Œ Pittsï¼Œ1943 å¹´ï¼‰ã€‚è¿™é¡¹å·¥ä½œæ˜¯åœ¨ Turingï¼ˆ1937 å¹´ï¼‰å…³äºé€šç”¨è®¡ç®—çš„åŸºç¡€æ€§å·¥ä½œä¹‹åï¼Œä½†åœ¨ä»»ä½•å®ç”¨çš„ç°ä»£è®¡ç®—æœºä¹‹å‰ã€‚è¿™é¡¹å·¥ä½œçš„ç›®æ ‡æ˜¯è¡¨æ˜ï¼Œå·²çŸ¥çš„å…³äºç¥ç»å…ƒçš„åŸºæœ¬äº‹å®è¶³ä»¥æ”¯æŒè®¡ç®—æœ¬è´¨ä¸Šçš„ä»»ä½•ä¸œè¥¿ã€‚ä¸€æ–¹é¢ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ç§¯æçš„ç†è®ºå‘å±•ï¼šå¤§è„‘å¯ä»¥åœ¨æ·±å±‚æ„ä¹‰ä¸Šæˆä¸ºä¸€å°è®¡ç®—æœºã€‚å¦ä¸€æ–¹é¢ï¼Œè¿™ä»¤äººå¤±æœ›ï¼Œå› ä¸ºå¦‚æœå¤§è„‘æ˜¯ä¸€å°é€šç”¨è®¡ç®—æœºï¼Œé‚£ä¹ˆå…³äºåŠ¨åŠ›å­¦å°±æ²¡æœ‰å¤ªå¤šå¯è¯´çš„äº†ã€‚</p>
<blockquote>
<p>The way in which computation emerges from neurons in this early work clearly involves interactions among large numbers of cells in a network. Although single neurons can have remarkably precise dynamics in relation to sensory inputs and motor outputs (Hires et al., 2015; Nemenman et al., 2008; Rieke et al., 1997; Srivastava et al., 2017), there are many indications that our perceptions and actions, thoughts and memories typically are connected to the activity in many hundreds, perhaps even hundreds of thousands of neurons. Relevant activity in these large networks must be coordinated or collective.</p>
</blockquote>
<p>åœ¨è¿™é¡¹æ—©æœŸå·¥ä½œä¸­ï¼Œè®¡ç®—æ˜¯å¦‚ä½•ä»ç¥ç»å…ƒä¸­æ¶Œç°å‡ºæ¥çš„ï¼Œæ˜¾ç„¶æ¶‰åŠç½‘ç»œä¸­å¤§é‡ç»†èƒä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚å°½ç®¡å•ä¸ªç¥ç»å…ƒåœ¨æ„Ÿå®˜è¾“å…¥å’Œè¿åŠ¨è¾“å‡ºæ–¹é¢å¯ä»¥å…·æœ‰éå¸¸ç²¾ç¡®çš„åŠ¨åŠ›å­¦ï¼ˆHires ç­‰äººï¼Œ2015 å¹´ï¼›Nemenman ç­‰äººï¼Œ2008 å¹´ï¼›Rieke ç­‰äººï¼Œ1997 å¹´ï¼›Srivastava ç­‰äººï¼Œ2017 å¹´ï¼‰ï¼Œä½†æœ‰è®¸å¤šè¿¹è±¡è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ„ŸçŸ¥å’Œè¡ŒåŠ¨ã€æ€æƒ³å’Œè®°å¿†é€šå¸¸ä¸æ•°ç™¾ç”šè‡³æ•°åä¸‡ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨æœ‰å…³ã€‚è¿™äº›å¤§å‹ç½‘ç»œä¸­çš„ç›¸å…³æ´»åŠ¨å¿…é¡»æ˜¯åè°ƒæˆ–é›†ä½“çš„ã€‚</p>
<blockquote>
<p>The idea that collective neural activity in the brain might be described with statistical mechanics was very much influenced by observations on the electroencephalogram or EEG (Wiener, 1958). The EEG is a macroscopic measure of activity, traditionally done simply by placing electrodes on the scalp, and the existence of the EEG is prima facie evidence that the electrical activity of many, many neurons must be correlated. There is also the remarkable story of a demonstration by Adrian, in which he sat quietly with his eyes closed with electrodes attached to his head. The signals, sent to an oscilloscope, showed the characteristic â€œalpha rhythmâ€ that occurs in resting states, roughly an oscillation at âˆ¼ 10 Hz. When asked to add two numbers in his head, the rhythm disappeared, replaced by less easily described patterns of activity (Adrian and Matthews, 1934). This should dispel any lingering doubts that your mental life is related to the electrical activity of your brain.</p>
</blockquote>
<p>å¤§è„‘ä¸­é›†ä½“ç¥ç»æ´»åŠ¨å¯èƒ½ç”¨ç»Ÿè®¡åŠ›å­¦æ¥æè¿°çš„æƒ³æ³•ï¼Œæ·±å—å¯¹è„‘ç”µå›¾æˆ– EEG è§‚å¯Ÿçš„å½±å“ï¼ˆWienerï¼Œ1958 å¹´ï¼‰ã€‚EEG æ˜¯ä¸€ç§å®è§‚çš„æ´»åŠ¨æµ‹é‡ï¼Œä¼ ç»Ÿä¸Šåªæ˜¯é€šè¿‡åœ¨å¤´çš®ä¸Šæ”¾ç½®ç”µææ¥å®Œæˆï¼ŒEEG çš„å­˜åœ¨æ˜¯ç¬¬ä¸€æ‰‹è¯æ®ï¼Œè¡¨æ˜è®¸å¤šç¥ç»å…ƒçš„ç”µæ´»åŠ¨å¿…é¡»æ˜¯ç›¸å…³çš„ã€‚è¿˜æœ‰ä¸€ä¸ªä»¤äººæƒŠè®¶çš„æ•…äº‹ï¼Œè®²è¿°äº† Adrian çš„ä¸€ä¸ªæ¼”ç¤ºï¼Œä»–å®‰é™åœ°åç€ï¼Œé—­ç€çœ¼ç›ï¼Œå¤´ä¸Šè¿æ¥ç€ç”µæã€‚ä¿¡å·è¢«å‘é€åˆ°ç¤ºæ³¢å™¨ä¸Šï¼Œæ˜¾ç¤ºå‡ºåœ¨é™æ¯çŠ¶æ€ä¸‹å‘ç”Ÿçš„ç‰¹å¾æ€§â€œÎ±èŠ‚å¾‹â€ï¼Œå¤§çº¦æ˜¯ âˆ¼ 10 Hz çš„æŒ¯è¡ã€‚å½“è¢«è¦æ±‚åœ¨è„‘æµ·ä¸­åŠ ä¸¤ä¸ªæ•°å­—æ—¶ï¼Œè¿™ç§èŠ‚å¾‹æ¶ˆå¤±äº†ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ä¸é‚£ä¹ˆå®¹æ˜“æè¿°çš„æ´»åŠ¨æ¨¡å¼ï¼ˆAdrian å’Œ Matthewsï¼Œ1934 å¹´ï¼‰ã€‚è¿™åº”è¯¥æ¶ˆé™¤ä»»ä½•å…³äºä½ å¯¹å¿ƒæ™ºä¸å¤§è„‘ç”µæ´»åŠ¨ç›¸å…³çš„è´¨ç–‘ã€‚</p>
<blockquote>
<p>In the simplest models for neural dynamics, we describe the state of each neuron i at time t by a binary or Ising variable $\sigma_{i}(t)$; $\sigma_{i}(t) = +1$ means that the neuron is active, and $\sigma_{i}(t) = 0$ means that the neuron is silent.2 We imagine the dynamics proceeding in discrete time steps $\Delta\tau$ . Each neuron sums inputs from other neurons, weighted by the strength $J_{ij}$ of the synapse or connection from cell $j\rightarrow i$, and neurons switch into the active state if the total input is above a threshold:</p>
<p>$$
\sigma_{i}(t+\Delta\tau) = \Theta\left[\sum_{j}J_{ij}\sigma_{j}(t)-\theta_{i}\right]
$$</p>
<p>The nature of the dynamics is encoded in the matrix $J_{ij}$ of synaptic strengths. If we think about arbitrary matrices, then the dynamics can be arbitrarily complex; progress depends on simplifying assumptions. It is useful to organize our discussion around two extreme simplifications. But keep in mind as we follow these threads that many of the developments occurred in parallel, and that there was considerable crosstalk.</p>
</blockquote>
<p>åœ¨ç¥ç»åŠ¨åŠ›å­¦çš„æœ€ç®€å•æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡äºŒè¿›åˆ¶æˆ– Ising å˜é‡ $\sigma_{i}(t)$ æ¥æè¿°æ¯ä¸ªç¥ç»å…ƒ i åœ¨æ—¶é—´ t çš„çŠ¶æ€ï¼›$\sigma_{i}(t) = +1$ æ„å‘³ç€ç¥ç»å…ƒå¤„äºæ´»è·ƒçŠ¶æ€ï¼Œ$\sigma_{i}(t) = 0$ æ„å‘³ç€ç¥ç»å…ƒå¤„äºé™é»˜çŠ¶æ€ã€‚æˆ‘ä»¬æƒ³è±¡åŠ¨åŠ›å­¦ä»¥ç¦»æ•£æ—¶é—´æ­¥é•¿ $\Delta\tau$ è¿›è¡Œã€‚æ¯ä¸ªç¥ç»å…ƒå¯¹æ¥è‡ªå…¶ä»–ç¥ç»å…ƒçš„è¾“å…¥è¿›è¡Œæ±‚å’Œï¼Œè¿™äº›è¾“å…¥ç”±ä»ç»†èƒ $j\rightarrow i$ çš„çªè§¦æˆ–è¿æ¥çš„å¼ºåº¦ $J_{ij}$ åŠ æƒï¼Œå¦‚æœæ€»è¾“å…¥è¶…è¿‡é˜ˆå€¼ï¼Œç¥ç»å…ƒå°±ä¼šåˆ‡æ¢åˆ°æ´»è·ƒçŠ¶æ€ï¼š</p>
<p>$$
\sigma_{i}(t+\Delta\tau) = \Theta\left[\sum_{j}J_{ij}\sigma_{j}(t)-\theta_{i}\right]
$$</p>
<p>åŠ¨åŠ›å­¦çš„æ€§è´¨ç¼–ç åœ¨çªè§¦å¼ºåº¦çŸ©é˜µ $J_{ij}$ ä¸­ã€‚å¦‚æœæˆ‘ä»¬è€ƒè™‘ä»»æ„çŸ©é˜µï¼Œé‚£ä¹ˆåŠ¨åŠ›å­¦å¯ä»¥æ˜¯ä»»æ„å¤æ‚çš„ï¼›è¿›å±•å–å†³äºç®€åŒ–å‡è®¾ã€‚å›´ç»•ä¸¤ä¸ªæç«¯ç®€åŒ–æ¥ç»„ç»‡æˆ‘ä»¬çš„è®¨è®ºæ˜¯æœ‰ç”¨çš„ã€‚ä½†è¯·è®°ä½ï¼Œåœ¨æˆ‘ä»¬è·Ÿéšè¿™äº›çº¿ç´¢æ—¶ï¼Œè®¸å¤šå‘å±•æ˜¯å¹¶è¡Œå‘ç”Ÿçš„ï¼Œå¹¶ä¸”å­˜åœ¨ç›¸å½“å¤§çš„ä¸²æ‰°ã€‚</p>
<h1 id="from-perceptrons-to-deep-networks">From perceptrons to deep networks<a hidden class="anchor" aria-hidden="true" href="#from-perceptrons-to-deep-networks">#</a></h1>
<blockquote>
<p>One popular simplification is to assume that $J_{ij}$ has a feedâ€“forward, layered structure. This is the â€œperceptronâ€ architecture (Block, 1962; Block et al., 1962; Rosenblatt, 1961), illustrated in Fig 1A, which is simpler to analyze precisely because there are no feedback loops. It is convenient to label the neurons also by the layer $l$ in which they reside, and to generalize from binary variables to continuous ones, so that</p>
<p>$$
x_{i}^{(l+1)} = g\left[\sum_{j}W_{ij}^{(l+1)}x_{j}^{(l)} - \theta_{i}^{(l+1)}\right]
$$</p>
<p>where the propagation through layers replaces propagation through time and $g[\cdot]$ is a monotonic nonlinear function. Thus each neuron computes a single projection of its possible inputs from the previous layer, and then outputs a nonlinear function of this projection.</p>
</blockquote>
<p>ä¸€ç§æµè¡Œçš„ç®€åŒ–æ˜¯å‡è®¾ $J_{ij}$ å…·æœ‰å‰é¦ˆçš„åˆ†å±‚ç»“æ„ã€‚è¿™æ˜¯â€œæ„ŸçŸ¥å™¨â€æ¶æ„ï¼ˆBlockï¼Œ1962 å¹´ï¼›Block ç­‰äººï¼Œ1962 å¹´ï¼›Rosenblattï¼Œ1961 å¹´ï¼‰ï¼Œå¦‚å›¾ 1A æ‰€ç¤ºï¼Œæ­£å› ä¸ºæ²¡æœ‰åé¦ˆå›è·¯ï¼Œæ‰€ä»¥æ›´å®¹æ˜“åˆ†æã€‚æ–¹ä¾¿çš„æ˜¯ï¼Œè¿˜å¯ä»¥æŒ‰å®ƒä»¬æ‰€åœ¨çš„å±‚ $l$ æ¥æ ‡è®°ç¥ç»å…ƒï¼Œå¹¶å°†äºŒè¿›åˆ¶å˜é‡æ¨å¹¿ä¸ºè¿ç»­å˜é‡ï¼Œå› æ­¤</p>
<p>$$
x_{i}^{(l+1)} = g\left[\sum_{j}W_{ij}^{(l+1)}x_{j}^{(l)} - \theta_{i}^{(l+1)}\right]
$$</p>
<p>å…¶ä¸­é€šè¿‡å±‚çš„ä¼ æ’­å–ä»£äº†é€šè¿‡æ—¶é—´çš„ä¼ æ’­ï¼Œ$g[\cdot]$ æ˜¯ä¸€ä¸ªå•è°ƒéçº¿æ€§å‡½æ•°ã€‚å› æ­¤ï¼Œæ¯ä¸ªç¥ç»å…ƒè®¡ç®—æ¥è‡ªå‰ä¸€å±‚çš„å¯èƒ½è¾“å…¥çš„å•ä¸ªæŠ•å½±ï¼Œç„¶åè¾“å‡ºè¯¥æŠ•å½±çš„éçº¿æ€§å‡½æ•°ã€‚</p>
<blockquote>
<p>In the limit that $g[\cdot]$ becomes a step function we recover binary variables and neuron $i$ in layer $l + 1$, can be thought of a dividing the space of its inputs in half, with a hyperplane perpendicular to the vector</p>
<p>$$
\vec{V} = \{V_{j}\} = \{W_{ij}^{(l+1)}\}
$$</p>
<p>Thus the elementary computation is a binary classification of inputs,</p>
<p>$$
x\rightarrow y = \Theta(\vec{V}\cdot\vec{x}-\theta)
$$</p>
<p>We could imagine having access to many examples of the input vector $\vec{x}$ labelled by the correct classification $y$, and thereby learning the optimal vector $\vec{V}$ . This picture of learning to classify was present already âˆ¼1960, although it would take the full power of modern statistical physics to say that we really understand it. Crucially, if we think of the the $\{x_{i}\}$ or $\{\sigma_{i}\}$ as being the microscopic variables in the system and the $J_{ij}$ as being the interactions among these variables, then learning is statistical mechanics in the space of interactions (Gardner, 1988; Gardner and Derrida, 1988; Levin et al., 1990; Watkin et al., 1993).</p>
</blockquote>
<p>åœ¨ $g[\cdot]$ å˜æˆé˜¶è·ƒå‡½æ•°çš„æé™ä¸‹ï¼Œæˆ‘ä»¬æ¢å¤äº†äºŒè¿›åˆ¶å˜é‡ï¼Œå±‚ $l + 1$ ä¸­çš„ç¥ç»å…ƒ $i$ å¯ä»¥è¢«è®¤ä¸ºæ˜¯å°†å…¶è¾“å…¥ç©ºé—´åˆ†æˆä¸¤åŠï¼Œå…·æœ‰å‚ç›´äºå‘é‡çš„è¶…å¹³é¢</p>
<p>$$
\vec{V} = \{V_{j}\} = \{W_{ij}^{(l+1)}\}
$$</p>
<p>å› æ­¤ï¼ŒåŸºæœ¬çš„è®¡ç®—æ˜¯å¯¹è¾“å…¥çš„äºŒå…ƒåˆ†ç±»ï¼Œ</p>
<p>$$
x\rightarrow y = \Theta(\vec{V}\cdot\vec{x}-\theta)
$$</p>
<p>æˆ‘ä»¬å¯ä»¥æƒ³è±¡è·å¾—è®¸å¤šè¾“å…¥å‘é‡ $\vec{x}$ çš„ç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹ç”±æ­£ç¡®çš„åˆ†ç±» $y$ æ ‡è®°ï¼Œä»è€Œå­¦ä¹ æœ€ä½³å‘é‡ $\vec{V}$ã€‚è¿™ç§å­¦ä¹ åˆ†ç±»çš„å›¾æ™¯å·²ç»å‡ºç°åœ¨å¤§çº¦ 1960 å¹´ï¼Œå°½ç®¡éœ€è¦ç°ä»£ç»Ÿè®¡ç‰©ç†å­¦çš„å…¨éƒ¨åŠ›é‡æ‰èƒ½è¯´æˆ‘ä»¬çœŸæ­£ç†è§£äº†å®ƒã€‚å…³é”®æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å°† $\{x_{i}\}$ æˆ– $\{\sigma_{i}\}$ è§†ä¸ºç³»ç»Ÿä¸­çš„å¾®è§‚å˜é‡ï¼Œè€Œå°† $J_{ij}$ è§†ä¸ºè¿™äº›å˜é‡ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œé‚£ä¹ˆå­¦ä¹ å°±æ˜¯åœ¨ç›¸äº’ä½œç”¨ç©ºé—´ä¸­çš„ç»Ÿè®¡åŠ›å­¦ï¼ˆGardnerï¼Œ1988 å¹´ï¼›Gardner å’Œ Derridaï¼Œ1988 å¹´ï¼›Levin ç­‰äººï¼Œ1990 å¹´ï¼›Watkin ç­‰äººï¼Œ1993 å¹´ï¼‰ã€‚</p>
<blockquote>
<p>Although many of the computations done by the brain can be framed as classification problems, such as attaching names or words to images, very few can be solved by a single step of linear separation. Again this was clear at the start, but development of these ideas took decades. Enthusiasm was dampened by an emphasis on what two layer networks could not do (Minsky and Papert, 1969), but eventually it became clear that multilayer perceptrons are much more powerful (Lapedes and Farber, 1988; LeCun, 1987), and theorems were proven to show that these systems can approximate any function (Hornik et al., 1989). As with the simple perceptron, optimal weights $W$ can be learned by fitting to many examples of input/output pairs. Importantly this doesnâ€™t require access to the â€œcorrectâ€ answers at every layer; instead if we work with continuous variables then the goodness of fit across many layers can be differentiated using the chain rule, and errors propagated back through the network to adjust the weights (Rumelhart et al., 1986).</p>
</blockquote>
<p>è™½ç„¶å¤§è„‘æ‰§è¡Œçš„è®¸å¤šè®¡ç®—éƒ½å¯ä»¥è¢«æ¡†å®šä¸ºåˆ†ç±»é—®é¢˜ï¼Œä¾‹å¦‚å°†åç§°æˆ–å•è¯é™„åŠ åˆ°å›¾åƒä¸Šï¼Œä½†å¾ˆå°‘æœ‰è®¡ç®—å¯ä»¥é€šè¿‡å•ä¸€æ­¥éª¤çš„çº¿æ€§åˆ†ç¦»æ¥è§£å†³ã€‚è¿™ä¸€äº‹å®ä»ä¸€å¼€å§‹å°±å¾ˆæ¸…æ¥šï¼Œä½†è¿™äº›æƒ³æ³•çš„å‘å±•èŠ±è´¹äº†å‡ åå¹´æ—¶é—´ã€‚ç”±äºå¼ºè°ƒä¸¤å±‚ç½‘ç»œæ— æ³•å®Œæˆçš„ä»»åŠ¡ï¼ˆMinsky å’Œ Papertï¼Œ1969 å¹´ï¼‰ï¼Œçƒ­æƒ…æœ‰æ‰€å‡å¼±ï¼Œä½†æœ€ç»ˆäººä»¬æ„è¯†åˆ°å¤šå±‚æ„ŸçŸ¥å™¨æ›´åŠ å¼ºå¤§ï¼ˆLapedes å’Œ Farberï¼Œ1988 å¹´ï¼›LeCunï¼Œ1987 å¹´ï¼‰ï¼Œå¹¶ä¸”å·²ç»è¯æ˜äº†è¿™äº›ç³»ç»Ÿå¯ä»¥è¿‘ä¼¼ä»»ä½•å‡½æ•°çš„å®šç†ï¼ˆHornik ç­‰äººï¼Œ1989 å¹´ï¼‰ã€‚ä¸ç®€å•æ„ŸçŸ¥å™¨ä¸€æ ·ï¼Œå¯ä»¥é€šè¿‡æ‹Ÿåˆè®¸å¤šè¾“å…¥/è¾“å‡ºå¯¹çš„ç¤ºä¾‹æ¥å­¦ä¹ æœ€ä½³æƒé‡ $W$ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ä¸éœ€è¦è®¿é—®æ¯ä¸€å±‚çš„â€œæ­£ç¡®â€ç­”æ¡ˆï¼›ç›¸åï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨è¿ç»­å˜é‡ï¼Œé‚£ä¹ˆé€šè¿‡é“¾å¼æ³•åˆ™å¯ä»¥åŒºåˆ†å¤šå±‚ä¹‹é—´çš„æ‹Ÿåˆä¼˜åº¦ï¼Œå¹¶å°†è¯¯å·®åå‘ä¼ æ’­é€šè¿‡ç½‘ç»œä»¥è°ƒæ•´æƒé‡ï¼ˆRumelhart ç­‰äººï¼Œ1986 å¹´ï¼‰ã€‚</p>
<blockquote>
<p>Fast forward from the late 1980s to the mid 2010s. The few layers of early perceptrons became the many layers of â€œdeep networks,â€ in the spirit of Fig 1B; comparing the two panels of Fig 1 emphasizes the continuity of ideas across the decades. Advances in computing power and storage made it possible not just to simulate these models efficiently, but to solve the problem of finding optimal synaptic weights by comparing against millions or even billions of examples. These explorations led to networks so large that the number of weights needed to specify the network vastly exceeded the number of examples. Contrary to well established intuitions these â€œover parameterizedâ€ models worked, generalizing to new examples rather than overâ€“fitting to the training data. Although we donâ€™t fully understand them, these developments have fueled a revolution in artificial intelligence (AI).</p>
</blockquote>
<p>ä» 1980 å¹´ä»£åæœŸå¿«è¿›åˆ° 2010 å¹´ä»£ä¸­æœŸã€‚æ—©æœŸæ„ŸçŸ¥å™¨çš„å‡ å±‚å˜æˆäº†â€œæ·±åº¦ç½‘ç»œâ€çš„å¤šå±‚ï¼Œå¦‚å›¾ 1B æ‰€ç¤ºï¼›æ¯”è¾ƒå›¾ 1 çš„ä¸¤ä¸ªé¢æ¿å¼ºè°ƒäº†å‡ åå¹´æ¥æ€æƒ³çš„è¿ç»­æ€§ã€‚è®¡ç®—èƒ½åŠ›å’Œå­˜å‚¨çš„è¿›æ­¥ä¸ä»…ä½¿å¾—é«˜æ•ˆåœ°æ¨¡æ‹Ÿè¿™äº›æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œè€Œä¸”é€šè¿‡ä¸æ•°ç™¾ä¸‡ç”šè‡³æ•°åäº¿ä¸ªç¤ºä¾‹è¿›è¡Œæ¯”è¾ƒï¼Œè§£å†³äº†å¯»æ‰¾æœ€ä½³çªè§¦æƒé‡çš„é—®é¢˜ã€‚è¿™äº›æ¢ç´¢å¯¼è‡´äº†å¦‚æ­¤åºå¤§çš„ç½‘ç»œï¼Œä»¥è‡³äºæŒ‡å®šç½‘ç»œæ‰€éœ€çš„æƒé‡æ•°é‡è¿œè¿œè¶…è¿‡äº†ç¤ºä¾‹çš„æ•°é‡ã€‚ä¸æ—¢å®šçš„ç›´è§‰ç›¸åï¼Œè¿™äº›â€œè¿‡å‚æ•°åŒ–â€æ¨¡å‹æœ‰æ•ˆï¼Œå¯¹æ–°ç¤ºä¾‹è¿›è¡Œäº†æ³›åŒ–ï¼Œè€Œä¸æ˜¯å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œäº†è¿‡æ‹Ÿåˆã€‚å°½ç®¡æˆ‘ä»¬å¹¶ä¸å®Œå…¨ç†è§£å®ƒä»¬ï¼Œä½†è¿™äº›å‘å±•æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„é©å‘½ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/13/ixK2Tu3bsUPZ9mf.png" alt=""  /></p>
<p>FIG. 1 Neural networks with a feed-forward architecture, or â€œperceptrons.â€ (A) An early version of the idea, from Block (1962). (B) A modern version, with additional hidden layers. The first steps in the modern AI revolution involved similar networks, with many hidden layers, that achieved human-level performance on image classification and other tasks (LeCun et al., 2015).</p>
</blockquote>
<p>å›¾ä¸€. ç¥ç»ç½‘ç»œå…·æœ‰å‰é¦ˆæ¶æ„ï¼Œæˆ–â€œæ„ŸçŸ¥å™¨â€ã€‚ï¼ˆAï¼‰è¯¥æƒ³æ³•çš„æ—©æœŸç‰ˆæœ¬ï¼Œæ¥è‡ª Blockï¼ˆ1962 å¹´ï¼‰ã€‚ï¼ˆBï¼‰ç°ä»£ç‰ˆæœ¬ï¼Œå…·æœ‰é¢å¤–çš„éšè—å±‚ã€‚ç°ä»£ AI é©å‘½çš„ç¬¬ä¸€æ­¥æ¶‰åŠç±»ä¼¼çš„ç½‘ç»œï¼Œå…·æœ‰è®¸å¤šéšè—å±‚ï¼Œåœ¨å›¾åƒåˆ†ç±»å’Œå…¶ä»–ä»»åŠ¡ä¸Šå®ç°äº†äººç±»æ°´å¹³çš„æ€§èƒ½ï¼ˆLeCun ç­‰äººï¼Œ2015 å¹´ï¼‰ã€‚</p>
</blockquote>
<h1 id="symmetric-networks">Symmetric networks<a hidden class="anchor" aria-hidden="true" href="#symmetric-networks">#</a></h1>
<blockquote>
<p>Feedâ€“forward networks have the property that if $J_{ij}$ is nonzero, then $J_{ji} = 0$. Hopfield (1982, 1984) considered the opposite simplification: if neuron $i$ is connected to neuron $j$, then neuron $j$ is connected to neuron $i$, and the strength of the connection is the same, so that $J_{ij} = J_{ji}$. In this case the dynamics in Eq (1) have a Lyapunov function: at each time step the â€œenergyâ€</p>
<p>$$
E = -\frac{1}{2}\sum_{ij}\sigma_{i}J_{ij}\sigma_{j} + \sum_{i}\theta_{i}\sigma_{i}
$$</p>
<p>either decreases or stays constant. The evolution of the network state stops at local minima of the energy $E$, and only at these local minima. We recognize this energy function as an Ising model with pairwise interactions among the spins (neurons). This very explicit connection of neural dynamics to statistical physics triggered an avalanche of work, and textbook accounts of these ideas appeared quickly (Amit, 1989; Hertz et al., 1991).</p>
</blockquote>
<p>å‰é¦ˆç½‘ç»œå…·æœ‰è¿™æ ·çš„å±æ€§ï¼šå¦‚æœ $J_{ij}$ éé›¶ï¼Œåˆ™ $J_{ji} = 0$ã€‚Hopfieldï¼ˆ1982 å¹´ï¼Œ1984 å¹´ï¼‰è€ƒè™‘äº†ç›¸åçš„ç®€åŒ–ï¼šå¦‚æœç¥ç»å…ƒ $i$ ä¸ç¥ç»å…ƒ $j$ ç›¸è¿ï¼Œé‚£ä¹ˆç¥ç»å…ƒ $j$ ä¹Ÿä¸ç¥ç»å…ƒ $i$ ç›¸è¿ï¼Œå¹¶ä¸”è¿æ¥çš„å¼ºåº¦ç›¸åŒï¼Œå› æ­¤ $J_{ij} = J_{ji}$ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ–¹ç¨‹ï¼ˆ1ï¼‰ä¸­çš„åŠ¨åŠ›å­¦å…·æœ‰ Lyapunov å‡½æ•°ï¼šåœ¨æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸­ï¼Œâ€œèƒ½é‡â€</p>
<p>$$
E = -\frac{1}{2}\sum_{ij}\sigma_{i}J_{ij}\sigma_{j} + \sum_{i}\theta_{i}\sigma_{i}
$$</p>
<p>è¦ä¹ˆå‡å°‘ï¼Œè¦ä¹ˆä¿æŒä¸å˜ã€‚ç½‘ç»œçŠ¶æ€çš„æ¼”å˜åœ¨èƒ½é‡ $E$ çš„å±€éƒ¨æå°å€¼å¤„åœæ­¢ï¼Œå¹¶ä¸”ä»…åœ¨è¿™äº›å±€éƒ¨æå°å€¼å¤„åœæ­¢ã€‚æˆ‘ä»¬å°†è¿™ä¸ªèƒ½é‡å‡½æ•°è¯†åˆ«ä¸ºå…·æœ‰è‡ªæ—‹ï¼ˆç¥ç»å…ƒï¼‰ä¹‹é—´æˆå¯¹ç›¸äº’ä½œç”¨çš„ Ising æ¨¡å‹ã€‚ç¥ç»åŠ¨åŠ›å­¦ä¸ç»Ÿè®¡ç‰©ç†å­¦çš„è¿™ç§éå¸¸æ˜ç¡®çš„è”ç³»å¼•å‘äº†ä¸€ç³»åˆ—å·¥ä½œï¼Œå¹¶ä¸”è¿™äº›æ€æƒ³çš„æ•™ç§‘ä¹¦æè¿°å¾ˆå¿«å°±å‡ºç°äº†ï¼ˆAmitï¼Œ1989 å¹´ï¼›Hertz ç­‰äººï¼Œ1991 å¹´ï¼‰ã€‚</p>
<blockquote>
<p>It was useful in visualizing the dynamics of symmetric networks that they can be realized by simple circuit components, using amplifiers with saturating outputs in place of neurons, as in Fig 2. As with perceptrons one generalize to soft spins, now in continuous time; one version of these dynamics is</p>
<p>$$
\tau\frac{\mathrm{d}x_{i}}{\mathrm{d}t} = -x_{i} + \sum_{j}J_{ij}g(x_{j})
$$</p>
<p>These models have the same collective behaviors as Ising spins (Hopfield, 1984).</p>
</blockquote>
<p>åœ¨å¯è§†åŒ–å¯¹ç§°ç½‘ç»œçš„åŠ¨åŠ›å­¦æ—¶ï¼Œå®ƒä»¬å¯ä»¥é€šè¿‡ç®€å•çš„ç”µè·¯ç»„ä»¶å®ç°ï¼Œè¿™å¾ˆæœ‰ç”¨ï¼Œä½¿ç”¨å…·æœ‰é¥±å’Œè¾“å‡ºçš„æ”¾å¤§å™¨ä»£æ›¿ç¥ç»å…ƒï¼Œå¦‚å›¾ 2 æ‰€ç¤ºã€‚ä¸æ„ŸçŸ¥å™¨ä¸€æ ·ï¼Œå¯ä»¥æ¨å¹¿åˆ°è½¯è‡ªæ—‹ï¼Œç°åœ¨æ˜¯åœ¨è¿ç»­æ—¶é—´ä¸­ï¼›è¿™äº›åŠ¨åŠ›å­¦çš„ä¸€ä¸ªç‰ˆæœ¬æ˜¯</p>
<p>$$
\tau\frac{\mathrm{d}x_{i}}{\mathrm{d}t} = -x_{i} + \sum_{j}J_{ij}g(x_{j})
$$</p>
<p>è¿™äº›æ¨¡å‹å…·æœ‰ä¸ Ising è‡ªæ—‹ç›¸åŒçš„é›†ä½“è¡Œä¸ºï¼ˆHopfieldï¼Œ1984 å¹´ï¼‰ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/13/bgHcuQy596BdxX3.png" alt=""  /></p>
<p>FIG. 2 Equivalent circuit and dynamics in a symmetric network (Hopfield and Tank, 1986). (A) &hellip; (B) Schematic energy function for the circuit in (A); solid contours are above a mean level and dashed contours below, with X marking fixed points at the bottoms of energy valleys. (C) Corresponding dynamics, shown as a flow field. \</p>
</blockquote>
<p>å›¾äºŒ. å¯¹ç§°ç½‘ç»œä¸­çš„ç­‰æ•ˆç”µè·¯å’ŒåŠ¨åŠ›å­¦ï¼ˆHopfield å’Œ Tankï¼Œ1986 å¹´ï¼‰ã€‚ï¼ˆAï¼‰&hellip;ï¼ˆBï¼‰ï¼ˆAï¼‰ä¸­ç”µè·¯çš„ç¤ºæ„èƒ½é‡å‡½æ•°ï¼›å®çº¿è½®å»“é«˜äºå¹³å‡æ°´å¹³ï¼Œè™šçº¿è½®å»“ä½äºå¹³å‡æ°´å¹³ï¼ŒX æ ‡è®°èƒ½é‡è°·åº•çš„å›ºå®šç‚¹ã€‚ï¼ˆCï¼‰ç›¸åº”çš„åŠ¨åŠ›å­¦ï¼Œæ˜¾ç¤ºä¸ºæµåœºã€‚</p>
</blockquote>
<blockquote>
<p>A crucial point is that one can â€œprogramâ€ symmetric networks to place local minima at desired states. Since the dynamics will flow spontaneously toward these minima and stop, we can think of this programming as storing memories in the network, which then can be recovered by initializing the state anywhere in the relevant basin of attraction. Taking the mapping of the Lyapunov function to an energy more seriously, this memory storage represents a sculpting of the energy landscape, which is a more general idea. As an example, we can think about the evolution of amino acid sequences in proteins sculpting the energy landscape for folding.</p>
</blockquote>
<p>ä¸€ä¸ªå…³é”®ç‚¹æ˜¯ï¼Œå¯ä»¥â€œç¼–ç¨‹â€å¯¹ç§°ç½‘ç»œä»¥åœ¨æ‰€éœ€çŠ¶æ€ä¸‹æ”¾ç½®å±€éƒ¨æå°å€¼ã€‚ç”±äºåŠ¨åŠ›å­¦å°†è‡ªå‘åœ°æµå‘è¿™äº›æå°å€¼å¹¶åœæ­¢ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ç§ç¼–ç¨‹è§†ä¸ºåœ¨ç½‘ç»œä¸­å­˜å‚¨è®°å¿†ï¼Œç„¶åå¯ä»¥é€šè¿‡åœ¨ç›¸å…³å¸å¼•ç›†åœ°ä¸­çš„ä»»ä½•ä½ç½®åˆå§‹åŒ–çŠ¶æ€æ¥æ¢å¤è¿™äº›è®°å¿†ã€‚æ›´è®¤çœŸåœ°è€ƒè™‘ Lyapunov å‡½æ•°åˆ°èƒ½é‡çš„æ˜ å°„ï¼Œè¿™ç§è®°å¿†å­˜å‚¨ä»£è¡¨äº†èƒ½é‡æ™¯è§‚çš„é›•åˆ»ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´ä¸€èˆ¬çš„æƒ³æ³•ã€‚ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘è›‹ç™½è´¨ä¸­æ°¨åŸºé…¸åºåˆ—çš„æ¼”åŒ–ï¼Œé›•åˆ»æŠ˜å çš„èƒ½é‡æ™¯è§‚ã€‚</p>
<blockquote>
<p>To illustrate the idea of memory storage, consider the case where the thresholds $\theta_{i} = 0$. Suppose we can construct a matrix of synaptic weights such that</p>
<p>$$
J_{ij} = J\xi_{i}\xi_{j}
$$</p>
<p>where the $\xi_{i} = +1$ are again a set of (now fixed) binary or Ising variables. Then the energy function becomes</p>
<p>$$
E = -\frac{J}{2}\sum_{ij}\sigma_{i}\xi_{i}\xi_{j}\sigma_{j} = -\frac{J}2{}\left(\sum_{i}\sigma_{i}\xi_{i}\right)^{2} = -\frac{J}{2}\left(\vec{\sigma}\cdot\vec{\xi}\right)^{2}
$$</p>
<p>Because both $\vec{\sigma}$ and $\vec{\xi}$ are binary vectors the energy is minimized when these vectors are equal. If want to be a bit fancier we can transform $\sigma_{i}\rightarrow \widetilde{\sigma}_{i} = \sigma_{i}\xi_{i}$, and we then realize that Eq (8) is gauge equivalent to the mean-field ferromagnet.</p>
</blockquote>
<p>ä¸ºäº†è¯´æ˜è®°å¿†å­˜å‚¨çš„æƒ³æ³•ï¼Œè€ƒè™‘é˜ˆå€¼ $\theta_{i} = 0$ çš„æƒ…å†µã€‚å‡è®¾æˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªçªè§¦æƒé‡çŸ©é˜µï¼Œä½¿å¾—</p>
<p>$$
J_{ij} = J\xi_{i}\xi_{j}
$$</p>
<p>å…¶ä¸­ $\xi_{i} = +1$ å†æ¬¡æ˜¯ä¸€ç»„ï¼ˆç°åœ¨å›ºå®šçš„ï¼‰äºŒè¿›åˆ¶æˆ– Ising å˜é‡ã€‚é‚£ä¹ˆèƒ½é‡å‡½æ•°å˜ä¸º</p>
<p>$$
E = -\frac{J}{2}\sum_{ij}\sigma_{i}\xi_{i}\xi_{j}\sigma_{j} = -\frac{J}2{}\left(\sum_{i}\sigma_{i}\xi_{i}\right)^{2} = -\frac{J}{2}\left(\vec{\sigma}\cdot\vec{\xi}\right)^{2}
$$</p>
<p>ç”±äº $\vec{\sigma}$ å’Œ $\vec{\xi}$ éƒ½æ˜¯äºŒè¿›åˆ¶å‘é‡ï¼Œå½“è¿™äº›å‘é‡ç›¸ç­‰æ—¶èƒ½é‡æœ€å°åŒ–ã€‚å¦‚æœæƒ³è¦æ›´èŠ±å“¨ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œå˜æ¢ $\sigma_{i}\rightarrow \widetilde{\sigma}_{i} = \sigma_{i}\xi_{i}$ï¼Œç„¶åæˆ‘ä»¬æ„è¯†åˆ°æ–¹ç¨‹ï¼ˆ8ï¼‰åœ¨è§„èŒƒä¸Šç­‰ä»·äºå¹³å‡åœºé“ç£ä½“ã€‚</p>
<blockquote>
<p>Crucially, we can generalize this construction,</p>
<p>$$
J_{ij} = J \left(\xi_{i}^{(1)}\xi_{j}^{(1)} + \xi_{i}^{(2)}\xi_{j}^{(2)} + \cdots + \xi_{i}^{(K)}\xi_{j}^{(K)}\right).
$$</p>
<p>If network has $N$ neurons, and the number of these terms $K\ll N$ , then typically the vectors $\vec{\xi}^{(\mu)}$ are orthogonal, and the energy function will have multiple minima at $\vec{\sigma} = \vec{\xi}^{(\mu)}$: we have a model that stores $K$ memories.</p>
</blockquote>
<p>å…³é”®æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥æ¨å¹¿è¿™ç§æ„é€ ï¼Œ</p>
<p>$$
J_{ij} = J \left(\xi_{i}^{(1)}\xi_{j}^{(1)} + \xi_{i}^{(2)}\xi_{j}^{(2)} + \cdots + \xi_{i}^{(K)}\xi_{j}^{(K)}\right).
$$</p>
<p>å¦‚æœç½‘ç»œæœ‰ $N$ ä¸ªç¥ç»å…ƒï¼Œå¹¶ä¸”è¿™äº›é¡¹çš„æ•°é‡ $K\ll N$ï¼Œé‚£ä¹ˆé€šå¸¸å‘é‡ $\vec{\xi}^{(\mu)}$ æ˜¯æ­£äº¤çš„ï¼Œå¹¶ä¸”èƒ½é‡å‡½æ•°å°†åœ¨ $\vec{\sigma} = \vec{\xi}^{(\mu)}$ å¤„å…·æœ‰å¤šä¸ªæå°å€¼ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªå­˜å‚¨ $K$ ä¸ªè®°å¿†çš„æ¨¡å‹ã€‚</p>
<blockquote>
<p>To make this more rigorous letâ€™s imagine that the states of the network are not just the minima of the energy function, but are drawn from a Boltzmann distribution at some inverse temperature $\beta$; it is plausible that this emerges from a noisy version of the dynamics in Eq (1). Then we have</p>
<p>$$
\begin{aligned}
P(\vec{\sigma}) &amp;= \frac{1}{Z}\exp{{-\beta E(\vec{\sigma})}} \\
E(\vec{\sigma}) &amp;= -\frac{J_{0}}{N}\sum_{ij=1}^{N}\sum_{\mu=1}^{K}\sigma_{i}\xi_{i}^{\mu}\xi_{j}^{\mu}\sigma_{j}
\end{aligned}
$$</p>
<p>where we use the usual normalization of interactions by a factor $N$ to insure a thermodynamic limit. Because the stored patterns are fixed, this is a statistical mechanics problem with quenched disorder, a special kind of meanfield spin glass. As a first try we can take the stored patterns to be random vectors, which might make sense if we are describing a region of the brain where the mapping between the features of what we remember and the identities of neurons is very abstract. We can measure the success of recalling memories by measuring the order parameters</p>
<p>$$
m_{\mu} = \overline{\langle\vec{\xi}^{\mu}\cdot\vec{\sigma}\rangle}
$$</p>
<p>where $\langle\cdots\rangle$ denotes an average over the â€œthermalâ€ fluctuations in the neural state $\vec{\sigma}$ and $\overline{\cdots}$ denotes an average over the random choice of the patterns $\vec{\xi}^{\mu}$.</p>
</blockquote>
<p>ä¸ºäº†ä½¿è¿™ä¸€ç‚¹æ›´åŠ ä¸¥æ ¼ï¼Œè®©æˆ‘ä»¬æƒ³è±¡ç½‘ç»œçš„çŠ¶æ€ä¸ä»…ä»…æ˜¯èƒ½é‡å‡½æ•°çš„æå°å€¼ï¼Œè€Œæ˜¯ä»æŸä¸ªé€†æ¸©åº¦ $\beta$ çš„ Boltzmann åˆ†å¸ƒä¸­æŠ½å–çš„ï¼›è¿™å¯èƒ½æ˜¯ä»æ–¹ç¨‹ï¼ˆ1ï¼‰ä¸­çš„åŠ¨åŠ›å­¦çš„å™ªå£°ç‰ˆæœ¬ä¸­å‡ºç°çš„ã€‚ç„¶åæˆ‘ä»¬æœ‰</p>
<p>$$
\begin{aligned}
P(\vec{\sigma}) &amp;= \frac{1}{Z}\exp{{-\beta E(\vec{\sigma})}} \\
E(\vec{\sigma}) &amp;= -\frac{J_{0}}{N}\sum_{ij=1}^{N}\sum_{\mu=1}^{K}\sigma_{i}\xi_{i}^{\mu}\xi_{j}^{\mu}\sigma_{j}
\end{aligned}
$$</p>
<p>å…¶ä¸­æˆ‘ä»¬ä½¿ç”¨é€šè¿‡å› å­ $N$ çš„ç›¸äº’ä½œç”¨çš„é€šå¸¸å½’ä¸€åŒ–æ¥ç¡®ä¿çƒ­åŠ›å­¦æé™ã€‚ç”±äºå­˜å‚¨çš„æ¨¡å¼æ˜¯å›ºå®šçš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æ·¬ç«æ— åºçš„ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œä¸€ç§ç‰¹æ®Šç±»å‹çš„å¹³å‡åœºè‡ªæ—‹ç»ç’ƒã€‚ä½œä¸ºç¬¬ä¸€æ¬¡å°è¯•ï¼Œæˆ‘ä»¬å¯ä»¥å°†å­˜å‚¨çš„æ¨¡å¼è§†ä¸ºéšæœºå‘é‡ï¼Œå¦‚æœæˆ‘ä»¬æ­£åœ¨æè¿°å¤§è„‘çš„ä¸€ä¸ªåŒºåŸŸï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬è®°å¿†çš„ç‰¹å¾ä¸ç¥ç»å…ƒçš„èº«ä»½ä¹‹é—´çš„æ˜ å°„éå¸¸æŠ½è±¡ï¼Œè¿™å¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æµ‹é‡åºå‚é‡æ¥è¡¡é‡å›å¿†è®°å¿†çš„æˆåŠŸ</p>
<p>$$
m_{\mu} = \overline{\langle\vec{\xi}^{\mu}\cdot\vec{\sigma}\rangle}
$$</p>
<p>å…¶ä¸­ $\langle\cdots\rangle$ è¡¨ç¤ºå¯¹ç¥ç»çŠ¶æ€ $\vec{\sigma}$ ä¸­çš„â€œçƒ­â€æ³¢åŠ¨çš„å¹³å‡ï¼Œ$\overline{\cdots}$ è¡¨ç¤ºå¯¹æ¨¡å¼ $\vec{\xi}^{\mu}$ çš„éšæœºé€‰æ‹©çš„å¹³å‡ã€‚</p>
<blockquote>
<p>Shortly before the introduction of these models, there had been dramatic developments in the statistical mechanics of disordered systems, including the solution of the fully meanâ€“field Sherringtonâ€“Kirkpatrick spin glass model (Mezard et al., 1987). These tools could be applied to neural networks, resulting in a phase diagram mapping the order parameters $\{m_{\mu}\}$ as function of the fictitious temperature and the storage density $\alpha = K/N$ , all in the thermodynamic limit $N\to\infty$ (Amit et al., 1985, 1987). In the limit of zero temperature, below a critical $\alpha_{c} = 0.138$ only one of the $m_{\mu}$ will be nonzero, and it takes values close to one; this survives to finite temperatures. Thus there is a whole phase in which this model provides effective even if not quite perfect recall. By now we think of neural network models not as an application of statistical mechanics, but as a source of problems.</p>
</blockquote>
<p>åœ¨å¼•å…¥è¿™äº›æ¨¡å‹ä¹‹å‰ï¼Œå…³äºæ— åºç³»ç»Ÿçš„ç»Ÿè®¡åŠ›å­¦å·²ç»æœ‰äº†æˆå‰§æ€§çš„è¿›å±•ï¼ŒåŒ…æ‹¬å®Œå…¨å¹³å‡åœº Sherringtonâ€“Kirkpatrick è‡ªæ—‹ç»ç’ƒæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆï¼ˆMezard ç­‰äººï¼Œ1987 å¹´ï¼‰ã€‚è¿™äº›å·¥å…·å¯ä»¥åº”ç”¨äºç¥ç»ç½‘ç»œï¼Œå¯¼è‡´äº†ä¸€ä¸ªç›¸å›¾ï¼Œå°†åºå‚é‡ $\{m_{\mu}\}$ æ˜ å°„ä¸ºè™šæ‹Ÿæ¸©åº¦å’Œå­˜å‚¨å¯†åº¦ $\alpha = K/N$ çš„å‡½æ•°ï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨çƒ­åŠ›å­¦æé™ $N\to\infty$ ä¸­ï¼ˆAmit ç­‰äººï¼Œ1985 å¹´ï¼Œ1987 å¹´ï¼‰ã€‚åœ¨é›¶æ¸©åº¦çš„æé™ä¸‹ï¼Œä½äºä¸´ç•Œå€¼ $\alpha_{c} = 0.138$ æ—¶ï¼Œåªæœ‰ä¸€ä¸ª $m_{\mu}$ å°†æ˜¯éé›¶çš„ï¼Œå¹¶ä¸”å®ƒçš„å€¼æ¥è¿‘äºä¸€ï¼›è¿™åœ¨æœ‰é™æ¸©åº¦ä¸‹ä»ç„¶å­˜åœ¨ã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸ªæ¨¡å‹æä¾›æœ‰æ•ˆä½†ä¸å®Œå…¨å®Œç¾å›å¿†çš„æ•´ä¸ªé˜¶æ®µä¸­ã€‚åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œæˆ‘ä»¬è®¤ä¸ºç¥ç»ç½‘ç»œæ¨¡å‹ä¸ä»…ä»…æ˜¯ç»Ÿè®¡åŠ›å­¦çš„ä¸€ä¸ªåº”ç”¨ï¼Œè€Œæ˜¯é—®é¢˜çš„æ¥æºã€‚</p>
<blockquote>
<p>An important feature of the dynamics is that it is â€œassociative.â€ Many initial states will relax to the same local minimum of the energy, which is equivalent to saying the same memory can be recalled from many different cues. In particular, we can imagine that the many bits represented by the state $\{\sigma_{i}\}$ can be grouped into features, e.g. parts of the image of a face, the sound of the personâ€™s voice, $\dots$ . Under many conditions if one set of features is given and the others randomized, the nearest local minimum will have all the features correctly aligned (Hopfield, 1982). The fact that our mind conjures an image in response to a sound or a fragrance had once seemed mysterious, and this provides a path to demystification, built on the idea that stored and recalled memories are collective states of the network.</p>
</blockquote>
<p>åŠ¨åŠ›å­¦çš„ä¸€ä¸ªé‡è¦ç‰¹å¾æ˜¯å®ƒæ˜¯â€œè”æƒ³çš„â€ã€‚è®¸å¤šåˆå§‹çŠ¶æ€å°†å¼›è±«åˆ°èƒ½é‡çš„åŒä¸€å±€éƒ¨æå°å€¼ï¼Œè¿™ç›¸å½“äºè¯´åŒä¸€è®°å¿†å¯ä»¥ä»è®¸å¤šä¸åŒçš„çº¿ç´¢ä¸­å›å¿†èµ·æ¥ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡ç”±çŠ¶æ€ $\{\sigma_{i}\}$ è¡¨ç¤ºçš„è®¸å¤šä½å¯ä»¥åˆ†ç»„ä¸ºç‰¹å¾ï¼Œä¾‹å¦‚é¢éƒ¨å›¾åƒçš„éƒ¨åˆ†ã€è¯¥äººçš„å£°éŸ³ç­‰ã€‚åœ¨è®¸å¤šæ¡ä»¶ä¸‹ï¼Œå¦‚æœç»™å‡ºä¸€ç»„ç‰¹å¾å¹¶å°†å…¶ä»–ç‰¹å¾éšæœºåŒ–ï¼Œåˆ™æœ€è¿‘çš„å±€éƒ¨æå°å€¼å°†ä½¿æ‰€æœ‰ç‰¹å¾æ­£ç¡®å¯¹é½ï¼ˆHopfieldï¼Œ1982 å¹´ï¼‰ã€‚æˆ‘ä»¬çš„å¿ƒçµå¯¹å£°éŸ³æˆ–é¦™å‘³äº§ç”Ÿå›¾åƒçš„äº‹å®æ›¾ç»æ˜¾å¾—ç¥ç§˜ï¼Œè€Œè¿™ä¸ºå»ç¥ç§˜åŒ–æä¾›äº†ä¸€æ¡é€”å¾„ï¼Œå»ºç«‹åœ¨å­˜å‚¨å’Œå›å¿†çš„è®°å¿†æ˜¯ç½‘ç»œçš„é›†ä½“çŠ¶æ€çš„æƒ³æ³•ä¹‹ä¸Šã€‚</p>
<blockquote>
<p>The synaptic matrix in Eq (9) has an important feature. Suppose that the network is currently in some state $\vec{\sigma}$ and we would like to add this state to the list of stored memoriesâ€”i.e. we would like the network to learn the current state. Following Eq (9) we should change the synaptic weights</p>
<p>$$
J_{ij}\rightarrow J_{ij} + J\sigma_{i}\sigma_{j}
$$</p>
<p>First we note that the connection between neurons $i$ and $j$ changes in a way that depends only on these two neurons. This locality of the learning rule is in a way remarkable, since we might have thought that sculpting the energy landscape would require more global manipulations. Second, the change in synaptic strength depends on the correlation between the preâ€“synaptic neuron $j$ and the postâ€“synaptic neuron $i$: if the cells are active together, the synapse should be strengthened. This simple rule sometimes is summarized by saying that neurons that â€œfire together wire together,â€ and there is considerable evidence that real synapses change in this way. Indeed, although this idea has its origins in classical discussions (Hebb, 1949; James, 1904), more direct measurements demonstrating that correlated activity leads to long lasting increases of synaptic strength came only in the decade before Hopfieldâ€™s work (Bliss and LÃ¸mo, 1973).</p>
</blockquote>
<p>æ–¹ç¨‹ï¼ˆ9ï¼‰ä¸­çš„çªè§¦çŸ©é˜µå…·æœ‰ä¸€ä¸ªé‡è¦ç‰¹å¾ã€‚å‡è®¾ç½‘ç»œå½“å‰å¤„äºæŸä¸ªçŠ¶æ€ $\vec{\sigma}$ï¼Œæˆ‘ä»¬æƒ³å°†è¯¥çŠ¶æ€æ·»åŠ åˆ°å­˜å‚¨çš„è®°å¿†åˆ—è¡¨ä¸­â€”â€”å³æˆ‘ä»¬å¸Œæœ›ç½‘ç»œå­¦ä¹ å½“å‰çŠ¶æ€ã€‚æ ¹æ®æ–¹ç¨‹ï¼ˆ9ï¼‰ï¼Œæˆ‘ä»¬åº”è¯¥æ”¹å˜çªè§¦æƒé‡</p>
<p>$$
J_{ij}\rightarrow J_{ij} + J\sigma_{i}\sigma_{j}
$$</p>
<p>é¦–å…ˆæˆ‘ä»¬æ³¨æ„åˆ°ï¼Œç¥ç»å…ƒ $i$ å’Œ $j$ ä¹‹é—´çš„è¿æ¥ä»¥ä»…å–å†³äºè¿™ä¸¤ä¸ªç¥ç»å…ƒçš„æ–¹å¼å‘ç”Ÿå˜åŒ–ã€‚è¿™ç§å­¦ä¹ è§„åˆ™çš„å±€éƒ¨æ€§åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯æ˜¾è‘—çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½è®¤ä¸ºé›•åˆ»èƒ½é‡æ™¯è§‚éœ€è¦æ›´å…¨å±€çš„æ“ä½œã€‚å…¶æ¬¡ï¼Œçªè§¦å¼ºåº¦çš„å˜åŒ–å–å†³äºçªè§¦å‰ç¥ç»å…ƒ $j$ å’Œçªè§¦åç¥ç»å…ƒ $i$ ä¹‹é—´çš„ç›¸å…³æ€§ï¼šå¦‚æœç»†èƒä¸€èµ·æ´»è·ƒï¼Œçªè§¦åº”è¯¥è¢«åŠ å¼ºã€‚è¿™æ¡ç®€å•çš„è§„åˆ™æœ‰æ—¶è¢«æ€»ç»“ä¸ºâ€œå…±åŒå‘å°„çš„ç¥ç»å…ƒå…±åŒè¿æ¥â€ï¼Œå¹¶ä¸”æœ‰ç›¸å½“å¤šçš„è¯æ®è¡¨æ˜çœŸå®çš„çªè§¦ä»¥è¿™ç§æ–¹å¼å‘ç”Ÿå˜åŒ–ã€‚äº‹å®ä¸Šï¼Œå°½ç®¡è¿™ä¸ªæƒ³æ³•èµ·æºäºç»å…¸è®¨è®ºï¼ˆHebbï¼Œ1949 å¹´ï¼›Jamesï¼Œ1904 å¹´ï¼‰ï¼Œä½†åœ¨ Hopfield çš„å·¥ä½œä¹‹å‰çš„åå¹´ä¸­ï¼Œæ‰æœ‰æ›´ç›´æ¥çš„æµ‹é‡è¡¨æ˜ç›¸å…³æ´»åŠ¨ä¼šå¯¼è‡´çªè§¦å¼ºåº¦çš„é•¿æœŸå¢åŠ ï¼ˆBliss å’Œ LÃ¸moï¼Œ1973 å¹´ï¼‰ã€‚</p>
<blockquote>
<p>In the first examples, the goal of computation was to recover a stored pattern from partial information (associative memory). Beyond memory, Hopfield and Tank (1985) soon showed that one could construct networks that solve classical optimization problems, and that many biologically relevant problems could be cast in this form (Hopfield and Tank, 1986). At the same time, the idea of simulated annealing (Kirkpatrick et al., 1983) led people to take much more seriously the mapping between â€œcomputationalâ€ problems of optimization and the â€œphysicalâ€ problems of finding minimum energy states of manyâ€“body systems. This led, for example, to connections between statistical mechanics and computational complexity (Kirkpatrick and Selman, 1994; Monasson et al., 1999). From an engineering point of view, models for neural networks connected immediately to the possibility of using modern chip design methods to build analog, rather than digital circuits (Mead, 1989). Taken together, these simple symmetric models of neural networks formed a nexus among statistical physics, computer science, neurobiology, and engineering.</p>
</blockquote>
<p>åœ¨æœ€åˆçš„ä¾‹å­ä¸­ï¼Œè®¡ç®—çš„ç›®æ ‡æ˜¯ä»éƒ¨åˆ†ä¿¡æ¯ä¸­æ¢å¤å­˜å‚¨çš„æ¨¡å¼ï¼ˆè”æƒ³è®°å¿†ï¼‰ã€‚è¶…è¶Šè®°å¿†ï¼ŒHopfield å’Œ Tankï¼ˆ1985 å¹´ï¼‰å¾ˆå¿«è¡¨æ˜ï¼Œå¯ä»¥æ„å»ºè§£å†³ç»å…¸ä¼˜åŒ–é—®é¢˜çš„ç½‘ç»œï¼Œå¹¶ä¸”è®¸å¤šç”Ÿç‰©å­¦ç›¸å…³çš„é—®é¢˜å¯ä»¥ä»¥è¿™ç§å½¢å¼è¿›è¡ŒæŠ•å°„ï¼ˆHopfield å’Œ Tankï¼Œ1986 å¹´ï¼‰ã€‚ä¸æ­¤åŒæ—¶ï¼Œæ¨¡æ‹Ÿé€€ç«çš„æƒ³æ³•ï¼ˆKirkpatrick ç­‰äººï¼Œ1983 å¹´ï¼‰ä½¿äººä»¬æ›´åŠ è®¤çœŸåœ°å¯¹å¾…â€œè®¡ç®—â€ä¼˜åŒ–é—®é¢˜ä¸å¯»æ‰¾å¤šä½“ç³»ç»Ÿæœ€ä½èƒ½é‡çŠ¶æ€çš„â€œç‰©ç†â€é—®é¢˜ä¹‹é—´çš„æ˜ å°„ã€‚è¿™å¯¼è‡´äº†ç»Ÿè®¡åŠ›å­¦ä¸è®¡ç®—å¤æ‚æ€§ä¹‹é—´çš„è”ç³»ï¼ˆKirkpatrick å’Œ Selmanï¼Œ1994 å¹´ï¼›Monasson ç­‰äººï¼Œ1999 å¹´ï¼‰ã€‚ä»å·¥ç¨‹çš„è§’åº¦æ¥çœ‹ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹ç«‹å³ä¸ä½¿ç”¨ç°ä»£èŠ¯ç‰‡è®¾è®¡æ–¹æ³•æ„å»ºæ¨¡æ‹Ÿè€Œéæ•°å­—ç”µè·¯çš„å¯èƒ½æ€§ç›¸å…³è”ï¼ˆMeadï¼Œ1989 å¹´ï¼‰ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç®€å•çš„å¯¹ç§°ç¥ç»ç½‘ç»œæ¨¡å‹å½¢æˆäº†ç»Ÿè®¡ç‰©ç†å­¦ã€è®¡ç®—æœºç§‘å­¦ã€ç¥ç»ç”Ÿç‰©å­¦å’Œå·¥ç¨‹å­¦ä¹‹é—´çš„çº½å¸¦ã€‚</p>
<h1 id="perspectives">Perspectives<a hidden class="anchor" aria-hidden="true" href="#perspectives">#</a></h1>
<blockquote>
<p>Our emphasis in this review is on networks of real neurons. But it would be foolish to ignore what is happening in the world of engineered, artificial networks, which proceeds at a terrifying pace, realizing many of the old dreams for artificial intelligence (AI). Not so long ago we would have emphasized the tremendous progress being made on problems such as image recognition or game playing, where deep networks achieved something that approximates human level performance. Today, popular discussion is focused on generative AI, with networks that produces text and images that have a striking realism. Our theoretical understanding of why these things work remains quite weak. There are engineering questions about what practical problems can be solved with confidence by such systems, and ethical questions about how humanity will interact with these machines. The successes of AI even have led to some to suggest that the physicistsâ€™ notions of understanding might themselves be superseded. In opposition to this, many physicists are hopeful that ideas from statistical mechanics will help us build a better understanding of modern AI (Carleo et al., 2019; Mehta et al., 2019; Roberts and Yaida, 2022).</p>
</blockquote>
<p>æœ¬æ–‡ç»¼è¿°çš„é‡ç‚¹æ˜¯ç°å®ç¥ç»å…ƒç½‘ç»œã€‚ä½†å¿½è§†å·¥ç¨‹åŒ–çš„äººå·¥ç½‘ç»œé¢†åŸŸæ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…æ˜¯æ„šè ¢çš„ï¼Œè¿™ä¸€é¢†åŸŸä»¥æƒŠäººçš„é€Ÿåº¦å‘å±•ï¼Œå®ç°äº†å¯¹äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è®¸å¤šæ—§æ¢¦æƒ³ã€‚ä¸ä¹…å‰ï¼Œæˆ‘ä»¬ä¼šå¼ºè°ƒåœ¨å›¾åƒè¯†åˆ«æˆ–æ¸¸æˆç­‰é—®é¢˜ä¸Šå–å¾—çš„å·¨å¤§è¿›å±•ï¼Œåœ¨è¿™äº›é—®é¢˜ä¸Šï¼Œæ·±åº¦ç½‘ç»œå®ç°äº†æ¥è¿‘äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚ä»Šå¤©ï¼Œæµè¡Œçš„è®¨è®ºé›†ä¸­åœ¨ç”Ÿæˆå¼ AI ä¸Šï¼Œç½‘ç»œç”Ÿæˆçš„æ–‡æœ¬å’Œå›¾åƒå…·æœ‰æƒŠäººçš„çœŸå®æ„Ÿã€‚æˆ‘ä»¬å¯¹è¿™äº›äº‹ç‰©ä¸ºä½•æœ‰æ•ˆçš„ç†è®ºç†è§£ä»ç„¶ç›¸å½“è–„å¼±ã€‚å…³äºè¿™äº›ç³»ç»Ÿå¯ä»¥è‡ªä¿¡åœ°è§£å†³å“ªäº›å®é™…é—®é¢˜å­˜åœ¨å·¥ç¨‹é—®é¢˜ï¼Œä»¥åŠå…³äºäººç±»å°†å¦‚ä½•ä¸è¿™äº›æœºå™¨äº’åŠ¨çš„ä¼¦ç†é—®é¢˜ã€‚AI çš„æˆåŠŸç”šè‡³å¯¼è‡´ä¸€äº›äººå»ºè®®ç‰©ç†å­¦å®¶çš„ç†è§£æ¦‚å¿µæœ¬èº«å¯èƒ½ä¼šè¢«å–ä»£ã€‚ä¸æ­¤ç›¸åï¼Œè®¸å¤šç‰©ç†å­¦å®¶å¸Œæœ›ç»Ÿè®¡åŠ›å­¦çš„æ€æƒ³å°†å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£ç°ä»£ AIï¼ˆCarleo ç­‰äººï¼Œ2019 å¹´ï¼›Mehta ç­‰äººï¼Œ2019 å¹´ï¼›Roberts å’Œ Yaidaï¼Œ2022 å¹´ï¼‰ã€‚</p>
<blockquote>
<p>In a different direction, many physicists have been interested in more explicitly dynamical models of neural networks (Vogels et al., 2005), as in Eq (6). Guided by the statistical physics of disordered systems, one can study networks in which the matrix of synaptic connections is drawn at random, perhaps from an ensemble that captures some established features of real connectivity patterns. These same ideas can be used for probabilistic models of binary neurons; notable developments include the development of a dynamical meanâ€“field theory for these systems (van Vreeswijk and Sompolinsky, 1998).</p>
</blockquote>
<p>åœ¨å¦ä¸€ä¸ªæ–¹å‘ä¸Šï¼Œè®¸å¤šç‰©ç†å­¦å®¶å¯¹ç¥ç»ç½‘ç»œçš„æ›´æ˜ç¡®çš„åŠ¨æ€æ¨¡å‹æ„Ÿå…´è¶£ï¼ˆVogels ç­‰äººï¼Œ2005 å¹´ï¼‰ï¼Œå¦‚æ–¹ç¨‹ï¼ˆ6ï¼‰æ‰€ç¤ºã€‚åœ¨æ— åºç³»ç»Ÿçš„ç»Ÿè®¡ç‰©ç†å­¦çš„æŒ‡å¯¼ä¸‹ï¼Œå¯ä»¥ç ”ç©¶çªè§¦è¿æ¥çŸ©é˜µæ˜¯éšæœºæŠ½å–çš„ç½‘ç»œï¼Œå¯èƒ½æ¥è‡ªæ•æ‰çœŸå®è¿æ¥æ¨¡å¼çš„ä¸€äº›å·²å»ºç«‹ç‰¹å¾çš„é›†åˆã€‚è¿™äº›ç›¸åŒçš„æ€æƒ³å¯ä»¥ç”¨äºäºŒè¿›åˆ¶ç¥ç»å…ƒçš„æ¦‚ç‡æ¨¡å‹ï¼›å€¼å¾—æ³¨æ„çš„å‘å±•åŒ…æ‹¬ä¸ºè¿™äº›ç³»ç»Ÿå¼€å‘åŠ¨æ€å¹³å‡åœºç†è®ºï¼ˆvan Vreeswijk å’Œ Sompolinskyï¼Œ1998 å¹´ï¼‰ã€‚</p>
<blockquote>
<p>Against the background of these theoretical developments, there has been a revolution in the experimental exploration of the brain, driven by techniques that combine methods from physics, chemistry and biology. We believe that this provides an unprecedented opportunity to connect statistical physics ideas to quantitative measurements on network dynamics in real brains. We turn first to an overview of the experimental state of the art.</p>
</blockquote>
<p>åœ¨è¿™äº›ç†è®ºå‘å±•çš„èƒŒæ™¯ä¸‹ï¼Œå¤§è„‘çš„å®éªŒæ¢ç´¢å‘ç”Ÿäº†é©å‘½ï¼Œè¿™å¾—ç›Šäºç»“åˆäº†ç‰©ç†å­¦ã€åŒ–å­¦å’Œç”Ÿç‰©å­¦æ–¹æ³•çš„æŠ€æœ¯ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™ä¸ºå°†ç»Ÿè®¡ç‰©ç†å­¦æ€æƒ³ä¸å¯¹çœŸå®å¤§è„‘ä¸­ç½‘ç»œåŠ¨åŠ›å­¦çš„å®šé‡æµ‹é‡è”ç³»èµ·æ¥æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šã€‚æˆ‘ä»¬é¦–å…ˆæ¥æ¦‚è¿°ä¸€ä¸‹å®éªŒçš„æœ€æ–°çŠ¶æ€ã€‚</p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/cs/skills/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>åšå®¢ä½¿ç”¨æŠ€å·§</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/read/theory-of-neural-computation/theory-of-neural-computation-2/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>The Hopfield Model</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  
</body>







<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
