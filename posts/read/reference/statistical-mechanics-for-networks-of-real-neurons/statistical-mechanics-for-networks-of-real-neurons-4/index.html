<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Maximmum entropy as a path to connect theory and experiment | æ— å¤„æƒ¹å°˜åŸƒ</title>
<meta name="keywords" content="">
<meta name="description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-4/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-4/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>







<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="Maximmum entropy as a path to connect theory and experiment" />
<meta property="og:description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-4/" />
<meta property="og:image" content="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-12T00:18:23+08:00" />
<meta property="article:modified_time" content="2025-11-12T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" />
<meta name="twitter:title" content="Maximmum entropy as a path to connect theory and experiment"/>
<meta name="twitter:description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ“• é˜…è¯»",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "ğŸ“• æ–‡çŒ®",
          "item": "https://Muatyz.github.io/posts/read/reference/"
        },

        {
          "@type": "ListItem",
          "position":  4 ,
          "name": "ğŸ“• Statistical mechanics for networks of real neurons",
          "item": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/"
        }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "Maximmum entropy as a path to connect theory and experiment",
      "item": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-4/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Maximmum entropy as a path to connect theory and experiment",
  "name": "Maximmum entropy as a path to connect theory and experiment",
  "description": "çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦",
  "keywords": [
    ""
  ],
  "articleBody": " New experimental methods create new opportunities to test our theories. For neural networks, monitoring the electrical activity of tens, hundreds, or thousands of neurons simultaneously should allow us to test statistical approaches to these systems in detail. Doing this requires taking much more seriously the connection between our models and real neurons, a connection that sometimes has been tenuous. Can we really take the spins $\\sigma_{i}$ in Eq (5) to represent the presence or absence of an action potential in cell $i$? We will indeed make this identification, and our goal will be an accurate description of the probability distribution out of which the â€œmicroscopicâ€ states of a large network are drawn. Note that, as in equilibrium statistical mechanics, this would be the beginning and not the end of our understanding.\næ–°çš„å®éªŒæ–¹æ³•ä¸ºæµ‹è¯•æˆ‘ä»¬çš„ç†è®ºåˆ›é€ äº†æ–°çš„æœºä¼šã€‚å¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼ŒåŒæ—¶ç›‘æµ‹æ•°åã€æ•°ç™¾æˆ–æ•°åƒä¸ªç¥ç»å…ƒçš„ç”µæ´»åŠ¨åº”è¯¥å…è®¸æˆ‘ä»¬è¯¦ç»†æµ‹è¯•è¿™äº›ç³»ç»Ÿçš„ç»Ÿè®¡æ–¹æ³•ã€‚åšåˆ°è¿™ä¸€ç‚¹éœ€è¦æˆ‘ä»¬æ›´åŠ è®¤çœŸåœ°å¯¹å¾…æˆ‘ä»¬çš„æ¨¡å‹ä¸çœŸå®ç¥ç»å…ƒä¹‹é—´çš„è”ç³»ï¼Œè€Œè¿™ç§è”ç³»æœ‰æ—¶æ˜¯è„†å¼±çš„ã€‚æˆ‘ä»¬çœŸçš„å¯ä»¥å°†æ–¹ç¨‹ï¼ˆ5ï¼‰ä¸­çš„è‡ªæ—‹ $\\sigma_{i}$ è§†ä¸ºç»†èƒ $i$ ä¸­åŠ¨ä½œç”µä½çš„å­˜åœ¨æˆ–ä¸å­˜åœ¨å—ï¼Ÿæˆ‘ä»¬ç¡®å®ä¼šåšå‡ºè¿™ç§è¯†åˆ«ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‡†ç¡®æè¿°ä»ä¸­æŠ½å–å¤§å‹ç½‘ç»œâ€œå¾®è§‚â€çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¯·æ³¨æ„ï¼Œä¸å¹³è¡¡ç»Ÿè®¡åŠ›å­¦ä¸€æ ·ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬ç†è§£çš„å¼€å§‹ï¼Œè€Œä¸æ˜¯ç»“æŸã€‚\nWe will see that maximum entropy models provide a path that starts with data and constructs models that have a very direct connection to statistical physics. Our focus here is on networks of neurons, but it is important that the same concepts and methods are being used to study a much wider range of living systems, and there are important lessons to be drawn from seeing all these problems as part of the same project (Appendix A).\næœ€å¤§ç†µæ¨¡å‹æä¾›äº†ä¸€æ¡ä»æ•°æ®å¼€å§‹å¹¶æ„å»ºä¸ç»Ÿè®¡ç‰©ç†æœ‰éå¸¸ç›´æ¥è”ç³»çš„æ¨¡å‹çš„è·¯å¾„ã€‚æˆ‘ä»¬è¿™é‡Œçš„é‡ç‚¹æ˜¯ç¥ç»å…ƒç½‘ç»œï¼Œä½†é‡è¦çš„æ˜¯ï¼ŒåŒæ ·çš„æ¦‚å¿µå’Œæ–¹æ³•æ­£åœ¨è¢«ç”¨æ¥ç ”ç©¶æ›´å¹¿æ³›çš„ç”Ÿç‰©ç³»ç»Ÿï¼Œå¹¶ä¸”ä»å°†æ‰€æœ‰è¿™äº›é—®é¢˜è§†ä¸ºåŒä¸€é¡¹ç›®çš„ä¸€éƒ¨åˆ†ä¸­å¯ä»¥å¾—å‡ºé‡è¦çš„æ•™è®­ï¼ˆé™„å½• Aï¼‰ã€‚\nBasics of maximum entropy Consider a network of neurons, labelled by $i = 1, 2, \\cdots , N$ , each with a state $\\sigma_{i}$. In the simplest case where these states of individual neurons are binaryactive/inactive, or spiking/silentâ€”then the network as a whole has access to $\\Omega = 2^{N}$ possible states\n$$ \\vec{\\sigma} = \\{\\sigma_{1},\\sigma_{2},\\cdots,\\sigma_{N}\\}. $$\nThese states mean something to the organism: they may represent sensory inputs, inferred features of the surrounding world, plans, motor commands, recalled memories, or internal thoughts. But before we can build a dictionary for these meanings we need a lexicon, describing which of the possible states actually occur, and how often. More formally, we would like to understand the probability distribution $P(\\vec{\\sigma})$. We might also be interested in sequences of states over time, $P [\\{\\vec{\\sigma}(t_1), \\vec{\\sigma}(t_2),\\cdots \\}]$, but for simplicity we focus first on states at a single moment in time.\nè€ƒè™‘ä¸€ä¸ªç¥ç»å…ƒç½‘ç»œï¼Œæ ‡è®°ä¸º $i = 1, 2, \\cdots , N$ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½æœ‰ä¸€ä¸ªçŠ¶æ€ $\\sigma_{i}$ã€‚åœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œè¿™äº›å•ä¸ªç¥ç»å…ƒçš„çŠ¶æ€æ˜¯äºŒè¿›åˆ¶çš„â€”â€”æ´»è·ƒ/ä¸æ´»è·ƒï¼Œæˆ–å°–å³°/é™é»˜â€”â€”é‚£ä¹ˆæ•´ä¸ªç½‘ç»œå¯ä»¥è®¿é—® $\\Omega = 2^{N}$ ä¸ªå¯èƒ½çš„çŠ¶æ€\n$$ \\vec{\\sigma} = \\{\\sigma_{1},\\sigma_{2},\\cdots,\\sigma_{N}\\}. $$\nè¿™äº›çŠ¶æ€å¯¹æœ‰æœºä½“æ¥è¯´æ˜¯æœ‰æ„ä¹‰çš„ï¼šå®ƒä»¬å¯èƒ½ä»£è¡¨æ„Ÿå®˜è¾“å…¥ã€å‘¨å›´ä¸–ç•Œçš„æ¨æ–­ç‰¹å¾ã€è®¡åˆ’ã€è¿åŠ¨å‘½ä»¤ã€å›å¿†çš„è®°å¿†æˆ–å†…éƒ¨æ€ç»´ã€‚ä½†åœ¨æˆ‘ä»¬èƒ½å¤Ÿä¸ºè¿™äº›å«ä¹‰æ„å»ºè¯å…¸ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œæè¿°å“ªäº›å¯èƒ½çš„çŠ¶æ€å®é™…ä¸Šä¼šå‘ç”Ÿï¼Œä»¥åŠå®ƒä»¬å‘ç”Ÿçš„é¢‘ç‡ã€‚æ›´æ­£å¼åœ°è¯´ï¼Œæˆ‘ä»¬æƒ³è¦ç†è§£æ¦‚ç‡åˆ†å¸ƒ $P(\\vec{\\sigma})$ã€‚æˆ‘ä»¬ä¹Ÿå¯èƒ½å¯¹éšæ—¶é—´å˜åŒ–çš„çŠ¶æ€åºåˆ—æ„Ÿå…´è¶£ï¼Œ$P [\\{\\vec{\\sigma}(t_1), \\vec{\\sigma}(t_2),\\cdots \\}]$ï¼Œä½†ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬é¦–å…ˆå…³æ³¨å•ä¸€æ—¶åˆ»çš„çŠ¶æ€ã€‚\nThe distribution $P(\\vec{\\sigma})$ is a list of $\\Omega$ numbers that sum to one. Even for modest size networks this is a very long list, $\\Omega\\sim 10^{30}$ for $N = 100$. To be clear, there is no way that we can measure all these numbers in any realistic experiment. More deeply, large networks could not visit all of their possible states in the age of the universe, let alone the lifetime of a single organism. This shouldnâ€™t bother us, since one can make similar observations about the states of molecules in the air around us, or the states of all the atoms in a tiny grain of sand. The fact that the number of possible states $\\Omega$ is (beyond) astronomically large does not stop us from asking questions about the distribution from which these states are drawn.\næ¦‚ç‡åˆ†å¸ƒ $P(\\vec{\\sigma})$ æ˜¯ä¸€ä¸ªåŒ…å« $\\Omega$ ä¸ªæ•°å­—çš„åˆ—è¡¨ï¼Œè¿™äº›æ•°å­—çš„æ€»å’Œä¸ºä¸€ã€‚å³ä½¿å¯¹äºé€‚åº¦å¤§å°çš„ç½‘ç»œæ¥è¯´ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸é•¿çš„åˆ—è¡¨ï¼Œå¯¹äº $N = 100$ï¼Œ$\\Omega\\sim 10^{30}$ã€‚æ˜ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬æ— æ³•åœ¨ä»»ä½•ç°å®çš„å®éªŒä¸­æµ‹é‡å‡ºæ‰€æœ‰è¿™äº›æ•°å­—ã€‚æ›´æ·±å±‚æ¬¡çš„æ˜¯ï¼Œå¤§å‹ç½‘ç»œä¸å¯èƒ½åœ¨å®‡å®™çš„å¹´é¾„å†…è®¿é—®å®ƒä»¬æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€ï¼Œæ›´ä¸ç”¨è¯´å•ä¸ªæœ‰æœºä½“çš„å¯¿å‘½äº†ã€‚è¿™ä¸åº”è¯¥å›°æ‰°æˆ‘ä»¬ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥å¯¹æˆ‘ä»¬å‘¨å›´ç©ºæ°”ä¸­çš„åˆ†å­çŠ¶æ€ï¼Œæˆ–å¾®å°æ²™ç²’ä¸­æ‰€æœ‰åŸå­çš„çŠ¶æ€åšå‡ºç±»ä¼¼çš„è§‚å¯Ÿã€‚å¯èƒ½çŠ¶æ€çš„æ•°é‡ $\\Omega$ï¼ˆè¶…å‡ºï¼‰å¤©æ–‡æ•°å­—çº§åˆ«ï¼Œå¹¶ä¸ä¼šé˜»æ­¢æˆ‘ä»¬å¯¹è¿™äº›çŠ¶æ€æ‰€æŠ½å–çš„åˆ†å¸ƒæå‡ºé—®é¢˜ã€‚\nThe enormous value of $\\Omega$ does mean, however, that answering questions about the distribution from which the states are drawn requires the answer to be, in some sense, simpler than it could be. If $P(\\vec{\\sigma})$ really were just a list of $\\Omega$ numbers with no underlying structure, we could never make a meaningful experimental prediction. Progress in the description of manyâ€“body systems depends on the discovery of some regularity or simplicity, and without such simplifying hypotheses nothing can be inferred from any reasonable amount of data. The maximum entropy method is a way of being explicit about our simplifying hypotheses.\nç„¶è€Œï¼Œ$\\Omega$ çš„å·¨å¤§å€¼ç¡®å®æ„å‘³ç€ï¼Œè¦å›ç­”æœ‰å…³çŠ¶æ€æ‰€æŠ½å–çš„åˆ†å¸ƒçš„é—®é¢˜ï¼Œç­”æ¡ˆåœ¨æŸç§æ„ä¹‰ä¸Šå¿…é¡»æ¯”å®ƒå¯èƒ½çš„å½¢å¼æ›´ç®€å•ã€‚å¦‚æœ $P(\\vec{\\sigma})$ çœŸçš„æ˜¯ä¸€ä¸ªæ²¡æœ‰æ½œåœ¨ç»“æ„çš„ $\\Omega$ ä¸ªæ•°å­—çš„åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†æ°¸è¿œæ— æ³•åšå‡ºæœ‰æ„ä¹‰çš„å®éªŒé¢„æµ‹ã€‚å¯¹å¤šä½“ç³»ç»Ÿæè¿°çš„è¿›å±•ä¾èµ–äºæŸç§è§„å¾‹æ€§æˆ–ç®€å•æ€§çš„å‘ç°ï¼Œå¦‚æœæ²¡æœ‰è¿™ç§ç®€åŒ–å‡è®¾ï¼Œä»ä»»ä½•åˆç†æ•°é‡çš„æ•°æ®ä¸­éƒ½æ— æ³•æ¨æ–­å‡ºä»»ä½•ä¸œè¥¿ã€‚æœ€å¤§ç†µæ–¹æ³•æ˜¯ä¸€ç§æ˜ç¡®è¡¨è¾¾æˆ‘ä»¬ç®€åŒ–å‡è®¾çš„æ–¹æ³•ã€‚\nWe can imagine mapping each microscopic state $\\vec{\\sigma}$ into some perhaps more macroscopic observable $f(\\vec{\\sigma})$, and from reasonable experiments we should be able to estimate the average of this observable $\\langle f(\\vec{\\sigma})\\rangle_{\\text{expt}}$. If we think this observable is an important and meaningful quantity, it makes sense to insist that any theory we write down for the distribution $P(\\vec{\\sigma})$ should predict this expectation value correctly,\n$$ \\langle f(\\vec{\\sigma})\\rangle_{P} = \\sum_{\\vec{\\sigma}} P(\\vec{\\sigma})f(\\vec{\\sigma}) = \\langle f(\\vec{\\sigma})\\rangle_{\\text{expt}}. $$\nThere might be several such meaningful observables, so we should have\n$$ \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{P} \\equiv \\sum_{\\vec{\\sigma}}P(\\vec{\\sigma})f_{\\mu}(\\vec{\\sigma}) = \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}} $$\nfor $\\mu = 1, 2, \\cdots , K$. These are strong constraints, but so long as the number of these observables $K \\ll \\Omega$ there are infinitely many distributions consistent with Eq (17). How do we choose among them?\næˆ‘ä»¬å¯ä»¥æƒ³è±¡å°†æ¯ä¸ªå¾®è§‚çŠ¶æ€ $\\vec{\\sigma}$ æ˜ å°„åˆ°æŸä¸ªå¯èƒ½æ›´å®è§‚çš„å¯è§‚å¯Ÿé‡ $f(\\vec{\\sigma})$ï¼Œå¹¶ä¸”é€šè¿‡åˆç†çš„å®éªŒï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿä¼°è®¡å‡ºè¿™ä¸ªå¯è§‚å¯Ÿé‡çš„å¹³å‡å€¼ $\\langle f(\\vec{\\sigma})\\rangle_{\\text{expt}}$ã€‚å¦‚æœæˆ‘ä»¬è®¤ä¸ºè¿™ä¸ªå¯è§‚å¯Ÿé‡æ˜¯ä¸€ä¸ªé‡è¦ä¸”æœ‰æ„ä¹‰çš„é‡ï¼Œé‚£ä¹ˆåšæŒä»»ä½•æˆ‘ä»¬ä¸ºåˆ†å¸ƒ $P(\\vec{\\sigma})$ å†™ä¸‹çš„ç†è®ºéƒ½åº”è¯¥æ­£ç¡®é¢„æµ‹è¿™ä¸ªæœŸæœ›å€¼æ˜¯æœ‰æ„ä¹‰çš„ï¼Œ\n$$ \\langle f(\\vec{\\sigma})\\rangle_{P} = \\sum_{\\vec{\\sigma}} P(\\vec{\\sigma})f(\\vec{\\sigma}) = \\langle f(\\vec{\\sigma})\\rangle_{\\text{expt}}. $$\nå¯èƒ½ä¼šæœ‰å‡ ä¸ªè¿™æ ·çš„æœ‰æ„ä¹‰çš„å¯è§‚å¯Ÿé‡ï¼Œå› æ­¤æˆ‘ä»¬åº”è¯¥æœ‰\n$$ \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{P} \\equiv \\sum_{\\vec{\\sigma}}P(\\vec{\\sigma})f_{\\mu}(\\vec{\\sigma}) = \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}} $$\nå¯¹äº $\\mu = 1, 2, \\cdots , K$ã€‚è¿™äº›æ˜¯å¼ºçº¦æŸï¼Œä½†åªè¦è¿™äº›å¯è§‚å¯Ÿé‡çš„æ•°é‡ $K \\ll \\Omega$ï¼Œå°±æœ‰æ— æ•°ä¸ªä¸æ–¹ç¨‹ï¼ˆ17ï¼‰ä¸€è‡´çš„åˆ†å¸ƒã€‚æˆ‘ä»¬å¦‚ä½•åœ¨å®ƒä»¬ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼Ÿ\nThere are many ways of saying, in words, how we would like to make our choice among the $P (\\sigma)$ that are consistent with the measured expectation values of observables. We would like to pick the simplest or least structured model. We would like not to inject into our model any information beyond what is given to us by the measurements $\\{\\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}}\\}$. From a different point of view, we would like drawing states out of the distribution $P(\\vec{\\sigma})$ to generate samples that are as random as possible while still obeying the constraints in Eq (17). It might seem that each choice of words generates a new discussionâ€”what do we mean, mathematically, by â€œleast structured,â€ or â€œas random as possibleâ€?\nåœ¨ä¸è§‚æµ‹é‡çš„æµ‹é‡æœŸæœ›å€¼ä¸€è‡´çš„ $P (\\sigma)$ ä¹‹é—´è¿›è¡Œé€‰æ‹©æ—¶ï¼Œæœ‰è®¸å¤šæ–¹æ³•å¯ä»¥ç”¨è¯­è¨€è¡¨è¾¾ã€‚æˆ‘ä»¬å¸Œæœ›é€‰æ‹©æœ€ç®€å•æˆ–ç»“æ„æœ€å°‘çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›ä¸è¦å°†ä»»ä½•è¶…å‡ºæµ‹é‡ $\\{\\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}}\\}$ æ‰€æä¾›çš„ä¿¡æ¯æ³¨å…¥åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚ä»ä¸åŒçš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¸Œæœ›ä»åˆ†å¸ƒ $P(\\vec{\\sigma})$ ä¸­æŠ½å–çŠ¶æ€ï¼Œä»¥ç”Ÿæˆå°½å¯èƒ½éšæœºçš„æ ·æœ¬ï¼ŒåŒæ—¶ä»ç„¶éµå®ˆæ–¹ç¨‹ï¼ˆ17ï¼‰ä¸­çš„çº¦æŸã€‚ä¼¼ä¹æ¯ç§æªè¾éƒ½ä¼šå¼•å‘æ–°çš„è®¨è®ºâ€”â€”æˆ‘ä»¬åœ¨æ•°å­¦ä¸Šæ˜¯ä»€ä¹ˆæ„æ€ï¼Œâ€œç»“æ„æœ€å°‘â€æˆ–â€œå°½å¯èƒ½éšæœºâ€ï¼Ÿ\nIntroductory courses in statistical mechanics make some remarks about entropy as a measure of our ignorance about the microscopic state of a system, but this connection often is left quite vague. In laying the foundations of information theory, Shannon made this connection precise (Shannon, 1948). If we ask a question, we have the intuition that we â€œgain informationâ€ when we hear the answer. If we want to attach a number to this information gain, then the unique measure that is consistent with natural constraints is the entropy of the distribution out of which the answers are drawn. Thus, if we ask for the microscopic state of a system, the information we gain on hearing the answer is (on average) the entropy of the distribution over these microscopic states. Conversely, if the entropy is less than its maximum possible value, this reduction in entropy measures how much we already know about the microscopic state even before we see it. As a result, for states to be as random as possibleâ€”to be sure that we do not inject extra information about these statesâ€”we need to find the distribution that has the maximum entropy.\nç»Ÿè®¡åŠ›å­¦çš„å…¥é—¨è¯¾ç¨‹å¯¹ç†µä½œä¸ºæˆ‘ä»¬å¯¹ç³»ç»Ÿå¾®è§‚çŠ¶æ€æ— çŸ¥çš„åº¦é‡åšäº†ä¸€äº›è¯„è®ºï¼Œä½†è¿™ç§è”ç³»é€šå¸¸ç›¸å½“æ¨¡ç³Šã€‚åœ¨å¥ å®šä¿¡æ¯ç†è®ºåŸºç¡€æ—¶ï¼Œé¦™å†œï¼ˆShannonï¼Œ1948ï¼‰ä½¿è¿™ç§è”ç³»å˜å¾—ç²¾ç¡®ã€‚å¦‚æœæˆ‘ä»¬æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æœ‰ä¸€ç§ç›´è§‰ï¼Œå½“æˆ‘ä»¬å¬åˆ°ç­”æ¡ˆæ—¶ï¼Œæˆ‘ä»¬â€œè·å¾—ä¿¡æ¯â€ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¸ºè¿™ç§ä¿¡æ¯å¢ç›Šé™„åŠ ä¸€ä¸ªæ•°å­—ï¼Œé‚£ä¹ˆä¸è‡ªç„¶çº¦æŸä¸€è‡´çš„å”¯ä¸€åº¦é‡å°±æ˜¯ä»ä¸­æŠ½å–ç­”æ¡ˆçš„åˆ†å¸ƒçš„ç†µã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬è¯¢é—®ç³»ç»Ÿçš„å¾®è§‚çŠ¶æ€ï¼Œé‚£ä¹ˆåœ¨å¬åˆ°ç­”æ¡ˆæ—¶æˆ‘ä»¬è·å¾—çš„ä¿¡æ¯ï¼ˆå¹³å‡è€Œè¨€ï¼‰å°±æ˜¯è¿™äº›å¾®è§‚çŠ¶æ€åˆ†å¸ƒçš„ç†µã€‚ç›¸åï¼Œå¦‚æœç†µå°äºå…¶æœ€å¤§å¯èƒ½å€¼ï¼Œè¿™ç§ç†µçš„å‡å°‘è¡¡é‡äº†å³ä½¿åœ¨çœ‹åˆ°å®ƒä¹‹å‰æˆ‘ä»¬å·²ç»çŸ¥é“äº†å¤šå°‘å…³äºå¾®è§‚çŠ¶æ€çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿çŠ¶æ€å°½å¯èƒ½éšæœºâ€”â€”ç¡®ä¿æˆ‘ä»¬ä¸ä¼šæ³¨å…¥å…³äºè¿™äº›çŠ¶æ€çš„é¢å¤–ä¿¡æ¯â€”â€”æˆ‘ä»¬éœ€è¦æ‰¾åˆ°å…·æœ‰æœ€å¤§ç†µçš„åˆ†å¸ƒã€‚\nMaximizing the entropy subject to constraints defines a variational problem, maximizing\n$$ \\widetilde{S} = -\\sum_{\\vec{\\sigma}}P(\\vec{\\sigma})\\ln{P(\\vec{\\sigma})} - \\sum_{\\mu = 1}^{K}\\left[\\sum_{\\sigma}P(\\vec{\\sigma})f_{\\mu}(\\vec{\\sigma}) - \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}}\\right] - \\lambda_{0}\\left[\\sum_{\\vec{\\sigma}}P(\\vec{\\sigma}) - 1\\right] $$\nwhere the $\\lambda_{\\mu}$ are Lagrange multipliers. We include an additional term ($\\propto \\lambda_{0}$) to constrain the normalization, so we can treat each entry in the distribution as an independent variable. Then\n$$ \\begin{aligned} \\frac{\\delta \\widetilde{S}}{\\delta P(\\vec{\\sigma})} \u0026= 0\\\\ \\Rightarrow P(\\vec{\\sigma}) \u0026= \\frac{1}{Z(\\{\\lambda_{\\mu}\\})}\\exp{[-E(\\vec{\\sigma})]}\\\\ E(\\vec{\\sigma}) \u0026= \\sum_{\\mu = 1}^{K}\\lambda_{\\mu}f_{\\mu}(\\vec{\\sigma}) \\end{aligned} $$\nThus the model we are looking for is equivalent to an equilibrium statistical mechanics problem in which the â€œenergyâ€ is a sum of terms, one for each of the observables whose expectation values we constrain; the Lagrange multipliers become coupling constants in the effective energy. To finish the construction we need to adjust these couplings $\\{\\lambda_{\\mu}\\}$ to satisfy Eq (17), and in general this is a hard problem; see Appendix B. Importantly, if we have some set of expectation values that we are matching, and we want to add one more, this just adds one more term to the form of the energy function, but in general implementing this extra constraint requires adjusting all the coupling constants.\næœ€å¤§åŒ–å—çº¦æŸçš„ç†µå®šä¹‰äº†ä¸€ä¸ªå˜åˆ†é—®é¢˜ï¼Œæœ€å¤§åŒ–\n$$ \\widetilde{S} = -\\sum_{\\vec{\\sigma}}P(\\vec{\\sigma})\\ln{P(\\vec{\\sigma})} - \\sum_{\\mu = 1}^{K}\\left[\\sum_{\\sigma}P(\\vec{\\sigma})f_{\\mu}(\\vec{\\sigma}) - \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}}\\right] - \\lambda_{0}\\left[\\sum_{\\vec{\\sigma}}P(\\vec{\\sigma}) - 1\\right] $$\nå…¶ä¸­ $\\lambda_{\\mu}$ æ˜¯æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ã€‚æˆ‘ä»¬åŒ…æ‹¬ä¸€ä¸ªé¢å¤–çš„é¡¹ï¼ˆ$\\propto \\lambda_{0}$ï¼‰æ¥çº¦æŸå½’ä¸€åŒ–ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†åˆ†å¸ƒä¸­çš„æ¯ä¸ªæ¡ç›®è§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„å˜é‡ã€‚ç„¶å\n$$ \\begin{aligned} \\frac{\\delta \\widetilde{S}}{\\delta P(\\vec{\\sigma})} \u0026= 0\\\\ \\Rightarrow P(\\vec{\\sigma}) \u0026= \\frac{1}{Z(\\{\\lambda_{\\mu}\\})}\\exp{[-E(\\vec{\\sigma})]}\\\\ E(\\vec{\\sigma}) \u0026= \\sum_{\\mu = 1}^{K}\\lambda_{\\mu}f_{\\mu}(\\vec{\\sigma}) \\end{aligned} $$\nå› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾çš„æ¨¡å‹ç­‰ä»·äºä¸€ä¸ªå¹³è¡¡ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œå…¶ä¸­â€œèƒ½é‡â€æ˜¯å„ä¸ªå¯è§‚å¯Ÿé‡çš„æœŸæœ›å€¼ä¹‹å’Œï¼›æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æˆä¸ºæœ‰æ•ˆèƒ½é‡ä¸­çš„è€¦åˆå¸¸æ•°ã€‚ä¸ºäº†å®Œæˆæ„å»ºï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´è¿™äº›è€¦åˆ $\\{\\lambda_{\\mu}\\}$ ä»¥æ»¡è¶³æ–¹ç¨‹ï¼ˆ17ï¼‰ï¼Œé€šå¸¸è¿™æ˜¯ä¸€ä¸ªå›°éš¾çš„é—®é¢˜ï¼›è§é™„å½• Bã€‚é‡è¦çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ç»„æˆ‘ä»¬æ­£åœ¨åŒ¹é…çš„æœŸæœ›å€¼ï¼Œå¹¶ä¸”æˆ‘ä»¬æƒ³å†æ·»åŠ ä¸€ä¸ªï¼Œè¿™åªä¼šåœ¨èƒ½é‡å‡½æ•°çš„å½¢å¼ä¸­æ·»åŠ ä¸€ä¸ªé¢å¤–çš„é¡¹ï¼Œä½†é€šå¸¸å®ç°è¿™ä¸ªé¢å¤–çš„çº¦æŸéœ€è¦è°ƒæ•´æ‰€æœ‰çš„è€¦åˆå¸¸æ•°ã€‚\nTo make the connections explicit, recall that we can define thermodynamic equilibrium as the state of maximum entropy given the constraint of fixed mean energy. This optimization problem is solved by the Boltzmann distribution. In this view the (inverse) temperature is a Lagrange multiplier that enforces the energy constraint, opposite to usual view of controlling the temperature and predicting the energy. The Boltzmann distribution generalizes if other expectation values are constrained (Landau and Lifshitz, 1977).\nä¸ºäº†æ˜ç¡®è¿æ¥ï¼Œå›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†çƒ­åŠ›å­¦å¹³è¡¡å®šä¹‰ä¸ºåœ¨å›ºå®šå¹³å‡èƒ½é‡çº¦æŸä¸‹çš„æœ€å¤§ç†µçŠ¶æ€ã€‚è¿™ä¸ªä¼˜åŒ–é—®é¢˜ç”± Boltzmann åˆ†å¸ƒè§£å†³ã€‚åœ¨è¿™ç§è§‚ç‚¹ä¸­ï¼Œï¼ˆåï¼‰æ¸©åº¦æ˜¯ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œç”¨äºå¼ºåˆ¶æ‰§è¡Œèƒ½é‡çº¦æŸï¼Œè¿™ä¸é€šå¸¸æ§åˆ¶æ¸©åº¦å¹¶é¢„æµ‹èƒ½é‡çš„è§‚ç‚¹ç›¸åã€‚å¦‚æœçº¦æŸäº†å…¶ä»–æœŸæœ›å€¼ï¼ŒBoltzmann åˆ†å¸ƒä¼šè¿›è¡Œæ¨å¹¿ï¼ˆLandau å’Œ Lifshitzï¼Œ1977ï¼‰ã€‚\nThe maximum entropy argument gives us the form of the probability distribution, but we also need the coupling constants. We can think of this as being an â€œinverse statistical mechanicsâ€ problem, since we are given expectation values or correlation functions and need to find the couplings, rather than the other way around. Different formulations of this problem have a long history in the mathematical physics community (Chayes et al., 1984; Keller and Zumino, 1959; Kunkin and Firsch, 1969). An early application to living systems involved reconstructing the forces that hold together the array of gap junction proteins which bridge the membranes of two cells in contact (Braun et al., 1984). As attention focused on networks of neurons, finding the relevant coupling constants came to be described as the â€œinverse Isingâ€ problem, as will become clear below.\næœ€å¤§ç†µè®ºç»™äº†æˆ‘ä»¬æ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œä½†æˆ‘ä»¬ä¹Ÿéœ€è¦è€¦åˆå¸¸æ•°ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸º â€œé€†ç»Ÿè®¡åŠ›å­¦â€ é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬ç»™å‡ºäº†æœŸæœ›å€¼æˆ–ç›¸å…³å‡½æ•°ï¼Œå¹¶ä¸”éœ€è¦æ‰¾åˆ°è€¦åˆï¼Œè€Œä¸æ˜¯ç›¸åã€‚è¿™ä¸ªé—®é¢˜çš„ä¸åŒè¡¨è¿°åœ¨æ•°å­¦ç‰©ç†å­¦ç•Œæœ‰ç€æ‚ ä¹…çš„å†å²ï¼ˆChayes ç­‰äººï¼Œ1984ï¼›Keller å’Œ Zuminoï¼Œ1959ï¼›Kunkin å’Œ Firschï¼Œ1969ï¼‰ã€‚å¯¹ç”Ÿç‰©ç³»ç»Ÿçš„æ—©æœŸåº”ç”¨æ¶‰åŠé‡å»ºå°†ä¸¤ä¸ªæ¥è§¦ç»†èƒçš„è†œè¿æ¥åœ¨ä¸€èµ·çš„é—´éš™è¿æ¥è›‹ç™½é˜µåˆ—çš„åŠ›ï¼ˆBraun ç­‰äººï¼Œ1984ï¼‰ã€‚éšç€æ³¨æ„åŠ›é›†ä¸­åœ¨ç¥ç»å…ƒç½‘ç»œä¸Šï¼Œæ‰¾åˆ°ç›¸å…³çš„è€¦åˆå¸¸æ•°è¢«æè¿°ä¸º â€œé€† Isingâ€ é—®é¢˜ï¼Œæ­£å¦‚ä¸‹é¢å°†å˜å¾—æ¸…æ¥šçš„é‚£æ ·ã€‚\nIn statistical physics there is in some sense a force driving systems toward equilibrium, as encapsulated in the Hâ€“theorem. In many cases this force triumphs, and what we see is a state with maximal entropy subject only to a very few constraints. In the networks of neurons that we study here, there is no Hâ€“theorem, and the list of constraints will be quite long compared to what we are used to in thermodynamics. This means that the probability distributions we write down will be mathematically equivalent to some equilibrium statistical mechanics problem, but they do not describe an equilibrium state of the system we are actually studying. This somewhat subtle relationship between maximum entropy as a description of thermal equilibrium and maximum entropy as a tool for inference was outlined long ago by Jaynes (1957, 1982).\nåœ¨ç»Ÿè®¡ç‰©ç†å­¦ä¸­ï¼Œåœ¨æŸç§æ„ä¹‰ä¸Šæœ‰ä¸€ç§é©±åŠ¨åŠ›å°†ç³»ç»Ÿæ¨å‘å¹³è¡¡ï¼Œæ­£å¦‚ H å®šç†æ‰€æ¦‚æ‹¬çš„é‚£æ ·ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¿™ç§åŠ›é‡å–å¾—äº†èƒœåˆ©ï¼Œæˆ‘ä»¬æ‰€çœ‹åˆ°çš„æ˜¯ä¸€ä¸ªä»…å—å¾ˆå°‘çº¦æŸçš„æœ€å¤§ç†µçŠ¶æ€ã€‚åœ¨æˆ‘ä»¬è¿™é‡Œç ”ç©¶çš„ç¥ç»å…ƒç½‘ç»œä¸­ï¼Œæ²¡æœ‰ H å®šç†ï¼Œå¹¶ä¸”ä¸æˆ‘ä»¬åœ¨çƒ­åŠ›å­¦ä¸­ä¹ æƒ¯çš„ç›¸æ¯”ï¼Œçº¦æŸåˆ—è¡¨å°†ç›¸å½“é•¿ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å†™ä¸‹çš„æ¦‚ç‡åˆ†å¸ƒåœ¨æ•°å­¦ä¸Šç­‰ä»·äºæŸä¸ªå¹³è¡¡ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œä½†å®ƒä»¬å¹¶ä¸æè¿°æˆ‘ä»¬å®é™…ç ”ç©¶çš„ç³»ç»Ÿçš„å¹³è¡¡çŠ¶æ€ã€‚Jaynesï¼ˆ1957ï¼Œ1982ï¼‰æ—©å·²æ¦‚è¿°äº†æœ€å¤§ç†µä½œä¸ºçƒ­å¹³è¡¡æè¿°å’Œæœ€å¤§ç†µä½œä¸ºæ¨ç†å·¥å…·ä¹‹é—´è¿™ç§å¾®å¦™çš„å…³ç³»ã€‚\nIf we donâ€™t have any constraints then the maximum entropy distribution is uniform over all $\\Omega$ states. Each observable whose expectation value we constrain lowers the maximum allowed value of the entropy, and if we add enough constraints we eventually reach the true entropy and hence the true distribution. Often it make sense to group the observables into oneâ€“body, twoâ€“body, threebody terms, etc.. Having constrained all the kâ€“body observables for $k\\leq K$, the maximum entropy model makes parameterâ€“free predictions for correlations among groups of $k \u003e K$ variables. This provides a powerful path to testing the model, and defines a natural generalization of connected correlations (Schneidman et al., 2003).\nå¦‚æœæˆ‘ä»¬æ²¡æœ‰ä»»ä½•çº¦æŸï¼Œé‚£ä¹ˆæœ€å¤§ç†µåˆ†å¸ƒåœ¨æ‰€æœ‰ $\\Omega$ çŠ¶æ€ä¸Šæ˜¯å‡åŒ€çš„ã€‚æˆ‘ä»¬çº¦æŸçš„æ¯ä¸ªå¯è§‚å¯Ÿé‡çš„æœŸæœ›å€¼éƒ½ä¼šé™ä½å…è®¸çš„æœ€å¤§ç†µå€¼ï¼Œå¦‚æœæˆ‘ä»¬æ·»åŠ è¶³å¤Ÿçš„çº¦æŸï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šè¾¾åˆ°çœŸå®çš„ç†µå€¼ï¼Œä»è€Œå¾—åˆ°çœŸå®çš„åˆ†å¸ƒã€‚é€šå¸¸ï¼Œå°†å¯è§‚å¯Ÿé‡åˆ†ä¸ºå•ä½“ã€äºŒä½“ã€ä¸‰ä½“ç­‰é¡¹æ˜¯æœ‰æ„ä¹‰çš„ã€‚åœ¨çº¦æŸäº†æ‰€æœ‰ $k\\leq K$ çš„ $k$ ä½“å¯è§‚å¯Ÿé‡ä¹‹åï¼Œæœ€å¤§ç†µæ¨¡å‹å¯¹ $k \u003e K$ å˜é‡ç»„ä¹‹é—´çš„ç›¸å…³æ€§åšå‡ºæ— å‚æ•°é¢„æµ‹ã€‚è¿™ä¸ºæµ‹è¯•æ¨¡å‹æä¾›äº†ä¸€æ¡å¼ºæœ‰åŠ›çš„è·¯å¾„ï¼Œå¹¶å®šä¹‰äº†è¿æ¥ç›¸å…³æ€§çš„è‡ªç„¶æ¨å¹¿ï¼ˆSchneidman ç­‰äººï¼Œ2003ï¼‰ã€‚\nThe connection of maximum entropy models to the Boltzmann distribution gives us intuition and practical computational tools. It can also leave the impression that we are describing a system in equilibrium, which would be a disaster. In fact the maximum entropy distribution describes thermal equilibrium only if the observable that we constrain is the energy in the mechanical sense. There is no obstacle to building maximum entropy models for the distribution of states in a nonâ€“equilibrium system.\næœ€å¤§ç†µæ¨¡å‹ä¸ Boltzmann åˆ†å¸ƒçš„è”ç³»ä¸ºæˆ‘ä»¬æä¾›äº†ç›´è§‰å’Œå®ç”¨çš„è®¡ç®—å·¥å…·ã€‚å®ƒä¹Ÿå¯èƒ½ç»™äººç•™ä¸‹æˆ‘ä»¬æ­£åœ¨æè¿°ä¸€ä¸ªå¹³è¡¡ç³»ç»Ÿçš„å°è±¡ï¼Œè¿™å°†æ˜¯ç¾éš¾æ€§çš„ã€‚äº‹å®ä¸Šï¼Œåªæœ‰å½“æˆ‘ä»¬çº¦æŸçš„å¯è§‚å¯Ÿé‡æ˜¯æœºæ¢°æ„ä¹‰ä¸Šçš„èƒ½é‡æ—¶ï¼Œæœ€å¤§ç†µåˆ†å¸ƒæ‰æè¿°çƒ­å¹³è¡¡ã€‚æ„å»ºéå¹³è¡¡ç³»ç»Ÿä¸­çŠ¶æ€åˆ†å¸ƒçš„æœ€å¤§ç†µæ¨¡å‹æ²¡æœ‰éšœç¢ã€‚\nAlthough we can usefully think of states distributed over an energy landscape, as we have formulated the maximum entropy construction this description works for states at one moment in time. Thus we cannot conclude that the dynamics by which the system moves from one state to another are analogous to Brownian motion on the effective energy surface. There are infinitely many models for the dynamics that are consistent with this description, and most of these will not obey detailed balance. Recent work shows how to explore a large family of dynamical models consistent with the maximum entropy distribution, and applies these ideas to collective animal behavior (Chen et al., 2023). There also are generalizations of the maximum entropy method to describe distributions of trajectories, as we discuss below (Â§IV.D); maximum entropy models for trajectories sometimes are called maximum caliber (Ghosh et al., 2020; Press Ìe et al., 2013). Finally we note that, for better or worse, the symmetries that are central to many problems in statistical physics in general are absent from the systems we will be studying; flocks and swarms are an exception, as discussed in Â§A.2.\nå°½ç®¡æˆ‘ä»¬å¯ä»¥æœ‰ç”¨åœ°å°†çŠ¶æ€åˆ†å¸ƒè§†ä¸ºèƒ½é‡æ™¯è§‚ï¼Œä½†æ­£å¦‚æˆ‘ä»¬æ‰€åˆ¶å®šçš„æœ€å¤§ç†µæ„é€ ï¼Œè¿™ç§æè¿°é€‚ç”¨äºæŸä¸€æ—¶åˆ»çš„çŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½å¾—å‡ºç³»ç»Ÿä»ä¸€ä¸ªçŠ¶æ€ç§»åŠ¨åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„åŠ¨åŠ›å­¦ç±»ä¼¼äºæœ‰æ•ˆèƒ½é‡è¡¨é¢ä¸Šçš„å¸ƒæœ—è¿åŠ¨çš„ç»“è®ºã€‚æœ‰æ— æ•°ç§åŠ¨åŠ›å­¦æ¨¡å‹ä¸è¿™ç§æè¿°ä¸€è‡´ï¼Œå…¶ä¸­å¤§å¤šæ•°ä¸ä¼šéµå®ˆè¯¦ç»†å¹³è¡¡ã€‚ æœ€è¿‘çš„å·¥ä½œå±•ç¤ºäº†å¦‚ä½•æ¢ç´¢ä¸æœ€å¤§ç†µåˆ†å¸ƒä¸€è‡´çš„å¤§é‡åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œå¹¶å°†è¿™äº›æ€æƒ³åº”ç”¨äºé›†ä½“åŠ¨ç‰©è¡Œä¸ºï¼ˆChen ç­‰äººï¼Œ2023ï¼‰ã€‚æœ€å¤§ç†µæ–¹æ³•ä¹Ÿæœ‰æ¨å¹¿ï¼Œç”¨äºæè¿°è½¨è¿¹åˆ†å¸ƒï¼Œæ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·ï¼ˆÂ§IV.Dï¼‰ï¼›è½¨è¿¹çš„æœ€å¤§ç†µæ¨¡å‹æœ‰æ—¶è¢«ç§°ä¸ºæœ€å¤§å£å¾„ï¼ˆGhosh ç­‰äººï¼Œ2020ï¼›Press Ìe ç­‰äººï¼Œ2013ï¼‰ã€‚æœ€åæˆ‘ä»¬æ³¨æ„åˆ°ï¼Œæ— è®ºå¥½åï¼Œè®¸å¤šç»Ÿè®¡ç‰©ç†å­¦é—®é¢˜ä¸­è‡³å…³é‡è¦çš„å¯¹ç§°æ€§åœ¨æˆ‘ä»¬å°†è¦ç ”ç©¶çš„ç³»ç»Ÿä¸­æ˜¯ç¼ºå¤±çš„ï¼›å¦‚ Â§A.2 æ‰€è®¨è®ºçš„ï¼Œé¸Ÿç¾¤å’Œè™«ç¾¤æ˜¯ä¸€ä¸ªä¾‹å¤–ã€‚\nTo conclude this introduction, we emphasize that maximum entropy is unlike usual theories. We donâ€™t start with a theoretical principle or even a model. Rather, we start with some features of the data and test the hypothesis that these features alone encode everything we need to describe the system. Whenever we use this approach we are referring back to the basic structure of the optimization problem defined in Eq (18), and its formal solution in Eqs (20, 21), but there is no single maximum entropy model, and each time we need to be explicit: Which are the observables $f_{\\mu}$ whose measured expectation values we want our model to reproduce? Can we find the corresponding Lagrange mutlipliers $\\lambda_{mu}$? Do these parameters have a natural interpretation? Once we answer these questions, we can ask whether these relatively simple statistical physics descriptions make predictions that agree with experiment. There is an unusually clean separation between learning the model (matching observed expectation values) and testing the model (predicting new expectation values). In this sense we can think of maximum entropy as predicting a set of parameter free relations among different aspects of the data. Finally, we will have to think carefully about what it means for models to â€œwork.â€ We begin with early explorations at relatively small $N$ (Â§IV.B), then turn to a wide variety of larger networks (Â§IV.C), and finally address how these analyses can catch up to the experimental frontier (Â§IV.D).\nä¸ºäº†ç»“æŸè¿™ä¸ªä»‹ç»ï¼Œæˆ‘ä»¬å¼ºè°ƒæœ€å¤§ç†µä¸åŒäºé€šå¸¸çš„ç†è®ºã€‚æˆ‘ä»¬ä¸æ˜¯ä»ä¸€ä¸ªç†è®ºåŸåˆ™ç”šè‡³ä¸€ä¸ªæ¨¡å‹å¼€å§‹ã€‚ç›¸åï¼Œæˆ‘ä»¬ä»æ•°æ®çš„ä¸€äº›ç‰¹å¾å¼€å§‹ï¼Œå¹¶æµ‹è¯•è¿™äº›ç‰¹å¾æ˜¯å¦ç¼–ç äº†æè¿°ç³»ç»Ÿæ‰€éœ€çš„ä¸€åˆ‡çš„å‡è®¾ã€‚æ¯å½“æˆ‘ä»¬ä½¿ç”¨è¿™ç§æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šå›åˆ°æ–¹ç¨‹ï¼ˆ18ï¼‰ä¸­å®šä¹‰çš„ä¼˜åŒ–é—®é¢˜çš„åŸºæœ¬ç»“æ„åŠå…¶åœ¨æ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰ä¸­çš„æ­£å¼è§£ï¼Œä½†æ²¡æœ‰å•ä¸€çš„æœ€å¤§ç†µæ¨¡å‹ï¼Œæ¯æ¬¡æˆ‘ä»¬éƒ½éœ€è¦æ˜ç¡®ï¼šå“ªäº›æ˜¯æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹é‡ç°å…¶æµ‹é‡æœŸæœ›å€¼çš„å¯è§‚å¯Ÿé‡ $f_{\\mu}$ï¼Ÿæˆ‘ä»¬èƒ½æ‰¾åˆ°ç›¸åº”çš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•° $\\lambda_{mu}$ å—ï¼Ÿè¿™äº›å‚æ•°æœ‰è‡ªç„¶çš„è§£é‡Šå—ï¼Ÿä¸€æ—¦æˆ‘ä»¬å›ç­”äº†è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å°±å¯ä»¥é—®è¿™äº›ç›¸å¯¹ç®€å•çš„ç»Ÿè®¡ç‰©ç†æè¿°æ˜¯å¦åšå‡ºäº†ä¸å®éªŒä¸€è‡´çš„é¢„æµ‹ã€‚åœ¨å­¦ä¹ æ¨¡å‹ï¼ˆåŒ¹é…è§‚å¯Ÿåˆ°çš„æœŸæœ›å€¼ï¼‰å’Œæµ‹è¯•æ¨¡å‹ï¼ˆé¢„æµ‹æ–°çš„æœŸæœ›å€¼ï¼‰ä¹‹é—´å­˜åœ¨ä¸€ç§å¼‚å¸¸æ¸…æ™°çš„åˆ†ç¦»ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè¯´ï¼Œæˆ‘ä»¬å¯ä»¥å°†æœ€å¤§ç†µè§†ä¸ºé¢„æµ‹æ•°æ®ä¸åŒæ–¹é¢ä¹‹é—´çš„ä¸€ç»„æ— å‚æ•°å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ä»”ç»†æ€è€ƒæ¨¡å‹â€œå·¥ä½œâ€çš„å«ä¹‰ã€‚æˆ‘ä»¬ä»ç›¸å¯¹è¾ƒå° $N$ çš„æ—©æœŸæ¢ç´¢å¼€å§‹ï¼ˆÂ§IV.Bï¼‰ï¼Œç„¶åè½¬å‘å„ç§æ›´å¤§çš„ç½‘ç»œï¼ˆÂ§IV.Cï¼‰ï¼Œæœ€åè§£å†³è¿™äº›åˆ†æå¦‚ä½•èµ¶ä¸Šå®éªŒå‰æ²¿çš„é—®é¢˜ï¼ˆÂ§IV.Dï¼‰ã€‚\nFirst connections to neurons Suppose we observe three neurons, and measure their mean activity as well as their pairwise correlations. Given these measurements, should we be surprised by how often the three neurons are active together? Maximum entropy provides a way of answering this question, generating a â€œnull modelâ€ prediction assuming all the correlation structure is captured in the pairs, and this was appreciated âˆ¼2000 (Martignon et al., 2000). Over the next several years a more ambitious idea emerged: could we build maximum entropy models for patterns of activity in larger populations of neurons? The first target for this analysis was a population of neurons in the salamander retina, as it responds to naturalistic visual inputs (Schneidman et al., 2006).\nå‡è®¾æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸‰ä¸ªç¥ç»å…ƒï¼Œå¹¶æµ‹é‡å®ƒä»¬çš„å¹³å‡æ´»åŠ¨ä»¥åŠå®ƒä»¬çš„æˆå¯¹ç›¸å…³æ€§ã€‚é‰´äºè¿™äº›æµ‹é‡ç»“æœï¼Œæˆ‘ä»¬æ˜¯å¦åº”è¯¥å¯¹è¿™ä¸‰ä¸ªç¥ç»å…ƒä¸€èµ·æ´»è·ƒçš„é¢‘ç‡æ„Ÿåˆ°æƒŠè®¶ï¼Ÿæœ€å¤§ç†µæä¾›äº†ä¸€ç§å›ç­”è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•ï¼Œç”Ÿæˆä¸€ä¸ªâ€œé›¶æ¨¡å‹â€é¢„æµ‹ï¼Œå‡è®¾æ‰€æœ‰çš„ç›¸å…³ç»“æ„éƒ½åŒ…å«åœ¨å¯¹ä¸­ï¼Œè¿™åœ¨å¤§çº¦ 2000 å¹´è¢«è®¤è¯†åˆ°ï¼ˆMartignon ç­‰äººï¼Œ2000ï¼‰ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ å¹´é‡Œï¼Œä¸€ä¸ªæ›´é›„å¿ƒå‹ƒå‹ƒçš„æƒ³æ³•å‡ºç°äº†ï¼šæˆ‘ä»¬èƒ½å¦ä¸ºæ›´å¤§ç¾¤ä½“çš„ç¥ç»å…ƒæ´»åŠ¨æ¨¡å¼æ„å»ºæœ€å¤§ç†µæ¨¡å‹ï¼Ÿè¿™ç§åˆ†æçš„ç¬¬ä¸€ä¸ªç›®æ ‡æ˜¯è¾èˆè§†ç½‘è†œä¸­çš„ä¸€ç¾¤ç¥ç»å…ƒï¼Œå› ä¸ºå®ƒå¯¹è‡ªç„¶è§†è§‰è¾“å…¥åšå‡ºååº”ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚\nIn response to natural movies, the output neurons of the retinaâ€”the â€œganglion cellsâ€ that carry visual signals from eye to brain, and which as a group form the optic nerveâ€”are sparsely activated, generating an average of just a few spikes per second each (Fig 7A, B). Those initial experiments monitored populations of up to forty neurons in a small patch of the retina, with recordings of up to one hour. Pairs of neurons have temporal correlations with a relatively sharp peak or trough on a broad background that tracks longer timescales in the visual input (Fig 7C). If we discretize time into bins of $\\Delta\\tau = 20$ ms then we capture most of the short time correlations but still have a very low probability of seeing two spikes in the same bin, so that responses of neuron i become binary, $\\sigma_{i} = \\{0, 1\\}$.\nä¸ºäº†å“åº”è‡ªç„¶æ´»åŠ¨ï¼Œè§†ç½‘è†œçš„è¾“å‡ºç¥ç»å…ƒâ€”â€”å°†è§†è§‰ä¿¡å·ä»çœ¼ç›ä¼ é€’åˆ°å¤§è„‘çš„â€œç¥ç»èŠ‚ç»†èƒâ€ï¼Œå®ƒä»¬ä½œä¸ºä¸€ä¸ªæ•´ä½“å½¢æˆè§†ç¥ç»â€”â€”è¢«ç¨€ç–æ¿€æ´»ï¼Œæ¯ä¸ªç¥ç»å…ƒå¹³å‡æ¯ç§’åªäº§ç”Ÿå‡ ä¸ªå°–å³°ï¼ˆå›¾ 7Aï¼ŒBï¼‰ã€‚é‚£äº›åˆå§‹å®éªŒç›‘æµ‹äº†è§†ç½‘è†œå°å—ä¸­å¤šè¾¾å››åä¸ªç¥ç»å…ƒçš„ç¾¤ä½“ï¼Œè®°å½•æ—¶é—´é•¿è¾¾ä¸€å°æ—¶ã€‚æˆå¯¹çš„ç¥ç»å…ƒå…·æœ‰æ—¶é—´ç›¸å…³æ€§ï¼Œåœ¨è§†è§‰è¾“å…¥çš„è¾ƒé•¿æ—¶é—´å°ºåº¦ä¸Šè·Ÿè¸ªè¾ƒå®½èƒŒæ™¯ä¸Šçš„ç›¸å¯¹å°–é”å³°å€¼æˆ–è°·å€¼ï¼ˆå›¾ 7Cï¼‰ã€‚å¦‚æœæˆ‘ä»¬å°†æ—¶é—´ç¦»æ•£åŒ–ä¸º $\\Delta\\tau = 20$ ms çš„æ—¶é—´æ®µï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ•æ‰åˆ°äº†å¤§éƒ¨åˆ†çŸ­æ—¶é—´ç›¸å…³æ€§ï¼Œä½†ä»ç„¶å¾ˆå°‘çœ‹åˆ°åŒä¸€æ—¶é—´æ®µå†…æœ‰ä¸¤ä¸ªå°–å³°ï¼Œå› æ­¤ç¥ç»å…ƒ i çš„å“åº”å˜ä¸ºäºŒè¿›åˆ¶ï¼Œ$\\sigma_{i} = \\{0, 1\\}$ã€‚\nFIG. 7 Responses of the salamander retina to naturalistic movies (Schneidman et al., 2006). (A) Raster plot of the action potentials from $N = 40$ neurons. Each dot represents a spike from one cell. (B) Expanded view of the green box in (A), showing the discretization of time into bins of width $\\Delta\\tau = 20$ms. The result (bottom) is that the state of the network is a binary word $\\{\\sigma_{i}\\}$. (C) Correlations between two neurons. Results are shown as the probability per unit time of a spike in cell $j$ (spike rate) given that there is a spike in cell $i$ at time $t = 0$; the plateau at long times should be the mean rate $r_{j} = \\langle\\sigma_{j}\\rangle/\\Delta\\tau$. There a peak with a width ~ 100 ms, related to time scales in the visual input, and a peak with width ~ 20ms emphasizes in the inset; this motivates the choice of bins size. (D) Distribution of (off-diagonal) correlation coefficients, from Eq (24), across the population of $N = 40$ neurons. (E) Probability that $K$ out of the $N = 40$ neurons are active in the same time bin (red) compared with expectations if activity of each neuron were independent of all the others (blue). Dashed lines are exponential (red) and Poisson (blue), to guide the eye. (F) Predicted occurrence rates of different binary patterns vs the observed rates, for the independent model $P_{1}$ [Eqs (29, 30), blue] and the pairwise maximum entropy model $P_{2}$ [Eqs (35, 33), red].\nå›¾ 7 è¾èˆè§†ç½‘è†œå¯¹è‡ªç„¶ç”µå½±çš„å“åº”ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚(A) $N = 40$ ä¸ªç¥ç»å…ƒçš„åŠ¨ä½œç”µä½å…‰æ …å›¾ã€‚æ¯ä¸ªç‚¹ä»£è¡¨ä¸€ä¸ªç»†èƒçš„ä¸€ä¸ªå°–å³°ã€‚(B) (A) ä¸­ç»¿è‰²æ¡†çš„æ”¾å¤§è§†å›¾ï¼Œæ˜¾ç¤ºæ—¶é—´è¢«ç¦»æ•£åŒ–ä¸ºå®½åº¦ä¸º $\\Delta\\tau = 20$ms çš„æ—¶é—´æ®µã€‚ç»“æœï¼ˆåº•éƒ¨ï¼‰æ˜¯ç½‘ç»œçš„çŠ¶æ€æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶å­— $\\{\\sigma_{i}\\}$ã€‚(C) ä¸¤ä¸ªç¥ç»å…ƒä¹‹é—´çš„ç›¸å…³æ€§ã€‚ç»“æœæ˜¾ç¤ºä¸ºåœ¨ç»†èƒ $i$ åœ¨æ—¶é—´ $t = 0$ å¤„æœ‰ä¸€ä¸ªå°–å³°çš„æƒ…å†µä¸‹ï¼Œç»†èƒ $j$ ä¸­å°–å³°çš„å•ä½æ—¶é—´æ¦‚ç‡ï¼ˆå°–å³°ç‡ï¼‰ï¼›é•¿æ—¶é—´å¤„çš„å¹³å°åº”è¯¥æ˜¯å¹³å‡ç‡ $r_{j} = \\langle\\sigma_{j}\\rangle/\\Delta\\tau$ã€‚æœ‰ä¸€ä¸ªå®½åº¦çº¦ä¸º 100 ms çš„å³°å€¼ï¼Œä¸è§†è§‰è¾“å…¥ä¸­çš„æ—¶é—´å°ºåº¦æœ‰å…³ï¼Œæ’å›¾ä¸­å¼ºè°ƒäº†ä¸€ä¸ªå®½åº¦çº¦ä¸º 20ms çš„å³°å€¼ï¼›è¿™æ¿€å‘äº†é€‰æ‹©ç®±å­å¤§å°çš„åŠ¨æœºã€‚(D) è·¨è¶Š $N = 40$ ä¸ªç¥ç»å…ƒç¾¤ä½“çš„ï¼ˆéå¯¹è§’çº¿ï¼‰ç›¸å…³ç³»æ•°çš„åˆ†å¸ƒï¼Œæ¥è‡ªæ–¹ç¨‹ï¼ˆ24ï¼‰ã€‚(E) åœ¨åŒä¸€æ—¶é—´æ®µå†… $N = 40$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªæ´»è·ƒçš„æ¦‚ç‡ï¼ˆçº¢è‰²ï¼‰ä¸å¦‚æœæ¯ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–ç¥ç»å…ƒçš„é¢„æœŸï¼ˆè“è‰²ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚è™šçº¿åˆ†åˆ«ä¸ºæŒ‡æ•°ï¼ˆçº¢è‰²ï¼‰å’Œæ³Šæ¾ï¼ˆè“è‰²ï¼‰ï¼Œä»¥å¼•å¯¼çœ¼ç›ã€‚(F) ä¸åŒäºŒè¿›åˆ¶æ¨¡å¼çš„é¢„æµ‹å‘ç”Ÿç‡ä¸è§‚å¯Ÿåˆ°çš„å‘ç”Ÿç‡ï¼Œå¯¹äºç‹¬ç«‹æ¨¡å‹ $P_{1}$ [æ–¹ç¨‹ï¼ˆ29ï¼Œ30ï¼‰ï¼Œè“è‰²] å’Œæˆå¯¹æœ€å¤§ç†µæ¨¡å‹ $P_{2}$ [æ–¹ç¨‹ï¼ˆ35ï¼Œ33ï¼‰ï¼Œçº¢è‰²]ã€‚\nIf we define as usual the fluctuations around the mean,\n$$ \\delta\\sigma_{i} = \\sigma_{i} - \\langle\\sigma_{i}\\rangle, $$\nthen the data sets were large enough to get good estimates of the covariance\n$$ C_{ij} = \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle = \\langle\\sigma_{i}\\sigma_{j}\\rangle_{c} $$\nwhere $\\langle\\cdots\\rangle_{c}$ denotes the connected part of the correlations; in many cases we have more intuition about the correlation matrix\n$$ \\widetilde{C}_{ij} = \\frac{C_{ij}}{\\sqrt{C_{ii}C_{jj}}} $$\nImportantly, these pairwise correlations are weak: almost all of the $|\\widetilde{C}_{i\\neq j}|\u003c0.1$, and the bulk of these correlations are just a few percent (Fig 7D). The recordings are long enough that these weak correlations are statistically significant, and almost none of the matrix elements are zero within errors. Correlations thus are weak and widespread, which seems to be common across many different regions of the brain.\nå¦‚æœæˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·å®šä¹‰å›´ç»•å‡å€¼çš„æ³¢åŠ¨ï¼Œ\n$$ \\delta\\sigma_{i} = \\sigma_{i} - \\langle\\sigma_{i}\\rangle, $$\né‚£ä¹ˆæ•°æ®é›†è¶³å¤Ÿå¤§ï¼Œå¯ä»¥å¾ˆå¥½åœ°ä¼°è®¡åæ–¹å·®\n$$ C_{ij} = \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle = \\langle\\sigma_{i}\\sigma_{j}\\rangle_{c} $$\nå…¶ä¸­ $\\langle\\cdots\\rangle_{c}$ è¡¨ç¤ºç›¸å…³æ€§çš„è¿æ¥éƒ¨åˆ†ï¼›åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯¹ç›¸å…³çŸ©é˜µæœ‰æ›´å¤šçš„ç›´è§‰\n$$ \\widetilde{C}_{ij} = \\frac{C_{ij}}{\\sqrt{C_{ii}C_{jj}}} $$\né‡è¦çš„æ˜¯ï¼Œè¿™äº›æˆå¯¹ç›¸å…³æ€§æ˜¯å¼±çš„ï¼šå‡ ä¹æ‰€æœ‰çš„ $|\\widetilde{C}_{i\\neq j}|\u003c0.1$ï¼Œè€Œä¸”è¿™äº›ç›¸å…³æ€§çš„ä¸»ä½“åªæœ‰å‡ ä¸ªç™¾åˆ†ç‚¹ï¼ˆå›¾ 7Dï¼‰ã€‚è®°å½•æ—¶é—´è¶³å¤Ÿé•¿ï¼Œè¿™äº›å¾®å¼±çš„ç›¸å…³æ€§åœ¨ç»Ÿè®¡ä¸Šæ˜¯æ˜¾è‘—çš„ï¼Œå¹¶ä¸”çŸ©é˜µå…ƒç´ å‡ ä¹æ²¡æœ‰åœ¨è¯¯å·®èŒƒå›´å†…ä¸ºé›¶ã€‚å› æ­¤ï¼Œç›¸å…³æ€§æ˜¯å¼±è€Œå¹¿æ³›çš„ï¼Œè¿™ä¼¼ä¹åœ¨å¤§è„‘çš„è®¸å¤šä¸åŒåŒºåŸŸä¸­å¾ˆå¸¸è§ã€‚\nIf we look just at two neurons, the approximation that they are independent of one another is very good, because the correlations are so weak. But if we look more globally then the widespread correlations combine to have qualitative effects. As an example, we can ask for the probability that $K$ out of $N = 40$ neurons are active in the same time bin, $P_{N}(K)$, and we find that this has a much longer tail than expected if the cells were independent (Fig 7E); simultaneous activity of $K = 10$ neurons already is $\\sim 10^{3}\\times$ more likely than in the independent model.\nå¦‚æœæˆ‘ä»¬åªçœ‹ä¸¤ä¸ªç¥ç»å…ƒï¼Œç”±äºç›¸å…³æ€§éå¸¸å¼±ï¼Œå®ƒä»¬ç›¸äº’ç‹¬ç«‹çš„è¿‘ä¼¼æ˜¯éå¸¸å¥½çš„ã€‚ä½†å¦‚æœæˆ‘ä»¬æ›´å…¨é¢åœ°è§‚å¯Ÿï¼Œé‚£ä¹ˆå¹¿æ³›çš„ç›¸å…³æ€§ä¼šç»“åˆèµ·æ¥äº§ç”Ÿå®šæ€§çš„å½±å“ã€‚ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è¯¢é—®åœ¨åŒä¸€æ—¶é—´æ®µå†… $N = 40$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªæ´»è·ƒçš„æ¦‚ç‡ $P_{N}(K)$ï¼Œæˆ‘ä»¬å‘ç°å¦‚æœç»†èƒæ˜¯ç‹¬ç«‹çš„ï¼Œè¿™ä¸ªæ¦‚ç‡çš„å°¾éƒ¨è¦é•¿å¾—å¤šï¼ˆå›¾ 7Eï¼‰ï¼›$K = 10$ ä¸ªç¥ç»å…ƒçš„åŒæ—¶æ´»åŠ¨å·²ç»æ¯”ç‹¬ç«‹æ¨¡å‹é«˜å‡ºçº¦ $10^{3}$ å€ã€‚\nIf we focus on $N = 10$ neurons then the experiments are long enough to sample all $\\Omega\\sim 10^{3}$ states, and the probabilities of these different binary words depart dramatically from the predictions of an independent model (Fig 7F). If we group the different binary words by the total number of active neurons, then the predictions of the independent model actually are antiâ€“correlated with the real data. We emphasize that these failures occur despite the fact that pairwise correlations are weak, and that they are visible at a relatively modest $N = 10$.\nå¦‚æœæˆ‘ä»¬å…³æ³¨ $N = 10$ ä¸ªç¥ç»å…ƒï¼Œé‚£ä¹ˆå®éªŒæ—¶é—´è¶³å¤Ÿé•¿ï¼Œå¯ä»¥å¯¹æ‰€æœ‰ $\\Omega\\sim 10^{3}$ çŠ¶æ€è¿›è¡Œé‡‡æ ·ï¼Œå¹¶ä¸”è¿™äº›ä¸åŒäºŒè¿›åˆ¶å­—çš„æ¦‚ç‡ä¸ç‹¬ç«‹æ¨¡å‹çš„é¢„æµ‹æœ‰æ˜¾è‘—åç¦»ï¼ˆå›¾ 7Fï¼‰ã€‚å¦‚æœæˆ‘ä»¬æŒ‰æ´»è·ƒç¥ç»å…ƒçš„æ€»æ•°å¯¹ä¸åŒçš„äºŒè¿›åˆ¶å­—è¿›è¡Œåˆ†ç»„ï¼Œé‚£ä¹ˆç‹¬ç«‹æ¨¡å‹çš„é¢„æµ‹å®é™…ä¸Šä¸çœŸå®æ•°æ®å‘ˆè´Ÿç›¸å…³ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œå°½ç®¡æˆå¯¹ç›¸å…³æ€§å¾ˆå¼±ï¼Œä½†è¿™äº›å¤±è´¥ä»ç„¶å‘ç”Ÿï¼Œå¹¶ä¸”å®ƒä»¬åœ¨ç›¸å¯¹é€‚åº¦çš„ $N = 10$ ä¸‹æ˜¯å¯è§çš„ã€‚\nIf we want to build a model for the patterns of activity in networks of neurons it certainly makes sense to insist that we match the mean activity of each cell. At the risk of being pedantic, what this means explicitly is that we are looking for a probability distribution over network states, $P_{1}(\\vec{\\sigma})$ that has the maximum entropy while correctly predicting the expectation values\n$$ m_{i} \\equiv \\langle\\sigma_{i}\\rangle_{\\text{expt}} = \\langle\\sigma_{i}\\rangle_{P_{1}} $$\nReferring back to Eq (17), the observables that we constrain become\n$$ \\{f_{\\mu}^{(1)}\\}\\rightarrow \\{\\sigma_{i}\\} $$\nnote that $i = 1, 2,\\cdots, N$, where $N$ is the number of neurons. To implement these constraints we need one Lagrange multiplier for each neuron, and it is convenient to write this multiplier as an â€œeffective fieldâ€ $h_{i}$, so that the general Eqs (20, 21) become\n$$ \\begin{aligned} P_{1}(\\vec{\\sigma}) \u0026= \\frac{1}{Z_{1}}\\exp{[-E_{1}(\\vec{\\sigma})]}\\\\ E_{1}(\\vec{\\sigma}) \u0026= \\sum_{\\mu}\\lambda_{\\mu}^{(1)}f_{\\mu}^{(1)}\\\\ \u0026= \\sum_{i=1}^{N}h_{i}\\sigma_{i} \\end{aligned} $$\nWe notice that $E_1$ is the energy function for independent spins in local fields, and so the probability distribution over states factorizes,\n$$ P_{1}(\\vec{\\sigma}) \\propto \\prod_{i=1}^{N}e^{-h_{i}\\sigma_{i}} $$\nThus a maximum entropy model which matches only the mean activities of individual neurons is a model in which the activity of each cell is independent of all the others. We have seen that this model is in dramatic disagreement with the data.\nå¦‚æœæˆ‘ä»¬æƒ³ä¸ºç¥ç»å…ƒç½‘ç»œä¸­çš„æ´»åŠ¨æ¨¡å¼æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼ŒåšæŒåŒ¹é…æ¯ä¸ªç»†èƒçš„å¹³å‡æ´»åŠ¨å½“ç„¶æ˜¯æœ‰æ„ä¹‰çš„ã€‚å†’ç€å•°å—¦çš„é£é™©ï¼Œè¿™æ˜ç¡®æ„å‘³ç€æˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªç½‘ç»œçŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒ $P_{1}(\\vec{\\sigma})$ï¼Œè¯¥åˆ†å¸ƒå…·æœ‰æœ€å¤§ç†µï¼ŒåŒæ—¶æ­£ç¡®é¢„æµ‹æœŸæœ›å€¼\n$$ m_{i} \\equiv \\langle\\sigma_{i}\\rangle_{\\text{expt}} = \\langle\\sigma_{i}\\rangle_{P_{1}} $$\nå›åˆ°æ–¹ç¨‹ï¼ˆ17ï¼‰ï¼Œæˆ‘ä»¬çº¦æŸçš„å¯è§‚å¯Ÿé‡å˜ä¸º\n$$ \\{f_{\\mu}^{(1)}\\}\\rightarrow \\{\\sigma_{i}\\} $$\næ³¨æ„ $i = 1, 2,\\cdots, N$ï¼Œå…¶ä¸­ $N$ æ˜¯ç¥ç»å…ƒçš„æ•°é‡ã€‚ä¸ºäº†å®ç°è¿™äº›çº¦æŸï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªç¥ç»å…ƒä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œå¹¶ä¸”å°†è¿™ä¸ªä¹˜æ•°å†™æˆâ€œæœ‰æ•ˆåœºâ€ $h_{i}$ æ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œå› æ­¤ä¸€èˆ¬çš„æ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰å˜ä¸º\n$$ \\begin{aligned} P_{1}(\\vec{\\sigma}) \u0026= \\frac{1}{Z_{1}}\\exp{[-E_{1}(\\vec{\\sigma})]}\\\\ E_{1}(\\vec{\\sigma}) \u0026= \\sum_{\\mu}\\lambda_{\\mu}^{(1)}f_{\\mu}^{(1)}\\\\ \u0026= \\sum_{i=1}^{N}h_{i}\\sigma_{i} \\end{aligned} $$\næˆ‘ä»¬æ³¨æ„åˆ° $E_1$ æ˜¯å±€éƒ¨åœºä¸­ç‹¬ç«‹è‡ªæ—‹çš„èƒ½é‡å‡½æ•°ï¼Œå› æ­¤çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥åˆ†è§£ä¸ºï¼Œ\n$$ P_{1}(\\vec{\\sigma}) \\propto \\prod_{i=1}^{N}e^{-h_{i}\\sigma_{i}} $$\nå› æ­¤ï¼Œä¸€ä¸ªä»…åŒ¹é…å•ä¸ªç¥ç»å…ƒå¹³å‡æ´»åŠ¨çš„æœ€å¤§ç†µæ¨¡å‹æ˜¯ä¸€ä¸ªæ¯ä¸ªç»†èƒçš„æ´»åŠ¨éƒ½ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–ç»†èƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°è¿™ä¸ªæ¨¡å‹ä¸æ•°æ®æœ‰æ˜¾è‘—çš„ä¸ä¸€è‡´ã€‚\nA natural first step in trying to capture the nonindependence of neurons is to build a maximum entropy model that matches pairwise correlations. Thus, we are looking for a distribution $P_{2}(\\sigma)$ that has maximum entropy while matching the mean activities as in Eq (25) and also the covariance of activity\n$$ C_{ij}\\equiv \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle_{\\text{expt}} = \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle_{P_{2}} $$\nIn the language of Eq (17) this means that we have a second set of relevant observables\n$$ \\{f_{\\nu}^{(2)}\\} \\rightarrow \\{\\sigma_{i}\\sigma_{j}\\} $$\nAs before we need one Lagrange multiplier for each constrained observable, and it is useful to think of the Lagrange multiplier that constrains $\\sigma_{i}\\sigma_{j}$ as being a â€œspinspinâ€ coupling $\\lambda_{ij} = J_{ij}$. Recalling that each extra constraint adds a term to the effective energy function, Eqs (20, 21) become\n$$ \\begin{aligned} P_{2}(\\vec{\\sigma}) \u0026= \\frac{1}{Z_{2}(\\{h_{i};J_{ij}\\})}e^{-E_{2}(\\vec{\\sigma})}\\\\ E_{2}(\\vec{\\sigma}) \u0026= \\sum_{\\mu}\\lambda_{\\mu}^{(1)}f_{\\mu}^{(1)} + \\sum_{\\mu}\\lambda_{\\mu}^{(2)}f_{\\mu}^{(2)}\\\\ \u0026= \\sum_{i=1}^{N}h_{i}\\sigma_{i} + \\frac{1}{2}\\sum_{i\\neq j}J_{ij}\\sigma_{i}\\sigma_{j} \\end{aligned} $$\nThis is exactly an Ising model with pairwise interactions among the spinsâ€”not an analogy but a mathematical equivalence.\næ•æ‰ç¥ç»å…ƒéç‹¬ç«‹æ€§çš„ä¸€ä¸ªè‡ªç„¶ç¬¬ä¸€æ­¥æ˜¯æ„å»ºä¸€ä¸ªåŒ¹é…æˆå¯¹ç›¸å…³æ€§çš„æœ€å¤§ç†µæ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªåˆ†å¸ƒ $P_{2}(\\sigma)$ï¼Œè¯¥åˆ†å¸ƒå…·æœ‰æœ€å¤§ç†µï¼ŒåŒæ—¶åŒ¹é…æ–¹ç¨‹ï¼ˆ25ï¼‰ä¸­çš„å¹³å‡æ´»åŠ¨ä»¥åŠæ´»åŠ¨çš„åæ–¹å·®\n$$ C_{ij}\\equiv \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle_{\\text{expt}} = \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle_{P_{2}} $$\nç”¨æ–¹ç¨‹ï¼ˆ17ï¼‰çš„è¯­è¨€æ¥è¯´ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æœ‰ç¬¬äºŒç»„ç›¸å…³çš„å¯è§‚å¯Ÿé‡\n$$ \\{f_{\\nu}^{(2)}\\} \\rightarrow \\{\\sigma_{i}\\sigma_{j}\\} $$\nåƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªå—çº¦æŸçš„å¯è§‚å¯Ÿé‡ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œå¹¶ä¸”å°†çº¦æŸ $\\sigma_{i}\\sigma_{j}$ çš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°è§†ä¸ºâ€œè‡ªæ—‹-è‡ªæ—‹â€è€¦åˆ $\\lambda_{ij} = J_{ij}$ æ˜¯æœ‰ç”¨çš„ã€‚å›æƒ³ä¸€ä¸‹ï¼Œæ¯ä¸ªé¢å¤–çš„çº¦æŸéƒ½ä¼šå‘æœ‰æ•ˆèƒ½é‡å‡½æ•°æ·»åŠ ä¸€é¡¹ï¼Œæ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰å˜ä¸º\n$$ \\begin{aligned} P_{2}(\\vec{\\sigma}) \u0026= \\frac{1}{Z_{2}(\\{h_{i};J_{ij}\\})}e^{-E_{2}(\\vec{\\sigma})}\\\\ E_{2}(\\vec{\\sigma}) \u0026= \\sum_{\\mu}\\lambda_{\\mu}^{(1)}f_{\\mu}^{(1)} + \\sum_{\\mu}\\lambda_{\\mu}^{(2)}f_{\\mu}^{(2)}\\\\ \u0026= \\sum_{i=1}^{N}h_{i}\\sigma_{i} + \\frac{1}{2}\\sum_{i\\neq j}J_{ij}\\sigma_{i}\\sigma_{j} \\end{aligned} $$\nè¿™æ­£æ˜¯å…·æœ‰è‡ªæ—‹ä¹‹é—´æˆå¯¹ç›¸äº’ä½œç”¨çš„ Ising æ¨¡å‹â€”â€”ä¸æ˜¯ç±»æ¯”ï¼Œè€Œæ˜¯æ•°å­¦ç­‰ä»·ã€‚\nIsing models for networks of neurons have a long history, as described in Â§II.C. In their earliest appearance, these models emerged from a hypothetical, simplified model of the underlying dynamics. Here they emerge as the least structured models consistent with measured properties of the network. As a result, we arrive not at some arbitrary Ising model, where we are free to choose the fields and couplings, but at a particular model that describes the actual network of neurons we are observing. To complete this construction we have to adjust the fields and couplings to match the observed mean activities and correlations. Concretely we have to solve Eqs (25, 31), which can be rewritten as\n$$ \\begin{aligned} \\langle \\sigma_{i}\\rangle_{\\text{expt}} \u0026= \\langle\\sigma_{i}\\rangle_{P_{2}} = \\frac{\\partial \\ln{Z_{2}(\\{h_{i};J_{ij}\\})}}{\\partial h_{i}}\\\\ \\langle\\sigma_{i}\\sigma_{j}\\rangle_{\\text{expt}} \u0026= \\langle\\sigma_{i}\\sigma_{j}\\rangle_{P_{2}} = \\frac{\\partial \\ln{Z_{2}(\\{h_{i};J_{ij}\\})}}{\\partial J_{ij}} \\end{aligned} $$\nWith $N = 10$ neurons this is challenging but can be done exactly, since the partition function is a sum over just $\\Omega\\sim 1000$ terms. Once we are done, the model is specified completely. Anything that we compute is a prediction, and there is no room to adjust parameters in search of better agreement with the data.\nç¥ç»å…ƒç½‘ç»œçš„ Ising æ¨¡å‹æœ‰ç€æ‚ ä¹…çš„å†å²ï¼Œå¦‚ Â§II.C æ‰€è¿°ã€‚åœ¨å®ƒä»¬æœ€æ—©å‡ºç°æ—¶ï¼Œè¿™äº›æ¨¡å‹æºè‡ªå¯¹æ½œåœ¨åŠ¨åŠ›å­¦çš„å‡è®¾æ€§ç®€åŒ–æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œå®ƒä»¬ä½œä¸ºä¸ç½‘ç»œçš„æµ‹é‡å±æ€§ä¸€è‡´çš„æœ€å°‘ç»“æ„åŒ–æ¨¡å‹å‡ºç°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸æ˜¯å¾—åˆ°æŸä¸ªä»»æ„çš„ Ising æ¨¡å‹ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å¯ä»¥è‡ªç”±é€‰æ‹©åœºå’Œè€¦åˆï¼Œè€Œæ˜¯å¾—åˆ°ä¸€ä¸ªæè¿°æˆ‘ä»¬æ­£åœ¨è§‚å¯Ÿçš„å®é™…ç¥ç»å…ƒç½‘ç»œçš„ç‰¹å®šæ¨¡å‹ã€‚ä¸ºäº†å®Œæˆè¿™ä¸ªæ„å»ºï¼Œæˆ‘ä»¬å¿…é¡»è°ƒæ•´åœºå’Œè€¦åˆä»¥åŒ¹é…è§‚å¯Ÿåˆ°çš„å¹³å‡æ´»åŠ¨å’Œç›¸å…³æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¿…é¡»è§£å†³æ–¹ç¨‹ï¼ˆ25ï¼Œ31ï¼‰ï¼Œå®ƒä»¬å¯ä»¥é‡å†™ä¸º\n$$ \\begin{aligned} \\langle \\sigma_{i}\\rangle_{\\text{expt}} \u0026= \\langle\\sigma_{i}\\rangle_{P_{2}} = \\frac{\\partial \\ln{Z_{2}(\\{h_{i};J_{ij}\\})}}{\\partial h_{i}}\\\\ \\langle\\sigma_{i}\\sigma_{j}\\rangle_{\\text{expt}} \u0026= \\langle\\sigma_{i}\\sigma_{j}\\rangle_{P_{2}} = \\frac{\\partial \\ln{Z_{2}(\\{h_{i};J_{ij}\\})}}{\\partial J_{ij}} \\end{aligned} $$\nå¯¹äº $N = 10$ ä¸ªç¥ç»å…ƒæ¥è¯´ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†å¯ä»¥ç²¾ç¡®å®Œæˆï¼Œå› ä¸ºé…åˆ†å‡½æ•°åªæ˜¯ $\\Omega\\sim 1000$ é¡¹çš„æ€»å’Œã€‚ä¸€æ—¦æˆ‘ä»¬å®Œæˆï¼Œæ¨¡å‹å°±å®Œå…¨æŒ‡å®šäº†ã€‚æˆ‘ä»¬è®¡ç®—çš„ä»»ä½•ä¸œè¥¿éƒ½æ˜¯ä¸€ä¸ªé¢„æµ‹ï¼Œæ²¡æœ‰è°ƒæ•´å‚æ•°ä»¥å¯»æ±‚ä¸æ•°æ®æ›´å¥½ä¸€è‡´çš„ä½™åœ°ã€‚\nAs noted above, with $N = 10$ neurons the experiments are long enough to get a reasonably full sampling of the probability distribution over $\\vec{\\sigma}$. This provides the most detailed possible test of the model $P_{2}$, and in Fig 7F we see that the agreement between theory and experiment is excellent, except for very rare patterns where errors in the estimate of the probability are larger. Similar results are obtained for other groups of $N = 10$ cells drawn out of the full population of $N = 40$. Quantitatively we can measure the Jensenâ€“Shannon divergence between the estimated distribution $P_{\\text{data}}(\\sigma)$ and the model $P_{2}(\\sigma)$; across multiple choices of ten cells this fluctuates by a factor of two around $D_{JS} = 0.001$ bits, which means that it takes thousands of independent observations to distinguish the model from the data.\næ­£å¦‚ä¸Šé¢æ‰€æŒ‡å‡ºçš„ï¼Œå¯¹äº $N = 10$ ä¸ªç¥ç»å…ƒï¼Œå®éªŒæ—¶é—´è¶³å¤Ÿé•¿ï¼Œå¯ä»¥å¯¹ $\\vec{\\sigma}$ ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œåˆç†å®Œæ•´çš„é‡‡æ ·ã€‚è¿™ä¸ºæ¨¡å‹ $P_{2}$ æä¾›äº†æœ€è¯¦ç»†çš„å¯èƒ½æµ‹è¯•ï¼Œåœ¨å›¾ 7F ä¸­æˆ‘ä»¬çœ‹åˆ°ç†è®ºä¸å®éªŒä¹‹é—´çš„ä¸€è‡´æ€§éå¸¸å¥½ï¼Œé™¤äº†éå¸¸ç½•è§çš„æ¨¡å¼ï¼Œå…¶ä¸­æ¦‚ç‡ä¼°è®¡çš„è¯¯å·®è¾ƒå¤§ã€‚ä»å®Œæ•´çš„ $N = 40$ ç¾¤ä½“ä¸­æŠ½å–çš„å…¶ä»– $N = 10$ ä¸ªç»†èƒç»„ä¹Ÿå¾—åˆ°äº†ç±»ä¼¼çš„ç»“æœã€‚æˆ‘ä»¬å¯ä»¥å®šé‡åœ°æµ‹é‡ä¼°è®¡åˆ†å¸ƒ $P_{\\text{data}}(\\sigma)$ ä¸æ¨¡å‹ $P_{2}(\\sigma)$ ä¹‹é—´çš„ Jensenâ€“Shannon æ•£åº¦ï¼›åœ¨å¤šä¸ªåä¸ªç»†èƒçš„é€‰æ‹©ä¸­ï¼Œè¿™ä¸ªå€¼å›´ç»• $D_{JS} = 0.001$ æ¯”ç‰¹æ³¢åŠ¨äº†ä¸¤å€ï¼Œè¿™æ„å‘³ç€éœ€è¦æ•°åƒæ¬¡ç‹¬ç«‹è§‚å¯Ÿæ‰èƒ½åŒºåˆ†æ¨¡å‹ä¸æ•°æ®ã€‚\nThe architecture of the retina is such that many individual output neurons can be driven or inhibited by a single common neuron that is internal to the circuitry. This is one of many reasons that one might expect significant combinatorial regulation in the patterns of activity, and there were serious efforts to search for these effects (Schnitzer and Meister, 2003). The success of a pairwise model thus came as a considerable surprise.\nè§†ç½‘è†œçš„ç»“æ„ä½¿å¾—è®¸å¤šä¸ªä½“è¾“å‡ºç¥ç»å…ƒå¯ä»¥è¢«ç”µè·¯å†…éƒ¨çš„å•ä¸ªå…±åŒç¥ç»å…ƒé©±åŠ¨æˆ–æŠ‘åˆ¶ã€‚è¿™æ˜¯è®¸å¤šåŸå› ä¹‹ä¸€ï¼Œäººä»¬å¯èƒ½ä¼šæœŸæœ›åœ¨æ´»åŠ¨æ¨¡å¼ä¸­å­˜åœ¨æ˜¾è‘—çš„ç»„åˆè°ƒèŠ‚ï¼Œå¹¶ä¸”æ›¾ç»æœ‰è®¤çœŸåŠªåŠ›å»å¯»æ‰¾è¿™äº›æ•ˆåº”ï¼ˆSchnitzer å’Œ Meisterï¼Œ2003ï¼‰ã€‚å› æ­¤ï¼Œæˆå¯¹æ¨¡å‹çš„æˆåŠŸä»¤äººç›¸å½“æƒŠè®¶ã€‚\nThe results in the salamander retina, with natural inputs, were quickly confirmed in the primate retina using simpler inputs (Shlens et al., 2006). Those experiments covered a larger area and thus could focus on subâ€“populations of neurons belonging to a single class, which are arrayed in a relatively regular lattice. In this case not only did the pairwise model work very well, but the effective interactions $J_{ij}$ were confined largely to nearest neighbors on this lattice.\nè¾èˆè§†ç½‘è†œä¸­ä½¿ç”¨è‡ªç„¶è¾“å…¥çš„ç»“æœå¾ˆå¿«åœ¨çµé•¿ç±»åŠ¨ç‰©è§†ç½‘è†œä¸­å¾—åˆ°äº†ç¡®è®¤ï¼Œä½¿ç”¨äº†æ›´ç®€å•çš„è¾“å…¥ï¼ˆShlens ç­‰äººï¼Œ2006ï¼‰ã€‚è¿™äº›å®éªŒè¦†ç›–äº†æ›´å¤§çš„åŒºåŸŸï¼Œå› æ­¤å¯ä»¥ä¸“æ³¨äºå±äºå•ä¸€ç±»åˆ«çš„ç¥ç»å…ƒäºšç¾¤ï¼Œè¿™äº›ç¥ç»å…ƒæ’åˆ—åœ¨ä¸€ä¸ªç›¸å¯¹è§„åˆ™çš„æ™¶æ ¼ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸ä»…æˆå¯¹æ¨¡å‹æ•ˆæœéå¸¸å¥½ï¼Œè€Œä¸”æœ‰æ•ˆç›¸äº’ä½œç”¨ $J_{ij}$ ä¸»è¦å±€é™äºè¯¥æ™¶æ ¼ä¸Šçš„æœ€è¿‘é‚»ã€‚\nPairwise maximum entropy models also were reasonably successful in describing patterns of activity across $N\\leq 10$ neurons sampled from a cluster of cortical neurons kept alive in a dish (Tang et al., 2008). This work also pointed to the fact that dynamics did not correspond to Brownian motion on the energy surface.\næˆå¯¹æœ€å¤§ç†µæ¨¡å‹åœ¨æè¿°ä»åŸ¹å…»çš¿ä¸­ä¿æŒæ´»åŠ›çš„ä¸€ç°‡çš®å±‚ç¥ç»å…ƒä¸­é‡‡æ ·çš„ $N\\leq 10$ ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨æ¨¡å¼æ–¹é¢ä¹Ÿç›¸å½“æˆåŠŸï¼ˆTang ç­‰äººï¼Œ2008ï¼‰ã€‚è¿™é¡¹å·¥ä½œè¿˜æŒ‡å‡ºï¼ŒåŠ¨åŠ›å­¦å¹¶ä¸å¯¹åº”äºèƒ½é‡è¡¨é¢ä¸Šçš„å¸ƒæœ—è¿åŠ¨ã€‚\nThese early successes with small numbers of neurons raised many questions. For example, the interaction matrix $J_{ij}$ contained a mix of positive and negative terms, suggesting that frustration could lead to many local minima of the energy function or equivalently local maxima of the probability $P(\\vec{\\sigma})$, as in the Hopfield model (Â§II.C); could these â€œattractorsâ€ have a function in representing the visual world? Relatedly, an important consequence of the collective behavior in the Ising model is that if we know that state of all neurons in the network but one, then we have a parameterâ€“free prediction for the probability that this last neuron will be active; does this allow for error correction? To address these and other issues one must go beyond $N\\sim 10$ cells, which was already possible experimentally. But at larger $N$ one needs more powerful methods for solving the inverse problem that is at the heart of the maximum entropy construction, as described in Appendix B.\nè¿™äº›æ—©æœŸåœ¨å°‘é‡ç¥ç»å…ƒä¸Šçš„æˆåŠŸå¼•å‘äº†è®¸å¤šé—®é¢˜ã€‚ä¾‹å¦‚ï¼Œäº¤äº’çŸ©é˜µ $J_{ij}$ åŒ…å«æ­£è´Ÿæ··åˆé¡¹ï¼Œè¡¨æ˜é˜»æŒ«å¯èƒ½å¯¼è‡´èƒ½é‡å‡½æ•°çš„è®¸å¤šå±€éƒ¨æå°å€¼ï¼Œæˆ–è€…ç­‰ä»·åœ°ï¼Œæ¦‚ç‡ $P(\\vec{\\sigma})$ çš„å±€éƒ¨æå¤§å€¼ï¼Œå°±åƒ Hopfield æ¨¡å‹ï¼ˆÂ§II.Cï¼‰ä¸­ä¸€æ ·ï¼›è¿™äº›â€œå¸å¼•å­â€åœ¨è¡¨ç¤ºè§†è§‰ä¸–ç•Œæ–¹é¢æ˜¯å¦å…·æœ‰åŠŸèƒ½ï¼Ÿç›¸å…³åœ°ï¼ŒIsing æ¨¡å‹ä¸­é›†ä½“è¡Œä¸ºçš„ä¸€ä¸ªé‡è¦åæœæ˜¯ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“ç½‘ç»œä¸­é™¤ä¸€ä¸ªç¥ç»å…ƒå¤–æ‰€æœ‰ç¥ç»å…ƒçš„çŠ¶æ€ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥æ— å‚æ•°åœ°é¢„æµ‹è¿™ä¸ªæœ€åä¸€ä¸ªç¥ç»å…ƒæ˜¯å¦ä¼šæ´»è·ƒï¼›è¿™æ˜¯å¦å…è®¸çº é”™ï¼Ÿä¸ºäº†å¤„ç†è¿™äº›å’Œå…¶ä»–é—®é¢˜ï¼Œå¿…é¡»è¶…è¶Š $N\\sim 10$ ä¸ªç»†èƒï¼Œè¿™åœ¨å®éªŒä¸Šå·²ç»æ˜¯å¯èƒ½çš„ã€‚ä½†åœ¨æ›´å¤§çš„ $N$ ä¸‹ï¼Œéœ€è¦æ›´å¼ºå¤§çš„æ–¹æ³•æ¥è§£å†³æœ€å¤§ç†µæ„é€ æ ¸å¿ƒçš„é€†é—®é¢˜ï¼Œå¦‚é™„å½• B æ‰€è¿°ã€‚\nThe equivalence to equilibrium models entices us to describe the couplings $J_{ij}$ as â€œinteractions,â€ but there is no reason to think that these correspond to genuine connections between cells. In particular, $J_{ij}$ is symmetric because it is an effective interaction driving the equaltime correlations of activity in cells $i$ and $j$, and these correlations are symmetric by definition. If we go beyond single time slices to describe trajectories of activity over time, then with multiple cells the effective interactions can become asymmetric and break timereversal invariance.\nä¸å¹³è¡¡æ¨¡å‹çš„ç­‰ä»·æ€§è¯±ä½¿æˆ‘ä»¬å°†è€¦åˆ $J_{ij}$ æè¿°ä¸ºâ€œç›¸äº’ä½œç”¨â€ï¼Œä½†æ²¡æœ‰ç†ç”±è®¤ä¸ºå®ƒä»¬å¯¹åº”äºç»†èƒä¹‹é—´çš„çœŸæ­£è¿æ¥ã€‚ç‰¹åˆ«æ˜¯ï¼Œ$J_{ij}$ æ˜¯å¯¹ç§°çš„ï¼Œå› ä¸ºå®ƒæ˜¯é©±åŠ¨ç»†èƒ $i$ å’Œ $j$ æ´»åŠ¨çš„åŒæ—¶ç›¸å…³æ€§çš„æœ‰æ•ˆç›¸äº’ä½œç”¨ï¼Œè€Œè¿™äº›ç›¸å…³æ€§æŒ‰å®šä¹‰æ˜¯å¯¹ç§°çš„ã€‚å¦‚æœæˆ‘ä»¬è¶…è¶Šå•ä¸ªæ—¶é—´ç‰‡æ¥æè¿°éšæ—¶é—´å˜åŒ–çš„æ´»åŠ¨è½¨è¿¹ï¼Œé‚£ä¹ˆå¯¹äºå¤šä¸ªç»†èƒï¼Œæœ‰æ•ˆç›¸äº’ä½œç”¨å¯ä»¥å˜å¾—ä¸å¯¹ç§°å¹¶æ‰“ç ´æ—¶é—´åæ¼”ä¸å˜æ€§ã€‚\nBefore leaving the early work, it is useful to step back and ask about the goals and hopes from that time. As reviewed above, the use of statistical physics models for neural networks has a deep history. Saying that the brain is described by an Ising model captured both the optimism and (one must admit) the naÄ± Ìˆvet Ìe of the physics community in approaching the phenomena of life. One could balance optimism and naÄ± Ìˆvet Ìe by retreating to the position that these models are metaphors, illustrating what could happen rather than being theories of what actually happens. The success of maximum entropy models in the retina gave an example of how statistical physics ideas could provide a quantitative theory for networks of real neurons.\nåœ¨ç¦»å¼€æ—©æœŸå·¥ä½œä¹‹å‰ï¼Œé€€ä¸€æ­¥é—®ä¸€ä¸‹å½“æ—¶çš„ç›®æ ‡å’Œå¸Œæœ›æ˜¯æœ‰ç”¨çš„ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œç¥ç»ç½‘ç»œä½¿ç”¨ç»Ÿè®¡ç‰©ç†æ¨¡å‹æœ‰ç€æ·±åšçš„å†å²ã€‚è¯´å¤§è„‘ç”± Ising æ¨¡å‹æè¿°æ—¢æ•æ‰äº†ä¹è§‚ä¸»ä¹‰ï¼Œä¹Ÿæ•æ‰äº†ï¼ˆå¿…é¡»æ‰¿è®¤ï¼‰ç‰©ç†å­¦ç•Œåœ¨æ¥è¿‘ç”Ÿå‘½ç°è±¡æ—¶çš„å¤©çœŸã€‚é€šè¿‡é€€å›åˆ°è¿™äº›æ¨¡å‹æ˜¯éšå–»çš„ä½ç½®ï¼Œå¯ä»¥å¹³è¡¡ä¹è§‚ä¸»ä¹‰å’Œå¤©çœŸï¼Œè¯´æ˜å¯èƒ½ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œè€Œä¸æ˜¯å®é™…å‘ç”Ÿçš„ç†è®ºã€‚è§†ç½‘è†œä¸­æœ€å¤§ç†µæ¨¡å‹çš„æˆåŠŸæä¾›äº†ä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ç»Ÿè®¡ç‰©ç†æ€æƒ³å¦‚ä½•ä¸ºçœŸå®ç¥ç»å…ƒç½‘ç»œæä¾›å®šé‡ç†è®ºã€‚\nLarger networks of neurons The use of maximum entropy for networks of real neurons quickly triggered almost all possible reactions: (a) It should never work, because systems are not in equilibrium, have combinational interactions, $\\cdots$ . (b) It could work, but only under uninteresting conditions. (c) It should always work, since these models are very expressive. (d) It works at small $N$ , but this is a poor guide to what will happen at large $N$ . (e) Sure, but why not use [favorite alternative], for which we have efficient algorithms?\nå¯¹äºçœŸå®ç¥ç»å…ƒç½‘ç»œä½¿ç”¨æœ€å¤§ç†µè¿…é€Ÿå¼•å‘äº†å‡ ä¹æ‰€æœ‰å¯èƒ½çš„ååº”ï¼šï¼ˆaï¼‰å®ƒæ°¸è¿œä¸ä¼šèµ·ä½œç”¨ï¼Œå› ä¸ºç³»ç»Ÿä¸å¤„äºå¹³è¡¡çŠ¶æ€ï¼Œå…·æœ‰ç»„åˆç›¸äº’ä½œç”¨ï¼Œ$\\cdots$ï¼ˆbï¼‰å®ƒå¯èƒ½èµ·ä½œç”¨ï¼Œä½†ä»…åœ¨æ— è¶£çš„æ¡ä»¶ä¸‹ã€‚ï¼ˆcï¼‰å®ƒåº”è¯¥æ€»æ˜¯æœ‰æ•ˆï¼Œå› ä¸ºè¿™äº›æ¨¡å‹éå¸¸æœ‰è¡¨ç°åŠ›ã€‚ï¼ˆdï¼‰å®ƒåœ¨å° $N$ ä¸‹æœ‰æ•ˆï¼Œä½†è¿™å¯¹å¤§ $N$ ä¼šå‘ç”Ÿä»€ä¹ˆæ²¡æœ‰å¾ˆå¥½çš„æŒ‡å¯¼æ„ä¹‰ã€‚ï¼ˆeï¼‰å½“ç„¶ï¼Œä½†ä¸ºä»€ä¹ˆä¸ä½¿ç”¨[æœ€å–œæ¬¢çš„æ›¿ä»£æ–¹æ¡ˆ]ï¼Œæˆ‘ä»¬æœ‰é«˜æ•ˆçš„ç®—æ³•ï¼Ÿ\nPerhaps the most concrete response to these issues is just to see what happens as we move to more examples, especially in larger networks. But we should do this with several questions in mind, some of which were very explicit in the early literature (Macke et al., 2011a; Roudi et al., 2009). First, finding the maximum entropy model that matches the desired constraintsâ€”that is, solving Eqs (17)â€”becomes more difficult at larger $N$ . Can we be sure that we are testing the maximum entropy idea, and our choice of constraints, rather than the efficacy of our algorithms for solving this problem?\nä¹Ÿè®¸å¯¹è¿™äº›é—®é¢˜æœ€å…·ä½“çš„å›åº”å°±æ˜¯çœ‹çœ‹å½“æˆ‘ä»¬è½¬å‘æ›´å¤šä¾‹å­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´å¤§ç½‘ç»œä¸­ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ä½†æˆ‘ä»¬åº”è¯¥ç‰¢è®°å‡ ä¸ªé—®é¢˜ï¼Œå…¶ä¸­ä¸€äº›åœ¨æ—©æœŸæ–‡çŒ®ä¸­éå¸¸æ˜ç¡®ï¼ˆMacke ç­‰äººï¼Œ2011aï¼›Roudi ç­‰äººï¼Œ2009ï¼‰ã€‚é¦–å…ˆï¼Œæ‰¾åˆ°åŒ¹é…æ‰€éœ€çº¦æŸçš„æœ€å¤§ç†µæ¨¡å‹â€”â€”å³è§£å†³æ–¹ç¨‹ï¼ˆ17ï¼‰â€”â€”åœ¨æ›´å¤§çš„ $N$ ä¸‹å˜å¾—æ›´åŠ å›°éš¾ã€‚æˆ‘ä»¬èƒ½å¦ç¡®å®šæˆ‘ä»¬æ­£åœ¨æµ‹è¯•æœ€å¤§ç†µçš„æƒ³æ³•ï¼Œä»¥åŠæˆ‘ä»¬é€‰æ‹©çš„çº¦æŸï¼Œè€Œä¸æ˜¯æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜çš„ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Ÿ\nSecond, as N increases the maximum entropy construction becomes very data hungry. This concern often is phrased as the usual problem of â€œoverâ€“fitting,â€ when the number of parameters in our model is too large to fully constrained by the data. But in the maximum entropy formulation the problem is even more fundamental. The maximum entropy construction builds the least structured model consistent with a set of known expectation values. With a finite amount of data, if our list of expectation values is too long then the claim that we â€œknowâ€ these features of the system just isnâ€™t true, and this problem arises even before we try to build the maximum entropy model.\nå…¶æ¬¡ï¼Œéšç€ N çš„å¢åŠ ï¼Œæœ€å¤§ç†µæ„é€ å˜å¾—éå¸¸éœ€è¦æ•°æ®ã€‚è¿™ä¸ªé—®é¢˜é€šå¸¸è¢«è¡¨è¿°ä¸ºâ€œè¿‡æ‹Ÿåˆâ€çš„å¸¸è§é—®é¢˜ï¼Œå½“æˆ‘ä»¬æ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡å¤ªå¤§è€Œæ— æ³•è¢«æ•°æ®å®Œå…¨çº¦æŸæ—¶ã€‚ä½†åœ¨æœ€å¤§ç†µå…¬å¼ä¸­ï¼Œè¿™ä¸ªé—®é¢˜ç”šè‡³æ›´ä¸ºæ ¹æœ¬ã€‚æœ€å¤§ç†µæ„é€ å»ºç«‹äº†ä¸€ä¸ªä¸ä¸€ç»„å·²çŸ¥æœŸæœ›å€¼ä¸€è‡´çš„æœ€å°‘ç»“æ„åŒ–æ¨¡å‹ã€‚å¯¹äºæœ‰é™çš„æ•°æ®é‡ï¼Œå¦‚æœæˆ‘ä»¬çš„æœŸæœ›å€¼åˆ—è¡¨å¤ªé•¿ï¼Œé‚£ä¹ˆæˆ‘ä»¬â€œçŸ¥é“â€ç³»ç»Ÿçš„è¿™äº›ç‰¹å¾çš„è¯´æ³•å°±ä¸æ˜¯çœŸçš„ï¼Œå³ä½¿åœ¨æˆ‘ä»¬å°è¯•æ„å»ºæœ€å¤§ç†µæ¨¡å‹ä¹‹å‰ï¼Œè¿™ä¸ªé—®é¢˜ä¹Ÿä¼šå‡ºç°ã€‚\nThird, because correlations are spread widely in these networks, if one develops a perturbation theory around the limit of independent neurons then factors of $N$ appear in the series, e.g. for the entropy per neuron. Success at modest $N$ might thus mean that we are in a perturbative regime, which would be much less interesting. The question of whether success is perturbative is subtle, since at finite $N$ all properties of the maximum entropy model are analytic functions of the correlations, and hence if we carry perturbation theory far enough we will get the right answer (Sessak and Monasson, 2009).\nç¬¬ä¸‰ï¼Œå› ä¸ºç›¸å…³æ€§åœ¨è¿™äº›ç½‘ç»œä¸­å¹¿æ³›ä¼ æ’­ï¼Œå¦‚æœæˆ‘ä»¬å›´ç»•ç‹¬ç«‹ç¥ç»å…ƒçš„æé™å‘å±•å¾®æ‰°ç†è®ºï¼Œé‚£ä¹ˆ $N$ çš„å› ç´ ä¼šå‡ºç°åœ¨çº§æ•°ä¸­ï¼Œä¾‹å¦‚æ¯ä¸ªç¥ç»å…ƒçš„ç†µã€‚å› æ­¤ï¼Œåœ¨é€‚åº¦çš„ $N$ ä¸‹çš„æˆåŠŸå¯èƒ½æ„å‘³ç€æˆ‘ä»¬å¤„äºå¾®æ‰°èŒƒå›´å†…ï¼Œè¿™å°†ä¸é‚£ä¹ˆæœ‰è¶£ã€‚æˆåŠŸæ˜¯å¦æ˜¯å¾®æ‰°çš„é—®é¢˜æ˜¯å¾®å¦™çš„ï¼Œå› ä¸ºåœ¨æœ‰é™çš„ $N$ ä¸‹ï¼Œæœ€å¤§ç†µæ¨¡å‹çš„æ‰€æœ‰å±æ€§éƒ½æ˜¯ç›¸å…³æ€§çš„è§£æå‡½æ•°ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å°†å¾®æ‰°ç†è®ºè¿›è¡Œå¾—è¶³å¤Ÿè¿œï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ­£ç¡®çš„ç­”æ¡ˆï¼ˆSessak å’Œ Monassonï¼Œ2009ï¼‰ã€‚\nFinally, in statistical mechanics we are used to the idea of a large $N$, thermodynamic limit. Although this carries over to model networks (Amit, 1989), it is not obvious how to use this idea in thinking about networks of real neurons. Naive extrapolation of results from maximum entropy models of $N = 10 âˆ’ 20$ neurons in the retina indicated that something special had to happen by $N\\sim 200$, or else the entropy would vanish; this was interesting because $N\\sim 200$ is the number cells that are â€œlookingâ€ at overlapping regions of the visual world (Schneidman et al., 2006). A more sophisticated extrapolation imagines a large population of neurons in which mean activities and pairwise correlations are drawn at random from the same distribution as found in recordings from smaller numbers of neurons (TkaË‡cik et al., 2006, 2009). This sort of extrapolation is motivated in part by the observation that â€œthermodynamicâ€ properties of the maximum entropy models learned for $N = 20$ or $N = 40$ retinal neurons match the behavior of such random models at the same $N$. If we now extrapolate to $N = 120$ there are striking collective behaviors, and we will ask if these are seen in real data from $N \u003e 100$ cells.\næœ€åï¼Œåœ¨ç»Ÿè®¡åŠ›å­¦ä¸­ï¼Œæˆ‘ä»¬ä¹ æƒ¯äºå¤§ $N$ã€çƒ­åŠ›å­¦æé™çš„æ¦‚å¿µã€‚å°½ç®¡è¿™å¯ä»¥è½¬ç§»åˆ°æ¨¡å‹ç½‘ç»œä¸­ï¼ˆAmitï¼Œ1989ï¼‰ï¼Œä½†åœ¨æ€è€ƒçœŸå®ç¥ç»å…ƒç½‘ç»œæ—¶å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæƒ³æ³•å¹¶ä¸æ˜æ˜¾ã€‚ä»è§†ç½‘è†œä¸­ $N = 10 âˆ’ 20$ ä¸ªç¥ç»å…ƒçš„æœ€å¤§ç†µæ¨¡å‹ç»“æœçš„å¤©çœŸå¤–æ¨è¡¨æ˜ï¼Œåˆ° $N\\sim 200$ æ—¶å¿…é¡»å‘ç”Ÿä¸€äº›ç‰¹æ®Šçš„äº‹æƒ…ï¼Œå¦åˆ™ç†µå°†æ¶ˆå¤±ï¼›è¿™æ˜¯æœ‰è¶£çš„ï¼Œå› ä¸º $N\\sim 200$ æ˜¯â€œè§‚å¯Ÿâ€è§†è§‰ä¸–ç•Œé‡å åŒºåŸŸçš„ç»†èƒæ•°é‡ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚æ›´å¤æ‚çš„å¤–æ¨è®¾æƒ³äº†ä¸€ä¸ªå¤§å‹ç¥ç»å…ƒç¾¤ä½“ï¼Œå…¶ä¸­å¹³å‡æ´»åŠ¨å’Œæˆå¯¹ç›¸å…³æ€§æ˜¯ä»ä¸è¾ƒå°‘ç¥ç»å…ƒè®°å½•ä¸­å‘ç°çš„ç›¸åŒåˆ†å¸ƒä¸­éšæœºæŠ½å–çš„ï¼ˆTkaË‡cik ç­‰äººï¼Œ2006ï¼Œ2009ï¼‰ã€‚è¿™ç§å¤–æ¨éƒ¨åˆ†æ˜¯ç”±è§‚å¯Ÿåˆ°ä¸º $N = 20$ æˆ– $N = 40$ è§†ç½‘è†œç¥ç»å…ƒå­¦ä¹ çš„æœ€å¤§ç†µæ¨¡å‹çš„â€œçƒ­åŠ›å­¦â€å±æ€§ä¸åŒä¸€ $N$ ä¸‹æ­¤ç±»éšæœºæ¨¡å‹çš„è¡Œä¸ºç›¸åŒ¹é…æ‰€æ¿€å‘çš„ã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨å¤–æ¨åˆ° $N = 120$ï¼Œä¼šå‡ºç°æ˜¾è‘—çš„é›†ä½“è¡Œä¸ºï¼Œæˆ‘ä»¬å°†è¯¢é—®åœ¨æ¥è‡ª $N \u003e 100$ ä¸ªç»†èƒçš„çœŸå®æ•°æ®ä¸­æ˜¯å¦çœ‹åˆ°äº†è¿™äº›è¡Œä¸ºã€‚\nEarly experiments in the retina already were monitoring $N = 40$ cells, and the development of numerical methods described in Appendix B quickly allowed analysis of these larger data sets (TkaË‡cik et al., 2006, 2009). With $N = 40$ cells one cannot check the predictions for probabilities of individual patterns $P(\\vec{\\sigma})$, but one can check the probability that $K$ out of $N$ cells are active in the same small time bin, as in Fig. 7E, or the correlations among triplets of neurons. At $N = 40$ we\n",
  "wordCount" : "14395",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png","datePublished": "2025-11-12T00:18:23+08:00",
  "dateModified": "2025-11-12T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-4/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "æ— å¤„æƒ¹å°˜åŸƒ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿— (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿—</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/">ğŸ“• é˜…è¯»</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/">ğŸ“• æ–‡çŒ®</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/">ğŸ“• Statistical mechanics for networks of real neurons</a></div>
            <h1 class="post-title">
                Maximmum entropy as a path to connect theory and experiment
            </h1>
            <div class="post-description">
                çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-11-12
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>14395å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>29åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#basics-of-maximum-entropy" aria-label="Basics of maximum entropy">Basics of maximum entropy</a></li>
                <li>
                    <a href="#first-connections-to-neurons" aria-label="First connections to neurons">First connections to neurons</a></li>
                <li>
                    <a href="#larger-networks-of-neurons" aria-label="Larger networks of neurons">Larger networks of neurons</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><blockquote>
<p>New experimental methods create new opportunities to test our theories. For neural networks, monitoring the electrical activity of tens, hundreds, or thousands of neurons simultaneously should allow us to test statistical approaches to these systems in detail. Doing this requires taking much more seriously the connection between our models and real neurons, a connection that sometimes has been tenuous. Can we really take the spins $\sigma_{i}$ in Eq (5) to represent the presence or absence of an action potential in cell $i$? We will indeed make this identification, and our goal will be an accurate description of the probability distribution out of which the â€œmicroscopicâ€ states of a large network are drawn. Note that, as in equilibrium statistical mechanics, this would be the beginning and not the end of our understanding.</p>
</blockquote>
<p>æ–°çš„å®éªŒæ–¹æ³•ä¸ºæµ‹è¯•æˆ‘ä»¬çš„ç†è®ºåˆ›é€ äº†æ–°çš„æœºä¼šã€‚å¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼ŒåŒæ—¶ç›‘æµ‹æ•°åã€æ•°ç™¾æˆ–æ•°åƒä¸ªç¥ç»å…ƒçš„ç”µæ´»åŠ¨åº”è¯¥å…è®¸æˆ‘ä»¬è¯¦ç»†æµ‹è¯•è¿™äº›ç³»ç»Ÿçš„ç»Ÿè®¡æ–¹æ³•ã€‚åšåˆ°è¿™ä¸€ç‚¹éœ€è¦æˆ‘ä»¬æ›´åŠ è®¤çœŸåœ°å¯¹å¾…æˆ‘ä»¬çš„æ¨¡å‹ä¸çœŸå®ç¥ç»å…ƒä¹‹é—´çš„è”ç³»ï¼Œè€Œè¿™ç§è”ç³»æœ‰æ—¶æ˜¯è„†å¼±çš„ã€‚æˆ‘ä»¬çœŸçš„å¯ä»¥å°†æ–¹ç¨‹ï¼ˆ5ï¼‰ä¸­çš„è‡ªæ—‹ $\sigma_{i}$ è§†ä¸ºç»†èƒ $i$ ä¸­åŠ¨ä½œç”µä½çš„å­˜åœ¨æˆ–ä¸å­˜åœ¨å—ï¼Ÿæˆ‘ä»¬ç¡®å®ä¼šåšå‡ºè¿™ç§è¯†åˆ«ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‡†ç¡®æè¿°ä»ä¸­æŠ½å–å¤§å‹ç½‘ç»œâ€œå¾®è§‚â€çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¯·æ³¨æ„ï¼Œä¸å¹³è¡¡ç»Ÿè®¡åŠ›å­¦ä¸€æ ·ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬ç†è§£çš„å¼€å§‹ï¼Œè€Œä¸æ˜¯ç»“æŸã€‚</p>
<blockquote>
<p>We will see that maximum entropy models provide a path that starts with data and constructs models that have a very direct connection to statistical physics. Our focus here is on networks of neurons, but it is important that the same concepts and methods are being used to study a much wider range of living systems, and there are important lessons to be drawn from seeing all these problems as part of the same project (Appendix A).</p>
</blockquote>
<p>æœ€å¤§ç†µæ¨¡å‹æä¾›äº†ä¸€æ¡ä»æ•°æ®å¼€å§‹å¹¶æ„å»ºä¸ç»Ÿè®¡ç‰©ç†æœ‰éå¸¸ç›´æ¥è”ç³»çš„æ¨¡å‹çš„è·¯å¾„ã€‚æˆ‘ä»¬è¿™é‡Œçš„é‡ç‚¹æ˜¯ç¥ç»å…ƒç½‘ç»œï¼Œä½†é‡è¦çš„æ˜¯ï¼ŒåŒæ ·çš„æ¦‚å¿µå’Œæ–¹æ³•æ­£åœ¨è¢«ç”¨æ¥ç ”ç©¶æ›´å¹¿æ³›çš„ç”Ÿç‰©ç³»ç»Ÿï¼Œå¹¶ä¸”ä»å°†æ‰€æœ‰è¿™äº›é—®é¢˜è§†ä¸ºåŒä¸€é¡¹ç›®çš„ä¸€éƒ¨åˆ†ä¸­å¯ä»¥å¾—å‡ºé‡è¦çš„æ•™è®­ï¼ˆé™„å½• Aï¼‰ã€‚</p>
<h1 id="basics-of-maximum-entropy">Basics of maximum entropy<a hidden class="anchor" aria-hidden="true" href="#basics-of-maximum-entropy">#</a></h1>
<blockquote>
<p>Consider a network of neurons, labelled by $i = 1, 2, \cdots , N$ , each with a state $\sigma_{i}$. In the simplest case where these states of individual neurons are binaryactive/inactive, or spiking/silentâ€”then the network as a whole has access to $\Omega = 2^{N}$ possible states</p>
<p>$$
\vec{\sigma} = \{\sigma_{1},\sigma_{2},\cdots,\sigma_{N}\}.
$$</p>
<p>These states mean something to the organism: they may represent sensory inputs, inferred features of the surrounding world, plans, motor commands, recalled memories, or internal thoughts. But before we can build a dictionary for these meanings we need a lexicon, describing which of the possible states actually occur, and how often. More formally, we would like to understand the probability distribution $P(\vec{\sigma})$. We might also be interested in sequences of states over time, $P [\{\vec{\sigma}(t_1), \vec{\sigma}(t_2),\cdots \}]$, but for simplicity we focus first on states at a single moment in time.</p>
</blockquote>
<p>è€ƒè™‘ä¸€ä¸ªç¥ç»å…ƒç½‘ç»œï¼Œæ ‡è®°ä¸º $i = 1, 2, \cdots , N$ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½æœ‰ä¸€ä¸ªçŠ¶æ€ $\sigma_{i}$ã€‚åœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œè¿™äº›å•ä¸ªç¥ç»å…ƒçš„çŠ¶æ€æ˜¯äºŒè¿›åˆ¶çš„â€”â€”æ´»è·ƒ/ä¸æ´»è·ƒï¼Œæˆ–å°–å³°/é™é»˜â€”â€”é‚£ä¹ˆæ•´ä¸ªç½‘ç»œå¯ä»¥è®¿é—® $\Omega = 2^{N}$ ä¸ªå¯èƒ½çš„çŠ¶æ€</p>
<p>$$
\vec{\sigma} = \{\sigma_{1},\sigma_{2},\cdots,\sigma_{N}\}.
$$</p>
<p>è¿™äº›çŠ¶æ€å¯¹æœ‰æœºä½“æ¥è¯´æ˜¯æœ‰æ„ä¹‰çš„ï¼šå®ƒä»¬å¯èƒ½ä»£è¡¨æ„Ÿå®˜è¾“å…¥ã€å‘¨å›´ä¸–ç•Œçš„æ¨æ–­ç‰¹å¾ã€è®¡åˆ’ã€è¿åŠ¨å‘½ä»¤ã€å›å¿†çš„è®°å¿†æˆ–å†…éƒ¨æ€ç»´ã€‚ä½†åœ¨æˆ‘ä»¬èƒ½å¤Ÿä¸ºè¿™äº›å«ä¹‰æ„å»ºè¯å…¸ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œæè¿°å“ªäº›å¯èƒ½çš„çŠ¶æ€å®é™…ä¸Šä¼šå‘ç”Ÿï¼Œä»¥åŠå®ƒä»¬å‘ç”Ÿçš„é¢‘ç‡ã€‚æ›´æ­£å¼åœ°è¯´ï¼Œæˆ‘ä»¬æƒ³è¦ç†è§£æ¦‚ç‡åˆ†å¸ƒ $P(\vec{\sigma})$ã€‚æˆ‘ä»¬ä¹Ÿå¯èƒ½å¯¹éšæ—¶é—´å˜åŒ–çš„çŠ¶æ€åºåˆ—æ„Ÿå…´è¶£ï¼Œ$P [\{\vec{\sigma}(t_1), \vec{\sigma}(t_2),\cdots \}]$ï¼Œä½†ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬é¦–å…ˆå…³æ³¨å•ä¸€æ—¶åˆ»çš„çŠ¶æ€ã€‚</p>
<blockquote>
<p>The distribution $P(\vec{\sigma})$ is a list of $\Omega$ numbers that sum to one. Even for modest size networks this is a very long list, $\Omega\sim 10^{30}$ for $N = 100$. To be clear, there is no way that we can measure all these numbers in any realistic experiment. More deeply, large networks could not visit all of their possible states in the age of the universe, let alone the lifetime of a single organism. This shouldnâ€™t bother us, since one can make similar observations about the states of molecules in the air around us, or the states of all the atoms in a tiny grain of sand. The fact that the number of possible states $\Omega$ is (beyond) astronomically large does not stop us from asking questions about the distribution from which these states are drawn.</p>
</blockquote>
<p>æ¦‚ç‡åˆ†å¸ƒ $P(\vec{\sigma})$ æ˜¯ä¸€ä¸ªåŒ…å« $\Omega$ ä¸ªæ•°å­—çš„åˆ—è¡¨ï¼Œè¿™äº›æ•°å­—çš„æ€»å’Œä¸ºä¸€ã€‚å³ä½¿å¯¹äºé€‚åº¦å¤§å°çš„ç½‘ç»œæ¥è¯´ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸é•¿çš„åˆ—è¡¨ï¼Œå¯¹äº $N = 100$ï¼Œ$\Omega\sim 10^{30}$ã€‚æ˜ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬æ— æ³•åœ¨ä»»ä½•ç°å®çš„å®éªŒä¸­æµ‹é‡å‡ºæ‰€æœ‰è¿™äº›æ•°å­—ã€‚æ›´æ·±å±‚æ¬¡çš„æ˜¯ï¼Œå¤§å‹ç½‘ç»œä¸å¯èƒ½åœ¨å®‡å®™çš„å¹´é¾„å†…è®¿é—®å®ƒä»¬æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€ï¼Œæ›´ä¸ç”¨è¯´å•ä¸ªæœ‰æœºä½“çš„å¯¿å‘½äº†ã€‚è¿™ä¸åº”è¯¥å›°æ‰°æˆ‘ä»¬ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥å¯¹æˆ‘ä»¬å‘¨å›´ç©ºæ°”ä¸­çš„åˆ†å­çŠ¶æ€ï¼Œæˆ–å¾®å°æ²™ç²’ä¸­æ‰€æœ‰åŸå­çš„çŠ¶æ€åšå‡ºç±»ä¼¼çš„è§‚å¯Ÿã€‚å¯èƒ½çŠ¶æ€çš„æ•°é‡ $\Omega$ï¼ˆè¶…å‡ºï¼‰å¤©æ–‡æ•°å­—çº§åˆ«ï¼Œå¹¶ä¸ä¼šé˜»æ­¢æˆ‘ä»¬å¯¹è¿™äº›çŠ¶æ€æ‰€æŠ½å–çš„åˆ†å¸ƒæå‡ºé—®é¢˜ã€‚</p>
<blockquote>
<p>The enormous value of $\Omega$ does mean, however, that answering questions about the distribution from which the states are drawn requires the answer to be, in some sense, simpler than it could be. If $P(\vec{\sigma})$ really were just a list of $\Omega$ numbers with no underlying structure, we could never make a meaningful experimental prediction. Progress in the description of manyâ€“body systems depends on the discovery of some regularity or simplicity, and without such simplifying hypotheses nothing can be inferred from any reasonable amount of data. The maximum entropy method is a way of being explicit about our simplifying hypotheses.</p>
</blockquote>
<p>ç„¶è€Œï¼Œ$\Omega$ çš„å·¨å¤§å€¼ç¡®å®æ„å‘³ç€ï¼Œè¦å›ç­”æœ‰å…³çŠ¶æ€æ‰€æŠ½å–çš„åˆ†å¸ƒçš„é—®é¢˜ï¼Œç­”æ¡ˆåœ¨æŸç§æ„ä¹‰ä¸Šå¿…é¡»æ¯”å®ƒå¯èƒ½çš„å½¢å¼æ›´ç®€å•ã€‚å¦‚æœ $P(\vec{\sigma})$ çœŸçš„æ˜¯ä¸€ä¸ªæ²¡æœ‰æ½œåœ¨ç»“æ„çš„ $\Omega$ ä¸ªæ•°å­—çš„åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†æ°¸è¿œæ— æ³•åšå‡ºæœ‰æ„ä¹‰çš„å®éªŒé¢„æµ‹ã€‚å¯¹å¤šä½“ç³»ç»Ÿæè¿°çš„è¿›å±•ä¾èµ–äºæŸç§è§„å¾‹æ€§æˆ–ç®€å•æ€§çš„å‘ç°ï¼Œå¦‚æœæ²¡æœ‰è¿™ç§ç®€åŒ–å‡è®¾ï¼Œä»ä»»ä½•åˆç†æ•°é‡çš„æ•°æ®ä¸­éƒ½æ— æ³•æ¨æ–­å‡ºä»»ä½•ä¸œè¥¿ã€‚æœ€å¤§ç†µæ–¹æ³•æ˜¯ä¸€ç§æ˜ç¡®è¡¨è¾¾æˆ‘ä»¬ç®€åŒ–å‡è®¾çš„æ–¹æ³•ã€‚</p>
<blockquote>
<p>We can imagine mapping each microscopic state $\vec{\sigma}$ into some perhaps more macroscopic observable $f(\vec{\sigma})$, and from reasonable experiments we should be able to estimate the average of this observable $\langle f(\vec{\sigma})\rangle_{\text{expt}}$. If we think this observable is an important and meaningful quantity, it makes sense to insist that any theory we write down for the distribution $P(\vec{\sigma})$ should predict this expectation value correctly,</p>
<p>$$
\langle f(\vec{\sigma})\rangle_{P} = \sum_{\vec{\sigma}} P(\vec{\sigma})f(\vec{\sigma}) = \langle f(\vec{\sigma})\rangle_{\text{expt}}.
$$</p>
<p>There might be several such meaningful observables, so we should have</p>
<p>$$
\langle f_{\mu}(\vec{\sigma})\rangle_{P} \equiv \sum_{\vec{\sigma}}P(\vec{\sigma})f_{\mu}(\vec{\sigma}) = \langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}
$$</p>
<p>for $\mu = 1, 2, \cdots , K$. These are strong constraints, but so long as the number of these observables $K \ll \Omega$ there are infinitely many distributions consistent with Eq (17). How do we choose among them?</p>
</blockquote>
<p>æˆ‘ä»¬å¯ä»¥æƒ³è±¡å°†æ¯ä¸ªå¾®è§‚çŠ¶æ€ $\vec{\sigma}$ æ˜ å°„åˆ°æŸä¸ªå¯èƒ½æ›´å®è§‚çš„å¯è§‚å¯Ÿé‡ $f(\vec{\sigma})$ï¼Œå¹¶ä¸”é€šè¿‡åˆç†çš„å®éªŒï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿä¼°è®¡å‡ºè¿™ä¸ªå¯è§‚å¯Ÿé‡çš„å¹³å‡å€¼ $\langle f(\vec{\sigma})\rangle_{\text{expt}}$ã€‚å¦‚æœæˆ‘ä»¬è®¤ä¸ºè¿™ä¸ªå¯è§‚å¯Ÿé‡æ˜¯ä¸€ä¸ªé‡è¦ä¸”æœ‰æ„ä¹‰çš„é‡ï¼Œé‚£ä¹ˆåšæŒä»»ä½•æˆ‘ä»¬ä¸ºåˆ†å¸ƒ $P(\vec{\sigma})$ å†™ä¸‹çš„ç†è®ºéƒ½åº”è¯¥æ­£ç¡®é¢„æµ‹è¿™ä¸ªæœŸæœ›å€¼æ˜¯æœ‰æ„ä¹‰çš„ï¼Œ</p>
<p>$$
\langle f(\vec{\sigma})\rangle_{P} = \sum_{\vec{\sigma}} P(\vec{\sigma})f(\vec{\sigma}) = \langle f(\vec{\sigma})\rangle_{\text{expt}}.
$$</p>
<p>å¯èƒ½ä¼šæœ‰å‡ ä¸ªè¿™æ ·çš„æœ‰æ„ä¹‰çš„å¯è§‚å¯Ÿé‡ï¼Œå› æ­¤æˆ‘ä»¬åº”è¯¥æœ‰</p>
<p>$$
\langle f_{\mu}(\vec{\sigma})\rangle_{P} \equiv \sum_{\vec{\sigma}}P(\vec{\sigma})f_{\mu}(\vec{\sigma}) = \langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}
$$</p>
<p>å¯¹äº $\mu = 1, 2, \cdots , K$ã€‚è¿™äº›æ˜¯å¼ºçº¦æŸï¼Œä½†åªè¦è¿™äº›å¯è§‚å¯Ÿé‡çš„æ•°é‡ $K \ll \Omega$ï¼Œå°±æœ‰æ— æ•°ä¸ªä¸æ–¹ç¨‹ï¼ˆ17ï¼‰ä¸€è‡´çš„åˆ†å¸ƒã€‚æˆ‘ä»¬å¦‚ä½•åœ¨å®ƒä»¬ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼Ÿ</p>
<blockquote>
<p>There are many ways of saying, in words, how we would like to make our choice among the $P (\sigma)$ that are consistent with the measured expectation values of observables. We would like to pick the simplest or least structured model. We would like not to inject into our model any information beyond what is given to us by the measurements $\{\langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}\}$. From a different point of view, we would like drawing states out of the distribution $P(\vec{\sigma})$ to generate samples that are as random as possible while still obeying the constraints in Eq (17). It might seem that each choice of words generates a new discussionâ€”what do we mean, mathematically, by â€œleast structured,â€ or â€œas random as possibleâ€?</p>
</blockquote>
<p>åœ¨ä¸è§‚æµ‹é‡çš„æµ‹é‡æœŸæœ›å€¼ä¸€è‡´çš„ $P (\sigma)$ ä¹‹é—´è¿›è¡Œé€‰æ‹©æ—¶ï¼Œæœ‰è®¸å¤šæ–¹æ³•å¯ä»¥ç”¨è¯­è¨€è¡¨è¾¾ã€‚æˆ‘ä»¬å¸Œæœ›é€‰æ‹©æœ€ç®€å•æˆ–ç»“æ„æœ€å°‘çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›ä¸è¦å°†ä»»ä½•è¶…å‡ºæµ‹é‡ $\{\langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}\}$ æ‰€æä¾›çš„ä¿¡æ¯æ³¨å…¥åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚ä»ä¸åŒçš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¸Œæœ›ä»åˆ†å¸ƒ $P(\vec{\sigma})$ ä¸­æŠ½å–çŠ¶æ€ï¼Œä»¥ç”Ÿæˆå°½å¯èƒ½éšæœºçš„æ ·æœ¬ï¼ŒåŒæ—¶ä»ç„¶éµå®ˆæ–¹ç¨‹ï¼ˆ17ï¼‰ä¸­çš„çº¦æŸã€‚ä¼¼ä¹æ¯ç§æªè¾éƒ½ä¼šå¼•å‘æ–°çš„è®¨è®ºâ€”â€”æˆ‘ä»¬åœ¨æ•°å­¦ä¸Šæ˜¯ä»€ä¹ˆæ„æ€ï¼Œâ€œç»“æ„æœ€å°‘â€æˆ–â€œå°½å¯èƒ½éšæœºâ€ï¼Ÿ</p>
<blockquote>
<p>Introductory courses in statistical mechanics make some remarks about entropy as a measure of our ignorance about the microscopic state of a system, but this connection often is left quite vague. In laying the foundations of information theory, Shannon made this connection precise (Shannon, 1948). If we ask a question, we have the intuition that we â€œgain informationâ€ when  we hear the answer. If we want to attach a number to this information gain, then the unique measure that is consistent with natural constraints is the entropy of the distribution out of which the answers are drawn. Thus, if we ask for the microscopic state of a system, the information we gain on hearing the answer is (on average) the entropy of the distribution over these microscopic states. Conversely, if the entropy is less than its maximum possible value, this reduction in entropy measures how much we already know about the microscopic state even before we see it. As a result, for states to be as random as possibleâ€”to be sure that we do not inject extra information about these statesâ€”we need to find the distribution that has the maximum entropy.</p>
</blockquote>
<p>ç»Ÿè®¡åŠ›å­¦çš„å…¥é—¨è¯¾ç¨‹å¯¹ç†µä½œä¸ºæˆ‘ä»¬å¯¹ç³»ç»Ÿå¾®è§‚çŠ¶æ€æ— çŸ¥çš„åº¦é‡åšäº†ä¸€äº›è¯„è®ºï¼Œä½†è¿™ç§è”ç³»é€šå¸¸ç›¸å½“æ¨¡ç³Šã€‚åœ¨å¥ å®šä¿¡æ¯ç†è®ºåŸºç¡€æ—¶ï¼Œé¦™å†œï¼ˆShannonï¼Œ1948ï¼‰ä½¿è¿™ç§è”ç³»å˜å¾—ç²¾ç¡®ã€‚å¦‚æœæˆ‘ä»¬æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æœ‰ä¸€ç§ç›´è§‰ï¼Œå½“æˆ‘ä»¬å¬åˆ°ç­”æ¡ˆæ—¶ï¼Œæˆ‘ä»¬â€œè·å¾—ä¿¡æ¯â€ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¸ºè¿™ç§ä¿¡æ¯å¢ç›Šé™„åŠ ä¸€ä¸ªæ•°å­—ï¼Œé‚£ä¹ˆä¸è‡ªç„¶çº¦æŸä¸€è‡´çš„å”¯ä¸€åº¦é‡å°±æ˜¯ä»ä¸­æŠ½å–ç­”æ¡ˆçš„åˆ†å¸ƒçš„ç†µã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬è¯¢é—®ç³»ç»Ÿçš„å¾®è§‚çŠ¶æ€ï¼Œé‚£ä¹ˆåœ¨å¬åˆ°ç­”æ¡ˆæ—¶æˆ‘ä»¬è·å¾—çš„ä¿¡æ¯ï¼ˆå¹³å‡è€Œè¨€ï¼‰å°±æ˜¯è¿™äº›å¾®è§‚çŠ¶æ€åˆ†å¸ƒçš„ç†µã€‚ç›¸åï¼Œå¦‚æœç†µå°äºå…¶æœ€å¤§å¯èƒ½å€¼ï¼Œè¿™ç§ç†µçš„å‡å°‘è¡¡é‡äº†å³ä½¿åœ¨çœ‹åˆ°å®ƒä¹‹å‰æˆ‘ä»¬å·²ç»çŸ¥é“äº†å¤šå°‘å…³äºå¾®è§‚çŠ¶æ€çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿çŠ¶æ€å°½å¯èƒ½éšæœºâ€”â€”ç¡®ä¿æˆ‘ä»¬ä¸ä¼šæ³¨å…¥å…³äºè¿™äº›çŠ¶æ€çš„é¢å¤–ä¿¡æ¯â€”â€”æˆ‘ä»¬éœ€è¦æ‰¾åˆ°å…·æœ‰æœ€å¤§ç†µçš„åˆ†å¸ƒã€‚</p>
<blockquote>
<p>Maximizing the entropy subject to constraints defines a variational problem, maximizing</p>
<p>$$
\widetilde{S} = -\sum_{\vec{\sigma}}P(\vec{\sigma})\ln{P(\vec{\sigma})} - \sum_{\mu = 1}^{K}\left[\sum_{\sigma}P(\vec{\sigma})f_{\mu}(\vec{\sigma}) - \langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}\right] - \lambda_{0}\left[\sum_{\vec{\sigma}}P(\vec{\sigma}) - 1\right]
$$</p>
<p>where the $\lambda_{\mu}$ are Lagrange multipliers. We include an additional term ($\propto \lambda_{0}$) to constrain the normalization, so we can treat each entry in the distribution as an independent variable. Then</p>
<p>$$
\begin{aligned}
\frac{\delta \widetilde{S}}{\delta P(\vec{\sigma})} &amp;= 0\\
\Rightarrow P(\vec{\sigma}) &amp;= \frac{1}{Z(\{\lambda_{\mu}\})}\exp{[-E(\vec{\sigma})]}\\
E(\vec{\sigma}) &amp;= \sum_{\mu = 1}^{K}\lambda_{\mu}f_{\mu}(\vec{\sigma})
\end{aligned}
$$</p>
<p>Thus the model we are looking for is equivalent to an equilibrium statistical mechanics problem in which the â€œenergyâ€ is a sum of terms, one for each of the observables whose expectation values we constrain; the Lagrange multipliers become coupling constants in the effective energy. To finish the construction we need to adjust these couplings $\{\lambda_{\mu}\}$ to satisfy Eq (17), and in general this is a hard problem; see Appendix B. Importantly, if we have some set of expectation values that we are matching, and we want to add one more, this just adds one more term to the form of the energy function, but in general implementing this extra constraint requires adjusting all the coupling constants.</p>
</blockquote>
<p>æœ€å¤§åŒ–å—çº¦æŸçš„ç†µå®šä¹‰äº†ä¸€ä¸ªå˜åˆ†é—®é¢˜ï¼Œæœ€å¤§åŒ–</p>
<p>$$
\widetilde{S} = -\sum_{\vec{\sigma}}P(\vec{\sigma})\ln{P(\vec{\sigma})} - \sum_{\mu = 1}^{K}\left[\sum_{\sigma}P(\vec{\sigma})f_{\mu}(\vec{\sigma}) - \langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}\right] - \lambda_{0}\left[\sum_{\vec{\sigma}}P(\vec{\sigma}) - 1\right]
$$</p>
<p>å…¶ä¸­ $\lambda_{\mu}$ æ˜¯æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ã€‚æˆ‘ä»¬åŒ…æ‹¬ä¸€ä¸ªé¢å¤–çš„é¡¹ï¼ˆ$\propto \lambda_{0}$ï¼‰æ¥çº¦æŸå½’ä¸€åŒ–ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†åˆ†å¸ƒä¸­çš„æ¯ä¸ªæ¡ç›®è§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„å˜é‡ã€‚ç„¶å</p>
<p>$$
\begin{aligned}
\frac{\delta \widetilde{S}}{\delta P(\vec{\sigma})} &amp;= 0\\
\Rightarrow P(\vec{\sigma}) &amp;= \frac{1}{Z(\{\lambda_{\mu}\})}\exp{[-E(\vec{\sigma})]}\\
E(\vec{\sigma}) &amp;= \sum_{\mu = 1}^{K}\lambda_{\mu}f_{\mu}(\vec{\sigma})
\end{aligned}
$$</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾çš„æ¨¡å‹ç­‰ä»·äºä¸€ä¸ªå¹³è¡¡ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œå…¶ä¸­â€œèƒ½é‡â€æ˜¯å„ä¸ªå¯è§‚å¯Ÿé‡çš„æœŸæœ›å€¼ä¹‹å’Œï¼›æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æˆä¸ºæœ‰æ•ˆèƒ½é‡ä¸­çš„è€¦åˆå¸¸æ•°ã€‚ä¸ºäº†å®Œæˆæ„å»ºï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´è¿™äº›è€¦åˆ $\{\lambda_{\mu}\}$ ä»¥æ»¡è¶³æ–¹ç¨‹ï¼ˆ17ï¼‰ï¼Œé€šå¸¸è¿™æ˜¯ä¸€ä¸ªå›°éš¾çš„é—®é¢˜ï¼›è§é™„å½• Bã€‚é‡è¦çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ç»„æˆ‘ä»¬æ­£åœ¨åŒ¹é…çš„æœŸæœ›å€¼ï¼Œå¹¶ä¸”æˆ‘ä»¬æƒ³å†æ·»åŠ ä¸€ä¸ªï¼Œè¿™åªä¼šåœ¨èƒ½é‡å‡½æ•°çš„å½¢å¼ä¸­æ·»åŠ ä¸€ä¸ªé¢å¤–çš„é¡¹ï¼Œä½†é€šå¸¸å®ç°è¿™ä¸ªé¢å¤–çš„çº¦æŸéœ€è¦è°ƒæ•´æ‰€æœ‰çš„è€¦åˆå¸¸æ•°ã€‚</p>
<blockquote>
<p>To make the connections explicit, recall that we can define thermodynamic equilibrium as the state of maximum entropy given the constraint of fixed mean energy. This optimization problem is solved by the Boltzmann distribution. In this view the (inverse) temperature is a Lagrange multiplier that enforces the energy constraint, opposite to usual view of controlling the temperature and predicting the energy. The Boltzmann distribution generalizes if other expectation values are constrained (Landau and Lifshitz, 1977).</p>
</blockquote>
<p>ä¸ºäº†æ˜ç¡®è¿æ¥ï¼Œå›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†çƒ­åŠ›å­¦å¹³è¡¡å®šä¹‰ä¸ºåœ¨å›ºå®šå¹³å‡èƒ½é‡çº¦æŸä¸‹çš„æœ€å¤§ç†µçŠ¶æ€ã€‚è¿™ä¸ªä¼˜åŒ–é—®é¢˜ç”± Boltzmann åˆ†å¸ƒè§£å†³ã€‚åœ¨è¿™ç§è§‚ç‚¹ä¸­ï¼Œï¼ˆåï¼‰æ¸©åº¦æ˜¯ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œç”¨äºå¼ºåˆ¶æ‰§è¡Œèƒ½é‡çº¦æŸï¼Œè¿™ä¸é€šå¸¸æ§åˆ¶æ¸©åº¦å¹¶é¢„æµ‹èƒ½é‡çš„è§‚ç‚¹ç›¸åã€‚å¦‚æœçº¦æŸäº†å…¶ä»–æœŸæœ›å€¼ï¼ŒBoltzmann åˆ†å¸ƒä¼šè¿›è¡Œæ¨å¹¿ï¼ˆLandau å’Œ Lifshitzï¼Œ1977ï¼‰ã€‚</p>
<blockquote>
<p>The maximum entropy argument gives us the form of the probability distribution, but we also need the coupling constants. We can think of this as being an â€œinverse statistical mechanicsâ€ problem, since we are given expectation values or correlation functions and need to find the couplings, rather than the other way around. Different formulations of this problem have a long history in the mathematical physics community (Chayes et al., 1984; Keller and Zumino, 1959; Kunkin and Firsch, 1969). An early application to living systems involved reconstructing the forces that hold together the array of gap junction proteins which bridge the membranes of two cells in contact (Braun et al., 1984). As attention focused on networks of neurons, finding the relevant coupling constants came to be described as the â€œinverse Isingâ€ problem, as will become clear below.</p>
</blockquote>
<p>æœ€å¤§ç†µè®ºç»™äº†æˆ‘ä»¬æ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œä½†æˆ‘ä»¬ä¹Ÿéœ€è¦è€¦åˆå¸¸æ•°ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸º â€œé€†ç»Ÿè®¡åŠ›å­¦â€ é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬ç»™å‡ºäº†æœŸæœ›å€¼æˆ–ç›¸å…³å‡½æ•°ï¼Œå¹¶ä¸”éœ€è¦æ‰¾åˆ°è€¦åˆï¼Œè€Œä¸æ˜¯ç›¸åã€‚è¿™ä¸ªé—®é¢˜çš„ä¸åŒè¡¨è¿°åœ¨æ•°å­¦ç‰©ç†å­¦ç•Œæœ‰ç€æ‚ ä¹…çš„å†å²ï¼ˆChayes ç­‰äººï¼Œ1984ï¼›Keller å’Œ Zuminoï¼Œ1959ï¼›Kunkin å’Œ Firschï¼Œ1969ï¼‰ã€‚å¯¹ç”Ÿç‰©ç³»ç»Ÿçš„æ—©æœŸåº”ç”¨æ¶‰åŠé‡å»ºå°†ä¸¤ä¸ªæ¥è§¦ç»†èƒçš„è†œè¿æ¥åœ¨ä¸€èµ·çš„é—´éš™è¿æ¥è›‹ç™½é˜µåˆ—çš„åŠ›ï¼ˆBraun ç­‰äººï¼Œ1984ï¼‰ã€‚éšç€æ³¨æ„åŠ›é›†ä¸­åœ¨ç¥ç»å…ƒç½‘ç»œä¸Šï¼Œæ‰¾åˆ°ç›¸å…³çš„è€¦åˆå¸¸æ•°è¢«æè¿°ä¸º â€œé€† Isingâ€ é—®é¢˜ï¼Œæ­£å¦‚ä¸‹é¢å°†å˜å¾—æ¸…æ¥šçš„é‚£æ ·ã€‚</p>
<blockquote>
<p>In statistical physics there is in some sense a force driving systems toward equilibrium, as encapsulated in the Hâ€“theorem. In many cases this force triumphs, and what we see is a state with maximal entropy subject only to a very few constraints. In the networks of neurons that we study here, there is no Hâ€“theorem, and the list of constraints will be quite long compared to what we are used to in thermodynamics. This means that the probability distributions we write down will be mathematically equivalent to some equilibrium statistical mechanics problem, but they do not describe an equilibrium state of the system we are actually studying. This somewhat subtle relationship between maximum entropy as a description of thermal equilibrium and maximum entropy as a tool for inference was outlined long ago by Jaynes (1957, 1982).</p>
</blockquote>
<p>åœ¨ç»Ÿè®¡ç‰©ç†å­¦ä¸­ï¼Œåœ¨æŸç§æ„ä¹‰ä¸Šæœ‰ä¸€ç§é©±åŠ¨åŠ›å°†ç³»ç»Ÿæ¨å‘å¹³è¡¡ï¼Œæ­£å¦‚ H å®šç†æ‰€æ¦‚æ‹¬çš„é‚£æ ·ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¿™ç§åŠ›é‡å–å¾—äº†èƒœåˆ©ï¼Œæˆ‘ä»¬æ‰€çœ‹åˆ°çš„æ˜¯ä¸€ä¸ªä»…å—å¾ˆå°‘çº¦æŸçš„æœ€å¤§ç†µçŠ¶æ€ã€‚åœ¨æˆ‘ä»¬è¿™é‡Œç ”ç©¶çš„ç¥ç»å…ƒç½‘ç»œä¸­ï¼Œæ²¡æœ‰ H å®šç†ï¼Œå¹¶ä¸”ä¸æˆ‘ä»¬åœ¨çƒ­åŠ›å­¦ä¸­ä¹ æƒ¯çš„ç›¸æ¯”ï¼Œçº¦æŸåˆ—è¡¨å°†ç›¸å½“é•¿ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å†™ä¸‹çš„æ¦‚ç‡åˆ†å¸ƒåœ¨æ•°å­¦ä¸Šç­‰ä»·äºæŸä¸ªå¹³è¡¡ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œä½†å®ƒä»¬å¹¶ä¸æè¿°æˆ‘ä»¬å®é™…ç ”ç©¶çš„ç³»ç»Ÿçš„å¹³è¡¡çŠ¶æ€ã€‚Jaynesï¼ˆ1957ï¼Œ1982ï¼‰æ—©å·²æ¦‚è¿°äº†æœ€å¤§ç†µä½œä¸ºçƒ­å¹³è¡¡æè¿°å’Œæœ€å¤§ç†µä½œä¸ºæ¨ç†å·¥å…·ä¹‹é—´è¿™ç§å¾®å¦™çš„å…³ç³»ã€‚</p>
<blockquote>
<p>If we donâ€™t have any constraints then the maximum entropy distribution is uniform over all $\Omega$ states. Each observable whose expectation value we constrain lowers the maximum allowed value of the entropy, and if we add enough constraints we eventually reach the true entropy and hence the true distribution. Often it make sense to group the observables into oneâ€“body, twoâ€“body, threebody terms, etc.. Having constrained all the kâ€“body observables for $k\leq K$, the maximum entropy model makes parameterâ€“free predictions for correlations among groups of $k &gt; K$ variables. This provides a powerful path to testing the model, and defines a natural generalization of connected correlations (Schneidman et al., 2003).</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬æ²¡æœ‰ä»»ä½•çº¦æŸï¼Œé‚£ä¹ˆæœ€å¤§ç†µåˆ†å¸ƒåœ¨æ‰€æœ‰ $\Omega$ çŠ¶æ€ä¸Šæ˜¯å‡åŒ€çš„ã€‚æˆ‘ä»¬çº¦æŸçš„æ¯ä¸ªå¯è§‚å¯Ÿé‡çš„æœŸæœ›å€¼éƒ½ä¼šé™ä½å…è®¸çš„æœ€å¤§ç†µå€¼ï¼Œå¦‚æœæˆ‘ä»¬æ·»åŠ è¶³å¤Ÿçš„çº¦æŸï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šè¾¾åˆ°çœŸå®çš„ç†µå€¼ï¼Œä»è€Œå¾—åˆ°çœŸå®çš„åˆ†å¸ƒã€‚é€šå¸¸ï¼Œå°†å¯è§‚å¯Ÿé‡åˆ†ä¸ºå•ä½“ã€äºŒä½“ã€ä¸‰ä½“ç­‰é¡¹æ˜¯æœ‰æ„ä¹‰çš„ã€‚åœ¨çº¦æŸäº†æ‰€æœ‰ $k\leq K$ çš„ $k$ ä½“å¯è§‚å¯Ÿé‡ä¹‹åï¼Œæœ€å¤§ç†µæ¨¡å‹å¯¹ $k &gt; K$ å˜é‡ç»„ä¹‹é—´çš„ç›¸å…³æ€§åšå‡ºæ— å‚æ•°é¢„æµ‹ã€‚è¿™ä¸ºæµ‹è¯•æ¨¡å‹æä¾›äº†ä¸€æ¡å¼ºæœ‰åŠ›çš„è·¯å¾„ï¼Œå¹¶å®šä¹‰äº†è¿æ¥ç›¸å…³æ€§çš„è‡ªç„¶æ¨å¹¿ï¼ˆSchneidman ç­‰äººï¼Œ2003ï¼‰ã€‚</p>
<blockquote>
<p>The connection of maximum entropy models to the Boltzmann distribution gives us intuition and practical computational tools. It can also leave the impression that we are describing a system in equilibrium, which would be a disaster. In fact the maximum entropy distribution describes thermal equilibrium only if the observable that we constrain is the energy in the mechanical sense. There is no obstacle to building maximum entropy models for the distribution of states in a nonâ€“equilibrium system.</p>
</blockquote>
<p>æœ€å¤§ç†µæ¨¡å‹ä¸ Boltzmann åˆ†å¸ƒçš„è”ç³»ä¸ºæˆ‘ä»¬æä¾›äº†ç›´è§‰å’Œå®ç”¨çš„è®¡ç®—å·¥å…·ã€‚å®ƒä¹Ÿå¯èƒ½ç»™äººç•™ä¸‹æˆ‘ä»¬æ­£åœ¨æè¿°ä¸€ä¸ªå¹³è¡¡ç³»ç»Ÿçš„å°è±¡ï¼Œè¿™å°†æ˜¯ç¾éš¾æ€§çš„ã€‚äº‹å®ä¸Šï¼Œåªæœ‰å½“æˆ‘ä»¬çº¦æŸçš„å¯è§‚å¯Ÿé‡æ˜¯æœºæ¢°æ„ä¹‰ä¸Šçš„èƒ½é‡æ—¶ï¼Œæœ€å¤§ç†µåˆ†å¸ƒæ‰æè¿°çƒ­å¹³è¡¡ã€‚æ„å»ºéå¹³è¡¡ç³»ç»Ÿä¸­çŠ¶æ€åˆ†å¸ƒçš„æœ€å¤§ç†µæ¨¡å‹æ²¡æœ‰éšœç¢ã€‚</p>
<blockquote>
<p>Although we can usefully think of states distributed over an energy landscape, as we have formulated the maximum entropy construction this description works for states at one moment in time. Thus we cannot conclude that the dynamics by which the system moves from one state to another are analogous to Brownian motion on the effective energy surface. There are infinitely many models for the dynamics that are consistent with this description, and most of these will not obey detailed balance.
Recent work shows how to explore a large family of dynamical models consistent with the maximum entropy distribution, and applies these ideas to collective animal behavior (Chen et al., 2023). There also are generalizations of the maximum entropy method to describe distributions of trajectories, as we discuss below (Â§IV.D); maximum entropy models for trajectories sometimes are called <strong>maximum caliber</strong> (Ghosh et al., 2020; Press Ìe et al., 2013). Finally we note that, for better or worse, the symmetries that are central to many problems in statistical physics in general are absent from the systems we will be studying; flocks and swarms are an exception, as discussed in Â§A.2.</p>
</blockquote>
<p>å°½ç®¡æˆ‘ä»¬å¯ä»¥æœ‰ç”¨åœ°å°†çŠ¶æ€åˆ†å¸ƒè§†ä¸ºèƒ½é‡æ™¯è§‚ï¼Œä½†æ­£å¦‚æˆ‘ä»¬æ‰€åˆ¶å®šçš„æœ€å¤§ç†µæ„é€ ï¼Œè¿™ç§æè¿°é€‚ç”¨äºæŸä¸€æ—¶åˆ»çš„çŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½å¾—å‡ºç³»ç»Ÿä»ä¸€ä¸ªçŠ¶æ€ç§»åŠ¨åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„åŠ¨åŠ›å­¦ç±»ä¼¼äºæœ‰æ•ˆèƒ½é‡è¡¨é¢ä¸Šçš„å¸ƒæœ—è¿åŠ¨çš„ç»“è®ºã€‚æœ‰æ— æ•°ç§åŠ¨åŠ›å­¦æ¨¡å‹ä¸è¿™ç§æè¿°ä¸€è‡´ï¼Œå…¶ä¸­å¤§å¤šæ•°ä¸ä¼šéµå®ˆè¯¦ç»†å¹³è¡¡ã€‚
æœ€è¿‘çš„å·¥ä½œå±•ç¤ºäº†å¦‚ä½•æ¢ç´¢ä¸æœ€å¤§ç†µåˆ†å¸ƒä¸€è‡´çš„å¤§é‡åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œå¹¶å°†è¿™äº›æ€æƒ³åº”ç”¨äºé›†ä½“åŠ¨ç‰©è¡Œä¸ºï¼ˆChen ç­‰äººï¼Œ2023ï¼‰ã€‚æœ€å¤§ç†µæ–¹æ³•ä¹Ÿæœ‰æ¨å¹¿ï¼Œç”¨äºæè¿°è½¨è¿¹åˆ†å¸ƒï¼Œæ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·ï¼ˆÂ§IV.Dï¼‰ï¼›è½¨è¿¹çš„æœ€å¤§ç†µæ¨¡å‹æœ‰æ—¶è¢«ç§°ä¸º<strong>æœ€å¤§å£å¾„</strong>ï¼ˆGhosh ç­‰äººï¼Œ2020ï¼›Press Ìe ç­‰äººï¼Œ2013ï¼‰ã€‚æœ€åæˆ‘ä»¬æ³¨æ„åˆ°ï¼Œæ— è®ºå¥½åï¼Œè®¸å¤šç»Ÿè®¡ç‰©ç†å­¦é—®é¢˜ä¸­è‡³å…³é‡è¦çš„å¯¹ç§°æ€§åœ¨æˆ‘ä»¬å°†è¦ç ”ç©¶çš„ç³»ç»Ÿä¸­æ˜¯ç¼ºå¤±çš„ï¼›å¦‚ Â§A.2 æ‰€è®¨è®ºçš„ï¼Œé¸Ÿç¾¤å’Œè™«ç¾¤æ˜¯ä¸€ä¸ªä¾‹å¤–ã€‚</p>
<blockquote>
<p>To conclude this introduction, we emphasize that maximum entropy is unlike usual theories. We donâ€™t start with a theoretical principle or even a model. Rather, we start with some features of the data and test the hypothesis that these features alone encode everything we need to describe the system. Whenever we use this approach we are referring back to the basic structure of the optimization problem defined in Eq (18), and its formal solution in Eqs (20, 21), but there is no single maximum entropy model, and each time we need to be explicit: Which are the observables $f_{\mu}$ whose measured expectation values we want our model to reproduce? Can we find the corresponding Lagrange mutlipliers $\lambda_{mu}$? Do these parameters have a natural interpretation? Once we answer these questions, we can ask whether these relatively simple statistical physics descriptions make predictions that agree with experiment. There is an unusually clean separation between learning the model (matching observed expectation values) and testing the model (predicting new expectation values). In this sense we can think of maximum entropy as predicting a set of parameter free relations among different aspects of the data. Finally, we will have to think carefully about what it means for models to â€œwork.â€ We begin with early explorations at relatively small $N$ (Â§IV.B), then turn to a wide variety of larger networks (Â§IV.C), and finally address how these analyses can catch up to the experimental frontier (Â§IV.D).</p>
</blockquote>
<p>ä¸ºäº†ç»“æŸè¿™ä¸ªä»‹ç»ï¼Œæˆ‘ä»¬å¼ºè°ƒæœ€å¤§ç†µä¸åŒäºé€šå¸¸çš„ç†è®ºã€‚æˆ‘ä»¬ä¸æ˜¯ä»ä¸€ä¸ªç†è®ºåŸåˆ™ç”šè‡³ä¸€ä¸ªæ¨¡å‹å¼€å§‹ã€‚ç›¸åï¼Œæˆ‘ä»¬ä»æ•°æ®çš„ä¸€äº›ç‰¹å¾å¼€å§‹ï¼Œå¹¶æµ‹è¯•è¿™äº›ç‰¹å¾æ˜¯å¦ç¼–ç äº†æè¿°ç³»ç»Ÿæ‰€éœ€çš„ä¸€åˆ‡çš„å‡è®¾ã€‚æ¯å½“æˆ‘ä»¬ä½¿ç”¨è¿™ç§æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šå›åˆ°æ–¹ç¨‹ï¼ˆ18ï¼‰ä¸­å®šä¹‰çš„ä¼˜åŒ–é—®é¢˜çš„åŸºæœ¬ç»“æ„åŠå…¶åœ¨æ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰ä¸­çš„æ­£å¼è§£ï¼Œä½†æ²¡æœ‰å•ä¸€çš„æœ€å¤§ç†µæ¨¡å‹ï¼Œæ¯æ¬¡æˆ‘ä»¬éƒ½éœ€è¦æ˜ç¡®ï¼šå“ªäº›æ˜¯æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹é‡ç°å…¶æµ‹é‡æœŸæœ›å€¼çš„å¯è§‚å¯Ÿé‡ $f_{\mu}$ï¼Ÿæˆ‘ä»¬èƒ½æ‰¾åˆ°ç›¸åº”çš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•° $\lambda_{mu}$ å—ï¼Ÿè¿™äº›å‚æ•°æœ‰è‡ªç„¶çš„è§£é‡Šå—ï¼Ÿä¸€æ—¦æˆ‘ä»¬å›ç­”äº†è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å°±å¯ä»¥é—®è¿™äº›ç›¸å¯¹ç®€å•çš„ç»Ÿè®¡ç‰©ç†æè¿°æ˜¯å¦åšå‡ºäº†ä¸å®éªŒä¸€è‡´çš„é¢„æµ‹ã€‚åœ¨å­¦ä¹ æ¨¡å‹ï¼ˆåŒ¹é…è§‚å¯Ÿåˆ°çš„æœŸæœ›å€¼ï¼‰å’Œæµ‹è¯•æ¨¡å‹ï¼ˆé¢„æµ‹æ–°çš„æœŸæœ›å€¼ï¼‰ä¹‹é—´å­˜åœ¨ä¸€ç§å¼‚å¸¸æ¸…æ™°çš„åˆ†ç¦»ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè¯´ï¼Œæˆ‘ä»¬å¯ä»¥å°†æœ€å¤§ç†µè§†ä¸ºé¢„æµ‹æ•°æ®ä¸åŒæ–¹é¢ä¹‹é—´çš„ä¸€ç»„æ— å‚æ•°å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ä»”ç»†æ€è€ƒæ¨¡å‹â€œå·¥ä½œâ€çš„å«ä¹‰ã€‚æˆ‘ä»¬ä»ç›¸å¯¹è¾ƒå° $N$ çš„æ—©æœŸæ¢ç´¢å¼€å§‹ï¼ˆÂ§IV.Bï¼‰ï¼Œç„¶åè½¬å‘å„ç§æ›´å¤§çš„ç½‘ç»œï¼ˆÂ§IV.Cï¼‰ï¼Œæœ€åè§£å†³è¿™äº›åˆ†æå¦‚ä½•èµ¶ä¸Šå®éªŒå‰æ²¿çš„é—®é¢˜ï¼ˆÂ§IV.Dï¼‰ã€‚</p>
<h1 id="first-connections-to-neurons">First connections to neurons<a hidden class="anchor" aria-hidden="true" href="#first-connections-to-neurons">#</a></h1>
<blockquote>
<p>Suppose we observe three neurons, and measure their mean activity as well as their pairwise correlations. Given these measurements, should we be surprised by how often the three neurons are active together? Maximum entropy provides a way of answering this question, generating a â€œnull modelâ€ prediction assuming all the correlation structure is captured in the pairs, and this was appreciated âˆ¼2000 (Martignon et al., 2000). Over the next several years a more ambitious idea emerged: could we build maximum entropy models for patterns of activity in larger populations of neurons? The first target for this analysis was a population of neurons in the salamander retina, as it responds to naturalistic visual inputs (Schneidman et al., 2006).</p>
</blockquote>
<p>å‡è®¾æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸‰ä¸ªç¥ç»å…ƒï¼Œå¹¶æµ‹é‡å®ƒä»¬çš„å¹³å‡æ´»åŠ¨ä»¥åŠå®ƒä»¬çš„æˆå¯¹ç›¸å…³æ€§ã€‚é‰´äºè¿™äº›æµ‹é‡ç»“æœï¼Œæˆ‘ä»¬æ˜¯å¦åº”è¯¥å¯¹è¿™ä¸‰ä¸ªç¥ç»å…ƒä¸€èµ·æ´»è·ƒçš„é¢‘ç‡æ„Ÿåˆ°æƒŠè®¶ï¼Ÿæœ€å¤§ç†µæä¾›äº†ä¸€ç§å›ç­”è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•ï¼Œç”Ÿæˆä¸€ä¸ªâ€œé›¶æ¨¡å‹â€é¢„æµ‹ï¼Œå‡è®¾æ‰€æœ‰çš„ç›¸å…³ç»“æ„éƒ½åŒ…å«åœ¨å¯¹ä¸­ï¼Œè¿™åœ¨å¤§çº¦ 2000 å¹´è¢«è®¤è¯†åˆ°ï¼ˆMartignon ç­‰äººï¼Œ2000ï¼‰ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ å¹´é‡Œï¼Œä¸€ä¸ªæ›´é›„å¿ƒå‹ƒå‹ƒçš„æƒ³æ³•å‡ºç°äº†ï¼šæˆ‘ä»¬èƒ½å¦ä¸ºæ›´å¤§ç¾¤ä½“çš„ç¥ç»å…ƒæ´»åŠ¨æ¨¡å¼æ„å»ºæœ€å¤§ç†µæ¨¡å‹ï¼Ÿè¿™ç§åˆ†æçš„ç¬¬ä¸€ä¸ªç›®æ ‡æ˜¯è¾èˆè§†ç½‘è†œä¸­çš„ä¸€ç¾¤ç¥ç»å…ƒï¼Œå› ä¸ºå®ƒå¯¹è‡ªç„¶è§†è§‰è¾“å…¥åšå‡ºååº”ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚</p>
<blockquote>
<p>In response to natural movies, the output neurons of the retinaâ€”the â€œganglion cellsâ€ that carry visual signals from eye to brain, and which as a group form the optic nerveâ€”are sparsely activated, generating an average of just a few spikes per second each (Fig 7A, B). Those initial experiments monitored populations of up to forty neurons in a small patch of the retina, with recordings of up to one hour. Pairs of neurons have temporal correlations with a relatively sharp peak or trough on a broad background that tracks longer timescales in the visual input (Fig 7C). If we discretize time into bins of $\Delta\tau = 20$ ms then we capture most of the short time correlations but still have a very low probability of seeing two spikes in the same bin, so that responses of neuron i become binary, $\sigma_{i} = \{0, 1\}$.</p>
</blockquote>
<p>ä¸ºäº†å“åº”è‡ªç„¶æ´»åŠ¨ï¼Œè§†ç½‘è†œçš„è¾“å‡ºç¥ç»å…ƒâ€”â€”å°†è§†è§‰ä¿¡å·ä»çœ¼ç›ä¼ é€’åˆ°å¤§è„‘çš„â€œç¥ç»èŠ‚ç»†èƒâ€ï¼Œå®ƒä»¬ä½œä¸ºä¸€ä¸ªæ•´ä½“å½¢æˆè§†ç¥ç»â€”â€”è¢«ç¨€ç–æ¿€æ´»ï¼Œæ¯ä¸ªç¥ç»å…ƒå¹³å‡æ¯ç§’åªäº§ç”Ÿå‡ ä¸ªå°–å³°ï¼ˆå›¾ 7Aï¼ŒBï¼‰ã€‚é‚£äº›åˆå§‹å®éªŒç›‘æµ‹äº†è§†ç½‘è†œå°å—ä¸­å¤šè¾¾å››åä¸ªç¥ç»å…ƒçš„ç¾¤ä½“ï¼Œè®°å½•æ—¶é—´é•¿è¾¾ä¸€å°æ—¶ã€‚æˆå¯¹çš„ç¥ç»å…ƒå…·æœ‰æ—¶é—´ç›¸å…³æ€§ï¼Œåœ¨è§†è§‰è¾“å…¥çš„è¾ƒé•¿æ—¶é—´å°ºåº¦ä¸Šè·Ÿè¸ªè¾ƒå®½èƒŒæ™¯ä¸Šçš„ç›¸å¯¹å°–é”å³°å€¼æˆ–è°·å€¼ï¼ˆå›¾ 7Cï¼‰ã€‚å¦‚æœæˆ‘ä»¬å°†æ—¶é—´ç¦»æ•£åŒ–ä¸º $\Delta\tau = 20$ ms çš„æ—¶é—´æ®µï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ•æ‰åˆ°äº†å¤§éƒ¨åˆ†çŸ­æ—¶é—´ç›¸å…³æ€§ï¼Œä½†ä»ç„¶å¾ˆå°‘çœ‹åˆ°åŒä¸€æ—¶é—´æ®µå†…æœ‰ä¸¤ä¸ªå°–å³°ï¼Œå› æ­¤ç¥ç»å…ƒ i çš„å“åº”å˜ä¸ºäºŒè¿›åˆ¶ï¼Œ$\sigma_{i} = \{0, 1\}$ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/13/FgdE8S4rsWtiBja.png" alt=""  /></p>
<p>FIG. 7 Responses of the salamander retina to naturalistic movies (Schneidman et al., 2006). (A) Raster plot of the action potentials from $N = 40$ neurons. Each dot represents a spike from one cell. (B) Expanded view of the green box in (A), showing the discretization of time into bins of width $\Delta\tau = 20$ms. The result (bottom) is that the state of the network is a binary word $\{\sigma_{i}\}$. (C) Correlations between two neurons. Results are shown as the probability per unit time of a spike in cell $j$ (spike rate) given that there is a spike in cell $i$ at time $t = 0$; the plateau at long times should be the mean rate $r_{j} = \langle\sigma_{j}\rangle/\Delta\tau$. There a peak with a width ~ 100 ms, related to time scales in the visual input, and a peak with width ~ 20ms emphasizes in the inset; this motivates the choice of bins size. (D) Distribution of (off-diagonal) correlation coefficients, from Eq (24), across the population of $N = 40$ neurons. (E) Probability that $K$ out of the $N = 40$ neurons are active in the same time bin (red) compared with expectations if activity of each neuron were independent of all the others (blue). Dashed lines are exponential (red) and Poisson (blue), to guide the eye. (F) Predicted occurrence rates of different binary patterns vs the observed rates, for the independent model $P_{1}$ [Eqs (29, 30), blue] and the pairwise maximum entropy model $P_{2}$ [Eqs (35, 33), red].</p>
</blockquote>
<p>å›¾ 7 è¾èˆè§†ç½‘è†œå¯¹è‡ªç„¶ç”µå½±çš„å“åº”ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚(A) $N = 40$ ä¸ªç¥ç»å…ƒçš„åŠ¨ä½œç”µä½å…‰æ …å›¾ã€‚æ¯ä¸ªç‚¹ä»£è¡¨ä¸€ä¸ªç»†èƒçš„ä¸€ä¸ªå°–å³°ã€‚(B) (A) ä¸­ç»¿è‰²æ¡†çš„æ”¾å¤§è§†å›¾ï¼Œæ˜¾ç¤ºæ—¶é—´è¢«ç¦»æ•£åŒ–ä¸ºå®½åº¦ä¸º $\Delta\tau = 20$ms çš„æ—¶é—´æ®µã€‚ç»“æœï¼ˆåº•éƒ¨ï¼‰æ˜¯ç½‘ç»œçš„çŠ¶æ€æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶å­— $\{\sigma_{i}\}$ã€‚(C) ä¸¤ä¸ªç¥ç»å…ƒä¹‹é—´çš„ç›¸å…³æ€§ã€‚ç»“æœæ˜¾ç¤ºä¸ºåœ¨ç»†èƒ $i$ åœ¨æ—¶é—´ $t = 0$ å¤„æœ‰ä¸€ä¸ªå°–å³°çš„æƒ…å†µä¸‹ï¼Œç»†èƒ $j$ ä¸­å°–å³°çš„å•ä½æ—¶é—´æ¦‚ç‡ï¼ˆå°–å³°ç‡ï¼‰ï¼›é•¿æ—¶é—´å¤„çš„å¹³å°åº”è¯¥æ˜¯å¹³å‡ç‡ $r_{j} = \langle\sigma_{j}\rangle/\Delta\tau$ã€‚æœ‰ä¸€ä¸ªå®½åº¦çº¦ä¸º 100 ms çš„å³°å€¼ï¼Œä¸è§†è§‰è¾“å…¥ä¸­çš„æ—¶é—´å°ºåº¦æœ‰å…³ï¼Œæ’å›¾ä¸­å¼ºè°ƒäº†ä¸€ä¸ªå®½åº¦çº¦ä¸º 20ms çš„å³°å€¼ï¼›è¿™æ¿€å‘äº†é€‰æ‹©ç®±å­å¤§å°çš„åŠ¨æœºã€‚(D) è·¨è¶Š $N = 40$ ä¸ªç¥ç»å…ƒç¾¤ä½“çš„ï¼ˆéå¯¹è§’çº¿ï¼‰ç›¸å…³ç³»æ•°çš„åˆ†å¸ƒï¼Œæ¥è‡ªæ–¹ç¨‹ï¼ˆ24ï¼‰ã€‚(E) åœ¨åŒä¸€æ—¶é—´æ®µå†… $N = 40$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªæ´»è·ƒçš„æ¦‚ç‡ï¼ˆçº¢è‰²ï¼‰ä¸å¦‚æœæ¯ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–ç¥ç»å…ƒçš„é¢„æœŸï¼ˆè“è‰²ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚è™šçº¿åˆ†åˆ«ä¸ºæŒ‡æ•°ï¼ˆçº¢è‰²ï¼‰å’Œæ³Šæ¾ï¼ˆè“è‰²ï¼‰ï¼Œä»¥å¼•å¯¼çœ¼ç›ã€‚(F) ä¸åŒäºŒè¿›åˆ¶æ¨¡å¼çš„é¢„æµ‹å‘ç”Ÿç‡ä¸è§‚å¯Ÿåˆ°çš„å‘ç”Ÿç‡ï¼Œå¯¹äºç‹¬ç«‹æ¨¡å‹ $P_{1}$ [æ–¹ç¨‹ï¼ˆ29ï¼Œ30ï¼‰ï¼Œè“è‰²] å’Œæˆå¯¹æœ€å¤§ç†µæ¨¡å‹ $P_{2}$ [æ–¹ç¨‹ï¼ˆ35ï¼Œ33ï¼‰ï¼Œçº¢è‰²]ã€‚</p>
</blockquote>
<blockquote>
<p>If we define as usual the fluctuations around the mean,</p>
<p>$$
\delta\sigma_{i} = \sigma_{i} - \langle\sigma_{i}\rangle,
$$</p>
<p>then the data sets were large enough to get good estimates of the covariance</p>
<p>$$
C_{ij} = \langle\delta\sigma_{i}\delta\sigma_{j}\rangle = \langle\sigma_{i}\sigma_{j}\rangle_{c}
$$</p>
<p>where $\langle\cdots\rangle_{c}$ denotes the connected part of the correlations; in many cases we have more intuition about the correlation matrix</p>
<p>$$
\widetilde{C}_{ij} = \frac{C_{ij}}{\sqrt{C_{ii}C_{jj}}}
$$</p>
<p>Importantly, these pairwise correlations are weak: almost all of the $|\widetilde{C}_{i\neq j}|&lt;0.1$, and the bulk of these correlations are just a few percent (Fig 7D). The recordings are long enough that these weak correlations are statistically significant, and almost none of the matrix elements are zero within errors. Correlations thus are weak and widespread, which seems to be common across many different regions of the brain.</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·å®šä¹‰å›´ç»•å‡å€¼çš„æ³¢åŠ¨ï¼Œ</p>
<p>$$
\delta\sigma_{i} = \sigma_{i} - \langle\sigma_{i}\rangle,
$$</p>
<p>é‚£ä¹ˆæ•°æ®é›†è¶³å¤Ÿå¤§ï¼Œå¯ä»¥å¾ˆå¥½åœ°ä¼°è®¡åæ–¹å·®</p>
<p>$$
C_{ij} = \langle\delta\sigma_{i}\delta\sigma_{j}\rangle = \langle\sigma_{i}\sigma_{j}\rangle_{c}
$$</p>
<p>å…¶ä¸­ $\langle\cdots\rangle_{c}$ è¡¨ç¤ºç›¸å…³æ€§çš„è¿æ¥éƒ¨åˆ†ï¼›åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯¹ç›¸å…³çŸ©é˜µæœ‰æ›´å¤šçš„ç›´è§‰</p>
<p>$$
\widetilde{C}_{ij} = \frac{C_{ij}}{\sqrt{C_{ii}C_{jj}}}
$$</p>
<p>é‡è¦çš„æ˜¯ï¼Œè¿™äº›æˆå¯¹ç›¸å…³æ€§æ˜¯å¼±çš„ï¼šå‡ ä¹æ‰€æœ‰çš„ $|\widetilde{C}_{i\neq j}|&lt;0.1$ï¼Œè€Œä¸”è¿™äº›ç›¸å…³æ€§çš„ä¸»ä½“åªæœ‰å‡ ä¸ªç™¾åˆ†ç‚¹ï¼ˆå›¾ 7Dï¼‰ã€‚è®°å½•æ—¶é—´è¶³å¤Ÿé•¿ï¼Œè¿™äº›å¾®å¼±çš„ç›¸å…³æ€§åœ¨ç»Ÿè®¡ä¸Šæ˜¯æ˜¾è‘—çš„ï¼Œå¹¶ä¸”çŸ©é˜µå…ƒç´ å‡ ä¹æ²¡æœ‰åœ¨è¯¯å·®èŒƒå›´å†…ä¸ºé›¶ã€‚å› æ­¤ï¼Œç›¸å…³æ€§æ˜¯å¼±è€Œå¹¿æ³›çš„ï¼Œè¿™ä¼¼ä¹åœ¨å¤§è„‘çš„è®¸å¤šä¸åŒåŒºåŸŸä¸­å¾ˆå¸¸è§ã€‚</p>
<blockquote>
<p>If we look just at two neurons, the approximation that they are independent of one another is very good, because the correlations are so weak. But if we look more globally then the widespread correlations combine to have qualitative effects. As an example, we can ask for the probability that $K$ out of $N = 40$ neurons are active in the same time bin, $P_{N}(K)$, and we find that this has a much longer tail than expected if the cells were independent (Fig 7E); simultaneous activity of $K = 10$ neurons already is $\sim 10^{3}\times$ more likely than in the independent model.</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬åªçœ‹ä¸¤ä¸ªç¥ç»å…ƒï¼Œç”±äºç›¸å…³æ€§éå¸¸å¼±ï¼Œå®ƒä»¬ç›¸äº’ç‹¬ç«‹çš„è¿‘ä¼¼æ˜¯éå¸¸å¥½çš„ã€‚ä½†å¦‚æœæˆ‘ä»¬æ›´å…¨é¢åœ°è§‚å¯Ÿï¼Œé‚£ä¹ˆå¹¿æ³›çš„ç›¸å…³æ€§ä¼šç»“åˆèµ·æ¥äº§ç”Ÿå®šæ€§çš„å½±å“ã€‚ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è¯¢é—®åœ¨åŒä¸€æ—¶é—´æ®µå†… $N = 40$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªæ´»è·ƒçš„æ¦‚ç‡ $P_{N}(K)$ï¼Œæˆ‘ä»¬å‘ç°å¦‚æœç»†èƒæ˜¯ç‹¬ç«‹çš„ï¼Œè¿™ä¸ªæ¦‚ç‡çš„å°¾éƒ¨è¦é•¿å¾—å¤šï¼ˆå›¾ 7Eï¼‰ï¼›$K = 10$ ä¸ªç¥ç»å…ƒçš„åŒæ—¶æ´»åŠ¨å·²ç»æ¯”ç‹¬ç«‹æ¨¡å‹é«˜å‡ºçº¦ $10^{3}$ å€ã€‚</p>
<blockquote>
<p>If we focus on $N = 10$ neurons then the experiments are long enough to sample all $\Omega\sim 10^{3}$ states, and the probabilities of these different binary words depart dramatically from the predictions of an independent model (Fig 7F). If we group the different binary words by the total number of active neurons, then the predictions of the independent model actually are antiâ€“correlated with the real data. We emphasize that these failures occur despite the fact that pairwise correlations are weak, and that they are visible at a relatively modest $N = 10$.</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬å…³æ³¨ $N = 10$ ä¸ªç¥ç»å…ƒï¼Œé‚£ä¹ˆå®éªŒæ—¶é—´è¶³å¤Ÿé•¿ï¼Œå¯ä»¥å¯¹æ‰€æœ‰ $\Omega\sim 10^{3}$ çŠ¶æ€è¿›è¡Œé‡‡æ ·ï¼Œå¹¶ä¸”è¿™äº›ä¸åŒäºŒè¿›åˆ¶å­—çš„æ¦‚ç‡ä¸ç‹¬ç«‹æ¨¡å‹çš„é¢„æµ‹æœ‰æ˜¾è‘—åç¦»ï¼ˆå›¾ 7Fï¼‰ã€‚å¦‚æœæˆ‘ä»¬æŒ‰æ´»è·ƒç¥ç»å…ƒçš„æ€»æ•°å¯¹ä¸åŒçš„äºŒè¿›åˆ¶å­—è¿›è¡Œåˆ†ç»„ï¼Œé‚£ä¹ˆç‹¬ç«‹æ¨¡å‹çš„é¢„æµ‹å®é™…ä¸Šä¸çœŸå®æ•°æ®å‘ˆè´Ÿç›¸å…³ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œå°½ç®¡æˆå¯¹ç›¸å…³æ€§å¾ˆå¼±ï¼Œä½†è¿™äº›å¤±è´¥ä»ç„¶å‘ç”Ÿï¼Œå¹¶ä¸”å®ƒä»¬åœ¨ç›¸å¯¹é€‚åº¦çš„ $N = 10$ ä¸‹æ˜¯å¯è§çš„ã€‚</p>
<blockquote>
<p>If we want to build a model for the patterns of activity in networks of neurons it certainly makes sense to insist that we match the mean activity of each cell. At the risk of being pedantic, what this means explicitly is that we are looking for a probability distribution over network states, $P_{1}(\vec{\sigma})$ that has the maximum entropy while correctly predicting the expectation values</p>
<p>$$
m_{i} \equiv \langle\sigma_{i}\rangle_{\text{expt}} = \langle\sigma_{i}\rangle_{P_{1}}
$$</p>
<p>Referring back to Eq (17), the observables that we constrain become</p>
<p>$$
\{f_{\mu}^{(1)}\}\rightarrow \{\sigma_{i}\}
$$</p>
<p>note that $i = 1, 2,\cdots, N$, where $N$ is the number of neurons. To implement these constraints we need one Lagrange multiplier for each neuron, and it is convenient to write this multiplier as an â€œeffective fieldâ€ $h_{i}$, so that the general Eqs (20, 21) become</p>
<p>$$
\begin{aligned}
P_{1}(\vec{\sigma}) &amp;= \frac{1}{Z_{1}}\exp{[-E_{1}(\vec{\sigma})]}\\
E_{1}(\vec{\sigma}) &amp;= \sum_{\mu}\lambda_{\mu}^{(1)}f_{\mu}^{(1)}\\
&amp;= \sum_{i=1}^{N}h_{i}\sigma_{i}
\end{aligned}
$$</p>
<p>We notice that $E_1$ is the energy function for independent spins in local fields, and so the probability distribution over states factorizes,</p>
<p>$$
P_{1}(\vec{\sigma}) \propto \prod_{i=1}^{N}e^{-h_{i}\sigma_{i}}
$$</p>
<p>Thus a maximum entropy model which matches only the mean activities of individual neurons is a model in which the activity of each cell is independent of all the others. We have seen that this model is in dramatic disagreement with the data.</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬æƒ³ä¸ºç¥ç»å…ƒç½‘ç»œä¸­çš„æ´»åŠ¨æ¨¡å¼æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼ŒåšæŒåŒ¹é…æ¯ä¸ªç»†èƒçš„å¹³å‡æ´»åŠ¨å½“ç„¶æ˜¯æœ‰æ„ä¹‰çš„ã€‚å†’ç€å•°å—¦çš„é£é™©ï¼Œè¿™æ˜ç¡®æ„å‘³ç€æˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªç½‘ç»œçŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒ $P_{1}(\vec{\sigma})$ï¼Œè¯¥åˆ†å¸ƒå…·æœ‰æœ€å¤§ç†µï¼ŒåŒæ—¶æ­£ç¡®é¢„æµ‹æœŸæœ›å€¼</p>
<p>$$
m_{i} \equiv \langle\sigma_{i}\rangle_{\text{expt}} = \langle\sigma_{i}\rangle_{P_{1}}
$$</p>
<p>å›åˆ°æ–¹ç¨‹ï¼ˆ17ï¼‰ï¼Œæˆ‘ä»¬çº¦æŸçš„å¯è§‚å¯Ÿé‡å˜ä¸º</p>
<p>$$
\{f_{\mu}^{(1)}\}\rightarrow \{\sigma_{i}\}
$$</p>
<p>æ³¨æ„ $i = 1, 2,\cdots, N$ï¼Œå…¶ä¸­ $N$ æ˜¯ç¥ç»å…ƒçš„æ•°é‡ã€‚ä¸ºäº†å®ç°è¿™äº›çº¦æŸï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªç¥ç»å…ƒä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œå¹¶ä¸”å°†è¿™ä¸ªä¹˜æ•°å†™æˆâ€œæœ‰æ•ˆåœºâ€ $h_{i}$ æ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œå› æ­¤ä¸€èˆ¬çš„æ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰å˜ä¸º</p>
<p>$$
\begin{aligned}
P_{1}(\vec{\sigma}) &amp;= \frac{1}{Z_{1}}\exp{[-E_{1}(\vec{\sigma})]}\\
E_{1}(\vec{\sigma}) &amp;= \sum_{\mu}\lambda_{\mu}^{(1)}f_{\mu}^{(1)}\\
&amp;= \sum_{i=1}^{N}h_{i}\sigma_{i}
\end{aligned}
$$</p>
<p>æˆ‘ä»¬æ³¨æ„åˆ° $E_1$ æ˜¯å±€éƒ¨åœºä¸­ç‹¬ç«‹è‡ªæ—‹çš„èƒ½é‡å‡½æ•°ï¼Œå› æ­¤çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥åˆ†è§£ä¸ºï¼Œ</p>
<p>$$
P_{1}(\vec{\sigma}) \propto \prod_{i=1}^{N}e^{-h_{i}\sigma_{i}}
$$</p>
<p>å› æ­¤ï¼Œä¸€ä¸ªä»…åŒ¹é…å•ä¸ªç¥ç»å…ƒå¹³å‡æ´»åŠ¨çš„æœ€å¤§ç†µæ¨¡å‹æ˜¯ä¸€ä¸ªæ¯ä¸ªç»†èƒçš„æ´»åŠ¨éƒ½ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–ç»†èƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°è¿™ä¸ªæ¨¡å‹ä¸æ•°æ®æœ‰æ˜¾è‘—çš„ä¸ä¸€è‡´ã€‚</p>
<blockquote>
<p>A natural first step in trying to capture the nonindependence of neurons is to build a maximum entropy model that matches pairwise correlations. Thus, we are looking for a distribution $P_{2}(\sigma)$ that has maximum entropy while matching the mean activities as in Eq (25) and also the covariance of activity</p>
<p>$$
C_{ij}\equiv \langle\delta\sigma_{i}\delta\sigma_{j}\rangle_{\text{expt}} = \langle\delta\sigma_{i}\delta\sigma_{j}\rangle_{P_{2}}
$$</p>
<p>In the language of Eq (17) this means that we have a second set of relevant observables</p>
<p>$$
\{f_{\nu}^{(2)}\} \rightarrow \{\sigma_{i}\sigma_{j}\}
$$</p>
<p>As before we need one Lagrange multiplier for each constrained observable, and it is useful to think of the Lagrange multiplier that constrains $\sigma_{i}\sigma_{j}$ as being a â€œspinspinâ€ coupling $\lambda_{ij} = J_{ij}$. Recalling that each extra constraint adds a term to the effective energy function, Eqs (20, 21) become</p>
<p>$$
\begin{aligned}
P_{2}(\vec{\sigma}) &amp;= \frac{1}{Z_{2}(\{h_{i};J_{ij}\})}e^{-E_{2}(\vec{\sigma})}\\
E_{2}(\vec{\sigma}) &amp;= \sum_{\mu}\lambda_{\mu}^{(1)}f_{\mu}^{(1)} + \sum_{\mu}\lambda_{\mu}^{(2)}f_{\mu}^{(2)}\\
&amp;= \sum_{i=1}^{N}h_{i}\sigma_{i} + \frac{1}{2}\sum_{i\neq j}J_{ij}\sigma_{i}\sigma_{j}
\end{aligned}
$$</p>
<p>This is exactly an Ising model with pairwise interactions among the spinsâ€”not an analogy but a mathematical equivalence.</p>
</blockquote>
<p>æ•æ‰ç¥ç»å…ƒéç‹¬ç«‹æ€§çš„ä¸€ä¸ªè‡ªç„¶ç¬¬ä¸€æ­¥æ˜¯æ„å»ºä¸€ä¸ªåŒ¹é…æˆå¯¹ç›¸å…³æ€§çš„æœ€å¤§ç†µæ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªåˆ†å¸ƒ $P_{2}(\sigma)$ï¼Œè¯¥åˆ†å¸ƒå…·æœ‰æœ€å¤§ç†µï¼ŒåŒæ—¶åŒ¹é…æ–¹ç¨‹ï¼ˆ25ï¼‰ä¸­çš„å¹³å‡æ´»åŠ¨ä»¥åŠæ´»åŠ¨çš„åæ–¹å·®</p>
<p>$$
C_{ij}\equiv \langle\delta\sigma_{i}\delta\sigma_{j}\rangle_{\text{expt}} = \langle\delta\sigma_{i}\delta\sigma_{j}\rangle_{P_{2}}
$$</p>
<p>ç”¨æ–¹ç¨‹ï¼ˆ17ï¼‰çš„è¯­è¨€æ¥è¯´ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æœ‰ç¬¬äºŒç»„ç›¸å…³çš„å¯è§‚å¯Ÿé‡</p>
<p>$$
\{f_{\nu}^{(2)}\} \rightarrow \{\sigma_{i}\sigma_{j}\}
$$</p>
<p>åƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªå—çº¦æŸçš„å¯è§‚å¯Ÿé‡ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œå¹¶ä¸”å°†çº¦æŸ $\sigma_{i}\sigma_{j}$ çš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°è§†ä¸ºâ€œè‡ªæ—‹-è‡ªæ—‹â€è€¦åˆ $\lambda_{ij} = J_{ij}$ æ˜¯æœ‰ç”¨çš„ã€‚å›æƒ³ä¸€ä¸‹ï¼Œæ¯ä¸ªé¢å¤–çš„çº¦æŸéƒ½ä¼šå‘æœ‰æ•ˆèƒ½é‡å‡½æ•°æ·»åŠ ä¸€é¡¹ï¼Œæ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰å˜ä¸º</p>
<p>$$
\begin{aligned}
P_{2}(\vec{\sigma}) &amp;= \frac{1}{Z_{2}(\{h_{i};J_{ij}\})}e^{-E_{2}(\vec{\sigma})}\\
E_{2}(\vec{\sigma}) &amp;= \sum_{\mu}\lambda_{\mu}^{(1)}f_{\mu}^{(1)} + \sum_{\mu}\lambda_{\mu}^{(2)}f_{\mu}^{(2)}\\
&amp;= \sum_{i=1}^{N}h_{i}\sigma_{i} + \frac{1}{2}\sum_{i\neq j}J_{ij}\sigma_{i}\sigma_{j}
\end{aligned}
$$</p>
<p>è¿™æ­£æ˜¯å…·æœ‰è‡ªæ—‹ä¹‹é—´æˆå¯¹ç›¸äº’ä½œç”¨çš„ Ising æ¨¡å‹â€”â€”ä¸æ˜¯ç±»æ¯”ï¼Œè€Œæ˜¯æ•°å­¦ç­‰ä»·ã€‚</p>
<blockquote>
<p>Ising models for networks of neurons have a long history, as described in Â§II.C. In their earliest appearance, these models emerged from a hypothetical, simplified model of the underlying dynamics. Here they emerge as the least structured models consistent with measured properties of the network. As a result, we arrive not at some arbitrary Ising model, where we are free to choose the fields and couplings, but at a particular model that describes the actual network of neurons we are observing. To complete this construction we have to adjust the fields and couplings to match the observed mean activities and correlations. Concretely we have to solve Eqs (25, 31), which can be rewritten as</p>
<p>$$
\begin{aligned}
\langle \sigma_{i}\rangle_{\text{expt}} &amp;= \langle\sigma_{i}\rangle_{P_{2}} = \frac{\partial \ln{Z_{2}(\{h_{i};J_{ij}\})}}{\partial h_{i}}\\
\langle\sigma_{i}\sigma_{j}\rangle_{\text{expt}} &amp;= \langle\sigma_{i}\sigma_{j}\rangle_{P_{2}} = \frac{\partial \ln{Z_{2}(\{h_{i};J_{ij}\})}}{\partial J_{ij}}
\end{aligned}
$$</p>
<p>With $N = 10$ neurons this is challenging but can be done exactly, since the partition function is a sum over just $\Omega\sim 1000$ terms. Once we are done, the model is specified completely. Anything that we compute is a prediction, and there is no room to adjust parameters in search of better agreement with the data.</p>
</blockquote>
<p>ç¥ç»å…ƒç½‘ç»œçš„ Ising æ¨¡å‹æœ‰ç€æ‚ ä¹…çš„å†å²ï¼Œå¦‚ Â§II.C æ‰€è¿°ã€‚åœ¨å®ƒä»¬æœ€æ—©å‡ºç°æ—¶ï¼Œè¿™äº›æ¨¡å‹æºè‡ªå¯¹æ½œåœ¨åŠ¨åŠ›å­¦çš„å‡è®¾æ€§ç®€åŒ–æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œå®ƒä»¬ä½œä¸ºä¸ç½‘ç»œçš„æµ‹é‡å±æ€§ä¸€è‡´çš„æœ€å°‘ç»“æ„åŒ–æ¨¡å‹å‡ºç°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸æ˜¯å¾—åˆ°æŸä¸ªä»»æ„çš„ Ising æ¨¡å‹ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å¯ä»¥è‡ªç”±é€‰æ‹©åœºå’Œè€¦åˆï¼Œè€Œæ˜¯å¾—åˆ°ä¸€ä¸ªæè¿°æˆ‘ä»¬æ­£åœ¨è§‚å¯Ÿçš„å®é™…ç¥ç»å…ƒç½‘ç»œçš„ç‰¹å®šæ¨¡å‹ã€‚ä¸ºäº†å®Œæˆè¿™ä¸ªæ„å»ºï¼Œæˆ‘ä»¬å¿…é¡»è°ƒæ•´åœºå’Œè€¦åˆä»¥åŒ¹é…è§‚å¯Ÿåˆ°çš„å¹³å‡æ´»åŠ¨å’Œç›¸å…³æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¿…é¡»è§£å†³æ–¹ç¨‹ï¼ˆ25ï¼Œ31ï¼‰ï¼Œå®ƒä»¬å¯ä»¥é‡å†™ä¸º</p>
<p>$$
\begin{aligned}
\langle \sigma_{i}\rangle_{\text{expt}} &amp;= \langle\sigma_{i}\rangle_{P_{2}} = \frac{\partial \ln{Z_{2}(\{h_{i};J_{ij}\})}}{\partial h_{i}}\\
\langle\sigma_{i}\sigma_{j}\rangle_{\text{expt}} &amp;= \langle\sigma_{i}\sigma_{j}\rangle_{P_{2}} = \frac{\partial \ln{Z_{2}(\{h_{i};J_{ij}\})}}{\partial J_{ij}}
\end{aligned}
$$</p>
<p>å¯¹äº $N = 10$ ä¸ªç¥ç»å…ƒæ¥è¯´ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†å¯ä»¥ç²¾ç¡®å®Œæˆï¼Œå› ä¸ºé…åˆ†å‡½æ•°åªæ˜¯ $\Omega\sim 1000$ é¡¹çš„æ€»å’Œã€‚ä¸€æ—¦æˆ‘ä»¬å®Œæˆï¼Œæ¨¡å‹å°±å®Œå…¨æŒ‡å®šäº†ã€‚æˆ‘ä»¬è®¡ç®—çš„ä»»ä½•ä¸œè¥¿éƒ½æ˜¯ä¸€ä¸ªé¢„æµ‹ï¼Œæ²¡æœ‰è°ƒæ•´å‚æ•°ä»¥å¯»æ±‚ä¸æ•°æ®æ›´å¥½ä¸€è‡´çš„ä½™åœ°ã€‚</p>
<blockquote>
<p>As noted above, with $N = 10$ neurons the experiments are long enough to get a reasonably full sampling of the probability distribution over $\vec{\sigma}$. This provides the most detailed possible test of the model $P_{2}$, and in Fig 7F we see that the agreement between theory and experiment is excellent, except for very rare patterns where errors in the estimate of the probability are larger. Similar results are obtained for other groups of $N = 10$ cells drawn out of the full population of $N = 40$. Quantitatively we can measure the Jensenâ€“Shannon divergence between the estimated distribution $P_{\text{data}}(\sigma)$ and the model $P_{2}(\sigma)$; across multiple choices of ten cells this fluctuates by a factor of two around $D_{JS} = 0.001$ bits, which means that it takes thousands of independent observations to distinguish the model from the data.</p>
</blockquote>
<p>æ­£å¦‚ä¸Šé¢æ‰€æŒ‡å‡ºçš„ï¼Œå¯¹äº $N = 10$ ä¸ªç¥ç»å…ƒï¼Œå®éªŒæ—¶é—´è¶³å¤Ÿé•¿ï¼Œå¯ä»¥å¯¹ $\vec{\sigma}$ ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œåˆç†å®Œæ•´çš„é‡‡æ ·ã€‚è¿™ä¸ºæ¨¡å‹ $P_{2}$ æä¾›äº†æœ€è¯¦ç»†çš„å¯èƒ½æµ‹è¯•ï¼Œåœ¨å›¾ 7F ä¸­æˆ‘ä»¬çœ‹åˆ°ç†è®ºä¸å®éªŒä¹‹é—´çš„ä¸€è‡´æ€§éå¸¸å¥½ï¼Œé™¤äº†éå¸¸ç½•è§çš„æ¨¡å¼ï¼Œå…¶ä¸­æ¦‚ç‡ä¼°è®¡çš„è¯¯å·®è¾ƒå¤§ã€‚ä»å®Œæ•´çš„ $N = 40$ ç¾¤ä½“ä¸­æŠ½å–çš„å…¶ä»– $N = 10$ ä¸ªç»†èƒç»„ä¹Ÿå¾—åˆ°äº†ç±»ä¼¼çš„ç»“æœã€‚æˆ‘ä»¬å¯ä»¥å®šé‡åœ°æµ‹é‡ä¼°è®¡åˆ†å¸ƒ $P_{\text{data}}(\sigma)$ ä¸æ¨¡å‹ $P_{2}(\sigma)$ ä¹‹é—´çš„ Jensenâ€“Shannon æ•£åº¦ï¼›åœ¨å¤šä¸ªåä¸ªç»†èƒçš„é€‰æ‹©ä¸­ï¼Œè¿™ä¸ªå€¼å›´ç»• $D_{JS} = 0.001$ æ¯”ç‰¹æ³¢åŠ¨äº†ä¸¤å€ï¼Œè¿™æ„å‘³ç€éœ€è¦æ•°åƒæ¬¡ç‹¬ç«‹è§‚å¯Ÿæ‰èƒ½åŒºåˆ†æ¨¡å‹ä¸æ•°æ®ã€‚</p>
<blockquote>
<p>The architecture of the retina is such that many individual output neurons can be driven or inhibited by a single common neuron that is internal to the circuitry. This is one of many reasons that one might expect significant combinatorial regulation in the patterns of activity, and there were serious efforts to search for these effects (Schnitzer and Meister, 2003). The success of a pairwise model thus came as a considerable surprise.</p>
</blockquote>
<p>è§†ç½‘è†œçš„ç»“æ„ä½¿å¾—è®¸å¤šä¸ªä½“è¾“å‡ºç¥ç»å…ƒå¯ä»¥è¢«ç”µè·¯å†…éƒ¨çš„å•ä¸ªå…±åŒç¥ç»å…ƒé©±åŠ¨æˆ–æŠ‘åˆ¶ã€‚è¿™æ˜¯è®¸å¤šåŸå› ä¹‹ä¸€ï¼Œäººä»¬å¯èƒ½ä¼šæœŸæœ›åœ¨æ´»åŠ¨æ¨¡å¼ä¸­å­˜åœ¨æ˜¾è‘—çš„ç»„åˆè°ƒèŠ‚ï¼Œå¹¶ä¸”æ›¾ç»æœ‰è®¤çœŸåŠªåŠ›å»å¯»æ‰¾è¿™äº›æ•ˆåº”ï¼ˆSchnitzer å’Œ Meisterï¼Œ2003ï¼‰ã€‚å› æ­¤ï¼Œæˆå¯¹æ¨¡å‹çš„æˆåŠŸä»¤äººç›¸å½“æƒŠè®¶ã€‚</p>
<blockquote>
<p>The results in the salamander retina, with natural inputs, were quickly confirmed in the primate retina using simpler inputs (Shlens et al., 2006). Those experiments covered a larger area and thus could focus on subâ€“populations of neurons belonging to a single class, which are arrayed in a relatively regular lattice. In this case not only did the pairwise model work very well, but the effective interactions $J_{ij}$ were confined largely to nearest neighbors on this lattice.</p>
</blockquote>
<p>è¾èˆè§†ç½‘è†œä¸­ä½¿ç”¨è‡ªç„¶è¾“å…¥çš„ç»“æœå¾ˆå¿«åœ¨çµé•¿ç±»åŠ¨ç‰©è§†ç½‘è†œä¸­å¾—åˆ°äº†ç¡®è®¤ï¼Œä½¿ç”¨äº†æ›´ç®€å•çš„è¾“å…¥ï¼ˆShlens ç­‰äººï¼Œ2006ï¼‰ã€‚è¿™äº›å®éªŒè¦†ç›–äº†æ›´å¤§çš„åŒºåŸŸï¼Œå› æ­¤å¯ä»¥ä¸“æ³¨äºå±äºå•ä¸€ç±»åˆ«çš„ç¥ç»å…ƒäºšç¾¤ï¼Œè¿™äº›ç¥ç»å…ƒæ’åˆ—åœ¨ä¸€ä¸ªç›¸å¯¹è§„åˆ™çš„æ™¶æ ¼ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸ä»…æˆå¯¹æ¨¡å‹æ•ˆæœéå¸¸å¥½ï¼Œè€Œä¸”æœ‰æ•ˆç›¸äº’ä½œç”¨ $J_{ij}$ ä¸»è¦å±€é™äºè¯¥æ™¶æ ¼ä¸Šçš„æœ€è¿‘é‚»ã€‚</p>
<blockquote>
<p>Pairwise maximum entropy models also were reasonably successful in describing patterns of activity across $N\leq 10$ neurons sampled from a cluster of cortical neurons kept alive in a dish (Tang et al., 2008). This work also pointed to the fact that dynamics did not correspond to Brownian motion on the energy surface.</p>
</blockquote>
<p>æˆå¯¹æœ€å¤§ç†µæ¨¡å‹åœ¨æè¿°ä»åŸ¹å…»çš¿ä¸­ä¿æŒæ´»åŠ›çš„ä¸€ç°‡çš®å±‚ç¥ç»å…ƒä¸­é‡‡æ ·çš„ $N\leq 10$ ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨æ¨¡å¼æ–¹é¢ä¹Ÿç›¸å½“æˆåŠŸï¼ˆTang ç­‰äººï¼Œ2008ï¼‰ã€‚è¿™é¡¹å·¥ä½œè¿˜æŒ‡å‡ºï¼ŒåŠ¨åŠ›å­¦å¹¶ä¸å¯¹åº”äºèƒ½é‡è¡¨é¢ä¸Šçš„å¸ƒæœ—è¿åŠ¨ã€‚</p>
<blockquote>
<p>These early successes with small numbers of neurons raised many questions. For example, the interaction matrix $J_{ij}$ contained a mix of positive and negative terms, suggesting that frustration could lead to many local minima of the energy function or equivalently local maxima of the probability $P(\vec{\sigma})$, as in the Hopfield model (Â§II.C); could these â€œattractorsâ€ have a function in representing the visual world? Relatedly, an important consequence of the collective behavior in the Ising model is that if we know that state of all neurons in the network but one, then we have a parameterâ€“free prediction for the probability that this last neuron will be active; does this allow for error correction? To address these and other issues one must go beyond $N\sim 10$ cells, which was already possible experimentally. But at larger $N$ one needs more powerful methods for solving the inverse problem that is at the heart of the maximum entropy construction, as described in Appendix B.</p>
</blockquote>
<p>è¿™äº›æ—©æœŸåœ¨å°‘é‡ç¥ç»å…ƒä¸Šçš„æˆåŠŸå¼•å‘äº†è®¸å¤šé—®é¢˜ã€‚ä¾‹å¦‚ï¼Œäº¤äº’çŸ©é˜µ $J_{ij}$ åŒ…å«æ­£è´Ÿæ··åˆé¡¹ï¼Œè¡¨æ˜é˜»æŒ«å¯èƒ½å¯¼è‡´èƒ½é‡å‡½æ•°çš„è®¸å¤šå±€éƒ¨æå°å€¼ï¼Œæˆ–è€…ç­‰ä»·åœ°ï¼Œæ¦‚ç‡ $P(\vec{\sigma})$ çš„å±€éƒ¨æå¤§å€¼ï¼Œå°±åƒ Hopfield æ¨¡å‹ï¼ˆÂ§II.Cï¼‰ä¸­ä¸€æ ·ï¼›è¿™äº›â€œå¸å¼•å­â€åœ¨è¡¨ç¤ºè§†è§‰ä¸–ç•Œæ–¹é¢æ˜¯å¦å…·æœ‰åŠŸèƒ½ï¼Ÿç›¸å…³åœ°ï¼ŒIsing æ¨¡å‹ä¸­é›†ä½“è¡Œä¸ºçš„ä¸€ä¸ªé‡è¦åæœæ˜¯ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“ç½‘ç»œä¸­é™¤ä¸€ä¸ªç¥ç»å…ƒå¤–æ‰€æœ‰ç¥ç»å…ƒçš„çŠ¶æ€ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥æ— å‚æ•°åœ°é¢„æµ‹è¿™ä¸ªæœ€åä¸€ä¸ªç¥ç»å…ƒæ˜¯å¦ä¼šæ´»è·ƒï¼›è¿™æ˜¯å¦å…è®¸çº é”™ï¼Ÿä¸ºäº†å¤„ç†è¿™äº›å’Œå…¶ä»–é—®é¢˜ï¼Œå¿…é¡»è¶…è¶Š $N\sim 10$ ä¸ªç»†èƒï¼Œè¿™åœ¨å®éªŒä¸Šå·²ç»æ˜¯å¯èƒ½çš„ã€‚ä½†åœ¨æ›´å¤§çš„ $N$ ä¸‹ï¼Œéœ€è¦æ›´å¼ºå¤§çš„æ–¹æ³•æ¥è§£å†³æœ€å¤§ç†µæ„é€ æ ¸å¿ƒçš„é€†é—®é¢˜ï¼Œå¦‚é™„å½• B æ‰€è¿°ã€‚</p>
<blockquote>
<p>The equivalence to equilibrium models entices us to describe the couplings $J_{ij}$ as â€œinteractions,â€ but there is no reason to think that these correspond to genuine connections between cells. In particular, $J_{ij}$ is symmetric because it is an effective interaction driving the equaltime correlations of activity in cells $i$ and $j$, and these correlations are symmetric by definition. If we go beyond single time slices to describe trajectories of activity over time, then with multiple cells the effective interactions can become asymmetric and break timereversal invariance.</p>
</blockquote>
<p>ä¸å¹³è¡¡æ¨¡å‹çš„ç­‰ä»·æ€§è¯±ä½¿æˆ‘ä»¬å°†è€¦åˆ $J_{ij}$ æè¿°ä¸ºâ€œç›¸äº’ä½œç”¨â€ï¼Œä½†æ²¡æœ‰ç†ç”±è®¤ä¸ºå®ƒä»¬å¯¹åº”äºç»†èƒä¹‹é—´çš„çœŸæ­£è¿æ¥ã€‚ç‰¹åˆ«æ˜¯ï¼Œ$J_{ij}$ æ˜¯å¯¹ç§°çš„ï¼Œå› ä¸ºå®ƒæ˜¯é©±åŠ¨ç»†èƒ $i$ å’Œ $j$ æ´»åŠ¨çš„åŒæ—¶ç›¸å…³æ€§çš„æœ‰æ•ˆç›¸äº’ä½œç”¨ï¼Œè€Œè¿™äº›ç›¸å…³æ€§æŒ‰å®šä¹‰æ˜¯å¯¹ç§°çš„ã€‚å¦‚æœæˆ‘ä»¬è¶…è¶Šå•ä¸ªæ—¶é—´ç‰‡æ¥æè¿°éšæ—¶é—´å˜åŒ–çš„æ´»åŠ¨è½¨è¿¹ï¼Œé‚£ä¹ˆå¯¹äºå¤šä¸ªç»†èƒï¼Œæœ‰æ•ˆç›¸äº’ä½œç”¨å¯ä»¥å˜å¾—ä¸å¯¹ç§°å¹¶æ‰“ç ´æ—¶é—´åæ¼”ä¸å˜æ€§ã€‚</p>
<blockquote>
<p>Before leaving the early work, it is useful to step back and ask about the goals and hopes from that time. As reviewed above, the use of statistical physics models for neural networks has a deep history. Saying that the brain is described by an Ising model captured both the optimism and (one must admit) the naÄ± Ìˆvet Ìe of the physics community in approaching the phenomena of life. One could balance optimism and naÄ± Ìˆvet Ìe by retreating to the position that these models are metaphors, illustrating what could happen rather than being theories of what actually happens. The success of maximum entropy models in the retina gave an example of how statistical physics ideas could provide a quantitative theory for networks of real neurons.</p>
</blockquote>
<p>åœ¨ç¦»å¼€æ—©æœŸå·¥ä½œä¹‹å‰ï¼Œé€€ä¸€æ­¥é—®ä¸€ä¸‹å½“æ—¶çš„ç›®æ ‡å’Œå¸Œæœ›æ˜¯æœ‰ç”¨çš„ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œç¥ç»ç½‘ç»œä½¿ç”¨ç»Ÿè®¡ç‰©ç†æ¨¡å‹æœ‰ç€æ·±åšçš„å†å²ã€‚è¯´å¤§è„‘ç”± Ising æ¨¡å‹æè¿°æ—¢æ•æ‰äº†ä¹è§‚ä¸»ä¹‰ï¼Œä¹Ÿæ•æ‰äº†ï¼ˆå¿…é¡»æ‰¿è®¤ï¼‰ç‰©ç†å­¦ç•Œåœ¨æ¥è¿‘ç”Ÿå‘½ç°è±¡æ—¶çš„å¤©çœŸã€‚é€šè¿‡é€€å›åˆ°è¿™äº›æ¨¡å‹æ˜¯éšå–»çš„ä½ç½®ï¼Œå¯ä»¥å¹³è¡¡ä¹è§‚ä¸»ä¹‰å’Œå¤©çœŸï¼Œè¯´æ˜å¯èƒ½ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œè€Œä¸æ˜¯å®é™…å‘ç”Ÿçš„ç†è®ºã€‚è§†ç½‘è†œä¸­æœ€å¤§ç†µæ¨¡å‹çš„æˆåŠŸæä¾›äº†ä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ç»Ÿè®¡ç‰©ç†æ€æƒ³å¦‚ä½•ä¸ºçœŸå®ç¥ç»å…ƒç½‘ç»œæä¾›å®šé‡ç†è®ºã€‚</p>
<h1 id="larger-networks-of-neurons">Larger networks of neurons<a hidden class="anchor" aria-hidden="true" href="#larger-networks-of-neurons">#</a></h1>
<blockquote>
<p>The use of maximum entropy for networks of real neurons quickly triggered almost all possible reactions: (a) It should never work, because systems are not in equilibrium, have combinational interactions, $\cdots$ . (b) It could work, but only under uninteresting conditions. (c) It should always work, since these models are very expressive. (d) It works at small $N$ , but this is a poor guide to what will happen at large $N$ . (e) Sure, but why not use [favorite alternative], for which we have efficient algorithms?</p>
</blockquote>
<p>å¯¹äºçœŸå®ç¥ç»å…ƒç½‘ç»œä½¿ç”¨æœ€å¤§ç†µè¿…é€Ÿå¼•å‘äº†å‡ ä¹æ‰€æœ‰å¯èƒ½çš„ååº”ï¼šï¼ˆaï¼‰å®ƒæ°¸è¿œä¸ä¼šèµ·ä½œç”¨ï¼Œå› ä¸ºç³»ç»Ÿä¸å¤„äºå¹³è¡¡çŠ¶æ€ï¼Œå…·æœ‰ç»„åˆç›¸äº’ä½œç”¨ï¼Œ$\cdots$ï¼ˆbï¼‰å®ƒå¯èƒ½èµ·ä½œç”¨ï¼Œä½†ä»…åœ¨æ— è¶£çš„æ¡ä»¶ä¸‹ã€‚ï¼ˆcï¼‰å®ƒåº”è¯¥æ€»æ˜¯æœ‰æ•ˆï¼Œå› ä¸ºè¿™äº›æ¨¡å‹éå¸¸æœ‰è¡¨ç°åŠ›ã€‚ï¼ˆdï¼‰å®ƒåœ¨å° $N$ ä¸‹æœ‰æ•ˆï¼Œä½†è¿™å¯¹å¤§ $N$ ä¼šå‘ç”Ÿä»€ä¹ˆæ²¡æœ‰å¾ˆå¥½çš„æŒ‡å¯¼æ„ä¹‰ã€‚ï¼ˆeï¼‰å½“ç„¶ï¼Œä½†ä¸ºä»€ä¹ˆä¸ä½¿ç”¨[æœ€å–œæ¬¢çš„æ›¿ä»£æ–¹æ¡ˆ]ï¼Œæˆ‘ä»¬æœ‰é«˜æ•ˆçš„ç®—æ³•ï¼Ÿ</p>
<blockquote>
<p>Perhaps the most concrete response to these issues is just to see what happens as we move to more examples, especially in larger networks. But we should do this with several questions in mind, some of which were very explicit in the early literature (Macke et al., 2011a; Roudi et al., 2009). First, finding the maximum entropy model that matches the desired constraintsâ€”that is, solving Eqs (17)â€”becomes more difficult at larger $N$ . Can we be sure that we are testing the maximum entropy idea, and our choice of constraints, rather than the efficacy of our algorithms for solving this problem?</p>
</blockquote>
<p>ä¹Ÿè®¸å¯¹è¿™äº›é—®é¢˜æœ€å…·ä½“çš„å›åº”å°±æ˜¯çœ‹çœ‹å½“æˆ‘ä»¬è½¬å‘æ›´å¤šä¾‹å­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´å¤§ç½‘ç»œä¸­ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ä½†æˆ‘ä»¬åº”è¯¥ç‰¢è®°å‡ ä¸ªé—®é¢˜ï¼Œå…¶ä¸­ä¸€äº›åœ¨æ—©æœŸæ–‡çŒ®ä¸­éå¸¸æ˜ç¡®ï¼ˆMacke ç­‰äººï¼Œ2011aï¼›Roudi ç­‰äººï¼Œ2009ï¼‰ã€‚é¦–å…ˆï¼Œæ‰¾åˆ°åŒ¹é…æ‰€éœ€çº¦æŸçš„æœ€å¤§ç†µæ¨¡å‹â€”â€”å³è§£å†³æ–¹ç¨‹ï¼ˆ17ï¼‰â€”â€”åœ¨æ›´å¤§çš„ $N$ ä¸‹å˜å¾—æ›´åŠ å›°éš¾ã€‚æˆ‘ä»¬èƒ½å¦ç¡®å®šæˆ‘ä»¬æ­£åœ¨æµ‹è¯•æœ€å¤§ç†µçš„æƒ³æ³•ï¼Œä»¥åŠæˆ‘ä»¬é€‰æ‹©çš„çº¦æŸï¼Œè€Œä¸æ˜¯æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜çš„ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Ÿ</p>
<blockquote>
<p>Second, as N increases the maximum entropy construction becomes very data hungry. This concern often is phrased as the usual problem of â€œoverâ€“fitting,â€ when the number of parameters in our model is too large to fully constrained by the data. But in the maximum entropy formulation the problem is even more fundamental. The maximum entropy construction builds the least structured model consistent with a set of known expectation values. With a finite amount of data, if our list of expectation values is too long then the claim that we â€œknowâ€ these features of the system just isnâ€™t true, and this problem arises even before we try to build the maximum entropy model.</p>
</blockquote>
<p>å…¶æ¬¡ï¼Œéšç€ N çš„å¢åŠ ï¼Œæœ€å¤§ç†µæ„é€ å˜å¾—éå¸¸éœ€è¦æ•°æ®ã€‚è¿™ä¸ªé—®é¢˜é€šå¸¸è¢«è¡¨è¿°ä¸ºâ€œè¿‡æ‹Ÿåˆâ€çš„å¸¸è§é—®é¢˜ï¼Œå½“æˆ‘ä»¬æ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡å¤ªå¤§è€Œæ— æ³•è¢«æ•°æ®å®Œå…¨çº¦æŸæ—¶ã€‚ä½†åœ¨æœ€å¤§ç†µå…¬å¼ä¸­ï¼Œè¿™ä¸ªé—®é¢˜ç”šè‡³æ›´ä¸ºæ ¹æœ¬ã€‚æœ€å¤§ç†µæ„é€ å»ºç«‹äº†ä¸€ä¸ªä¸ä¸€ç»„å·²çŸ¥æœŸæœ›å€¼ä¸€è‡´çš„æœ€å°‘ç»“æ„åŒ–æ¨¡å‹ã€‚å¯¹äºæœ‰é™çš„æ•°æ®é‡ï¼Œå¦‚æœæˆ‘ä»¬çš„æœŸæœ›å€¼åˆ—è¡¨å¤ªé•¿ï¼Œé‚£ä¹ˆæˆ‘ä»¬â€œçŸ¥é“â€ç³»ç»Ÿçš„è¿™äº›ç‰¹å¾çš„è¯´æ³•å°±ä¸æ˜¯çœŸçš„ï¼Œå³ä½¿åœ¨æˆ‘ä»¬å°è¯•æ„å»ºæœ€å¤§ç†µæ¨¡å‹ä¹‹å‰ï¼Œè¿™ä¸ªé—®é¢˜ä¹Ÿä¼šå‡ºç°ã€‚</p>
<blockquote>
<p>Third, because correlations are spread widely in these networks, if one develops a perturbation theory around the limit of independent neurons then factors of $N$ appear in the series, e.g. for the entropy per neuron. Success at modest $N$ might thus mean that we are in a perturbative regime, which would be much less interesting. The question of whether success is perturbative is subtle, since at finite $N$ all properties of the maximum entropy model are analytic functions of the correlations, and hence if we carry perturbation theory far enough we will get the right answer (Sessak and Monasson, 2009).</p>
</blockquote>
<p>ç¬¬ä¸‰ï¼Œå› ä¸ºç›¸å…³æ€§åœ¨è¿™äº›ç½‘ç»œä¸­å¹¿æ³›ä¼ æ’­ï¼Œå¦‚æœæˆ‘ä»¬å›´ç»•ç‹¬ç«‹ç¥ç»å…ƒçš„æé™å‘å±•å¾®æ‰°ç†è®ºï¼Œé‚£ä¹ˆ $N$ çš„å› ç´ ä¼šå‡ºç°åœ¨çº§æ•°ä¸­ï¼Œä¾‹å¦‚æ¯ä¸ªç¥ç»å…ƒçš„ç†µã€‚å› æ­¤ï¼Œåœ¨é€‚åº¦çš„ $N$ ä¸‹çš„æˆåŠŸå¯èƒ½æ„å‘³ç€æˆ‘ä»¬å¤„äºå¾®æ‰°èŒƒå›´å†…ï¼Œè¿™å°†ä¸é‚£ä¹ˆæœ‰è¶£ã€‚æˆåŠŸæ˜¯å¦æ˜¯å¾®æ‰°çš„é—®é¢˜æ˜¯å¾®å¦™çš„ï¼Œå› ä¸ºåœ¨æœ‰é™çš„ $N$ ä¸‹ï¼Œæœ€å¤§ç†µæ¨¡å‹çš„æ‰€æœ‰å±æ€§éƒ½æ˜¯ç›¸å…³æ€§çš„è§£æå‡½æ•°ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å°†å¾®æ‰°ç†è®ºè¿›è¡Œå¾—è¶³å¤Ÿè¿œï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ­£ç¡®çš„ç­”æ¡ˆï¼ˆSessak å’Œ Monassonï¼Œ2009ï¼‰ã€‚</p>
<blockquote>
<p>Finally, in statistical mechanics we are used to the idea of a large $N$, thermodynamic limit. Although this carries over to model networks (Amit, 1989), it is not obvious how to use this idea in thinking about networks of real neurons. Naive extrapolation of results from maximum entropy models of $N = 10 âˆ’ 20$ neurons in the retina indicated that something special had to happen by $N\sim 200$, or else the entropy would vanish; this was interesting because $N\sim 200$ is the number cells that are â€œlookingâ€ at overlapping regions of the visual world (Schneidman et al., 2006). A more sophisticated extrapolation imagines a large population of neurons in which mean activities and pairwise correlations are drawn at random from the same distribution as found in recordings from smaller numbers of neurons (TkaË‡cik et al., 2006, 2009). This sort of extrapolation is motivated in part by the observation that â€œthermodynamicâ€ properties of the maximum entropy models learned for $N = 20$ or $N = 40$ retinal neurons match the behavior of such random models at the same $N$. If we now extrapolate to $N = 120$ there are striking collective behaviors, and we will ask if these are seen in real data from $N &gt; 100$ cells.</p>
</blockquote>
<p>æœ€åï¼Œåœ¨ç»Ÿè®¡åŠ›å­¦ä¸­ï¼Œæˆ‘ä»¬ä¹ æƒ¯äºå¤§ $N$ã€çƒ­åŠ›å­¦æé™çš„æ¦‚å¿µã€‚å°½ç®¡è¿™å¯ä»¥è½¬ç§»åˆ°æ¨¡å‹ç½‘ç»œä¸­ï¼ˆAmitï¼Œ1989ï¼‰ï¼Œä½†åœ¨æ€è€ƒçœŸå®ç¥ç»å…ƒç½‘ç»œæ—¶å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæƒ³æ³•å¹¶ä¸æ˜æ˜¾ã€‚ä»è§†ç½‘è†œä¸­ $N = 10 âˆ’ 20$ ä¸ªç¥ç»å…ƒçš„æœ€å¤§ç†µæ¨¡å‹ç»“æœçš„å¤©çœŸå¤–æ¨è¡¨æ˜ï¼Œåˆ° $N\sim 200$ æ—¶å¿…é¡»å‘ç”Ÿä¸€äº›ç‰¹æ®Šçš„äº‹æƒ…ï¼Œå¦åˆ™ç†µå°†æ¶ˆå¤±ï¼›è¿™æ˜¯æœ‰è¶£çš„ï¼Œå› ä¸º $N\sim 200$ æ˜¯â€œè§‚å¯Ÿâ€è§†è§‰ä¸–ç•Œé‡å åŒºåŸŸçš„ç»†èƒæ•°é‡ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚æ›´å¤æ‚çš„å¤–æ¨è®¾æƒ³äº†ä¸€ä¸ªå¤§å‹ç¥ç»å…ƒç¾¤ä½“ï¼Œå…¶ä¸­å¹³å‡æ´»åŠ¨å’Œæˆå¯¹ç›¸å…³æ€§æ˜¯ä»ä¸è¾ƒå°‘ç¥ç»å…ƒè®°å½•ä¸­å‘ç°çš„ç›¸åŒåˆ†å¸ƒä¸­éšæœºæŠ½å–çš„ï¼ˆTkaË‡cik ç­‰äººï¼Œ2006ï¼Œ2009ï¼‰ã€‚è¿™ç§å¤–æ¨éƒ¨åˆ†æ˜¯ç”±è§‚å¯Ÿåˆ°ä¸º $N = 20$ æˆ– $N = 40$ è§†ç½‘è†œç¥ç»å…ƒå­¦ä¹ çš„æœ€å¤§ç†µæ¨¡å‹çš„â€œçƒ­åŠ›å­¦â€å±æ€§ä¸åŒä¸€ $N$ ä¸‹æ­¤ç±»éšæœºæ¨¡å‹çš„è¡Œä¸ºç›¸åŒ¹é…æ‰€æ¿€å‘çš„ã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨å¤–æ¨åˆ° $N = 120$ï¼Œä¼šå‡ºç°æ˜¾è‘—çš„é›†ä½“è¡Œä¸ºï¼Œæˆ‘ä»¬å°†è¯¢é—®åœ¨æ¥è‡ª $N &gt; 100$ ä¸ªç»†èƒçš„çœŸå®æ•°æ®ä¸­æ˜¯å¦çœ‹åˆ°äº†è¿™äº›è¡Œä¸ºã€‚</p>
<blockquote>
<p>Early experiments in the retina already were monitoring $N = 40$ cells, and the development of numerical methods described in Appendix B quickly allowed analysis of these larger data sets (TkaË‡cik et al., 2006, 2009). With $N = 40$ cells one cannot check the predictions for probabilities of individual patterns $P(\vec{\sigma})$, but one can check the probability that $K$ out of $N$ cells are active in the same small time bin, as in Fig. 7E, or the correlations among triplets of neurons. At $N = 40$ we</p>
</blockquote>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-3/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>New experimental methods</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-5/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>A unique test</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  
</body>







<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
