<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Maximum entropy as a path to connect theory and experiment | æ— å¤„æƒ¹å°˜åŸƒ</title>
<meta name="keywords" content="">
<meta name="description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-4/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-4/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>







<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="Maximum entropy as a path to connect theory and experiment" />
<meta property="og:description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-4/" />
<meta property="og:image" content="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-12T00:18:23+08:00" />
<meta property="article:modified_time" content="2025-11-12T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" />
<meta name="twitter:title" content="Maximum entropy as a path to connect theory and experiment"/>
<meta name="twitter:description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ“• é˜…è¯»",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "ğŸ“• æ–‡çŒ®",
          "item": "https://Muatyz.github.io/posts/read/reference/"
        },

        {
          "@type": "ListItem",
          "position":  4 ,
          "name": "ğŸ“• Statistical mechanics for networks of real neurons",
          "item": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/"
        }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "Maximum entropy as a path to connect theory and experiment",
      "item": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-4/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Maximum entropy as a path to connect theory and experiment",
  "name": "Maximum entropy as a path to connect theory and experiment",
  "description": "çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦",
  "keywords": [
    ""
  ],
  "articleBody": " New experimental methods create new opportunities to test our theories. For neural networks, monitoring the electrical activity of tens, hundreds, or thousands of neurons simultaneously should allow us to test statistical approaches to these systems in detail. Doing this requires taking much more seriously the connection between our models and real neurons, a connection that sometimes has been tenuous. Can we really take the spins $\\sigma_{i}$ in Eq (5) to represent the presence or absence of an action potential in cell $i$? We will indeed make this identification, and our goal will be an accurate description of the probability distribution out of which the â€œmicroscopicâ€ states of a large network are drawn. Note that, as in equilibrium statistical mechanics, this would be the beginning and not the end of our understanding.\næ–°çš„å®éªŒæ–¹æ³•ä¸ºæµ‹è¯•æˆ‘ä»¬çš„ç†è®ºåˆ›é€ äº†æ–°çš„æœºä¼šã€‚å¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼ŒåŒæ—¶ç›‘æµ‹æ•°åã€æ•°ç™¾æˆ–æ•°åƒä¸ªç¥ç»å…ƒçš„ç”µæ´»åŠ¨åº”è¯¥å…è®¸æˆ‘ä»¬è¯¦ç»†æµ‹è¯•è¿™äº›ç³»ç»Ÿçš„ç»Ÿè®¡æ–¹æ³•ã€‚åšåˆ°è¿™ä¸€ç‚¹éœ€è¦æˆ‘ä»¬æ›´åŠ è®¤çœŸåœ°å¯¹å¾…æˆ‘ä»¬çš„æ¨¡å‹ä¸çœŸå®ç¥ç»å…ƒä¹‹é—´çš„è”ç³»ï¼Œè€Œè¿™ç§è”ç³»æœ‰æ—¶æ˜¯è„†å¼±çš„ã€‚æˆ‘ä»¬çœŸçš„å¯ä»¥å°†æ–¹ç¨‹ï¼ˆ5ï¼‰ä¸­çš„è‡ªæ—‹ $\\sigma_{i}$ è§†ä¸ºç»†èƒ $i$ ä¸­åŠ¨ä½œç”µä½çš„å­˜åœ¨æˆ–ä¸å­˜åœ¨å—ï¼Ÿæˆ‘ä»¬ç¡®å®ä¼šåšå‡ºè¿™ç§è¯†åˆ«ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‡†ç¡®æè¿°ä»ä¸­æŠ½å–å¤§å‹ç½‘ç»œâ€œå¾®è§‚â€çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¯·æ³¨æ„ï¼Œä¸å¹³è¡¡ç»Ÿè®¡åŠ›å­¦ä¸€æ ·ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬ç†è§£çš„å¼€å§‹ï¼Œè€Œä¸æ˜¯ç»“æŸã€‚\nWe will see that maximum entropy models provide a path that starts with data and constructs models that have a very direct connection to statistical physics. Our focus here is on networks of neurons, but it is important that the same concepts and methods are being used to study a much wider range of living systems, and there are important lessons to be drawn from seeing all these problems as part of the same project (Appendix A).\næœ€å¤§ç†µæ¨¡å‹æä¾›äº†ä¸€æ¡ä»æ•°æ®å¼€å§‹å¹¶æ„å»ºä¸ç»Ÿè®¡ç‰©ç†æœ‰éå¸¸ç›´æ¥è”ç³»çš„æ¨¡å‹çš„è·¯å¾„ã€‚æˆ‘ä»¬è¿™é‡Œçš„é‡ç‚¹æ˜¯ç¥ç»å…ƒç½‘ç»œï¼Œä½†é‡è¦çš„æ˜¯ï¼ŒåŒæ ·çš„æ¦‚å¿µå’Œæ–¹æ³•æ­£åœ¨è¢«ç”¨æ¥ç ”ç©¶æ›´å¹¿æ³›çš„ç”Ÿç‰©ç³»ç»Ÿï¼Œå¹¶ä¸”ä»å°†æ‰€æœ‰è¿™äº›é—®é¢˜è§†ä¸ºåŒä¸€é¡¹ç›®çš„ä¸€éƒ¨åˆ†ä¸­å¯ä»¥å¾—å‡ºé‡è¦çš„æ•™è®­ï¼ˆé™„å½• Aï¼‰ã€‚\nBasics of maximum entropy Consider a network of neurons, labelled by $i = 1, 2, \\cdots , N$ , each with a state $\\sigma_{i}$. In the simplest case where these states of individual neurons are binaryactive/inactive, or spiking/silentâ€”then the network as a whole has access to $\\Omega = 2^{N}$ possible states\n$$ \\vec{\\sigma} = \\{\\sigma_{1},\\sigma_{2},\\cdots,\\sigma_{N}\\}. $$\nThese states mean something to the organism: they may represent sensory inputs, inferred features of the surrounding world, plans, motor commands, recalled memories, or internal thoughts. But before we can build a dictionary for these meanings we need a lexicon, describing which of the possible states actually occur, and how often. More formally, we would like to understand the probability distribution $P(\\vec{\\sigma})$. We might also be interested in sequences of states over time, $P [\\{\\vec{\\sigma}(t_1), \\vec{\\sigma}(t_2),\\cdots \\}]$, but for simplicity we focus first on states at a single moment in time.\nè€ƒè™‘ä¸€ä¸ªç¥ç»å…ƒç½‘ç»œï¼Œæ ‡è®°ä¸º $i = 1, 2, \\cdots , N$ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½æœ‰ä¸€ä¸ªçŠ¶æ€ $\\sigma_{i}$ã€‚åœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œè¿™äº›å•ä¸ªç¥ç»å…ƒçš„çŠ¶æ€æ˜¯äºŒè¿›åˆ¶çš„â€”â€”æ´»è·ƒ/ä¸æ´»è·ƒï¼Œæˆ–å°–å³°/é™é»˜â€”â€”é‚£ä¹ˆæ•´ä¸ªç½‘ç»œå¯ä»¥è®¿é—® $\\Omega = 2^{N}$ ä¸ªå¯èƒ½çš„çŠ¶æ€\n$$ \\vec{\\sigma} = \\{\\sigma_{1},\\sigma_{2},\\cdots,\\sigma_{N}\\}. $$\nè¿™äº›çŠ¶æ€å¯¹æœ‰æœºä½“æ¥è¯´æ˜¯æœ‰æ„ä¹‰çš„ï¼šå®ƒä»¬å¯èƒ½ä»£è¡¨æ„Ÿå®˜è¾“å…¥ã€å‘¨å›´ä¸–ç•Œçš„æ¨æ–­ç‰¹å¾ã€è®¡åˆ’ã€è¿åŠ¨å‘½ä»¤ã€å›å¿†çš„è®°å¿†æˆ–å†…éƒ¨æ€ç»´ã€‚ä½†åœ¨æˆ‘ä»¬èƒ½å¤Ÿä¸ºè¿™äº›å«ä¹‰æ„å»ºè¯å…¸ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œæè¿°å“ªäº›å¯èƒ½çš„çŠ¶æ€å®é™…ä¸Šä¼šå‘ç”Ÿï¼Œä»¥åŠå®ƒä»¬å‘ç”Ÿçš„é¢‘ç‡ã€‚æ›´æ­£å¼åœ°è¯´ï¼Œæˆ‘ä»¬æƒ³è¦ç†è§£æ¦‚ç‡åˆ†å¸ƒ $P(\\vec{\\sigma})$ã€‚æˆ‘ä»¬ä¹Ÿå¯èƒ½å¯¹éšæ—¶é—´å˜åŒ–çš„çŠ¶æ€åºåˆ—æ„Ÿå…´è¶£ï¼Œ$P [\\{\\vec{\\sigma}(t_1), \\vec{\\sigma}(t_2),\\cdots \\}]$ï¼Œä½†ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬é¦–å…ˆå…³æ³¨å•ä¸€æ—¶åˆ»çš„çŠ¶æ€ã€‚\nThe distribution $P(\\vec{\\sigma})$ is a list of $\\Omega$ numbers that sum to one. Even for modest size networks this is a very long list, $\\Omega\\sim 10^{30}$ for $N = 100$. To be clear, there is no way that we can measure all these numbers in any realistic experiment. More deeply, large networks could not visit all of their possible states in the age of the universe, let alone the lifetime of a single organism. This shouldnâ€™t bother us, since one can make similar observations about the states of molecules in the air around us, or the states of all the atoms in a tiny grain of sand. The fact that the number of possible states $\\Omega$ is (beyond) astronomically large does not stop us from asking questions about the distribution from which these states are drawn.\næ¦‚ç‡åˆ†å¸ƒ $P(\\vec{\\sigma})$ æ˜¯ä¸€ä¸ªåŒ…å« $\\Omega$ ä¸ªæ•°å­—çš„åˆ—è¡¨ï¼Œè¿™äº›æ•°å­—çš„æ€»å’Œä¸ºä¸€ã€‚å³ä½¿å¯¹äºé€‚åº¦å¤§å°çš„ç½‘ç»œæ¥è¯´ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸é•¿çš„åˆ—è¡¨ï¼Œå¯¹äº $N = 100$ï¼Œ$\\Omega\\sim 10^{30}$ã€‚æ˜ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬æ— æ³•åœ¨ä»»ä½•ç°å®çš„å®éªŒä¸­æµ‹é‡å‡ºæ‰€æœ‰è¿™äº›æ•°å­—ã€‚æ›´æ·±å±‚æ¬¡çš„æ˜¯ï¼Œå¤§å‹ç½‘ç»œä¸å¯èƒ½åœ¨å®‡å®™çš„å¹´é¾„å†…è®¿é—®å®ƒä»¬æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€ï¼Œæ›´ä¸ç”¨è¯´å•ä¸ªæœ‰æœºä½“çš„å¯¿å‘½äº†ã€‚è¿™ä¸åº”è¯¥å›°æ‰°æˆ‘ä»¬ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥å¯¹æˆ‘ä»¬å‘¨å›´ç©ºæ°”ä¸­çš„åˆ†å­çŠ¶æ€ï¼Œæˆ–å¾®å°æ²™ç²’ä¸­æ‰€æœ‰åŸå­çš„çŠ¶æ€åšå‡ºç±»ä¼¼çš„è§‚å¯Ÿã€‚å¯èƒ½çŠ¶æ€çš„æ•°é‡ $\\Omega$ï¼ˆè¶…å‡ºï¼‰å¤©æ–‡æ•°å­—çº§åˆ«ï¼Œå¹¶ä¸ä¼šé˜»æ­¢æˆ‘ä»¬å¯¹è¿™äº›çŠ¶æ€æ‰€æŠ½å–çš„åˆ†å¸ƒæå‡ºé—®é¢˜ã€‚\nThe enormous value of $\\Omega$ does mean, however, that answering questions about the distribution from which the states are drawn requires the answer to be, in some sense, simpler than it could be. If $P(\\vec{\\sigma})$ really were just a list of $\\Omega$ numbers with no underlying structure, we could never make a meaningful experimental prediction. Progress in the description of manyâ€“body systems depends on the discovery of some regularity or simplicity, and without such simplifying hypotheses nothing can be inferred from any reasonable amount of data. The maximum entropy method is a way of being explicit about our simplifying hypotheses.\nç„¶è€Œï¼Œ$\\Omega$ çš„å·¨å¤§å€¼ç¡®å®æ„å‘³ç€ï¼Œè¦å›ç­”æœ‰å…³çŠ¶æ€æ‰€æŠ½å–çš„åˆ†å¸ƒçš„é—®é¢˜ï¼Œç­”æ¡ˆåœ¨æŸç§æ„ä¹‰ä¸Šå¿…é¡»æ¯”å®ƒå¯èƒ½çš„å½¢å¼æ›´ç®€å•ã€‚å¦‚æœ $P(\\vec{\\sigma})$ çœŸçš„æ˜¯ä¸€ä¸ªæ²¡æœ‰æ½œåœ¨ç»“æ„çš„ $\\Omega$ ä¸ªæ•°å­—çš„åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†æ°¸è¿œæ— æ³•åšå‡ºæœ‰æ„ä¹‰çš„å®éªŒé¢„æµ‹ã€‚å¯¹å¤šä½“ç³»ç»Ÿæè¿°çš„è¿›å±•ä¾èµ–äºæŸç§è§„å¾‹æ€§æˆ–ç®€å•æ€§çš„å‘ç°ï¼Œå¦‚æœæ²¡æœ‰è¿™ç§ç®€åŒ–å‡è®¾ï¼Œä»ä»»ä½•åˆç†æ•°é‡çš„æ•°æ®ä¸­éƒ½æ— æ³•æ¨æ–­å‡ºä»»ä½•ä¸œè¥¿ã€‚æœ€å¤§ç†µæ–¹æ³•æ˜¯ä¸€ç§æ˜ç¡®è¡¨è¾¾æˆ‘ä»¬ç®€åŒ–å‡è®¾çš„æ–¹æ³•ã€‚\nWe can imagine mapping each microscopic state $\\vec{\\sigma}$ into some perhaps more macroscopic observable $f(\\vec{\\sigma})$, and from reasonable experiments we should be able to estimate the average of this observable $\\langle f(\\vec{\\sigma})\\rangle_{\\text{expt}}$. If we think this observable is an important and meaningful quantity, it makes sense to insist that any theory we write down for the distribution $P(\\vec{\\sigma})$ should predict this expectation value correctly,\n$$ \\langle f(\\vec{\\sigma})\\rangle_{P} = \\sum_{\\vec{\\sigma}} P(\\vec{\\sigma})f(\\vec{\\sigma}) = \\langle f(\\vec{\\sigma})\\rangle_{\\text{expt}}. $$\nThere might be several such meaningful observables, so we should have\n$$ \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{P} \\equiv \\sum_{\\vec{\\sigma}}P(\\vec{\\sigma})f_{\\mu}(\\vec{\\sigma}) = \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}} $$\nfor $\\mu = 1, 2, \\cdots , K$. These are strong constraints, but so long as the number of these observables $K \\ll \\Omega$ there are infinitely many distributions consistent with Eq (17). How do we choose among them?\næˆ‘ä»¬å¯ä»¥æƒ³è±¡å°†æ¯ä¸ªå¾®è§‚çŠ¶æ€ $\\vec{\\sigma}$ æ˜ å°„åˆ°æŸä¸ªå¯èƒ½æ›´å®è§‚çš„å¯è§‚å¯Ÿé‡ $f(\\vec{\\sigma})$ï¼Œå¹¶ä¸”é€šè¿‡åˆç†çš„å®éªŒï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿä¼°è®¡å‡ºè¿™ä¸ªå¯è§‚å¯Ÿé‡çš„å¹³å‡å€¼ $\\langle f(\\vec{\\sigma})\\rangle_{\\text{expt}}$ã€‚å¦‚æœæˆ‘ä»¬è®¤ä¸ºè¿™ä¸ªå¯è§‚å¯Ÿé‡æ˜¯ä¸€ä¸ªé‡è¦ä¸”æœ‰æ„ä¹‰çš„é‡ï¼Œé‚£ä¹ˆåšæŒä»»ä½•æˆ‘ä»¬ä¸ºåˆ†å¸ƒ $P(\\vec{\\sigma})$ å†™ä¸‹çš„ç†è®ºéƒ½åº”è¯¥æ­£ç¡®é¢„æµ‹è¿™ä¸ªæœŸæœ›å€¼æ˜¯æœ‰æ„ä¹‰çš„ï¼Œ\n$$ \\langle f(\\vec{\\sigma})\\rangle_{P} = \\sum_{\\vec{\\sigma}} P(\\vec{\\sigma})f(\\vec{\\sigma}) = \\langle f(\\vec{\\sigma})\\rangle_{\\text{expt}}. $$\nå¯èƒ½ä¼šæœ‰å‡ ä¸ªè¿™æ ·çš„æœ‰æ„ä¹‰çš„å¯è§‚å¯Ÿé‡ï¼Œå› æ­¤æˆ‘ä»¬åº”è¯¥æœ‰\n$$ \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{P} \\equiv \\sum_{\\vec{\\sigma}}P(\\vec{\\sigma})f_{\\mu}(\\vec{\\sigma}) = \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}} $$\nå¯¹äº $\\mu = 1, 2, \\cdots , K$ã€‚è¿™äº›æ˜¯å¼ºçº¦æŸï¼Œä½†åªè¦è¿™äº›å¯è§‚å¯Ÿé‡çš„æ•°é‡ $K \\ll \\Omega$ï¼Œå°±æœ‰æ— æ•°ä¸ªä¸æ–¹ç¨‹ï¼ˆ17ï¼‰ä¸€è‡´çš„åˆ†å¸ƒã€‚æˆ‘ä»¬å¦‚ä½•åœ¨å®ƒä»¬ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼Ÿ\nThere are many ways of saying, in words, how we would like to make our choice among the $P (\\sigma)$ that are consistent with the measured expectation values of observables. We would like to pick the simplest or least structured model. We would like not to inject into our model any information beyond what is given to us by the measurements $\\{\\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}}\\}$. From a different point of view, we would like drawing states out of the distribution $P(\\vec{\\sigma})$ to generate samples that are as random as possible while still obeying the constraints in Eq (17). It might seem that each choice of words generates a new discussionâ€”what do we mean, mathematically, by â€œleast structured,â€ or â€œas random as possibleâ€?\nåœ¨ä¸è§‚æµ‹é‡çš„æµ‹é‡æœŸæœ›å€¼ä¸€è‡´çš„ $P (\\sigma)$ ä¹‹é—´è¿›è¡Œé€‰æ‹©æ—¶ï¼Œæœ‰è®¸å¤šæ–¹æ³•å¯ä»¥ç”¨è¯­è¨€è¡¨è¾¾ã€‚æˆ‘ä»¬å¸Œæœ›é€‰æ‹©æœ€ç®€å•æˆ–ç»“æ„æœ€å°‘çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›ä¸è¦å°†ä»»ä½•è¶…å‡ºæµ‹é‡ $\\{\\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}}\\}$ æ‰€æä¾›çš„ä¿¡æ¯æ³¨å…¥åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚ä»ä¸åŒçš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¸Œæœ›ä»åˆ†å¸ƒ $P(\\vec{\\sigma})$ ä¸­æŠ½å–çŠ¶æ€ï¼Œä»¥ç”Ÿæˆå°½å¯èƒ½éšæœºçš„æ ·æœ¬ï¼ŒåŒæ—¶ä»ç„¶éµå®ˆæ–¹ç¨‹ï¼ˆ17ï¼‰ä¸­çš„çº¦æŸã€‚ä¼¼ä¹æ¯ç§æªè¾éƒ½ä¼šå¼•å‘æ–°çš„è®¨è®ºâ€”â€”æˆ‘ä»¬åœ¨æ•°å­¦ä¸Šæ˜¯ä»€ä¹ˆæ„æ€ï¼Œâ€œç»“æ„æœ€å°‘â€æˆ–â€œå°½å¯èƒ½éšæœºâ€ï¼Ÿ\nIntroductory courses in statistical mechanics make some remarks about entropy as a measure of our ignorance about the microscopic state of a system, but this connection often is left quite vague. In laying the foundations of information theory, Shannon made this connection precise (Shannon, 1948). If we ask a question, we have the intuition that we â€œgain informationâ€ when we hear the answer. If we want to attach a number to this information gain, then the unique measure that is consistent with natural constraints is the entropy of the distribution out of which the answers are drawn. Thus, if we ask for the microscopic state of a system, the information we gain on hearing the answer is (on average) the entropy of the distribution over these microscopic states. Conversely, if the entropy is less than its maximum possible value, this reduction in entropy measures how much we already know about the microscopic state even before we see it. As a result, for states to be as random as possibleâ€”to be sure that we do not inject extra information about these statesâ€”we need to find the distribution that has the maximum entropy.\nç»Ÿè®¡åŠ›å­¦çš„å…¥é—¨è¯¾ç¨‹å¯¹ç†µä½œä¸ºæˆ‘ä»¬å¯¹ç³»ç»Ÿå¾®è§‚çŠ¶æ€æ— çŸ¥çš„åº¦é‡åšäº†ä¸€äº›è¯„è®ºï¼Œä½†è¿™ç§è”ç³»é€šå¸¸ç›¸å½“æ¨¡ç³Šã€‚åœ¨å¥ å®šä¿¡æ¯ç†è®ºåŸºç¡€æ—¶ï¼Œé¦™å†œï¼ˆShannonï¼Œ1948ï¼‰ä½¿è¿™ç§è”ç³»å˜å¾—ç²¾ç¡®ã€‚å¦‚æœæˆ‘ä»¬æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æœ‰ä¸€ç§ç›´è§‰ï¼Œå½“æˆ‘ä»¬å¬åˆ°ç­”æ¡ˆæ—¶ï¼Œæˆ‘ä»¬â€œè·å¾—ä¿¡æ¯â€ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¸ºè¿™ç§ä¿¡æ¯å¢ç›Šé™„åŠ ä¸€ä¸ªæ•°å­—ï¼Œé‚£ä¹ˆä¸è‡ªç„¶çº¦æŸä¸€è‡´çš„å”¯ä¸€åº¦é‡å°±æ˜¯ä»ä¸­æŠ½å–ç­”æ¡ˆçš„åˆ†å¸ƒçš„ç†µã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬è¯¢é—®ç³»ç»Ÿçš„å¾®è§‚çŠ¶æ€ï¼Œé‚£ä¹ˆåœ¨å¬åˆ°ç­”æ¡ˆæ—¶æˆ‘ä»¬è·å¾—çš„ä¿¡æ¯ï¼ˆå¹³å‡è€Œè¨€ï¼‰å°±æ˜¯è¿™äº›å¾®è§‚çŠ¶æ€åˆ†å¸ƒçš„ç†µã€‚ç›¸åï¼Œå¦‚æœç†µå°äºå…¶æœ€å¤§å¯èƒ½å€¼ï¼Œè¿™ç§ç†µçš„å‡å°‘è¡¡é‡äº†å³ä½¿åœ¨çœ‹åˆ°å®ƒä¹‹å‰æˆ‘ä»¬å·²ç»çŸ¥é“äº†å¤šå°‘å…³äºå¾®è§‚çŠ¶æ€çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿çŠ¶æ€å°½å¯èƒ½éšæœºâ€”â€”ç¡®ä¿æˆ‘ä»¬ä¸ä¼šæ³¨å…¥å…³äºè¿™äº›çŠ¶æ€çš„é¢å¤–ä¿¡æ¯â€”â€”æˆ‘ä»¬éœ€è¦æ‰¾åˆ°å…·æœ‰æœ€å¤§ç†µçš„åˆ†å¸ƒã€‚\nMaximizing the entropy subject to constraints defines a variational problem, maximizing\n$$ \\widetilde{S} = -\\sum_{\\vec{\\sigma}}P(\\vec{\\sigma})\\ln{P(\\vec{\\sigma})} - \\sum_{\\mu = 1}^{K}\\left[\\sum_{\\sigma}P(\\vec{\\sigma})f_{\\mu}(\\vec{\\sigma}) - \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}}\\right] - \\lambda_{0}\\left[\\sum_{\\vec{\\sigma}}P(\\vec{\\sigma}) - 1\\right] $$\nwhere the $\\lambda_{\\mu}$ are Lagrange multipliers. We include an additional term ($\\propto \\lambda_{0}$) to constrain the normalization, so we can treat each entry in the distribution as an independent variable. Then\n$$ \\begin{aligned} \\frac{\\delta \\widetilde{S}}{\\delta P(\\vec{\\sigma})} \u0026= 0\\\\ \\Rightarrow P(\\vec{\\sigma}) \u0026= \\frac{1}{Z(\\{\\lambda_{\\mu}\\})}\\exp{[-E(\\vec{\\sigma})]}\\\\ E(\\vec{\\sigma}) \u0026= \\sum_{\\mu = 1}^{K}\\lambda_{\\mu}f_{\\mu}(\\vec{\\sigma}) \\end{aligned} $$\nThus the model we are looking for is equivalent to an equilibrium statistical mechanics problem in which the â€œenergyâ€ is a sum of terms, one for each of the observables whose expectation values we constrain; the Lagrange multipliers become coupling constants in the effective energy. To finish the construction we need to adjust these couplings $\\{\\lambda_{\\mu}\\}$ to satisfy Eq (17), and in general this is a hard problem; see Appendix B. Importantly, if we have some set of expectation values that we are matching, and we want to add one more, this just adds one more term to the form of the energy function, but in general implementing this extra constraint requires adjusting all the coupling constants.\næœ€å¤§åŒ–å—çº¦æŸçš„ç†µå®šä¹‰äº†ä¸€ä¸ªå˜åˆ†é—®é¢˜ï¼Œæœ€å¤§åŒ–\n$$ \\widetilde{S} = -\\sum_{\\vec{\\sigma}}P(\\vec{\\sigma})\\ln{P(\\vec{\\sigma})} - \\sum_{\\mu = 1}^{K}\\left[\\sum_{\\sigma}P(\\vec{\\sigma})f_{\\mu}(\\vec{\\sigma}) - \\langle f_{\\mu}(\\vec{\\sigma})\\rangle_{\\text{expt}}\\right] - \\lambda_{0}\\left[\\sum_{\\vec{\\sigma}}P(\\vec{\\sigma}) - 1\\right] $$\nå…¶ä¸­ $\\lambda_{\\mu}$ æ˜¯æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ã€‚æˆ‘ä»¬åŒ…æ‹¬ä¸€ä¸ªé¢å¤–çš„é¡¹ï¼ˆ$\\propto \\lambda_{0}$ï¼‰æ¥çº¦æŸå½’ä¸€åŒ–ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†åˆ†å¸ƒä¸­çš„æ¯ä¸ªæ¡ç›®è§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„å˜é‡ã€‚ç„¶å\n$$ \\begin{aligned} \\frac{\\delta \\widetilde{S}}{\\delta P(\\vec{\\sigma})} \u0026= 0\\\\ \\Rightarrow P(\\vec{\\sigma}) \u0026= \\frac{1}{Z(\\{\\lambda_{\\mu}\\})}\\exp{[-E(\\vec{\\sigma})]}\\\\ E(\\vec{\\sigma}) \u0026= \\sum_{\\mu = 1}^{K}\\lambda_{\\mu}f_{\\mu}(\\vec{\\sigma}) \\end{aligned} $$\nå› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾çš„æ¨¡å‹ç­‰ä»·äºä¸€ä¸ªå¹³è¡¡ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œå…¶ä¸­â€œèƒ½é‡â€æ˜¯å„ä¸ªå¯è§‚å¯Ÿé‡çš„æœŸæœ›å€¼ä¹‹å’Œï¼›æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æˆä¸ºæœ‰æ•ˆèƒ½é‡ä¸­çš„è€¦åˆå¸¸æ•°ã€‚ä¸ºäº†å®Œæˆæ„å»ºï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´è¿™äº›è€¦åˆ $\\{\\lambda_{\\mu}\\}$ ä»¥æ»¡è¶³æ–¹ç¨‹ï¼ˆ17ï¼‰ï¼Œé€šå¸¸è¿™æ˜¯ä¸€ä¸ªå›°éš¾çš„é—®é¢˜ï¼›è§é™„å½• Bã€‚é‡è¦çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ç»„æˆ‘ä»¬æ­£åœ¨åŒ¹é…çš„æœŸæœ›å€¼ï¼Œå¹¶ä¸”æˆ‘ä»¬æƒ³å†æ·»åŠ ä¸€ä¸ªï¼Œè¿™åªä¼šåœ¨èƒ½é‡å‡½æ•°çš„å½¢å¼ä¸­æ·»åŠ ä¸€ä¸ªé¢å¤–çš„é¡¹ï¼Œä½†é€šå¸¸å®ç°è¿™ä¸ªé¢å¤–çš„çº¦æŸéœ€è¦è°ƒæ•´æ‰€æœ‰çš„è€¦åˆå¸¸æ•°ã€‚\nTo make the connections explicit, recall that we can define thermodynamic equilibrium as the state of maximum entropy given the constraint of fixed mean energy. This optimization problem is solved by the Boltzmann distribution. In this view the (inverse) temperature is a Lagrange multiplier that enforces the energy constraint, opposite to usual view of controlling the temperature and predicting the energy. The Boltzmann distribution generalizes if other expectation values are constrained (Landau and Lifshitz, 1977).\nä¸ºäº†æ˜ç¡®è¿æ¥ï¼Œå›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†çƒ­åŠ›å­¦å¹³è¡¡å®šä¹‰ä¸ºåœ¨å›ºå®šå¹³å‡èƒ½é‡çº¦æŸä¸‹çš„æœ€å¤§ç†µçŠ¶æ€ã€‚è¿™ä¸ªä¼˜åŒ–é—®é¢˜ç”± Boltzmann åˆ†å¸ƒè§£å†³ã€‚åœ¨è¿™ç§è§‚ç‚¹ä¸­ï¼Œï¼ˆåï¼‰æ¸©åº¦æ˜¯ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œç”¨äºå¼ºåˆ¶æ‰§è¡Œèƒ½é‡çº¦æŸï¼Œè¿™ä¸é€šå¸¸æ§åˆ¶æ¸©åº¦å¹¶é¢„æµ‹èƒ½é‡çš„è§‚ç‚¹ç›¸åã€‚å¦‚æœçº¦æŸäº†å…¶ä»–æœŸæœ›å€¼ï¼ŒBoltzmann åˆ†å¸ƒä¼šè¿›è¡Œæ¨å¹¿ï¼ˆLandau å’Œ Lifshitzï¼Œ1977ï¼‰ã€‚\nThe maximum entropy argument gives us the form of the probability distribution, but we also need the coupling constants. We can think of this as being an â€œinverse statistical mechanicsâ€ problem, since we are given expectation values or correlation functions and need to find the couplings, rather than the other way around. Different formulations of this problem have a long history in the mathematical physics community (Chayes et al., 1984; Keller and Zumino, 1959; Kunkin and Firsch, 1969). An early application to living systems involved reconstructing the forces that hold together the array of gap junction proteins which bridge the membranes of two cells in contact (Braun et al., 1984). As attention focused on networks of neurons, finding the relevant coupling constants came to be described as the â€œinverse Isingâ€ problem, as will become clear below.\næœ€å¤§ç†µè®ºç»™äº†æˆ‘ä»¬æ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œä½†æˆ‘ä»¬ä¹Ÿéœ€è¦è€¦åˆå¸¸æ•°ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸º â€œé€†ç»Ÿè®¡åŠ›å­¦â€ é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬ç»™å‡ºäº†æœŸæœ›å€¼æˆ–ç›¸å…³å‡½æ•°ï¼Œå¹¶ä¸”éœ€è¦æ‰¾åˆ°è€¦åˆï¼Œè€Œä¸æ˜¯ç›¸åã€‚è¿™ä¸ªé—®é¢˜çš„ä¸åŒè¡¨è¿°åœ¨æ•°å­¦ç‰©ç†å­¦ç•Œæœ‰ç€æ‚ ä¹…çš„å†å²ï¼ˆChayes ç­‰äººï¼Œ1984ï¼›Keller å’Œ Zuminoï¼Œ1959ï¼›Kunkin å’Œ Firschï¼Œ1969ï¼‰ã€‚å¯¹ç”Ÿç‰©ç³»ç»Ÿçš„æ—©æœŸåº”ç”¨æ¶‰åŠé‡å»ºå°†ä¸¤ä¸ªæ¥è§¦ç»†èƒçš„è†œè¿æ¥åœ¨ä¸€èµ·çš„é—´éš™è¿æ¥è›‹ç™½é˜µåˆ—çš„åŠ›ï¼ˆBraun ç­‰äººï¼Œ1984ï¼‰ã€‚éšç€æ³¨æ„åŠ›é›†ä¸­åœ¨ç¥ç»å…ƒç½‘ç»œä¸Šï¼Œæ‰¾åˆ°ç›¸å…³çš„è€¦åˆå¸¸æ•°è¢«æè¿°ä¸º â€œé€† Isingâ€ é—®é¢˜ï¼Œæ­£å¦‚ä¸‹é¢å°†å˜å¾—æ¸…æ¥šçš„é‚£æ ·ã€‚\nIn statistical physics there is in some sense a force driving systems toward equilibrium, as encapsulated in the Hâ€“theorem. In many cases this force triumphs, and what we see is a state with maximal entropy subject only to a very few constraints. In the networks of neurons that we study here, there is no Hâ€“theorem, and the list of constraints will be quite long compared to what we are used to in thermodynamics. This means that the probability distributions we write down will be mathematically equivalent to some equilibrium statistical mechanics problem, but they do not describe an equilibrium state of the system we are actually studying. This somewhat subtle relationship between maximum entropy as a description of thermal equilibrium and maximum entropy as a tool for inference was outlined long ago by Jaynes (1957, 1982).\nåœ¨ç»Ÿè®¡ç‰©ç†å­¦ä¸­ï¼Œåœ¨æŸç§æ„ä¹‰ä¸Šæœ‰ä¸€ç§é©±åŠ¨åŠ›å°†ç³»ç»Ÿæ¨å‘å¹³è¡¡ï¼Œæ­£å¦‚ H å®šç†æ‰€æ¦‚æ‹¬çš„é‚£æ ·ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¿™ç§åŠ›é‡å–å¾—äº†èƒœåˆ©ï¼Œæˆ‘ä»¬æ‰€çœ‹åˆ°çš„æ˜¯ä¸€ä¸ªä»…å—å¾ˆå°‘çº¦æŸçš„æœ€å¤§ç†µçŠ¶æ€ã€‚åœ¨æˆ‘ä»¬è¿™é‡Œç ”ç©¶çš„ç¥ç»å…ƒç½‘ç»œä¸­ï¼Œæ²¡æœ‰ H å®šç†ï¼Œå¹¶ä¸”ä¸æˆ‘ä»¬åœ¨çƒ­åŠ›å­¦ä¸­ä¹ æƒ¯çš„ç›¸æ¯”ï¼Œçº¦æŸåˆ—è¡¨å°†ç›¸å½“é•¿ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å†™ä¸‹çš„æ¦‚ç‡åˆ†å¸ƒåœ¨æ•°å­¦ä¸Šç­‰ä»·äºæŸä¸ªå¹³è¡¡ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œä½†å®ƒä»¬å¹¶ä¸æè¿°æˆ‘ä»¬å®é™…ç ”ç©¶çš„ç³»ç»Ÿçš„å¹³è¡¡çŠ¶æ€ã€‚Jaynesï¼ˆ1957ï¼Œ1982ï¼‰æ—©å·²æ¦‚è¿°äº†æœ€å¤§ç†µä½œä¸ºçƒ­å¹³è¡¡æè¿°å’Œæœ€å¤§ç†µä½œä¸ºæ¨ç†å·¥å…·ä¹‹é—´è¿™ç§å¾®å¦™çš„å…³ç³»ã€‚\nIf we donâ€™t have any constraints then the maximum entropy distribution is uniform over all $\\Omega$ states. Each observable whose expectation value we constrain lowers the maximum allowed value of the entropy, and if we add enough constraints we eventually reach the true entropy and hence the true distribution. Often it make sense to group the observables into oneâ€“body, twoâ€“body, threebody terms, etc.. Having constrained all the kâ€“body observables for $k\\leq K$, the maximum entropy model makes parameterâ€“free predictions for correlations among groups of $k \u003e K$ variables. This provides a powerful path to testing the model, and defines a natural generalization of connected correlations (Schneidman et al., 2003).\nå¦‚æœæˆ‘ä»¬æ²¡æœ‰ä»»ä½•çº¦æŸï¼Œé‚£ä¹ˆæœ€å¤§ç†µåˆ†å¸ƒåœ¨æ‰€æœ‰ $\\Omega$ çŠ¶æ€ä¸Šæ˜¯å‡åŒ€çš„ã€‚æˆ‘ä»¬çº¦æŸçš„æ¯ä¸ªå¯è§‚å¯Ÿé‡çš„æœŸæœ›å€¼éƒ½ä¼šé™ä½å…è®¸çš„æœ€å¤§ç†µå€¼ï¼Œå¦‚æœæˆ‘ä»¬æ·»åŠ è¶³å¤Ÿçš„çº¦æŸï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šè¾¾åˆ°çœŸå®çš„ç†µå€¼ï¼Œä»è€Œå¾—åˆ°çœŸå®çš„åˆ†å¸ƒã€‚é€šå¸¸ï¼Œå°†å¯è§‚å¯Ÿé‡åˆ†ä¸ºå•ä½“ã€äºŒä½“ã€ä¸‰ä½“ç­‰é¡¹æ˜¯æœ‰æ„ä¹‰çš„ã€‚åœ¨çº¦æŸäº†æ‰€æœ‰ $k\\leq K$ çš„ $k$ ä½“å¯è§‚å¯Ÿé‡ä¹‹åï¼Œæœ€å¤§ç†µæ¨¡å‹å¯¹ $k \u003e K$ å˜é‡ç»„ä¹‹é—´çš„ç›¸å…³æ€§åšå‡ºæ— å‚æ•°é¢„æµ‹ã€‚è¿™ä¸ºæµ‹è¯•æ¨¡å‹æä¾›äº†ä¸€æ¡å¼ºæœ‰åŠ›çš„è·¯å¾„ï¼Œå¹¶å®šä¹‰äº†è¿æ¥ç›¸å…³æ€§çš„è‡ªç„¶æ¨å¹¿ï¼ˆSchneidman ç­‰äººï¼Œ2003ï¼‰ã€‚\nThe connection of maximum entropy models to the Boltzmann distribution gives us intuition and practical computational tools. It can also leave the impression that we are describing a system in equilibrium, which would be a disaster. In fact the maximum entropy distribution describes thermal equilibrium only if the observable that we constrain is the energy in the mechanical sense. There is no obstacle to building maximum entropy models for the distribution of states in a nonâ€“equilibrium system.\næœ€å¤§ç†µæ¨¡å‹ä¸ Boltzmann åˆ†å¸ƒçš„è”ç³»ä¸ºæˆ‘ä»¬æä¾›äº†ç›´è§‰å’Œå®ç”¨çš„è®¡ç®—å·¥å…·ã€‚å®ƒä¹Ÿå¯èƒ½ç»™äººç•™ä¸‹æˆ‘ä»¬æ­£åœ¨æè¿°ä¸€ä¸ªå¹³è¡¡ç³»ç»Ÿçš„å°è±¡ï¼Œè¿™å°†æ˜¯ç¾éš¾æ€§çš„ã€‚äº‹å®ä¸Šï¼Œåªæœ‰å½“æˆ‘ä»¬çº¦æŸçš„å¯è§‚å¯Ÿé‡æ˜¯æœºæ¢°æ„ä¹‰ä¸Šçš„èƒ½é‡æ—¶ï¼Œæœ€å¤§ç†µåˆ†å¸ƒæ‰æè¿°çƒ­å¹³è¡¡ã€‚æ„å»ºéå¹³è¡¡ç³»ç»Ÿä¸­çŠ¶æ€åˆ†å¸ƒçš„æœ€å¤§ç†µæ¨¡å‹æ²¡æœ‰éšœç¢ã€‚\nAlthough we can usefully think of states distributed over an energy landscape, as we have formulated the maximum entropy construction this description works for states at one moment in time. Thus we cannot conclude that the dynamics by which the system moves from one state to another are analogous to Brownian motion on the effective energy surface. There are infinitely many models for the dynamics that are consistent with this description, and most of these will not obey detailed balance. Recent work shows how to explore a large family of dynamical models consistent with the maximum entropy distribution, and applies these ideas to collective animal behavior (Chen et al., 2023). There also are generalizations of the maximum entropy method to describe distributions of trajectories, as we discuss below (Â§IV.D); maximum entropy models for trajectories sometimes are called maximum caliber (Ghosh et al., 2020; Press Ìe et al., 2013). Finally we note that, for better or worse, the symmetries that are central to many problems in statistical physics in general are absent from the systems we will be studying; flocks and swarms are an exception, as discussed in Â§A.2.\nå°½ç®¡æˆ‘ä»¬å¯ä»¥æœ‰ç”¨åœ°å°†çŠ¶æ€åˆ†å¸ƒè§†ä¸ºèƒ½é‡æ™¯è§‚ï¼Œä½†æ­£å¦‚æˆ‘ä»¬æ‰€åˆ¶å®šçš„æœ€å¤§ç†µæ„é€ ï¼Œè¿™ç§æè¿°é€‚ç”¨äºæŸä¸€æ—¶åˆ»çš„çŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½å¾—å‡ºç³»ç»Ÿä»ä¸€ä¸ªçŠ¶æ€ç§»åŠ¨åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„åŠ¨åŠ›å­¦ç±»ä¼¼äºæœ‰æ•ˆèƒ½é‡è¡¨é¢ä¸Šçš„å¸ƒæœ—è¿åŠ¨çš„ç»“è®ºã€‚æœ‰æ— æ•°ç§åŠ¨åŠ›å­¦æ¨¡å‹ä¸è¿™ç§æè¿°ä¸€è‡´ï¼Œå…¶ä¸­å¤§å¤šæ•°ä¸ä¼šéµå®ˆè¯¦ç»†å¹³è¡¡ã€‚ æœ€è¿‘çš„å·¥ä½œå±•ç¤ºäº†å¦‚ä½•æ¢ç´¢ä¸æœ€å¤§ç†µåˆ†å¸ƒä¸€è‡´çš„å¤§é‡åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œå¹¶å°†è¿™äº›æ€æƒ³åº”ç”¨äºé›†ä½“åŠ¨ç‰©è¡Œä¸ºï¼ˆChen ç­‰äººï¼Œ2023ï¼‰ã€‚æœ€å¤§ç†µæ–¹æ³•ä¹Ÿæœ‰æ¨å¹¿ï¼Œç”¨äºæè¿°è½¨è¿¹åˆ†å¸ƒï¼Œæ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·ï¼ˆÂ§IV.Dï¼‰ï¼›è½¨è¿¹çš„æœ€å¤§ç†µæ¨¡å‹æœ‰æ—¶è¢«ç§°ä¸ºæœ€å¤§å£å¾„ï¼ˆGhosh ç­‰äººï¼Œ2020ï¼›Press Ìe ç­‰äººï¼Œ2013ï¼‰ã€‚æœ€åæˆ‘ä»¬æ³¨æ„åˆ°ï¼Œæ— è®ºå¥½åï¼Œè®¸å¤šç»Ÿè®¡ç‰©ç†å­¦é—®é¢˜ä¸­è‡³å…³é‡è¦çš„å¯¹ç§°æ€§åœ¨æˆ‘ä»¬å°†è¦ç ”ç©¶çš„ç³»ç»Ÿä¸­æ˜¯ç¼ºå¤±çš„ï¼›å¦‚ Â§A.2 æ‰€è®¨è®ºçš„ï¼Œé¸Ÿç¾¤å’Œè™«ç¾¤æ˜¯ä¸€ä¸ªä¾‹å¤–ã€‚\nTo conclude this introduction, we emphasize that maximum entropy is unlike usual theories. We donâ€™t start with a theoretical principle or even a model. Rather, we start with some features of the data and test the hypothesis that these features alone encode everything we need to describe the system. Whenever we use this approach we are referring back to the basic structure of the optimization problem defined in Eq (18), and its formal solution in Eqs (20, 21), but there is no single maximum entropy model, and each time we need to be explicit: Which are the observables $f_{\\mu}$ whose measured expectation values we want our model to reproduce? Can we find the corresponding Lagrange mutlipliers $\\lambda_{mu}$? Do these parameters have a natural interpretation? Once we answer these questions, we can ask whether these relatively simple statistical physics descriptions make predictions that agree with experiment. There is an unusually clean separation between learning the model (matching observed expectation values) and testing the model (predicting new expectation values). In this sense we can think of maximum entropy as predicting a set of parameter free relations among different aspects of the data. Finally, we will have to think carefully about what it means for models to â€œwork.â€ We begin with early explorations at relatively small $N$ (Â§IV.B), then turn to a wide variety of larger networks (Â§IV.C), and finally address how these analyses can catch up to the experimental frontier (Â§IV.D).\nä¸ºäº†ç»“æŸè¿™ä¸ªä»‹ç»ï¼Œæˆ‘ä»¬å¼ºè°ƒæœ€å¤§ç†µä¸åŒäºé€šå¸¸çš„ç†è®ºã€‚æˆ‘ä»¬ä¸æ˜¯ä»ä¸€ä¸ªç†è®ºåŸåˆ™ç”šè‡³ä¸€ä¸ªæ¨¡å‹å¼€å§‹ã€‚ç›¸åï¼Œæˆ‘ä»¬ä»æ•°æ®çš„ä¸€äº›ç‰¹å¾å¼€å§‹ï¼Œå¹¶æµ‹è¯•è¿™äº›ç‰¹å¾æ˜¯å¦ç¼–ç äº†æè¿°ç³»ç»Ÿæ‰€éœ€çš„ä¸€åˆ‡çš„å‡è®¾ã€‚æ¯å½“æˆ‘ä»¬ä½¿ç”¨è¿™ç§æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šå›åˆ°æ–¹ç¨‹ï¼ˆ18ï¼‰ä¸­å®šä¹‰çš„ä¼˜åŒ–é—®é¢˜çš„åŸºæœ¬ç»“æ„åŠå…¶åœ¨æ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰ä¸­çš„æ­£å¼è§£ï¼Œä½†æ²¡æœ‰å•ä¸€çš„æœ€å¤§ç†µæ¨¡å‹ï¼Œæ¯æ¬¡æˆ‘ä»¬éƒ½éœ€è¦æ˜ç¡®ï¼šå“ªäº›æ˜¯æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹é‡ç°å…¶æµ‹é‡æœŸæœ›å€¼çš„å¯è§‚å¯Ÿé‡ $f_{\\mu}$ï¼Ÿæˆ‘ä»¬èƒ½æ‰¾åˆ°ç›¸åº”çš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•° $\\lambda_{mu}$ å—ï¼Ÿè¿™äº›å‚æ•°æœ‰è‡ªç„¶çš„è§£é‡Šå—ï¼Ÿä¸€æ—¦æˆ‘ä»¬å›ç­”äº†è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å°±å¯ä»¥é—®è¿™äº›ç›¸å¯¹ç®€å•çš„ç»Ÿè®¡ç‰©ç†æè¿°æ˜¯å¦åšå‡ºäº†ä¸å®éªŒä¸€è‡´çš„é¢„æµ‹ã€‚åœ¨å­¦ä¹ æ¨¡å‹ï¼ˆåŒ¹é…è§‚å¯Ÿåˆ°çš„æœŸæœ›å€¼ï¼‰å’Œæµ‹è¯•æ¨¡å‹ï¼ˆé¢„æµ‹æ–°çš„æœŸæœ›å€¼ï¼‰ä¹‹é—´å­˜åœ¨ä¸€ç§å¼‚å¸¸æ¸…æ™°çš„åˆ†ç¦»ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè¯´ï¼Œæˆ‘ä»¬å¯ä»¥å°†æœ€å¤§ç†µè§†ä¸ºé¢„æµ‹æ•°æ®ä¸åŒæ–¹é¢ä¹‹é—´çš„ä¸€ç»„æ— å‚æ•°å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ä»”ç»†æ€è€ƒæ¨¡å‹â€œå·¥ä½œâ€çš„å«ä¹‰ã€‚æˆ‘ä»¬ä»ç›¸å¯¹è¾ƒå° $N$ çš„æ—©æœŸæ¢ç´¢å¼€å§‹ï¼ˆÂ§IV.Bï¼‰ï¼Œç„¶åè½¬å‘å„ç§æ›´å¤§çš„ç½‘ç»œï¼ˆÂ§IV.Cï¼‰ï¼Œæœ€åè§£å†³è¿™äº›åˆ†æå¦‚ä½•èµ¶ä¸Šå®éªŒå‰æ²¿çš„é—®é¢˜ï¼ˆÂ§IV.Dï¼‰ã€‚\nFirst connections to neurons Suppose we observe three neurons, and measure their mean activity as well as their pairwise correlations. Given these measurements, should we be surprised by how often the three neurons are active together? Maximum entropy provides a way of answering this question, generating a â€œnull modelâ€ prediction assuming all the correlation structure is captured in the pairs, and this was appreciated âˆ¼2000 (Martignon et al., 2000). Over the next several years a more ambitious idea emerged: could we build maximum entropy models for patterns of activity in larger populations of neurons? The first target for this analysis was a population of neurons in the salamander retina, as it responds to naturalistic visual inputs (Schneidman et al., 2006).\nå‡è®¾æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸‰ä¸ªç¥ç»å…ƒï¼Œå¹¶æµ‹é‡å®ƒä»¬çš„å¹³å‡æ´»åŠ¨ä»¥åŠå®ƒä»¬çš„æˆå¯¹ç›¸å…³æ€§ã€‚é‰´äºè¿™äº›æµ‹é‡ç»“æœï¼Œæˆ‘ä»¬æ˜¯å¦åº”è¯¥å¯¹è¿™ä¸‰ä¸ªç¥ç»å…ƒä¸€èµ·æ´»è·ƒçš„é¢‘ç‡æ„Ÿåˆ°æƒŠè®¶ï¼Ÿæœ€å¤§ç†µæä¾›äº†ä¸€ç§å›ç­”è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•ï¼Œç”Ÿæˆä¸€ä¸ªâ€œé›¶æ¨¡å‹â€é¢„æµ‹ï¼Œå‡è®¾æ‰€æœ‰çš„ç›¸å…³ç»“æ„éƒ½åŒ…å«åœ¨å¯¹ä¸­ï¼Œè¿™åœ¨å¤§çº¦ 2000 å¹´è¢«è®¤è¯†åˆ°ï¼ˆMartignon ç­‰äººï¼Œ2000ï¼‰ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ å¹´é‡Œï¼Œä¸€ä¸ªæ›´é›„å¿ƒå‹ƒå‹ƒçš„æƒ³æ³•å‡ºç°äº†ï¼šæˆ‘ä»¬èƒ½å¦ä¸ºæ›´å¤§ç¾¤ä½“çš„ç¥ç»å…ƒæ´»åŠ¨æ¨¡å¼æ„å»ºæœ€å¤§ç†µæ¨¡å‹ï¼Ÿè¿™ç§åˆ†æçš„ç¬¬ä¸€ä¸ªç›®æ ‡æ˜¯è¾èˆè§†ç½‘è†œä¸­çš„ä¸€ç¾¤ç¥ç»å…ƒï¼Œå› ä¸ºå®ƒå¯¹è‡ªç„¶è§†è§‰è¾“å…¥åšå‡ºååº”ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚\nIn response to natural movies, the output neurons of the retinaâ€”the â€œganglion cellsâ€ that carry visual signals from eye to brain, and which as a group form the optic nerveâ€”are sparsely activated, generating an average of just a few spikes per second each (Fig 7A, B). Those initial experiments monitored populations of up to forty neurons in a small patch of the retina, with recordings of up to one hour. Pairs of neurons have temporal correlations with a relatively sharp peak or trough on a broad background that tracks longer timescales in the visual input (Fig 7C). If we discretize time into bins of $\\Delta\\tau = 20$ ms then we capture most of the short time correlations but still have a very low probability of seeing two spikes in the same bin, so that responses of neuron i become binary, $\\sigma_{i} = \\{0, 1\\}$.\nä¸ºäº†å“åº”è‡ªç„¶æ´»åŠ¨ï¼Œè§†ç½‘è†œçš„è¾“å‡ºç¥ç»å…ƒâ€”â€”å°†è§†è§‰ä¿¡å·ä»çœ¼ç›ä¼ é€’åˆ°å¤§è„‘çš„â€œç¥ç»èŠ‚ç»†èƒâ€ï¼Œå®ƒä»¬ä½œä¸ºä¸€ä¸ªæ•´ä½“å½¢æˆè§†ç¥ç»â€”â€”è¢«ç¨€ç–æ¿€æ´»ï¼Œæ¯ä¸ªç¥ç»å…ƒå¹³å‡æ¯ç§’åªäº§ç”Ÿå‡ ä¸ªå°–å³°ï¼ˆå›¾ 7Aï¼ŒBï¼‰ã€‚é‚£äº›åˆå§‹å®éªŒç›‘æµ‹äº†è§†ç½‘è†œå°å—ä¸­å¤šè¾¾å››åä¸ªç¥ç»å…ƒçš„ç¾¤ä½“ï¼Œè®°å½•æ—¶é—´é•¿è¾¾ä¸€å°æ—¶ã€‚æˆå¯¹çš„ç¥ç»å…ƒå…·æœ‰æ—¶é—´ç›¸å…³æ€§ï¼Œåœ¨è§†è§‰è¾“å…¥çš„è¾ƒé•¿æ—¶é—´å°ºåº¦ä¸Šè·Ÿè¸ªè¾ƒå®½èƒŒæ™¯ä¸Šçš„ç›¸å¯¹å°–é”å³°å€¼æˆ–è°·å€¼ï¼ˆå›¾ 7Cï¼‰ã€‚å¦‚æœæˆ‘ä»¬å°†æ—¶é—´ç¦»æ•£åŒ–ä¸º $\\Delta\\tau = 20$ ms çš„æ—¶é—´æ®µï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ•æ‰åˆ°äº†å¤§éƒ¨åˆ†çŸ­æ—¶é—´ç›¸å…³æ€§ï¼Œä½†ä»ç„¶å¾ˆå°‘çœ‹åˆ°åŒä¸€æ—¶é—´æ®µå†…æœ‰ä¸¤ä¸ªå°–å³°ï¼Œå› æ­¤ç¥ç»å…ƒ i çš„å“åº”å˜ä¸ºäºŒè¿›åˆ¶ï¼Œ$\\sigma_{i} = \\{0, 1\\}$ã€‚\nFIG. 7 Responses of the salamander retina to naturalistic movies (Schneidman et al., 2006). (A) Raster plot of the action potentials from $N = 40$ neurons. Each dot represents a spike from one cell. (B) Expanded view of the green box in (A), showing the discretization of time into bins of width $\\Delta\\tau = 20$ms. The result (bottom) is that the state of the network is a binary word $\\{\\sigma_{i}\\}$. (C) Correlations between two neurons. Results are shown as the probability per unit time of a spike in cell $j$ (spike rate) given that there is a spike in cell $i$ at time $t = 0$; the plateau at long times should be the mean rate $r_{j} = \\langle\\sigma_{j}\\rangle/\\Delta\\tau$. There a peak with a width ~ 100 ms, related to time scales in the visual input, and a peak with width ~ 20ms emphasizes in the inset; this motivates the choice of bins size. (D) Distribution of (off-diagonal) correlation coefficients, from Eq (24), across the population of $N = 40$ neurons. (E) Probability that $K$ out of the $N = 40$ neurons are active in the same time bin (red) compared with expectations if activity of each neuron were independent of all the others (blue). Dashed lines are exponential (red) and Poisson (blue), to guide the eye. (F) Predicted occurrence rates of different binary patterns vs the observed rates, for the independent model $P_{1}$ [Eqs (29, 30), blue] and the pairwise maximum entropy model $P_{2}$ [Eqs (35, 33), red].\nå›¾ 7 è¾èˆè§†ç½‘è†œå¯¹è‡ªç„¶ç”µå½±çš„å“åº”ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚(A) $N = 40$ ä¸ªç¥ç»å…ƒçš„åŠ¨ä½œç”µä½å…‰æ …å›¾ã€‚æ¯ä¸ªç‚¹ä»£è¡¨ä¸€ä¸ªç»†èƒçš„ä¸€ä¸ªå°–å³°ã€‚(B) (A) ä¸­ç»¿è‰²æ¡†çš„æ”¾å¤§è§†å›¾ï¼Œæ˜¾ç¤ºæ—¶é—´è¢«ç¦»æ•£åŒ–ä¸ºå®½åº¦ä¸º $\\Delta\\tau = 20$ms çš„æ—¶é—´æ®µã€‚ç»“æœï¼ˆåº•éƒ¨ï¼‰æ˜¯ç½‘ç»œçš„çŠ¶æ€æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶å­— $\\{\\sigma_{i}\\}$ã€‚(C) ä¸¤ä¸ªç¥ç»å…ƒä¹‹é—´çš„ç›¸å…³æ€§ã€‚ç»“æœæ˜¾ç¤ºä¸ºåœ¨ç»†èƒ $i$ åœ¨æ—¶é—´ $t = 0$ å¤„æœ‰ä¸€ä¸ªå°–å³°çš„æƒ…å†µä¸‹ï¼Œç»†èƒ $j$ ä¸­å°–å³°çš„å•ä½æ—¶é—´æ¦‚ç‡ï¼ˆå°–å³°ç‡ï¼‰ï¼›é•¿æ—¶é—´å¤„çš„å¹³å°åº”è¯¥æ˜¯å¹³å‡ç‡ $r_{j} = \\langle\\sigma_{j}\\rangle/\\Delta\\tau$ã€‚æœ‰ä¸€ä¸ªå®½åº¦çº¦ä¸º 100 ms çš„å³°å€¼ï¼Œä¸è§†è§‰è¾“å…¥ä¸­çš„æ—¶é—´å°ºåº¦æœ‰å…³ï¼Œæ’å›¾ä¸­å¼ºè°ƒäº†ä¸€ä¸ªå®½åº¦çº¦ä¸º 20ms çš„å³°å€¼ï¼›è¿™æ¿€å‘äº†é€‰æ‹©ç®±å­å¤§å°çš„åŠ¨æœºã€‚(D) è·¨è¶Š $N = 40$ ä¸ªç¥ç»å…ƒç¾¤ä½“çš„ï¼ˆéå¯¹è§’çº¿ï¼‰ç›¸å…³ç³»æ•°çš„åˆ†å¸ƒï¼Œæ¥è‡ªæ–¹ç¨‹ï¼ˆ24ï¼‰ã€‚(E) åœ¨åŒä¸€æ—¶é—´æ®µå†… $N = 40$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªæ´»è·ƒçš„æ¦‚ç‡ï¼ˆçº¢è‰²ï¼‰ä¸å¦‚æœæ¯ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–ç¥ç»å…ƒçš„é¢„æœŸï¼ˆè“è‰²ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚è™šçº¿åˆ†åˆ«ä¸ºæŒ‡æ•°ï¼ˆçº¢è‰²ï¼‰å’Œæ³Šæ¾ï¼ˆè“è‰²ï¼‰ï¼Œä»¥å¼•å¯¼çœ¼ç›ã€‚(F) ä¸åŒäºŒè¿›åˆ¶æ¨¡å¼çš„é¢„æµ‹å‘ç”Ÿç‡ä¸è§‚å¯Ÿåˆ°çš„å‘ç”Ÿç‡ï¼Œå¯¹äºç‹¬ç«‹æ¨¡å‹ $P_{1}$ [æ–¹ç¨‹ï¼ˆ29ï¼Œ30ï¼‰ï¼Œè“è‰²] å’Œæˆå¯¹æœ€å¤§ç†µæ¨¡å‹ $P_{2}$ [æ–¹ç¨‹ï¼ˆ35ï¼Œ33ï¼‰ï¼Œçº¢è‰²]ã€‚\nIf we define as usual the fluctuations around the mean,\n$$ \\delta\\sigma_{i} = \\sigma_{i} - \\langle\\sigma_{i}\\rangle, $$\nthen the data sets were large enough to get good estimates of the covariance\n$$ C_{ij} = \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle = \\langle\\sigma_{i}\\sigma_{j}\\rangle_{c} $$\nwhere $\\langle\\cdots\\rangle_{c}$ denotes the connected part of the correlations; in many cases we have more intuition about the correlation matrix\n$$ \\widetilde{C}_{ij} = \\frac{C_{ij}}{\\sqrt{C_{ii}C_{jj}}} $$\nImportantly, these pairwise correlations are weak: almost all of the $|\\widetilde{C}_{i\\neq j}|\u003c0.1$, and the bulk of these correlations are just a few percent (Fig 7D). The recordings are long enough that these weak correlations are statistically significant, and almost none of the matrix elements are zero within errors. Correlations thus are weak and widespread, which seems to be common across many different regions of the brain.\nå¦‚æœæˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·å®šä¹‰å›´ç»•å‡å€¼çš„æ³¢åŠ¨ï¼Œ\n$$ \\delta\\sigma_{i} = \\sigma_{i} - \\langle\\sigma_{i}\\rangle, $$\né‚£ä¹ˆæ•°æ®é›†è¶³å¤Ÿå¤§ï¼Œå¯ä»¥å¾ˆå¥½åœ°ä¼°è®¡åæ–¹å·®\n$$ C_{ij} = \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle = \\langle\\sigma_{i}\\sigma_{j}\\rangle_{c} $$\nå…¶ä¸­ $\\langle\\cdots\\rangle_{c}$ è¡¨ç¤ºç›¸å…³æ€§çš„è¿æ¥éƒ¨åˆ†ï¼›åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯¹ç›¸å…³çŸ©é˜µæœ‰æ›´å¤šçš„ç›´è§‰\n$$ \\widetilde{C}_{ij} = \\frac{C_{ij}}{\\sqrt{C_{ii}C_{jj}}} $$\né‡è¦çš„æ˜¯ï¼Œè¿™äº›æˆå¯¹ç›¸å…³æ€§æ˜¯å¼±çš„ï¼šå‡ ä¹æ‰€æœ‰çš„ $|\\widetilde{C}_{i\\neq j}|\u003c0.1$ï¼Œè€Œä¸”è¿™äº›ç›¸å…³æ€§çš„ä¸»ä½“åªæœ‰å‡ ä¸ªç™¾åˆ†ç‚¹ï¼ˆå›¾ 7Dï¼‰ã€‚è®°å½•æ—¶é—´è¶³å¤Ÿé•¿ï¼Œè¿™äº›å¾®å¼±çš„ç›¸å…³æ€§åœ¨ç»Ÿè®¡ä¸Šæ˜¯æ˜¾è‘—çš„ï¼Œå¹¶ä¸”çŸ©é˜µå…ƒç´ å‡ ä¹æ²¡æœ‰åœ¨è¯¯å·®èŒƒå›´å†…ä¸ºé›¶ã€‚å› æ­¤ï¼Œç›¸å…³æ€§æ˜¯å¼±è€Œå¹¿æ³›çš„ï¼Œè¿™ä¼¼ä¹åœ¨å¤§è„‘çš„è®¸å¤šä¸åŒåŒºåŸŸä¸­å¾ˆå¸¸è§ã€‚\nIf we look just at two neurons, the approximation that they are independent of one another is very good, because the correlations are so weak. But if we look more globally then the widespread correlations combine to have qualitative effects. As an example, we can ask for the probability that $K$ out of $N = 40$ neurons are active in the same time bin, $P_{N}(K)$, and we find that this has a much longer tail than expected if the cells were independent (Fig 7E); simultaneous activity of $K = 10$ neurons already is $\\sim 10^{3}\\times$ more likely than in the independent model.\nå¦‚æœæˆ‘ä»¬åªçœ‹ä¸¤ä¸ªç¥ç»å…ƒï¼Œç”±äºç›¸å…³æ€§éå¸¸å¼±ï¼Œå®ƒä»¬ç›¸äº’ç‹¬ç«‹çš„è¿‘ä¼¼æ˜¯éå¸¸å¥½çš„ã€‚ä½†å¦‚æœæˆ‘ä»¬æ›´å…¨é¢åœ°è§‚å¯Ÿï¼Œé‚£ä¹ˆå¹¿æ³›çš„ç›¸å…³æ€§ä¼šç»“åˆèµ·æ¥äº§ç”Ÿå®šæ€§çš„å½±å“ã€‚ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è¯¢é—®åœ¨åŒä¸€æ—¶é—´æ®µå†… $N = 40$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªæ´»è·ƒçš„æ¦‚ç‡ $P_{N}(K)$ï¼Œæˆ‘ä»¬å‘ç°å¦‚æœç»†èƒæ˜¯ç‹¬ç«‹çš„ï¼Œè¿™ä¸ªæ¦‚ç‡çš„å°¾éƒ¨è¦é•¿å¾—å¤šï¼ˆå›¾ 7Eï¼‰ï¼›$K = 10$ ä¸ªç¥ç»å…ƒçš„åŒæ—¶æ´»åŠ¨å·²ç»æ¯”ç‹¬ç«‹æ¨¡å‹é«˜å‡ºçº¦ $10^{3}$ å€ã€‚\nIf we focus on $N = 10$ neurons then the experiments are long enough to sample all $\\Omega\\sim 10^{3}$ states, and the probabilities of these different binary words depart dramatically from the predictions of an independent model (Fig 7F). If we group the different binary words by the total number of active neurons, then the predictions of the independent model actually are antiâ€“correlated with the real data. We emphasize that these failures occur despite the fact that pairwise correlations are weak, and that they are visible at a relatively modest $N = 10$.\nå¦‚æœæˆ‘ä»¬å…³æ³¨ $N = 10$ ä¸ªç¥ç»å…ƒï¼Œé‚£ä¹ˆå®éªŒæ—¶é—´è¶³å¤Ÿé•¿ï¼Œå¯ä»¥å¯¹æ‰€æœ‰ $\\Omega\\sim 10^{3}$ çŠ¶æ€è¿›è¡Œé‡‡æ ·ï¼Œå¹¶ä¸”è¿™äº›ä¸åŒäºŒè¿›åˆ¶å­—çš„æ¦‚ç‡ä¸ç‹¬ç«‹æ¨¡å‹çš„é¢„æµ‹æœ‰æ˜¾è‘—åç¦»ï¼ˆå›¾ 7Fï¼‰ã€‚å¦‚æœæˆ‘ä»¬æŒ‰æ´»è·ƒç¥ç»å…ƒçš„æ€»æ•°å¯¹ä¸åŒçš„äºŒè¿›åˆ¶å­—è¿›è¡Œåˆ†ç»„ï¼Œé‚£ä¹ˆç‹¬ç«‹æ¨¡å‹çš„é¢„æµ‹å®é™…ä¸Šä¸çœŸå®æ•°æ®å‘ˆè´Ÿç›¸å…³ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œå°½ç®¡æˆå¯¹ç›¸å…³æ€§å¾ˆå¼±ï¼Œä½†è¿™äº›å¤±è´¥ä»ç„¶å‘ç”Ÿï¼Œå¹¶ä¸”å®ƒä»¬åœ¨ç›¸å¯¹é€‚åº¦çš„ $N = 10$ ä¸‹æ˜¯å¯è§çš„ã€‚\nIf we want to build a model for the patterns of activity in networks of neurons it certainly makes sense to insist that we match the mean activity of each cell. At the risk of being pedantic, what this means explicitly is that we are looking for a probability distribution over network states, $P_{1}(\\vec{\\sigma})$ that has the maximum entropy while correctly predicting the expectation values\n$$ m_{i} \\equiv \\langle\\sigma_{i}\\rangle_{\\text{expt}} = \\langle\\sigma_{i}\\rangle_{P_{1}} $$\nReferring back to Eq (17), the observables that we constrain become\n$$ \\{f_{\\mu}^{(1)}\\}\\rightarrow \\{\\sigma_{i}\\} $$\nnote that $i = 1, 2,\\cdots, N$, where $N$ is the number of neurons. To implement these constraints we need one Lagrange multiplier for each neuron, and it is convenient to write this multiplier as an â€œeffective fieldâ€ $h_{i}$, so that the general Eqs (20, 21) become\n$$ \\begin{aligned} P_{1}(\\vec{\\sigma}) \u0026= \\frac{1}{Z_{1}}\\exp{[-E_{1}(\\vec{\\sigma})]}\\\\ E_{1}(\\vec{\\sigma}) \u0026= \\sum_{\\mu}\\lambda_{\\mu}^{(1)}f_{\\mu}^{(1)}\\\\ \u0026= \\sum_{i=1}^{N}h_{i}\\sigma_{i} \\end{aligned} $$\nWe notice that $E_1$ is the energy function for independent spins in local fields, and so the probability distribution over states factorizes,\n$$ P_{1}(\\vec{\\sigma}) \\propto \\prod_{i=1}^{N}e^{-h_{i}\\sigma_{i}} $$\nThus a maximum entropy model which matches only the mean activities of individual neurons is a model in which the activity of each cell is independent of all the others. We have seen that this model is in dramatic disagreement with the data.\nå¦‚æœæˆ‘ä»¬æƒ³ä¸ºç¥ç»å…ƒç½‘ç»œä¸­çš„æ´»åŠ¨æ¨¡å¼æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼ŒåšæŒåŒ¹é…æ¯ä¸ªç»†èƒçš„å¹³å‡æ´»åŠ¨å½“ç„¶æ˜¯æœ‰æ„ä¹‰çš„ã€‚å†’ç€å•°å—¦çš„é£é™©ï¼Œè¿™æ˜ç¡®æ„å‘³ç€æˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªç½‘ç»œçŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒ $P_{1}(\\vec{\\sigma})$ï¼Œè¯¥åˆ†å¸ƒå…·æœ‰æœ€å¤§ç†µï¼ŒåŒæ—¶æ­£ç¡®é¢„æµ‹æœŸæœ›å€¼\n$$ m_{i} \\equiv \\langle\\sigma_{i}\\rangle_{\\text{expt}} = \\langle\\sigma_{i}\\rangle_{P_{1}} $$\nå›åˆ°æ–¹ç¨‹ï¼ˆ17ï¼‰ï¼Œæˆ‘ä»¬çº¦æŸçš„å¯è§‚å¯Ÿé‡å˜ä¸º\n$$ \\{f_{\\mu}^{(1)}\\}\\rightarrow \\{\\sigma_{i}\\} $$\næ³¨æ„ $i = 1, 2,\\cdots, N$ï¼Œå…¶ä¸­ $N$ æ˜¯ç¥ç»å…ƒçš„æ•°é‡ã€‚ä¸ºäº†å®ç°è¿™äº›çº¦æŸï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªç¥ç»å…ƒä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œå¹¶ä¸”å°†è¿™ä¸ªä¹˜æ•°å†™æˆâ€œæœ‰æ•ˆåœºâ€ $h_{i}$ æ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œå› æ­¤ä¸€èˆ¬çš„æ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰å˜ä¸º\n$$ \\begin{aligned} P_{1}(\\vec{\\sigma}) \u0026= \\frac{1}{Z_{1}}\\exp{[-E_{1}(\\vec{\\sigma})]}\\\\ E_{1}(\\vec{\\sigma}) \u0026= \\sum_{\\mu}\\lambda_{\\mu}^{(1)}f_{\\mu}^{(1)}\\\\ \u0026= \\sum_{i=1}^{N}h_{i}\\sigma_{i} \\end{aligned} $$\næˆ‘ä»¬æ³¨æ„åˆ° $E_1$ æ˜¯å±€éƒ¨åœºä¸­ç‹¬ç«‹è‡ªæ—‹çš„èƒ½é‡å‡½æ•°ï¼Œå› æ­¤çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥åˆ†è§£ä¸ºï¼Œ\n$$ P_{1}(\\vec{\\sigma}) \\propto \\prod_{i=1}^{N}e^{-h_{i}\\sigma_{i}} $$\nå› æ­¤ï¼Œä¸€ä¸ªä»…åŒ¹é…å•ä¸ªç¥ç»å…ƒå¹³å‡æ´»åŠ¨çš„æœ€å¤§ç†µæ¨¡å‹æ˜¯ä¸€ä¸ªæ¯ä¸ªç»†èƒçš„æ´»åŠ¨éƒ½ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–ç»†èƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°è¿™ä¸ªæ¨¡å‹ä¸æ•°æ®æœ‰æ˜¾è‘—çš„ä¸ä¸€è‡´ã€‚\nA natural first step in trying to capture the nonindependence of neurons is to build a maximum entropy model that matches pairwise correlations. Thus, we are looking for a distribution $P_{2}(\\sigma)$ that has maximum entropy while matching the mean activities as in Eq (25) and also the covariance of activity\n$$ C_{ij}\\equiv \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle_{\\text{expt}} = \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle_{P_{2}} $$\nIn the language of Eq (17) this means that we have a second set of relevant observables\n$$ \\{f_{\\nu}^{(2)}\\} \\rightarrow \\{\\sigma_{i}\\sigma_{j}\\} $$\nAs before we need one Lagrange multiplier for each constrained observable, and it is useful to think of the Lagrange multiplier that constrains $\\sigma_{i}\\sigma_{j}$ as being a â€œspinspinâ€ coupling $\\lambda_{ij} = J_{ij}$. Recalling that each extra constraint adds a term to the effective energy function, Eqs (20, 21) become\n$$ \\begin{aligned} P_{2}(\\vec{\\sigma}) \u0026= \\frac{1}{Z_{2}(\\{h_{i};J_{ij}\\})}e^{-E_{2}(\\vec{\\sigma})}\\\\ E_{2}(\\vec{\\sigma}) \u0026= \\sum_{\\mu}\\lambda_{\\mu}^{(1)}f_{\\mu}^{(1)} + \\sum_{\\mu}\\lambda_{\\mu}^{(2)}f_{\\mu}^{(2)}\\\\ \u0026= \\sum_{i=1}^{N}h_{i}\\sigma_{i} + \\frac{1}{2}\\sum_{i\\neq j}J_{ij}\\sigma_{i}\\sigma_{j} \\end{aligned} $$\nThis is exactly an Ising model with pairwise interactions among the spinsâ€”not an analogy but a mathematical equivalence.\næ•æ‰ç¥ç»å…ƒéç‹¬ç«‹æ€§çš„ä¸€ä¸ªè‡ªç„¶ç¬¬ä¸€æ­¥æ˜¯æ„å»ºä¸€ä¸ªåŒ¹é…æˆå¯¹ç›¸å…³æ€§çš„æœ€å¤§ç†µæ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªåˆ†å¸ƒ $P_{2}(\\sigma)$ï¼Œè¯¥åˆ†å¸ƒå…·æœ‰æœ€å¤§ç†µï¼ŒåŒæ—¶åŒ¹é…æ–¹ç¨‹ï¼ˆ25ï¼‰ä¸­çš„å¹³å‡æ´»åŠ¨ä»¥åŠæ´»åŠ¨çš„åæ–¹å·®\n$$ C_{ij}\\equiv \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle_{\\text{expt}} = \\langle\\delta\\sigma_{i}\\delta\\sigma_{j}\\rangle_{P_{2}} $$\nç”¨æ–¹ç¨‹ï¼ˆ17ï¼‰çš„è¯­è¨€æ¥è¯´ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æœ‰ç¬¬äºŒç»„ç›¸å…³çš„å¯è§‚å¯Ÿé‡\n$$ \\{f_{\\nu}^{(2)}\\} \\rightarrow \\{\\sigma_{i}\\sigma_{j}\\} $$\nåƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªå—çº¦æŸçš„å¯è§‚å¯Ÿé‡ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œå¹¶ä¸”å°†çº¦æŸ $\\sigma_{i}\\sigma_{j}$ çš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°è§†ä¸ºâ€œè‡ªæ—‹-è‡ªæ—‹â€è€¦åˆ $\\lambda_{ij} = J_{ij}$ æ˜¯æœ‰ç”¨çš„ã€‚å›æƒ³ä¸€ä¸‹ï¼Œæ¯ä¸ªé¢å¤–çš„çº¦æŸéƒ½ä¼šå‘æœ‰æ•ˆèƒ½é‡å‡½æ•°æ·»åŠ ä¸€é¡¹ï¼Œæ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰å˜ä¸º\n$$ \\begin{aligned} P_{2}(\\vec{\\sigma}) \u0026= \\frac{1}{Z_{2}(\\{h_{i};J_{ij}\\})}e^{-E_{2}(\\vec{\\sigma})}\\\\ E_{2}(\\vec{\\sigma}) \u0026= \\sum_{\\mu}\\lambda_{\\mu}^{(1)}f_{\\mu}^{(1)} + \\sum_{\\mu}\\lambda_{\\mu}^{(2)}f_{\\mu}^{(2)}\\\\ \u0026= \\sum_{i=1}^{N}h_{i}\\sigma_{i} + \\frac{1}{2}\\sum_{i\\neq j}J_{ij}\\sigma_{i}\\sigma_{j} \\end{aligned} $$\nè¿™æ­£æ˜¯å…·æœ‰è‡ªæ—‹ä¹‹é—´æˆå¯¹ç›¸äº’ä½œç”¨çš„ Ising æ¨¡å‹â€”â€”ä¸æ˜¯ç±»æ¯”ï¼Œè€Œæ˜¯æ•°å­¦ç­‰ä»·ã€‚\nIsing models for networks of neurons have a long history, as described in Â§II.C. In their earliest appearance, these models emerged from a hypothetical, simplified model of the underlying dynamics. Here they emerge as the least structured models consistent with measured properties of the network. As a result, we arrive not at some arbitrary Ising model, where we are free to choose the fields and couplings, but at a particular model that describes the actual network of neurons we are observing. To complete this construction we have to adjust the fields and couplings to match the observed mean activities and correlations. Concretely we have to solve Eqs (25, 31), which can be rewritten as\n$$ \\begin{aligned} \\langle \\sigma_{i}\\rangle_{\\text{expt}} \u0026= \\langle\\sigma_{i}\\rangle_{P_{2}} = \\frac{\\partial \\ln{Z_{2}(\\{h_{i};J_{ij}\\})}}{\\partial h_{i}}\\\\ \\langle\\sigma_{i}\\sigma_{j}\\rangle_{\\text{expt}} \u0026= \\langle\\sigma_{i}\\sigma_{j}\\rangle_{P_{2}} = \\frac{\\partial \\ln{Z_{2}(\\{h_{i};J_{ij}\\})}}{\\partial J_{ij}} \\end{aligned} $$\nWith $N = 10$ neurons this is challenging but can be done exactly, since the partition function is a sum over just $\\Omega\\sim 1000$ terms. Once we are done, the model is specified completely. Anything that we compute is a prediction, and there is no room to adjust parameters in search of better agreement with the data.\nç¥ç»å…ƒç½‘ç»œçš„ Ising æ¨¡å‹æœ‰ç€æ‚ ä¹…çš„å†å²ï¼Œå¦‚ Â§II.C æ‰€è¿°ã€‚åœ¨å®ƒä»¬æœ€æ—©å‡ºç°æ—¶ï¼Œè¿™äº›æ¨¡å‹æºè‡ªå¯¹æ½œåœ¨åŠ¨åŠ›å­¦çš„å‡è®¾æ€§ç®€åŒ–æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œå®ƒä»¬ä½œä¸ºä¸ç½‘ç»œçš„æµ‹é‡å±æ€§ä¸€è‡´çš„æœ€å°‘ç»“æ„åŒ–æ¨¡å‹å‡ºç°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸æ˜¯å¾—åˆ°æŸä¸ªä»»æ„çš„ Ising æ¨¡å‹ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å¯ä»¥è‡ªç”±é€‰æ‹©åœºå’Œè€¦åˆï¼Œè€Œæ˜¯å¾—åˆ°ä¸€ä¸ªæè¿°æˆ‘ä»¬æ­£åœ¨è§‚å¯Ÿçš„å®é™…ç¥ç»å…ƒç½‘ç»œçš„ç‰¹å®šæ¨¡å‹ã€‚ä¸ºäº†å®Œæˆè¿™ä¸ªæ„å»ºï¼Œæˆ‘ä»¬å¿…é¡»è°ƒæ•´åœºå’Œè€¦åˆä»¥åŒ¹é…è§‚å¯Ÿåˆ°çš„å¹³å‡æ´»åŠ¨å’Œç›¸å…³æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¿…é¡»è§£å†³æ–¹ç¨‹ï¼ˆ25ï¼Œ31ï¼‰ï¼Œå®ƒä»¬å¯ä»¥é‡å†™ä¸º\n$$ \\begin{aligned} \\langle \\sigma_{i}\\rangle_{\\text{expt}} \u0026= \\langle\\sigma_{i}\\rangle_{P_{2}} = \\frac{\\partial \\ln{Z_{2}(\\{h_{i};J_{ij}\\})}}{\\partial h_{i}}\\\\ \\langle\\sigma_{i}\\sigma_{j}\\rangle_{\\text{expt}} \u0026= \\langle\\sigma_{i}\\sigma_{j}\\rangle_{P_{2}} = \\frac{\\partial \\ln{Z_{2}(\\{h_{i};J_{ij}\\})}}{\\partial J_{ij}} \\end{aligned} $$\nå¯¹äº $N = 10$ ä¸ªç¥ç»å…ƒæ¥è¯´ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†å¯ä»¥ç²¾ç¡®å®Œæˆï¼Œå› ä¸ºé…åˆ†å‡½æ•°åªæ˜¯ $\\Omega\\sim 1000$ é¡¹çš„æ€»å’Œã€‚ä¸€æ—¦æˆ‘ä»¬å®Œæˆï¼Œæ¨¡å‹å°±å®Œå…¨æŒ‡å®šäº†ã€‚æˆ‘ä»¬è®¡ç®—çš„ä»»ä½•ä¸œè¥¿éƒ½æ˜¯ä¸€ä¸ªé¢„æµ‹ï¼Œæ²¡æœ‰è°ƒæ•´å‚æ•°ä»¥å¯»æ±‚ä¸æ•°æ®æ›´å¥½ä¸€è‡´çš„ä½™åœ°ã€‚\nAs noted above, with $N = 10$ neurons the experiments are long enough to get a reasonably full sampling of the probability distribution over $\\vec{\\sigma}$. This provides the most detailed possible test of the model $P_{2}$, and in Fig 7F we see that the agreement between theory and experiment is excellent, except for very rare patterns where errors in the estimate of the probability are larger. Similar results are obtained for other groups of $N = 10$ cells drawn out of the full population of $N = 40$. Quantitatively we can measure the Jensenâ€“Shannon divergence between the estimated distribution $P_{\\text{data}}(\\sigma)$ and the model $P_{2}(\\sigma)$; across multiple choices of ten cells this fluctuates by a factor of two around $D_{JS} = 0.001$ bits, which means that it takes thousands of independent observations to distinguish the model from the data.\næ­£å¦‚ä¸Šé¢æ‰€æŒ‡å‡ºçš„ï¼Œå¯¹äº $N = 10$ ä¸ªç¥ç»å…ƒï¼Œå®éªŒæ—¶é—´è¶³å¤Ÿé•¿ï¼Œå¯ä»¥å¯¹ $\\vec{\\sigma}$ ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œåˆç†å®Œæ•´çš„é‡‡æ ·ã€‚è¿™ä¸ºæ¨¡å‹ $P_{2}$ æä¾›äº†æœ€è¯¦ç»†çš„å¯èƒ½æµ‹è¯•ï¼Œåœ¨å›¾ 7F ä¸­æˆ‘ä»¬çœ‹åˆ°ç†è®ºä¸å®éªŒä¹‹é—´çš„ä¸€è‡´æ€§éå¸¸å¥½ï¼Œé™¤äº†éå¸¸ç½•è§çš„æ¨¡å¼ï¼Œå…¶ä¸­æ¦‚ç‡ä¼°è®¡çš„è¯¯å·®è¾ƒå¤§ã€‚ä»å®Œæ•´çš„ $N = 40$ ç¾¤ä½“ä¸­æŠ½å–çš„å…¶ä»– $N = 10$ ä¸ªç»†èƒç»„ä¹Ÿå¾—åˆ°äº†ç±»ä¼¼çš„ç»“æœã€‚æˆ‘ä»¬å¯ä»¥å®šé‡åœ°æµ‹é‡ä¼°è®¡åˆ†å¸ƒ $P_{\\text{data}}(\\sigma)$ ä¸æ¨¡å‹ $P_{2}(\\sigma)$ ä¹‹é—´çš„ Jensenâ€“Shannon æ•£åº¦ï¼›åœ¨å¤šä¸ªåä¸ªç»†èƒçš„é€‰æ‹©ä¸­ï¼Œè¿™ä¸ªå€¼å›´ç»• $D_{JS} = 0.001$ æ¯”ç‰¹æ³¢åŠ¨äº†ä¸¤å€ï¼Œè¿™æ„å‘³ç€éœ€è¦æ•°åƒæ¬¡ç‹¬ç«‹è§‚å¯Ÿæ‰èƒ½åŒºåˆ†æ¨¡å‹ä¸æ•°æ®ã€‚\nThe architecture of the retina is such that many individual output neurons can be driven or inhibited by a single common neuron that is internal to the circuitry. This is one of many reasons that one might expect significant combinatorial regulation in the patterns of activity, and there were serious efforts to search for these effects (Schnitzer and Meister, 2003). The success of a pairwise model thus came as a considerable surprise.\nè§†ç½‘è†œçš„ç»“æ„ä½¿å¾—è®¸å¤šä¸ªä½“è¾“å‡ºç¥ç»å…ƒå¯ä»¥è¢«ç”µè·¯å†…éƒ¨çš„å•ä¸ªå…±åŒç¥ç»å…ƒé©±åŠ¨æˆ–æŠ‘åˆ¶ã€‚è¿™æ˜¯è®¸å¤šåŸå› ä¹‹ä¸€ï¼Œäººä»¬å¯èƒ½ä¼šæœŸæœ›åœ¨æ´»åŠ¨æ¨¡å¼ä¸­å­˜åœ¨æ˜¾è‘—çš„ç»„åˆè°ƒèŠ‚ï¼Œå¹¶ä¸”æ›¾ç»æœ‰è®¤çœŸåŠªåŠ›å»å¯»æ‰¾è¿™äº›æ•ˆåº”ï¼ˆSchnitzer å’Œ Meisterï¼Œ2003ï¼‰ã€‚å› æ­¤ï¼Œæˆå¯¹æ¨¡å‹çš„æˆåŠŸä»¤äººç›¸å½“æƒŠè®¶ã€‚\nThe results in the salamander retina, with natural inputs, were quickly confirmed in the primate retina using simpler inputs (Shlens et al., 2006). Those experiments covered a larger area and thus could focus on subâ€“populations of neurons belonging to a single class, which are arrayed in a relatively regular lattice. In this case not only did the pairwise model work very well, but the effective interactions $J_{ij}$ were confined largely to nearest neighbors on this lattice.\nè¾èˆè§†ç½‘è†œä¸­ä½¿ç”¨è‡ªç„¶è¾“å…¥çš„ç»“æœå¾ˆå¿«åœ¨çµé•¿ç±»åŠ¨ç‰©è§†ç½‘è†œä¸­å¾—åˆ°äº†ç¡®è®¤ï¼Œä½¿ç”¨äº†æ›´ç®€å•çš„è¾“å…¥ï¼ˆShlens ç­‰äººï¼Œ2006ï¼‰ã€‚è¿™äº›å®éªŒè¦†ç›–äº†æ›´å¤§çš„åŒºåŸŸï¼Œå› æ­¤å¯ä»¥ä¸“æ³¨äºå±äºå•ä¸€ç±»åˆ«çš„ç¥ç»å…ƒäºšç¾¤ï¼Œè¿™äº›ç¥ç»å…ƒæ’åˆ—åœ¨ä¸€ä¸ªç›¸å¯¹è§„åˆ™çš„æ™¶æ ¼ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸ä»…æˆå¯¹æ¨¡å‹æ•ˆæœéå¸¸å¥½ï¼Œè€Œä¸”æœ‰æ•ˆç›¸äº’ä½œç”¨ $J_{ij}$ ä¸»è¦å±€é™äºè¯¥æ™¶æ ¼ä¸Šçš„æœ€è¿‘é‚»ã€‚\nPairwise maximum entropy models also were reasonably successful in describing patterns of activity across $N\\leq 10$ neurons sampled from a cluster of cortical neurons kept alive in a dish (Tang et al., 2008). This work also pointed to the fact that dynamics did not correspond to Brownian motion on the energy surface.\næˆå¯¹æœ€å¤§ç†µæ¨¡å‹åœ¨æè¿°ä»åŸ¹å…»çš¿ä¸­ä¿æŒæ´»åŠ›çš„ä¸€ç°‡çš®å±‚ç¥ç»å…ƒä¸­é‡‡æ ·çš„ $N\\leq 10$ ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨æ¨¡å¼æ–¹é¢ä¹Ÿç›¸å½“æˆåŠŸï¼ˆTang ç­‰äººï¼Œ2008ï¼‰ã€‚è¿™é¡¹å·¥ä½œè¿˜æŒ‡å‡ºï¼ŒåŠ¨åŠ›å­¦å¹¶ä¸å¯¹åº”äºèƒ½é‡è¡¨é¢ä¸Šçš„å¸ƒæœ—è¿åŠ¨ã€‚\nThese early successes with small numbers of neurons raised many questions. For example, the interaction matrix $J_{ij}$ contained a mix of positive and negative terms, suggesting that frustration could lead to many local minima of the energy function or equivalently local maxima of the probability $P(\\vec{\\sigma})$, as in the Hopfield model (Â§II.C); could these â€œattractorsâ€ have a function in representing the visual world? Relatedly, an important consequence of the collective behavior in the Ising model is that if we know that state of all neurons in the network but one, then we have a parameterâ€“free prediction for the probability that this last neuron will be active; does this allow for error correction? To address these and other issues one must go beyond $N\\sim 10$ cells, which was already possible experimentally. But at larger $N$ one needs more powerful methods for solving the inverse problem that is at the heart of the maximum entropy construction, as described in Appendix B.\nè¿™äº›æ—©æœŸåœ¨å°‘é‡ç¥ç»å…ƒä¸Šçš„æˆåŠŸå¼•å‘äº†è®¸å¤šé—®é¢˜ã€‚ä¾‹å¦‚ï¼Œäº¤äº’çŸ©é˜µ $J_{ij}$ åŒ…å«æ­£è´Ÿæ··åˆé¡¹ï¼Œè¡¨æ˜é˜»æŒ«å¯èƒ½å¯¼è‡´èƒ½é‡å‡½æ•°çš„è®¸å¤šå±€éƒ¨æå°å€¼ï¼Œæˆ–è€…ç­‰ä»·åœ°ï¼Œæ¦‚ç‡ $P(\\vec{\\sigma})$ çš„å±€éƒ¨æå¤§å€¼ï¼Œå°±åƒ Hopfield æ¨¡å‹ï¼ˆÂ§II.Cï¼‰ä¸­ä¸€æ ·ï¼›è¿™äº›â€œå¸å¼•å­â€åœ¨è¡¨ç¤ºè§†è§‰ä¸–ç•Œæ–¹é¢æ˜¯å¦å…·æœ‰åŠŸèƒ½ï¼Ÿç›¸å…³åœ°ï¼ŒIsing æ¨¡å‹ä¸­é›†ä½“è¡Œä¸ºçš„ä¸€ä¸ªé‡è¦åæœæ˜¯ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“ç½‘ç»œä¸­é™¤ä¸€ä¸ªç¥ç»å…ƒå¤–æ‰€æœ‰ç¥ç»å…ƒçš„çŠ¶æ€ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥æ— å‚æ•°åœ°é¢„æµ‹è¿™ä¸ªæœ€åä¸€ä¸ªç¥ç»å…ƒæ˜¯å¦ä¼šæ´»è·ƒï¼›è¿™æ˜¯å¦å…è®¸çº é”™ï¼Ÿä¸ºäº†å¤„ç†è¿™äº›å’Œå…¶ä»–é—®é¢˜ï¼Œå¿…é¡»è¶…è¶Š $N\\sim 10$ ä¸ªç»†èƒï¼Œè¿™åœ¨å®éªŒä¸Šå·²ç»æ˜¯å¯èƒ½çš„ã€‚ä½†åœ¨æ›´å¤§çš„ $N$ ä¸‹ï¼Œéœ€è¦æ›´å¼ºå¤§çš„æ–¹æ³•æ¥è§£å†³æœ€å¤§ç†µæ„é€ æ ¸å¿ƒçš„é€†é—®é¢˜ï¼Œå¦‚é™„å½• B æ‰€è¿°ã€‚\nThe equivalence to equilibrium models entices us to describe the couplings $J_{ij}$ as â€œinteractions,â€ but there is no reason to think that these correspond to genuine connections between cells. In particular, $J_{ij}$ is symmetric because it is an effective interaction driving the equaltime correlations of activity in cells $i$ and $j$, and these correlations are symmetric by definition. If we go beyond single time slices to describe trajectories of activity over time, then with multiple cells the effective interactions can become asymmetric and break timereversal invariance.\nä¸å¹³è¡¡æ¨¡å‹çš„ç­‰ä»·æ€§è¯±ä½¿æˆ‘ä»¬å°†è€¦åˆ $J_{ij}$ æè¿°ä¸ºâ€œç›¸äº’ä½œç”¨â€ï¼Œä½†æ²¡æœ‰ç†ç”±è®¤ä¸ºå®ƒä»¬å¯¹åº”äºç»†èƒä¹‹é—´çš„çœŸæ­£è¿æ¥ã€‚ç‰¹åˆ«æ˜¯ï¼Œ$J_{ij}$ æ˜¯å¯¹ç§°çš„ï¼Œå› ä¸ºå®ƒæ˜¯é©±åŠ¨ç»†èƒ $i$ å’Œ $j$ æ´»åŠ¨çš„åŒæ—¶ç›¸å…³æ€§çš„æœ‰æ•ˆç›¸äº’ä½œç”¨ï¼Œè€Œè¿™äº›ç›¸å…³æ€§æŒ‰å®šä¹‰æ˜¯å¯¹ç§°çš„ã€‚å¦‚æœæˆ‘ä»¬è¶…è¶Šå•ä¸ªæ—¶é—´ç‰‡æ¥æè¿°éšæ—¶é—´å˜åŒ–çš„æ´»åŠ¨è½¨è¿¹ï¼Œé‚£ä¹ˆå¯¹äºå¤šä¸ªç»†èƒï¼Œæœ‰æ•ˆç›¸äº’ä½œç”¨å¯ä»¥å˜å¾—ä¸å¯¹ç§°å¹¶æ‰“ç ´æ—¶é—´åæ¼”ä¸å˜æ€§ã€‚\nBefore leaving the early work, it is useful to step back and ask about the goals and hopes from that time. As reviewed above, the use of statistical physics models for neural networks has a deep history. Saying that the brain is described by an Ising model captured both the optimism and (one must admit) the naÄ± Ìˆvet Ìe of the physics community in approaching the phenomena of life. One could balance optimism and naÄ± Ìˆvet Ìe by retreating to the position that these models are metaphors, illustrating what could happen rather than being theories of what actually happens. The success of maximum entropy models in the retina gave an example of how statistical physics ideas could provide a quantitative theory for networks of real neurons.\nåœ¨ç¦»å¼€æ—©æœŸå·¥ä½œä¹‹å‰ï¼Œé€€ä¸€æ­¥é—®ä¸€ä¸‹å½“æ—¶çš„ç›®æ ‡å’Œå¸Œæœ›æ˜¯æœ‰ç”¨çš„ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œç¥ç»ç½‘ç»œä½¿ç”¨ç»Ÿè®¡ç‰©ç†æ¨¡å‹æœ‰ç€æ·±åšçš„å†å²ã€‚è¯´å¤§è„‘ç”± Ising æ¨¡å‹æè¿°æ—¢æ•æ‰äº†ä¹è§‚ä¸»ä¹‰ï¼Œä¹Ÿæ•æ‰äº†ï¼ˆå¿…é¡»æ‰¿è®¤ï¼‰ç‰©ç†å­¦ç•Œåœ¨æ¥è¿‘ç”Ÿå‘½ç°è±¡æ—¶çš„å¤©çœŸã€‚é€šè¿‡é€€å›åˆ°è¿™äº›æ¨¡å‹æ˜¯éšå–»çš„ä½ç½®ï¼Œå¯ä»¥å¹³è¡¡ä¹è§‚ä¸»ä¹‰å’Œå¤©çœŸï¼Œè¯´æ˜å¯èƒ½ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œè€Œä¸æ˜¯å®é™…å‘ç”Ÿçš„ç†è®ºã€‚è§†ç½‘è†œä¸­æœ€å¤§ç†µæ¨¡å‹çš„æˆåŠŸæä¾›äº†ä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ç»Ÿè®¡ç‰©ç†æ€æƒ³å¦‚ä½•ä¸ºçœŸå®ç¥ç»å…ƒç½‘ç»œæä¾›å®šé‡ç†è®ºã€‚\nLarger networks of neurons The use of maximum entropy for networks of real neurons quickly triggered almost all possible reactions: (a) It should never work, because systems are not in equilibrium, have combinational interactions, $\\cdots$ . (b) It could work, but only under uninteresting conditions. (c) It should always work, since these models are very expressive. (d) It works at small $N$ , but this is a poor guide to what will happen at large $N$ . (e) Sure, but why not use [favorite alternative], for which we have efficient algorithms?\nå¯¹äºçœŸå®ç¥ç»å…ƒç½‘ç»œä½¿ç”¨æœ€å¤§ç†µè¿…é€Ÿå¼•å‘äº†å‡ ä¹æ‰€æœ‰å¯èƒ½çš„ååº”ï¼šï¼ˆaï¼‰å®ƒæ°¸è¿œä¸ä¼šèµ·ä½œç”¨ï¼Œå› ä¸ºç³»ç»Ÿä¸å¤„äºå¹³è¡¡çŠ¶æ€ï¼Œå…·æœ‰ç»„åˆç›¸äº’ä½œç”¨ï¼Œ$\\cdots$ï¼ˆbï¼‰å®ƒå¯èƒ½èµ·ä½œç”¨ï¼Œä½†ä»…åœ¨æ— è¶£çš„æ¡ä»¶ä¸‹ã€‚ï¼ˆcï¼‰å®ƒåº”è¯¥æ€»æ˜¯æœ‰æ•ˆï¼Œå› ä¸ºè¿™äº›æ¨¡å‹éå¸¸æœ‰è¡¨ç°åŠ›ã€‚ï¼ˆdï¼‰å®ƒåœ¨å° $N$ ä¸‹æœ‰æ•ˆï¼Œä½†è¿™å¯¹å¤§ $N$ ä¼šå‘ç”Ÿä»€ä¹ˆæ²¡æœ‰å¾ˆå¥½çš„æŒ‡å¯¼æ„ä¹‰ã€‚ï¼ˆeï¼‰å½“ç„¶ï¼Œä½†ä¸ºä»€ä¹ˆä¸ä½¿ç”¨[æœ€å–œæ¬¢çš„æ›¿ä»£æ–¹æ¡ˆ]ï¼Œæˆ‘ä»¬æœ‰é«˜æ•ˆçš„ç®—æ³•ï¼Ÿ\nPerhaps the most concrete response to these issues is just to see what happens as we move to more examples, especially in larger networks. But we should do this with several questions in mind, some of which were very explicit in the early literature (Macke et al., 2011a; Roudi et al., 2009). First, finding the maximum entropy model that matches the desired constraintsâ€”that is, solving Eqs (17)â€”becomes more difficult at larger $N$ . Can we be sure that we are testing the maximum entropy idea, and our choice of constraints, rather than the efficacy of our algorithms for solving this problem?\nä¹Ÿè®¸å¯¹è¿™äº›é—®é¢˜æœ€å…·ä½“çš„å›åº”å°±æ˜¯çœ‹çœ‹å½“æˆ‘ä»¬è½¬å‘æ›´å¤šä¾‹å­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´å¤§ç½‘ç»œä¸­ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ä½†æˆ‘ä»¬åº”è¯¥ç‰¢è®°å‡ ä¸ªé—®é¢˜ï¼Œå…¶ä¸­ä¸€äº›åœ¨æ—©æœŸæ–‡çŒ®ä¸­éå¸¸æ˜ç¡®ï¼ˆMacke ç­‰äººï¼Œ2011aï¼›Roudi ç­‰äººï¼Œ2009ï¼‰ã€‚é¦–å…ˆï¼Œæ‰¾åˆ°åŒ¹é…æ‰€éœ€çº¦æŸçš„æœ€å¤§ç†µæ¨¡å‹â€”â€”å³è§£å†³æ–¹ç¨‹ï¼ˆ17ï¼‰â€”â€”åœ¨æ›´å¤§çš„ $N$ ä¸‹å˜å¾—æ›´åŠ å›°éš¾ã€‚æˆ‘ä»¬èƒ½å¦ç¡®å®šæˆ‘ä»¬æ­£åœ¨æµ‹è¯•æœ€å¤§ç†µçš„æƒ³æ³•ï¼Œä»¥åŠæˆ‘ä»¬é€‰æ‹©çš„çº¦æŸï¼Œè€Œä¸æ˜¯æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜çš„ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Ÿ\nSecond, as N increases the maximum entropy construction becomes very data hungry. This concern often is phrased as the usual problem of â€œoverâ€“fitting,â€ when the number of parameters in our model is too large to fully constrained by the data. But in the maximum entropy formulation the problem is even more fundamental. The maximum entropy construction builds the least structured model consistent with a set of known expectation values. With a finite amount of data, if our list of expectation values is too long then the claim that we â€œknowâ€ these features of the system just isnâ€™t true, and this problem arises even before we try to build the maximum entropy model.\nå…¶æ¬¡ï¼Œéšç€ N çš„å¢åŠ ï¼Œæœ€å¤§ç†µæ„é€ å˜å¾—éå¸¸éœ€è¦æ•°æ®ã€‚è¿™ä¸ªé—®é¢˜é€šå¸¸è¢«è¡¨è¿°ä¸ºâ€œè¿‡æ‹Ÿåˆâ€çš„å¸¸è§é—®é¢˜ï¼Œå½“æˆ‘ä»¬æ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡å¤ªå¤§è€Œæ— æ³•è¢«æ•°æ®å®Œå…¨çº¦æŸæ—¶ã€‚ä½†åœ¨æœ€å¤§ç†µå…¬å¼ä¸­ï¼Œè¿™ä¸ªé—®é¢˜ç”šè‡³æ›´ä¸ºæ ¹æœ¬ã€‚æœ€å¤§ç†µæ„é€ å»ºç«‹äº†ä¸€ä¸ªä¸ä¸€ç»„å·²çŸ¥æœŸæœ›å€¼ä¸€è‡´çš„æœ€å°‘ç»“æ„åŒ–æ¨¡å‹ã€‚å¯¹äºæœ‰é™çš„æ•°æ®é‡ï¼Œå¦‚æœæˆ‘ä»¬çš„æœŸæœ›å€¼åˆ—è¡¨å¤ªé•¿ï¼Œé‚£ä¹ˆæˆ‘ä»¬â€œçŸ¥é“â€ç³»ç»Ÿçš„è¿™äº›ç‰¹å¾çš„è¯´æ³•å°±ä¸æ˜¯çœŸçš„ï¼Œå³ä½¿åœ¨æˆ‘ä»¬å°è¯•æ„å»ºæœ€å¤§ç†µæ¨¡å‹ä¹‹å‰ï¼Œè¿™ä¸ªé—®é¢˜ä¹Ÿä¼šå‡ºç°ã€‚\nThird, because correlations are spread widely in these networks, if one develops a perturbation theory around the limit of independent neurons then factors of $N$ appear in the series, e.g. for the entropy per neuron. Success at modest $N$ might thus mean that we are in a perturbative regime, which would be much less interesting. The question of whether success is perturbative is subtle, since at finite $N$ all properties of the maximum entropy model are analytic functions of the correlations, and hence if we carry perturbation theory far enough we will get the right answer (Sessak and Monasson, 2009).\nç¬¬ä¸‰ï¼Œå› ä¸ºç›¸å…³æ€§åœ¨è¿™äº›ç½‘ç»œä¸­å¹¿æ³›ä¼ æ’­ï¼Œå¦‚æœæˆ‘ä»¬å›´ç»•ç‹¬ç«‹ç¥ç»å…ƒçš„æé™å‘å±•å¾®æ‰°ç†è®ºï¼Œé‚£ä¹ˆ $N$ çš„å› ç´ ä¼šå‡ºç°åœ¨çº§æ•°ä¸­ï¼Œä¾‹å¦‚æ¯ä¸ªç¥ç»å…ƒçš„ç†µã€‚å› æ­¤ï¼Œåœ¨é€‚åº¦çš„ $N$ ä¸‹çš„æˆåŠŸå¯èƒ½æ„å‘³ç€æˆ‘ä»¬å¤„äºå¾®æ‰°èŒƒå›´å†…ï¼Œè¿™å°†ä¸é‚£ä¹ˆæœ‰è¶£ã€‚æˆåŠŸæ˜¯å¦æ˜¯å¾®æ‰°çš„é—®é¢˜æ˜¯å¾®å¦™çš„ï¼Œå› ä¸ºåœ¨æœ‰é™çš„ $N$ ä¸‹ï¼Œæœ€å¤§ç†µæ¨¡å‹çš„æ‰€æœ‰å±æ€§éƒ½æ˜¯ç›¸å…³æ€§çš„è§£æå‡½æ•°ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å°†å¾®æ‰°ç†è®ºè¿›è¡Œå¾—è¶³å¤Ÿè¿œï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ­£ç¡®çš„ç­”æ¡ˆï¼ˆSessak å’Œ Monassonï¼Œ2009ï¼‰ã€‚\nFinally, in statistical mechanics we are used to the idea of a large $N$, thermodynamic limit. Although this carries over to model networks (Amit, 1989), it is not obvious how to use this idea in thinking about networks of real neurons. Naive extrapolation of results from maximum entropy models of $N = 10 âˆ’ 20$ neurons in the retina indicated that something special had to happen by $N\\sim 200$, or else the entropy would vanish; this was interesting because $N\\sim 200$ is the number cells that are â€œlookingâ€ at overlapping regions of the visual world (Schneidman et al., 2006). A more sophisticated extrapolation imagines a large population of neurons in which mean activities and pairwise correlations are drawn at random from the same distribution as found in recordings from smaller numbers of neurons (TkaË‡cik et al., 2006, 2009). This sort of extrapolation is motivated in part by the observation that â€œthermodynamicâ€ properties of the maximum entropy models learned for $N = 20$ or $N = 40$ retinal neurons match the behavior of such random models at the same $N$. If we now extrapolate to $N = 120$ there are striking collective behaviors, and we will ask if these are seen in real data from $N \u003e 100$ cells.\næœ€åï¼Œåœ¨ç»Ÿè®¡åŠ›å­¦ä¸­ï¼Œæˆ‘ä»¬ä¹ æƒ¯äºå¤§ $N$ã€çƒ­åŠ›å­¦æé™çš„æ¦‚å¿µã€‚å°½ç®¡è¿™å¯ä»¥è½¬ç§»åˆ°æ¨¡å‹ç½‘ç»œä¸­ï¼ˆAmitï¼Œ1989ï¼‰ï¼Œä½†åœ¨æ€è€ƒçœŸå®ç¥ç»å…ƒç½‘ç»œæ—¶å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæƒ³æ³•å¹¶ä¸æ˜æ˜¾ã€‚ä»è§†ç½‘è†œä¸­ $N = 10 âˆ’ 20$ ä¸ªç¥ç»å…ƒçš„æœ€å¤§ç†µæ¨¡å‹ç»“æœçš„å¤©çœŸå¤–æ¨è¡¨æ˜ï¼Œåˆ° $N\\sim 200$ æ—¶å¿…é¡»å‘ç”Ÿä¸€äº›ç‰¹æ®Šçš„äº‹æƒ…ï¼Œå¦åˆ™ç†µå°†æ¶ˆå¤±ï¼›è¿™æ˜¯æœ‰è¶£çš„ï¼Œå› ä¸º $N\\sim 200$ æ˜¯â€œè§‚å¯Ÿâ€è§†è§‰ä¸–ç•Œé‡å åŒºåŸŸçš„ç»†èƒæ•°é‡ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚æ›´å¤æ‚çš„å¤–æ¨è®¾æƒ³äº†ä¸€ä¸ªå¤§å‹ç¥ç»å…ƒç¾¤ä½“ï¼Œå…¶ä¸­å¹³å‡æ´»åŠ¨å’Œæˆå¯¹ç›¸å…³æ€§æ˜¯ä»ä¸è¾ƒå°‘ç¥ç»å…ƒè®°å½•ä¸­å‘ç°çš„ç›¸åŒåˆ†å¸ƒä¸­éšæœºæŠ½å–çš„ï¼ˆTkaË‡cik ç­‰äººï¼Œ2006ï¼Œ2009ï¼‰ã€‚è¿™ç§å¤–æ¨éƒ¨åˆ†æ˜¯ç”±è§‚å¯Ÿåˆ°ä¸º $N = 20$ æˆ– $N = 40$ è§†ç½‘è†œç¥ç»å…ƒå­¦ä¹ çš„æœ€å¤§ç†µæ¨¡å‹çš„â€œçƒ­åŠ›å­¦â€å±æ€§ä¸åŒä¸€ $N$ ä¸‹æ­¤ç±»éšæœºæ¨¡å‹çš„è¡Œä¸ºç›¸åŒ¹é…æ‰€æ¿€å‘çš„ã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨å¤–æ¨åˆ° $N = 120$ï¼Œä¼šå‡ºç°æ˜¾è‘—çš„é›†ä½“è¡Œä¸ºï¼Œæˆ‘ä»¬å°†è¯¢é—®åœ¨æ¥è‡ª $N \u003e 100$ ä¸ªç»†èƒçš„çœŸå®æ•°æ®ä¸­æ˜¯å¦çœ‹åˆ°äº†è¿™äº›è¡Œä¸ºã€‚\nEarly experiments in the retina already were monitoring $N = 40$ cells, and the development of numerical methods described in Appendix B quickly allowed analysis of these larger data sets (TkaË‡cik et al., 2006, 2009). With $N = 40$ cells one cannot check the predictions for probabilities of individual patterns $P(\\vec{\\sigma})$, but one can check the probability that $K$ out of $N$ cells are active in the same small time bin, as in Fig. 7E, or the correlations among triplets of neurons. At $N = 40$ we see the first hints that constraining pairwise correlations is not quite enough to capture the full structure of the network. There are disagreements between theory and experiment in the tails of the distribution $P_{N}(K)$, and more importantly a few percent disagreement at $K = 0$. This may not seem like much, but since the network is completely silent in roughly half of the $\\Delta\\tau = 20$ ms time bins, the data determine $P_{N}(K = 0)$ very precisely, and a one percent discrepancy is hugely significant.\nè§†ç½‘è†œä¸­çš„æ—©æœŸå®éªŒå·²ç»ç›‘æµ‹äº† $N = 40$ ä¸ªç»†èƒï¼Œå¹¶ä¸”é™„å½• B ä¸­æè¿°çš„æ•°å€¼æ–¹æ³•çš„å‘å±•å¾ˆå¿«å…è®¸åˆ†æè¿™äº›æ›´å¤§çš„æ•°æ®é›†ï¼ˆTkaË‡cik ç­‰äººï¼Œ2006ï¼Œ2009ï¼‰ã€‚å¯¹äº $N = 40$ ä¸ªç»†èƒï¼Œæˆ‘ä»¬æ— æ³•æ£€æŸ¥å•ä¸ªæ¨¡å¼ $P(\\vec{\\sigma})$ æ¦‚ç‡çš„é¢„æµ‹ï¼Œä½†æˆ‘ä»¬å¯ä»¥æ£€æŸ¥åœ¨åŒä¸€å°æ—¶é—´æ®µå†… $N$ ä¸ªç»†èƒä¸­æœ‰ $K$ ä¸ªæ´»è·ƒçš„æ¦‚ç‡ï¼Œå¦‚å›¾ 7E æ‰€ç¤ºï¼Œæˆ–ç¥ç»å…ƒä¸‰å…ƒç»„ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åœ¨ $N = 40$ æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›è¿¹è±¡è¡¨æ˜ï¼Œä»…çº¦æŸæˆå¯¹ç›¸å…³æ€§è¿˜ä¸è¶³ä»¥æ•æ‰ç½‘ç»œçš„å®Œæ•´ç»“æ„ã€‚åˆ†å¸ƒ $P_{N}(K)$ çš„å°¾éƒ¨å­˜åœ¨ç†è®ºä¸å®éªŒä¹‹é—´çš„ä¸ä¸€è‡´ï¼Œæ›´é‡è¦çš„æ˜¯åœ¨ $K = 0$ å¤„å­˜åœ¨å‡ ä¸ªç™¾åˆ†ç‚¹çš„ä¸ä¸€è‡´ã€‚è¿™ä¼¼ä¹ä¸å¤šï¼Œä½†ç”±äºç½‘ç»œåœ¨å¤§çº¦ä¸€åŠçš„ $\\Delta\\tau = 20$ æ¯«ç§’æ—¶é—´æ®µå†…å®Œå…¨é™é»˜ï¼Œæ•°æ®éå¸¸ç²¾ç¡®åœ°ç¡®å®šäº† $P_{N}(K = 0)$ï¼Œè€Œä¸”ç™¾åˆ†ä¹‹ä¸€çš„å·®å¼‚æ˜¯éå¸¸æ˜¾è‘—çš„ã€‚\nA new generation of electrode arrays made it possible to record $N = 100 âˆ’ 200$ cells, densely sampling a small patch of the retina (Â§III.A). As an example, these experiments could capture the signals from $N_{\\text{max}} = 160$ ganglion cells in a $(450 \\mu\\text{m})^2$ area of the salamander retina that contains a total of $N\\sim 200$ cells, and these recordings are stable for $\\sim 1.5$ hr.\næ–°ä¸€ä»£ç”µæé˜µåˆ—ä½¿å¾—è®°å½• $N = 100 âˆ’ 200$ ä¸ªç»†èƒæˆä¸ºå¯èƒ½ï¼Œå¯†é›†é‡‡æ ·è§†ç½‘è†œçš„å°å—ï¼ˆÂ§III.Aï¼‰ã€‚ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œè¿™äº›å®éªŒå¯ä»¥æ•æ‰åˆ°æ¥è‡ªè¾èˆè§†ç½‘è†œä¸­ $(450 \\mu\\text{m})^2$ åŒºåŸŸçš„ $N_{\\text{max}} = 160$ ä¸ªç¥ç»èŠ‚ç»†èƒçš„ä¿¡å·ï¼Œè¯¥åŒºåŸŸæ€»å…±æœ‰ $N\\sim 200$ ä¸ªç»†èƒï¼Œå¹¶ä¸”è¿™äº›è®°å½•åœ¨ $\\sim 1.5$ å°æ—¶å†…æ˜¯ç¨³å®šçš„ã€‚\nAs explained in Appendix B, we can build maximum entropy models at larger $N$ by using Monte Carlo simulation to estimate expectation values in the model, comparing with the measured expectation values, and then adjusting the coupling constants to improve the agreement. Necessarily this doesnâ€™t yield an exact solution to the constraint Eqs (17), but this seems acceptable since we are trying to match expectation values that are estimated from experiment and these have errors. Figure 8A shows that with $N = 100$ we can match the observed pairwise correlations within experimental error (TkaË‡cik et al., 2014). More precisely the errors in predicting the elements of the covariance matrix $C_{ij}$ [Eq (23)] are nearly Gaussian, with a variance equal to the variance of the measurement errors. This suggests, strongly, that one can successfully fit, but not overâ€“fit, a maximum entropy model to these data.\nå¦‚é™„å½• B æ‰€è¿°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿæ¥ä¼°è®¡æ¨¡å‹ä¸­çš„æœŸæœ›å€¼ï¼Œå°†å…¶ä¸æµ‹é‡çš„æœŸæœ›å€¼è¿›è¡Œæ¯”è¾ƒï¼Œç„¶åè°ƒæ•´è€¦åˆå¸¸æ•°ä»¥æ”¹å–„ä¸€è‡´æ€§ï¼Œä»è€Œåœ¨æ›´å¤§çš„ $N$ ä¸‹æ„å»ºæœ€å¤§ç†µæ¨¡å‹ã€‚å¿…ç„¶åœ°ï¼Œè¿™ä¸ä¼šäº§ç”Ÿå¯¹çº¦æŸæ–¹ç¨‹ï¼ˆ17ï¼‰çš„ç²¾ç¡®è§£ï¼Œä½†è¿™ä¼¼ä¹æ˜¯å¯ä»¥æ¥å—çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨å°è¯•åŒ¹é…ä»å®éªŒä¸­ä¼°è®¡çš„æœŸæœ›å€¼ï¼Œè€Œè¿™äº›æœŸæœ›å€¼å­˜åœ¨è¯¯å·®ã€‚å›¾ 8A æ˜¾ç¤ºï¼Œåœ¨ $N = 100$ æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å®éªŒè¯¯å·®èŒƒå›´å†…åŒ¹é…è§‚å¯Ÿåˆ°çš„æˆå¯¹ç›¸å…³æ€§ï¼ˆTkaË‡cik ç­‰äººï¼Œ2014ï¼‰ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œé¢„æµ‹åæ–¹å·®çŸ©é˜µ $C_{ij}$ [æ–¹ç¨‹ï¼ˆ23ï¼‰] å…ƒç´ çš„è¯¯å·®å‡ ä¹æ˜¯é«˜æ–¯åˆ†å¸ƒçš„ï¼Œå…¶æ–¹å·®ç­‰äºæµ‹é‡è¯¯å·®çš„æ–¹å·®ã€‚è¿™å¼ºçƒˆè¡¨æ˜ï¼Œå¯ä»¥æˆåŠŸåœ°æ‹Ÿåˆï¼Œä½†ä¸ä¼šè¿‡æ‹Ÿåˆè¿™äº›æ•°æ®çš„æœ€å¤§ç†µæ¨¡å‹ã€‚\nFIG. 8 Fitting, but not over-fitting, with $N\\sim 100$ neurons (TkaÃ©ik et al., 2014). (A) Distribution of errors in the prediction of pairwise correlations, after adjusting the parameters $\\{h_{i}; J_{ij}\\}$, for $N = 100$. Prediction errors are in units of the measurement error $\\Delta C_{ij}$ for each element of the covariance matrix. Red line shows a Gaussian with zero mean and unit variance. (B) Logâ€”likelihood [Eq (38)] of test data not used in constructing the maximum entropy model, in units of the result for the training data. At $N = 10$ it is not surprising that these agree, since the number of parameters $\\{h_{i}; J_{ij}\\}$ is small. But we see this agreement persists at the $\\sim 1%$ level out to $N = 120$, showing that even models for relatively large networks are not overfit.\nå›¾ 8 ä½¿ç”¨ $N\\sim 100$ ä¸ªç¥ç»å…ƒè¿›è¡Œæ‹Ÿåˆï¼Œä½†æ²¡æœ‰è¿‡æ‹Ÿåˆï¼ˆTkaÃ©ik ç­‰äººï¼Œ2014ï¼‰ã€‚ï¼ˆAï¼‰åœ¨è°ƒæ•´å‚æ•° $\\{h_{i}; J_{ij}\\}$ åï¼Œå¯¹äº $N = 100$ï¼Œé¢„æµ‹æˆå¯¹ç›¸å…³æ€§çš„è¯¯å·®åˆ†å¸ƒã€‚é¢„æµ‹è¯¯å·®ä»¥åæ–¹å·®çŸ©é˜µæ¯ä¸ªå…ƒç´ çš„æµ‹é‡è¯¯å·® $\\Delta C_{ij}$ ä¸ºå•ä½ã€‚çº¢çº¿æ˜¾ç¤ºäº†å‡å€¼ä¸ºé›¶ã€æ–¹å·®ä¸ºä¸€çš„é«˜æ–¯åˆ†å¸ƒã€‚ï¼ˆBï¼‰æµ‹è¯•æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ [æ–¹ç¨‹ï¼ˆ38ï¼‰]ï¼Œä»¥è®­ç»ƒæ•°æ®ç»“æœä¸ºå•ä½ã€‚åœ¨ $N = 10$ æ—¶ï¼Œè¿™äº›ç»“æœä¸€è‡´å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œå› ä¸ºå‚æ•°æ•°é‡ $\\{h_{i}; J_{ij}\\}$ å¾ˆå°‘ã€‚ä½†æˆ‘ä»¬çœ‹åˆ°è¿™ç§ä¸€è‡´æ€§åœ¨ $N = 120$ æ—¶ä»ç„¶ä¿æŒåœ¨çº¦ $1%$ çš„æ°´å¹³ï¼Œè¡¨æ˜å³ä½¿æ˜¯ç›¸å¯¹è¾ƒå¤§ç½‘ç»œçš„æ¨¡å‹ä¹Ÿæ²¡æœ‰è¿‡æ‹Ÿåˆã€‚\nThe test for fitting vs overâ€“fitting in Fig 8A looks at each pair of cells individually, but part of the worry is that at large N we can have accurate estimates of individual elements Cij while underâ€“determining the global properties of the matrix. We can take a familiar empirical approach, measuring the means âŸ¨ÏƒiâŸ© and covariances âŸ¨Î´ÏƒiÎ´ÏƒjâŸ©c in 90% of the data, using these to infer the parameters {hi; Jij} in a maximum entropy model, and then testing the predictions of the model [Eqs (35, 33)] on the remaining 10%. The fundamental measure of model quality is the logâ€“likelihood of the data, which we can normalize per sample and per neuron\n$$ \\mathcal{L} = \\frac{1}{N}\\langle \\log{P(\\vec{\\sigma})}\\rangle_{\\text{expt}} $$\nFigure 8B shows that $\\mathcal{L}$ is the same, to better than one percent, whether we evaluate it over the training data or over the test data. This is true at $N = 10$, where surely there can be no question that we have enough samples, and it is true at $N = 120$.\næ‹Ÿåˆä¸è¿‡æ‹Ÿåˆçš„æµ‹è¯•å¦‚å›¾ 8A æ‰€ç¤ºï¼Œå•ç‹¬æŸ¥çœ‹æ¯å¯¹ç»†èƒï¼Œä½†éƒ¨åˆ†æ‹…å¿§æ˜¯ï¼Œåœ¨å¤§ N ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å‡†ç¡®ä¼°è®¡å•ä¸ªå…ƒç´  Cijï¼ŒåŒæ—¶æœªç¡®å®šçŸ©é˜µçš„å…¨å±€å±æ€§ã€‚æˆ‘ä»¬å¯ä»¥é‡‡ç”¨ç†Ÿæ‚‰çš„ç»éªŒæ–¹æ³•ï¼Œåœ¨ 90% çš„æ•°æ®ä¸­æµ‹é‡å‡å€¼ âŸ¨ÏƒiâŸ© å’Œåæ–¹å·® âŸ¨Î´ÏƒiÎ´ÏƒjâŸ©cï¼Œä½¿ç”¨è¿™äº›æ¥æ¨æ–­æœ€å¤§ç†µæ¨¡å‹ä¸­çš„å‚æ•° {hi; Jij}ï¼Œç„¶ååœ¨å‰©ä½™çš„ 10% ä¸Šæµ‹è¯•æ¨¡å‹çš„é¢„æµ‹ [æ–¹ç¨‹ï¼ˆ35ï¼Œ33ï¼‰]ã€‚æ¨¡å‹è´¨é‡çš„åŸºæœ¬åº¦é‡æ˜¯æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰æ ·æœ¬å’Œæ¯ä¸ªç¥ç»å…ƒè¿›è¡Œå½’ä¸€åŒ–\n$$ \\mathcal{L} = \\frac{1}{N}\\langle \\log{P(\\vec{\\sigma})}\\rangle_{\\text{expt}} $$\nå›¾ 8B æ˜¾ç¤ºï¼Œæ— è®ºæˆ‘ä»¬æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸Šè¿˜æ˜¯åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼° $\\mathcal{L}$ï¼Œå…¶å€¼éƒ½ç›¸åŒï¼Œè¯¯å·®å°äºç™¾åˆ†ä¹‹ä¸€ã€‚è¿™åœ¨ $N = 10$ æ—¶æ˜¯çœŸçš„ï¼Œåœ¨é‚£é‡Œè‚¯å®šæ²¡æœ‰é—®é¢˜ï¼Œæˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œå¹¶ä¸”åœ¨ $N = 120$ æ—¶ä¹Ÿæ˜¯çœŸçš„ã€‚\nDifferent networks of neurons, in different organisms and different regions of the brain, have different correlation structures. One should thus be wary of generalizations such as â€œan hour is enough data for one hundred neurons.â€ But at least in the context of experiments on the retina, there is no question that maximum entropy models can be learned reliably from the available data, and that there is no overâ€“fitting. Said another way, the models really are the solutions to the mathematical problem that we set out to solve (Â§IV.A): What is the minimal model consistent with a set of expectation values measured in experiment? These models do not carry signatures of the algorithm that we used to find them, nor are they systematically perturbed by the finiteness of the data on which they are based. This answers the first two questions formulated above.\nä¸åŒç”Ÿç‰©ä½“å’Œå¤§è„‘ä¸åŒåŒºåŸŸçš„ç¥ç»å…ƒç½‘ç»œå…·æœ‰ä¸åŒçš„ç›¸å…³ç»“æ„ã€‚å› æ­¤ï¼Œäººä»¬åº”è¯¥å¯¹â€œä¸€å°æ—¶è¶³å¤Ÿä¸€ç™¾ä¸ªç¥ç»å…ƒçš„æ•°æ®â€ä¹‹ç±»çš„æ¦‚æ‹¬æŒè°¨æ…æ€åº¦ã€‚ä½†è‡³å°‘åœ¨è§†ç½‘è†œå®éªŒçš„èƒŒæ™¯ä¸‹ï¼Œæ¯«æ— ç–‘é—®ï¼Œæœ€å¤§ç†µæ¨¡å‹å¯ä»¥ä»å¯ç”¨æ•°æ®ä¸­å¯é åœ°å­¦ä¹ ï¼Œå¹¶ä¸”æ²¡æœ‰è¿‡æ‹Ÿåˆã€‚æ¢å¥è¯è¯´ï¼Œè¿™äº›æ¨¡å‹ç¡®å®æ˜¯æˆ‘ä»¬ç€æ‰‹è§£å†³çš„æ•°å­¦é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼ˆÂ§IV.Aï¼‰ï¼šä¸å®éªŒä¸­æµ‹é‡çš„ä¸€ç»„æœŸæœ›å€¼ä¸€è‡´çš„æœ€å°æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿè¿™äº›æ¨¡å‹ä¸æºå¸¦æˆ‘ä»¬ç”¨æ¥æ‰¾åˆ°å®ƒä»¬çš„ç®—æ³•çš„ç‰¹å¾ï¼Œä¹Ÿä¸ä¼šè¢«å®ƒä»¬æ‰€åŸºäºçš„æ•°æ®çš„æœ‰é™æ€§ç³»ç»Ÿåœ°æ‰°åŠ¨ã€‚è¿™å›ç­”äº†ä¸Šé¢æå‡ºçš„å‰ä¸¤ä¸ªé—®é¢˜ã€‚\nGiven that we can construct the maximum entropy models reliably, what do we learn? To begin, the small discrepancies in predicting the probability that $K$ out $N$ neurons are active simultaneously, $P_{N}(K)$, become larger as $N$ increases. The simplest solution to this problem is to add one more constraint, insisting that the maximum entropy model match the observed $P_{N}(K)$ exactly. This adds only $\\sim N$ constraints to a problem in which we already have $N(N + 1)/2$, so the resulting â€œ$K$â€“pairwiseâ€ models are not significantly more complex.\né‰´äºæˆ‘ä»¬å¯ä»¥å¯é åœ°æ„å»ºæœ€å¤§ç†µæ¨¡å‹ï¼Œæˆ‘ä»¬å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿé¦–å…ˆï¼Œé¢„æµ‹ $K$ ä¸ªç¥ç»å…ƒåŒæ—¶æ´»è·ƒçš„æ¦‚ç‡ $P_{N}(K)$ æ—¶çš„å°å·®å¼‚éšç€ $N$ çš„å¢åŠ è€Œå˜å¤§ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„æœ€ç®€å•æ–¹æ³•æ˜¯æ·»åŠ ä¸€ä¸ªé¢å¤–çš„çº¦æŸï¼ŒåšæŒè®©æœ€å¤§ç†µæ¨¡å‹å‡†ç¡®åŒ¹é…è§‚å¯Ÿåˆ°çš„ $P_{N}(K)$ã€‚è¿™åœ¨æˆ‘ä»¬å·²ç»æœ‰ $N(N + 1)/2$ ä¸ªçº¦æŸçš„é—®é¢˜ä¸­åªå¢åŠ äº† $\\sim N$ ä¸ªçº¦æŸï¼Œå› æ­¤å¾—åˆ°çš„â€œ$K$â€“æˆå¯¹â€æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾è‘—æ›´å¤æ‚ã€‚\nAgain, at the risk of being pedantic letâ€™s formulate matching of the observed $P_{N}(K)$ as constraining expectation values. If we introduce the Kronecker delta for integers $n$ and $m$,\n$$ \\begin{aligned} \\delta \u0026= 1\\quad n=m\\\\ \u0026= 0\\quad n\\neq m \\end{aligned} $$\nthen\n$$ P_{N}(K) = \\left\\langle \\delta\\left( K,\\sum_{i}^{N}\\sigma_{i} \\right)\\right\\rangle $$\nThus to match $P_{N}(K)$ we want to enlarge our set of observables to include\n$$ \\{f_{\\mu}^{(\\text{counts})}\\} \\rightarrow \\left\\{\\delta\\left(K,\\sum_{i}^{N}\\sigma_{i}\\right)\\right\\} $$\nAs before, each new constraint adds a term to the effective energy,\n$$ E(\\vec{\\sigma}) = \\sum_{\\mu}\\lambda_{\\mu}^{(\\text{counts})}f_{\\mu}^{(\\text{counts})} = \\sum_{K=0}^{N}\\lambda_{K}\\delta\\left(K, \\sum_{i}^{N}\\sigma_{i}\\right) $$\nIt is useful to think of this as an effective potential that acts on the summed activity,\n$$ \\sum_{K=0}^{N}\\lambda_{K}\\delta\\left(K,\\sum_{i}^{N}\\sigma_{i}\\right) = V\\left(\\sum_{i=1}^{N}\\sigma_{i}\\right) $$\nå†æ¬¡ï¼Œä¸ºäº†é¿å…è¿‡äºè¿‚è…ï¼Œè®©æˆ‘ä»¬å°†åŒ¹é…è§‚å¯Ÿåˆ°çš„ $P_{N}(K)$ è¡¨è¿°ä¸ºçº¦æŸæœŸæœ›å€¼ã€‚å¦‚æœæˆ‘ä»¬å¼•å…¥æ•´æ•° $n$ å’Œ $m$ çš„ Kronecker deltaï¼Œ\n$$ \\begin{aligned} \\delta \u0026= 1\\quad n=m\\\\ \u0026= 0\\quad n\\neq m \\end{aligned} $$\né‚£ä¹ˆ\n$$ P_{N}(K) = \\left\\langle \\delta\\left( K,\\sum_{i}^{N}\\sigma_{i} \\right)\\right\\rangle $$\nå› æ­¤ï¼Œä¸ºäº†åŒ¹é… $P_{N}(K)$ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æˆ‘ä»¬çš„å¯è§‚å¯Ÿé‡é›†åˆæ‰©å¤§åˆ°åŒ…æ‹¬\n$$ \\{f_{\\mu}^{(\\text{counts})}\\} \\rightarrow \\left\\{\\delta\\left(K,\\sum_{i}^{N}\\sigma_{i}\\right)\\right\\} $$\nå¦‚å‰æ‰€è¿°ï¼Œæ¯ä¸ªæ–°çº¦æŸéƒ½ä¼šå‘æœ‰æ•ˆèƒ½é‡æ·»åŠ ä¸€é¡¹ï¼Œ\n$$ E(\\vec{\\sigma}) = \\sum_{\\mu}\\lambda_{\\mu}^{(\\text{counts})}f_{\\mu}^{(\\text{counts})} = \\sum_{K=0}^{N}\\lambda_{K}\\delta\\left(K, \\sum_{i}^{N}\\sigma_{i}\\right) $$\nå°†å…¶è§†ä¸ºä½œç”¨äºæ€»æ´»åŠ¨çš„æœ‰æ•ˆåŠ¿æ˜¯æœ‰ç”¨çš„ï¼Œ\n$$ \\sum_{K=0}^{N}\\lambda_{K}\\delta\\left(K,\\sum_{i}^{N}\\sigma_{i}\\right) = V\\left(\\sum_{i=1}^{N}\\sigma_{i}\\right) $$\nPutting the pieces together, the maximum entropy model that matches the mean activity of individual neurons, the correlations between pairs of neurons, and the probability that $K$ out of $N$ are active simultaneously takes the form\n$$ \\begin{aligned} P_{2k}(\\vec{\\sigma}) \u0026= \\frac{1}{Z_{2k}}e^{-E_{2k}(\\vec{\\sigma})}\\\\ E_{2k}(\\vec{\\sigma}) \u0026= \\sum_{i=1}^{N}h_{i}\\sigma_{i} + \\frac{1}{2}\\sum_{i\\neq j}J_{ij}\\sigma_{i}\\sigma_{j} + V\\left(\\sum_{i=1}^{N}\\sigma_{i}\\right) \\end{aligned} $$\nWe refer to this as the â€œ$K$â€“pairwiseâ€ model (TkaË‡cik et al., 2014).\nå°†å„ä¸ªéƒ¨åˆ†ç»„åˆåœ¨ä¸€èµ·ï¼ŒåŒ¹é…å•ä¸ªç¥ç»å…ƒçš„å¹³å‡æ´»åŠ¨ã€ç¥ç»å…ƒå¯¹ä¹‹é—´çš„ç›¸å…³æ€§ä»¥åŠ $N$ ä¸ªä¸­æœ‰ $K$ ä¸ªåŒæ—¶æ´»è·ƒçš„æ¦‚ç‡çš„æœ€å¤§ç†µæ¨¡å‹å½¢å¼ä¸º\n$$ \\begin{aligned} P_{2k}(\\vec{\\sigma}) \u0026= \\frac{1}{Z_{2k}}e^{-E_{2k}(\\vec{\\sigma})}\\\\ E_{2k}(\\vec{\\sigma}) \u0026= \\sum_{i=1}^{N}h_{i}\\sigma_{i} + \\frac{1}{2}\\sum_{i\\neq j}J_{ij}\\sigma_{i}\\sigma_{j} + V\\left(\\sum_{i=1}^{N}\\sigma_{i}\\right) \\end{aligned} $$\nWe can test this model immediately by estimating the correlations among triplets of neurons,\n$$ C_{ijk} = \\langle (\\sigma_{i}-\\langle\\sigma_{i}\\rangle)(\\sigma_{j}-\\langle\\sigma_{j}\\rangle)(\\sigma_{k}-\\langle\\sigma_{k}\\rangle)\\rangle $$\nFigure 9 shows the results with averages computed in both the pairwise and Kâ€“pairwise models, plotted vs. the experimental values. The discrepancies are very small, although still roughly three times larger than the experimental errors in the estimates of the correlations themselves (TkacË‡ik et al., 2014); we will see that one can sometimes get even better agreement (Â§V). Note that the potential $V$ which we add to match the constraint on $P_{N}(K)$ does not carry any information about the identities of the individual neurons. It thus is interesting that including this term improves the prediction of all the triplet correlations, which do depend on neural identity.\næˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼°è®¡ç¥ç»å…ƒä¸‰å…ƒç»„ä¹‹é—´çš„ç›¸å…³æ€§æ¥ç«‹å³æµ‹è¯•è¿™ä¸ªæ¨¡å‹ï¼Œ\n$$ C_{ijk} = \\langle (\\sigma_{i}-\\langle\\sigma_{i}\\rangle)(\\sigma_{j}-\\langle\\sigma_{j}\\rangle)(\\sigma_{k}-\\langle\\sigma_{k}\\rangle)\\rangle $$\nå›¾ 9 æ˜¾ç¤ºäº†åœ¨æˆå¯¹å’Œ Kâ€“æˆå¯¹æ¨¡å‹ä¸­è®¡ç®—çš„å¹³å‡å€¼ä¸å®éªŒå€¼çš„ç»“æœã€‚å°½ç®¡ä¸ç›¸å…³æ€§æœ¬èº«ä¼°è®¡çš„å®éªŒè¯¯å·®ç›¸æ¯”ä»ç„¶å¤§çº¦å¤§ä¸‰å€ï¼Œä½†å·®å¼‚éå¸¸å°ï¼ˆTkacË‡ik ç­‰äººï¼Œ2014ï¼‰ï¼›æˆ‘ä»¬å°†çœ‹åˆ°æœ‰æ—¶å¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼ˆÂ§Vï¼‰ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ ä»¥åŒ¹é… $P_{N}(K)$ çº¦æŸçš„åŠ¿ $V$ ä¸åŒ…å«æœ‰å…³å•ä¸ªç¥ç»å…ƒèº«ä»½çš„ä»»ä½•ä¿¡æ¯ã€‚å› æ­¤ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŒ…æ‹¬è¿™ä¸€é¡¹æ”¹å–„äº†å¯¹æ‰€æœ‰ä¸‰å…ƒç»„ç›¸å…³æ€§çš„é¢„æµ‹ï¼Œè€Œè¿™äº›ç›¸å…³æ€§ç¡®å®å–å†³äºç¥ç»å…ƒçš„èº«ä»½ã€‚\nTriplet correlations for $N = 100$ cells in the retina (TkaË‡cik et al., 2014). Measured $C_{ijk}$ (x-axis) vs predicted by the model (y-axis), shown for a single subgroup. The $\\sim 1.6 \\times 10^{5}$ distinct triplets are grouped into 1000 equally populated bins; error bars in x are s.d. across the bin. The corresponding values for the predictions are grouped together, yielding the mean and the s.d. of the prediction (y-axis). Inset zooms in on the bulk of the predictions at small correlation, for the $K$â€“pairwise model. The original reference used $\\sigma_{i} = \\pm 1$, so that all the $C_{ijk}$ shown here are $8\\times$ larger than they would be in the $\\sigma_{i} = {0, 1}$ representation.\nå›¾ 9 è§†ç½‘è†œä¸­ $N = 100$ ä¸ªç»†èƒçš„ä¸‰å…ƒç»„ç›¸å…³æ€§ï¼ˆTkaË‡cik ç­‰äººï¼Œ2014ï¼‰ã€‚æµ‹é‡çš„ $C_{ijk}$ï¼ˆx è½´ï¼‰ä¸æ¨¡å‹é¢„æµ‹çš„å€¼ï¼ˆy è½´ï¼‰ï¼Œæ˜¾ç¤ºä¸ºå•ä¸ªå­ç»„ã€‚å¤§çº¦ $1.6 \\times 10^{5}$ ä¸ªä¸åŒçš„ä¸‰å…ƒç»„è¢«åˆ†æˆ 1000 ä¸ªåŒç­‰äººå£çš„ç®±ï¼›x è½´ä¸Šçš„è¯¯å·®æ¡æ˜¯è·¨ç®±çš„æ ‡å‡†å·®ã€‚é¢„æµ‹çš„ç›¸åº”å€¼è¢«åˆ†ç»„åœ¨ä¸€èµ·ï¼Œäº§ç”Ÿé¢„æµ‹çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆy è½´ï¼‰ã€‚æ’å›¾æ”¾å¤§äº† $K$â€“æˆå¯¹æ¨¡å‹ä¸­å°ç›¸å…³æ€§çš„é¢„æµ‹ä¸»ä½“ã€‚åŸå§‹å‚è€ƒä½¿ç”¨ $\\sigma_{i} = \\pm 1$ï¼Œå› æ­¤è¿™é‡Œæ˜¾ç¤ºçš„æ‰€æœ‰ $C_{ijk}$ éƒ½æ¯” $\\sigma_{i} = {0, 1}$ è¡¨ç¤ºæ³•ä¸­çš„å€¼å¤§ $8\\times$ã€‚\nWith $N = 100$ cells we cannot check, as in Fig 7F, the probability of every state of the network. But the model assigns to every state an energy $E_{2k}(\\vec{\\sigma})$, and we can ask about the distribution of this energy over the states that we see in the experiment vs. the expectation if states are drawn out of the model. To emphasize the extremes we look at the high energy tail,\n$$ \\Phi(E) = \\langle\\Theta[E - E_{2k}(\\vec{\\sigma})]\\rangle $$\nwhere $\\Theta(x)$ is the unit step function and the expectation value can be taken over the data or the theory. Figure 10 shows the comparison between theory and experiment. Note that the plot extends far past the point where individual states are predicted to occur once over the duration of the experiment, but we can make meaningful statements in this regime because there are (exponentially) many such states. Close agreement between theory and experiment extends out to $E\\sim 25$, corresponding to states that are predicted to occur roughly once per fifty years.\nå¯¹äº $N = 100$ ä¸ªç»†èƒï¼Œæˆ‘ä»¬æ— æ³•åƒå›¾ 7F é‚£æ ·æ£€æŸ¥ç½‘ç»œæ¯ä¸ªçŠ¶æ€çš„æ¦‚ç‡ã€‚ä½†æ¨¡å‹ä¸ºæ¯ä¸ªçŠ¶æ€åˆ†é…äº†ä¸€ä¸ªèƒ½é‡ $E_{2k}(\\vec{\\sigma})$ï¼Œæˆ‘ä»¬å¯ä»¥è¯¢é—®å®éªŒä¸­çœ‹åˆ°çš„çŠ¶æ€çš„èƒ½é‡åˆ†å¸ƒä¸ä»æ¨¡å‹ä¸­æŠ½å–çŠ¶æ€æ—¶çš„æœŸæœ›å€¼ã€‚ä¸ºäº†å¼ºè°ƒæç«¯æƒ…å†µï¼Œæˆ‘ä»¬æŸ¥çœ‹é«˜èƒ½å°¾éƒ¨ï¼Œ\n$$ \\Phi(E) = \\langle\\Theta[E - E_{2k}(\\vec{\\sigma})]\\rangle $$\nå…¶ä¸­ $\\Theta(x)$ æ˜¯å•ä½é˜¶è·ƒå‡½æ•°ï¼ŒæœŸæœ›å€¼å¯ä»¥åœ¨æ•°æ®æˆ–ç†è®ºä¸Šè¿›è¡Œã€‚å›¾ 10 æ˜¾ç¤ºäº†ç†è®ºä¸å®éªŒä¹‹é—´çš„æ¯”è¾ƒã€‚è¯·æ³¨æ„ï¼Œè¯¥å›¾è¿œè¿œè¶…å‡ºäº†é¢„æµ‹åœ¨å®éªŒæŒç»­æ—¶é—´å†…å•ç‹¬çŠ¶æ€å‡ºç°ä¸€æ¬¡çš„ç‚¹ï¼Œä½†æˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ªèŒƒå›´å†…åšå‡ºæœ‰æ„ä¹‰çš„é™ˆè¿°ï¼Œå› ä¸ºå­˜åœ¨ï¼ˆæŒ‡æ•°çº§ï¼‰è®¸å¤šè¿™æ ·çš„çŠ¶æ€ã€‚ç†è®ºä¸å®éªŒä¹‹é—´çš„å¯†åˆ‡ä¸€è‡´å»¶ä¼¸åˆ° $E\\sim 25$ï¼Œå¯¹åº”äºå¤§çº¦æ¯äº”åå¹´é¢„æµ‹å‡ºç°ä¸€æ¬¡çš„çŠ¶æ€ã€‚\nFIG. 10 The cumulative distribution of energies for $N = 120$ neurons (TkacË‡ik et al., 2014). $\\Phi(E)$ is defined in Eq (48), and averages are over data (black) or the theory (red). Dashed vertical line denotes an energy $E_{2k}(\\sigma)$ such that the particular state $\\sigma$ should occur on average once during the duration of the experiment.\nå›¾ 10 $N = 120$ ä¸ªç¥ç»å…ƒçš„èƒ½é‡ç´¯ç§¯åˆ†å¸ƒï¼ˆTkacË‡ik ç­‰äººï¼Œ2014ï¼‰ã€‚$\\Phi(E)$ åœ¨æ–¹ç¨‹ï¼ˆ48ï¼‰ä¸­å®šä¹‰ï¼Œå¹³å‡å€¼æ˜¯åŸºäºæ•°æ®ï¼ˆé»‘è‰²ï¼‰æˆ–ç†è®ºï¼ˆçº¢è‰²ï¼‰ã€‚è™šçº¿å‚ç›´çº¿è¡¨ç¤ºèƒ½é‡ $E_{2k}(\\sigma)$ï¼Œä½¿å¾—ç‰¹å®šçŠ¶æ€ $\\sigma$ åœ¨å®éªŒæŒç»­æ—¶é—´å†…å¹³å‡å‡ºç°ä¸€æ¬¡ã€‚\nThis class of models predicts that neural activity is collective. Thus in a population of $N$ cells, if we know the state of $N âˆ’1$ we can make a prediction of the probability that the last cell will be active,\n$$ P(\\sigma_{i}=1|\\{\\sigma_{i\\neq j}\\}) = \\frac{1}{1 + \\exp{[-h_{i}^{\\text{eff}}(\\{\\sigma_{i\\neq j}\\})]}} $$\nwhere we can think of the other neurons as applying an effective field to the one neuron that we focus on,\n$$ \\begin{aligned} h_{i}^{\\text{eff}}(\\{\\sigma_{i\\neq j}\\}) = \u0026E(\\sigma_{1},\\sigma_{2},\\cdots,\\sigma_{i}=1,\\cdots,\\sigma_{N}) \\\\ -\u0026E(\\sigma_{1},\\sigma_{2},\\cdots,\\sigma_{i}=0,\\cdots,\\sigma_{N}) \\end{aligned} $$\nFor each neuron and for each moment in time we can calculate the effective field predicted by the theory, with no free parameters, and we can group together all instances in which this field is in some narrow range and ask if the probability of the cell being active agrees with Eq (B8). Results are shown in Fig. 11A.\nè¿™ç§æ¨¡å‹ç±»é¢„æµ‹ç¥ç»æ´»åŠ¨æ˜¯é›†ä½“çš„ã€‚å› æ­¤ï¼Œåœ¨ $N$ ä¸ªç»†èƒçš„ç¾¤ä½“ä¸­ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“ $N âˆ’1$ çš„çŠ¶æ€ï¼Œæˆ‘ä»¬å¯ä»¥é¢„æµ‹æœ€åä¸€ä¸ªç»†èƒæ´»è·ƒçš„æ¦‚ç‡ï¼Œ\n$$ P(\\sigma_{i}=1|\\{\\sigma_{i\\neq j}\\}) = \\frac{1}{1 + \\exp{[-h_{i}^{\\text{eff}}(\\{\\sigma_{i\\neq j}\\})]}} $$\nå…¶ä¸­æˆ‘ä»¬å¯ä»¥å°†å…¶ä»–ç¥ç»å…ƒè§†ä¸ºå¯¹æˆ‘ä»¬å…³æ³¨çš„ä¸€ä¸ªç¥ç»å…ƒæ–½åŠ æœ‰æ•ˆåœºï¼Œ\n$$ \\begin{aligned} h_{i}^{\\text{eff}}(\\{\\sigma_{i\\neq j}\\}) = \u0026E(\\sigma_{1},\\sigma_{2},\\cdots,\\sigma_{i}=1,\\cdots,\\sigma_{N}) \\\\ -\u0026E(\\sigma_{1},\\sigma_{2},\\cdots,\\sigma_{i}=0,\\cdots,\\sigma_{N}) \\end{aligned} $$\nå¯¹äºæ¯ä¸ªç¥ç»å…ƒå’Œæ¯ä¸ªæ—¶é—´ç‚¹ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥è®¡ç®—ç†è®ºé¢„æµ‹çš„æœ‰æ•ˆåœºï¼Œæ²¡æœ‰è‡ªç”±å‚æ•°ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰å®ä¾‹åˆ†ç»„åœ¨ä¸€èµ·ï¼Œå…¶ä¸­è¯¥åœºå¤„äºæŸä¸ªç‹­çª„èŒƒå›´å†…ï¼Œå¹¶è¯¢é—®ç»†èƒæ´»è·ƒçš„æ¦‚ç‡æ˜¯å¦ä¸æ–¹ç¨‹ï¼ˆB8ï¼‰ä¸€è‡´ã€‚ç»“æœå¦‚å›¾ 11A æ‰€ç¤ºã€‚\nFIG. 11 Effective fields and the collective character of neural activity in the retina (TkacË‡ik et al., 2014). (A) The probability that a single neuron is active given the state of the rest of the network, with $N = 120$. Points with error bars are the data, with the effective field computed from the model as in Eq (50). Red line is the prediction from Eq (49), and grey points are results with the purely pairwise rather than â€œ$K$â€“pairwiseâ€ model. Shaded grey region shows the distribution of fields across the experiment, emphasizing that the errors at large positive field are in the tail of the distribution. Inset shows the same results on a logarithmic scale for probability. (B) Probability of a single neuron being active as a function of time in a repeated naturalistic movie, normalized as the probability per unit time of an action potential (spikes/s). Top, in red, experimental data. Lower traces, in black, predictions based on states of other neurons in an $N$â€“cell group, based on Eqs (B8, 50). Solid lines are the mean prediction across all repetitions of the movie, and thin lines are the envelope $\\pm$ one standard deviation.\nå›¾ 11 è§†ç½‘è†œä¸­ç¥ç»æ´»åŠ¨çš„æœ‰æ•ˆåœºå’Œé›†ä½“ç‰¹å¾ï¼ˆTkacË‡ik ç­‰äººï¼Œ2014ï¼‰ã€‚ï¼ˆAï¼‰åœ¨ $N = 120$ çš„æƒ…å†µä¸‹ï¼Œç»™å®šç½‘ç»œå…¶ä½™éƒ¨åˆ†çŠ¶æ€æ—¶å•ä¸ªç¥ç»å…ƒæ´»è·ƒçš„æ¦‚ç‡ã€‚å¸¦è¯¯å·®æ¡çš„ç‚¹æ˜¯æ•°æ®ï¼Œæœ‰æ•ˆåœºæ˜¯æ ¹æ®æ–¹ç¨‹ï¼ˆ50ï¼‰ä»æ¨¡å‹ä¸­è®¡ç®—å¾—å‡ºçš„ã€‚çº¢çº¿æ˜¯æ–¹ç¨‹ï¼ˆ49ï¼‰çš„é¢„æµ‹ï¼Œç°è‰²ç‚¹æ˜¯çº¯æˆå¯¹è€Œä¸æ˜¯â€œ$K$â€“æˆå¯¹â€æ¨¡å‹çš„ç»“æœã€‚é˜´å½±ç°è‰²åŒºåŸŸæ˜¾ç¤ºäº†å®éªŒä¸­åœºçš„åˆ†å¸ƒï¼Œå¼ºè°ƒäº†åœ¨å¤§æ­£åœºå¤„çš„è¯¯å·®å¤„äºåˆ†å¸ƒçš„å°¾éƒ¨ã€‚æ’å›¾ä»¥æ¦‚ç‡çš„å¯¹æ•°åˆ»åº¦æ˜¾ç¤ºç›¸åŒçš„ç»“æœã€‚ï¼ˆBï¼‰åœ¨é‡å¤çš„è‡ªç„¶ç”µå½±ä¸­ï¼Œå•ä¸ªç¥ç»å…ƒæ´»è·ƒçš„æ¦‚ç‡ï¼Œå½’ä¸€åŒ–ä¸ºæ¯å•ä½æ—¶é—´å†…åŠ¨ä½œç”µä½ï¼ˆå°–å³°/ç§’ï¼‰çš„æ¦‚ç‡ã€‚é¡¶éƒ¨ä¸ºçº¢è‰²ï¼Œè¡¨ç¤ºå®éªŒæ•°æ®ã€‚ä¸‹æ–¹è½¨è¿¹ä¸ºé»‘è‰²ï¼ŒåŸºäº $N$ ä¸ªç»†èƒç»„ä¸­å…¶ä»–ç¥ç»å…ƒçŠ¶æ€çš„é¢„æµ‹ï¼ŒåŸºäºæ–¹ç¨‹ï¼ˆB8ï¼Œ50ï¼‰ã€‚å®çº¿æ˜¯ç”µå½±æ‰€æœ‰é‡å¤çš„å¹³å‡é¢„æµ‹ï¼Œç»†çº¿æ˜¯åŒ…ç»œçº¿ $\\pm$ ä¸€ä¸ªæ ‡å‡†å·®ã€‚\nWe see that the predictions of Eqs (B8) and (50) in the $K$â€“pairwise model agree well with experiment throughout the bulk of the distribution of effective fields, but that discrepancies arise in the tails. These deviations are $\\sim 1.5\\times$ the error bars of the measurement, but have some systematic structure, suggesting that we are capturing much but not quite all of the collective behavior under conditions where neurons are driven most strongly.\næˆ‘ä»¬çœ‹åˆ° $K$â€“æˆå¯¹æ¨¡å‹ä¸­æ–¹ç¨‹ï¼ˆB8ï¼‰å’Œï¼ˆ50ï¼‰çš„é¢„æµ‹ä¸å®éªŒåœ¨æœ‰æ•ˆåœºåˆ†å¸ƒçš„ä¸»ä½“éƒ¨åˆ†å¾ˆå¥½åœ°ä¸€è‡´ï¼Œä½†åœ¨å°¾éƒ¨å‡ºç°äº†åå·®ã€‚è¿™äº›åå·®çº¦ä¸ºæµ‹é‡è¯¯å·®æ¡çš„ $\\sim 1.5\\times$ï¼Œä½†å…·æœ‰ä¸€äº›ç³»ç»Ÿç»“æ„ï¼Œè¡¨æ˜æˆ‘ä»¬æ­£åœ¨æ•æ‰å¤§éƒ¨åˆ†ä½†å¹¶éå…¨éƒ¨çš„é›†ä½“è¡Œä¸ºï¼Œåœ¨ç¥ç»å…ƒå—åˆ°æœ€å¼ºé©±åŠ¨çš„æ¡ä»¶ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚\nThe results in Fig. 11A combine data across all times to estimate the probability of activity in one cell given the state of the rest of the network. It is interesting to unfold these results in time. In particular, the structure of the experiment was such that the retina saw the same movie many times, and so we can condition on a particular moment in the movie, as shown for one neuron in Fig 11B. It is conventional to plot not the probability of being active in a small bin but the corresponding â€œrateâ€ (Rieke et al., 1997)\n$$ r_{i}(t) = \\langle\\sigma_{i}(t)\\rangle/\\Delta\\tau $$\nwhere $\\sigma_{i}(t)$ denotes the state of neuron $i$ at time $t$ relative to (in this case) the visual inputs. We see in the top trace of Fig. 11B that single neurons are active very rarely, with essentially zero probability of spiking between brief transients that generate on average one or a few spikes. This pattern is common in response to naturalistic stimuli, and very difficult to reproduce in models (Maheswaranathan et al., 2023).\nç¿»è¯‘\nå›¾ 11A ä¸­çš„ç»“æœç»“åˆäº†æ‰€æœ‰æ—¶é—´çš„æ•°æ®ï¼Œä»¥ä¼°è®¡åœ¨ç»™å®šç½‘ç»œå…¶ä½™éƒ¨åˆ†çŠ¶æ€æ—¶ä¸€ä¸ªç»†èƒçš„æ´»åŠ¨æ¦‚ç‡ã€‚æœ‰è¶£çš„æ˜¯å°†è¿™äº›ç»“æœå±•å¼€åœ¨æ—¶é—´ä¸Šã€‚ç‰¹åˆ«æ˜¯ï¼Œå®éªŒçš„ç»“æ„æ˜¯è§†ç½‘è†œå¤šæ¬¡çœ‹åˆ°åŒä¸€éƒ¨ç”µå½±ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä»¥ç”µå½±ä¸­çš„ç‰¹å®šæ—¶åˆ»ä¸ºæ¡ä»¶ï¼Œå¦‚å›¾ 11B ä¸­æ‰€ç¤ºçš„ä¸€ä¸ªç¥ç»å…ƒã€‚é€šå¸¸ä¸ç»˜åˆ¶åœ¨å°ç®±ä¸­æ´»è·ƒçš„æ¦‚ç‡ï¼Œè€Œæ˜¯ç»˜åˆ¶ç›¸åº”çš„â€œé€Ÿç‡â€ï¼ˆRieke ç­‰äººï¼Œ1997ï¼‰\n$$ r_{i}(t) = \\langle\\sigma_{i}(t)\\rangle/\\Delta\\tau $$\nå…¶ä¸­ $\\sigma_{i}(t)$ è¡¨ç¤ºç›¸å¯¹äºï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼‰è§†è§‰è¾“å…¥çš„æ—¶é—´ $t$ ç¥ç»å…ƒ $i$ çš„çŠ¶æ€ã€‚æˆ‘ä»¬åœ¨å›¾ 11B çš„é¡¶éƒ¨è½¨è¿¹ä¸­çœ‹åˆ°ï¼Œå•ä¸ªç¥ç»å…ƒéå¸¸ç½•è§åœ°æ´»è·ƒï¼Œåœ¨äº§ç”Ÿå¹³å‡ä¸€ä¸ªæˆ–å‡ ä¸ªå°–å³°çš„çŸ­æš‚ç¬å˜ä¹‹é—´å‡ ä¹æ²¡æœ‰å°–å³°çš„æ¦‚ç‡ã€‚è¿™ç§æ¨¡å¼åœ¨å¯¹è‡ªç„¶åˆºæ¿€çš„å“åº”ä¸­å¾ˆå¸¸è§ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹ä¸­å¾ˆéš¾å†ç°ï¼ˆMaheswaranathan ç­‰äººï¼Œ2023ï¼‰ã€‚\nThe maximum entropy models provide an extreme opposite point of view, making no reference to the visual inputs; instead activity is determined by the state of the rest of the network. We see that this approach correctly predicts sparse activity, with near zero rate between transients that are timed correctly relative to the input. Although here we see just one cell, the average neuron exhibits an $r_{i}(t)$ that has $\\sim 80%$ correlation with the theoretical predictions at $N = 120$. There is no sign of saturation, and it seems likely we would make even more precise predictions from models based on all $N\\sim 200$ cells in this small patch of the retina. The possibility of predicting activity without reference to the visual input suggests that the â€œvocabularyâ€ of the retinaâ€™s output is restricted, and that as with spelling rules this should allow for error-correction(Loback et al., 2017).\næœ€å¤§ç†µæ¨¡å‹æä¾›äº†ä¸€ä¸ªæç«¯ç›¸åçš„è§‚ç‚¹ï¼Œä¸å‚è€ƒè§†è§‰è¾“å…¥ï¼›ç›¸åï¼Œæ´»åŠ¨ç”±ç½‘ç»œå…¶ä½™éƒ¨åˆ†çš„çŠ¶æ€å†³å®šã€‚æˆ‘ä»¬çœ‹åˆ°è¿™ç§æ–¹æ³•æ­£ç¡®åœ°é¢„æµ‹äº†ç¨€ç–æ´»åŠ¨ï¼Œåœ¨ä¸è¾“å…¥ç›¸å¯¹æ­£ç¡®å®šæ—¶çš„ç¬å˜ä¹‹é—´é€Ÿç‡æ¥è¿‘é›¶ã€‚è™½ç„¶è¿™é‡Œæˆ‘ä»¬åªçœ‹åˆ°ä¸€ä¸ªç»†èƒï¼Œä½†å¹³å‡ç¥ç»å…ƒè¡¨ç°å‡ºä¸ç†è®ºé¢„æµ‹åœ¨ $N = 120$ æ—¶çº¦æœ‰ $\\sim 80%$ çš„ç›¸å…³æ€§çš„ $r_{i}(t)$ã€‚æ²¡æœ‰é¥±å’Œçš„è¿¹è±¡ï¼Œå¹¶ä¸”ä¼¼ä¹æˆ‘ä»¬å¯ä»¥ä»åŸºäºè§†ç½‘è†œè¿™ä¸ªå°å—ä¸­æ‰€æœ‰ $N\\sim 200$ ä¸ªç»†èƒçš„æ¨¡å‹ä¸­åšå‡ºæ›´ç²¾ç¡®çš„é¢„æµ‹ã€‚åœ¨ä¸å‚è€ƒè§†è§‰è¾“å…¥çš„æƒ…å†µä¸‹é¢„æµ‹æ´»åŠ¨çš„å¯èƒ½æ€§è¡¨æ˜ï¼Œè§†ç½‘è†œè¾“å‡ºçš„â€œè¯æ±‡â€æ˜¯å—é™çš„ï¼Œå¹¶ä¸”å°±åƒæ‹¼å†™è§„åˆ™ä¸€æ ·ï¼Œè¿™åº”è¯¥å…è®¸è¿›è¡Œçº é”™ï¼ˆLoback ç­‰äººï¼Œ2017ï¼‰ã€‚\nPerhaps the most basic prediction from maximum entropy models is the entropy itself. There are several ways that we can estimate the entropy. First, in the $K$-pairwise model we can see that the effective energy of the completely silent state, from Eq (46), is zero, which means that the probability of this state is just the inverse of the partition function. Further, in this model, the probability of complete silence matches what we observe experimentally. Thus we can estimate the free energy of the model from the data, and then we can estimate the mean energy of the model from Monte Carlo, giving us an estimate of the entropy. An alternative is to generalize the model by introducing a fictitious temperature, as will be discussed in Â§VI.B. Then at $T = 0$ the entropy must be zero and at $T\\to\\infty$ the entropy must be $N\\log{2}$, while the derivative of the entropy is related as always to the heat capacity. Thus the entropy of our model for the real system at $T = 1$ becomes\n$$ S_{N}(T=1) = \\int_{0}^{1}\\mathrm{d}T\\frac{C_{v}(T)}{T} = N\\log{2} - \\int_{1}^{\\infty}\\mathrm{d}T\\frac{C_{v}(T)}{T} $$\nwhere the heat capacity is related as usual to the variance of the energy, $\\begin{aligned}C_{v} = \\frac{\\langle (\\delta E)^{2}\\rangle}{T^{2}}\\end{aligned}$, that we can estimate from Monte Carlo simulations at each $T$. There is also a check that the two estimates in Eq (52) should agree. All of these methods agree with one another at the percent level, with results shown in Fig. 12A.\nä¹Ÿè®¸æœ€å¤§ç†µæ¨¡å‹çš„æœ€åŸºæœ¬é¢„æµ‹æ˜¯ç†µæœ¬èº«ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å‡ ç§æ–¹å¼æ¥ä¼°è®¡ç†µã€‚é¦–å…ˆï¼Œåœ¨ $K$-æˆå¯¹æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®Œå…¨é™æ­¢çŠ¶æ€çš„æœ‰æ•ˆèƒ½é‡ï¼Œä»æ–¹ç¨‹ï¼ˆ46ï¼‰æ¥çœ‹ä¸ºé›¶ï¼Œè¿™æ„å‘³ç€è¯¥çŠ¶æ€çš„æ¦‚ç‡åªæ˜¯é…åˆ†å‡½æ•°çš„å€’æ•°ã€‚æ­¤å¤–ï¼Œåœ¨è¯¥æ¨¡å‹ä¸­ï¼Œå®Œå…¨é™æ­¢çš„æ¦‚ç‡ä¸æˆ‘ä»¬åœ¨å®éªŒä¸­è§‚å¯Ÿåˆ°çš„ç›¸åŒ¹é…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ•°æ®ä¸­ä¼°è®¡æ¨¡å‹çš„è‡ªç”±èƒ½ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä»è’™ç‰¹å¡ç½—ä¸­ä¼°è®¡æ¨¡å‹çš„å¹³å‡èƒ½é‡ï¼Œä»è€Œç»™å‡ºç†µçš„ä¼°è®¡ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡å¼•å…¥è™šæ„æ¸©åº¦æ¥æ¨å¹¿æ¨¡å‹ï¼Œå¦‚ Â§VI.B ä¸­å°†è®¨è®ºçš„é‚£æ ·ã€‚é‚£ä¹ˆåœ¨ $T = 0$ æ—¶ç†µå¿…é¡»ä¸ºé›¶ï¼Œè€Œåœ¨ $T\\to\\infty$ æ—¶ç†µå¿…é¡»ä¸º $N\\log{2}$ï¼Œè€Œç†µçš„å¯¼æ•°ä¸€å¦‚æ—¢å¾€åœ°ä¸çƒ­å®¹æœ‰å…³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹çœŸå®ç³»ç»Ÿåœ¨ $T = 1$ æ—¶æ¨¡å‹çš„ç†µå˜ä¸º\n$$ S_{N}(T=1) = \\int_{0}^{1}\\mathrm{d}T\\frac{C_{v}(T)}{T} = N\\log{2} - \\int_{1}^{\\infty}\\mathrm{d}T\\frac{C_{v}(T)}{T} $$\nå…¶ä¸­çƒ­å®¹ä¸èƒ½é‡çš„æ–¹å·®æœ‰å…³ï¼Œ$\\begin{aligned}C_{v} = \\frac{\\langle (\\delta E)^{2}\\rangle}{T^{2}}\\end{aligned}$ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ¯ä¸ª $T$ çš„è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿä¸­ä¼°è®¡å®ƒã€‚è¿˜æœ‰ä¸€ä¸ªæ£€æŸ¥ï¼Œå³æ–¹ç¨‹ï¼ˆ52ï¼‰ä¸­çš„ä¸¤ä¸ªä¼°è®¡åº”è¯¥æ˜¯ä¸€è‡´çš„ã€‚æ‰€æœ‰è¿™äº›æ–¹æ³•åœ¨ç™¾åˆ†æ¯”æ°´å¹³ä¸Šå½¼æ­¤ä¸€è‡´ï¼Œç»“æœå¦‚å›¾ 12A æ‰€ç¤ºã€‚\nFIG. 12 Entropy and coincidences in the activity of the retinal network (TkaË‡cik et al., 2014). (A) Entropy predicted in Kâ€“pairwise models (red) and in the approximation that all neurons are independent (grey). Models are constructed independently for many subgroups of size $N$ chosen out of the total population $N_{\\text{max}} = 160$, and error bars include the variance across these groups. (B) Probability that two randomly chosen states of the network are the same, again for many subgroups of size $N$. Results for real data (black), shuffled data (grey), and the $K$â€“pairwise models (red).\nå›¾ 12 è§†ç½‘è†œç½‘ç»œæ´»åŠ¨ä¸­çš„ç†µå’Œå·§åˆï¼ˆTkaË‡cik ç­‰äººï¼Œ2014ï¼‰ã€‚ï¼ˆAï¼‰Kâ€“æˆå¯¹æ¨¡å‹ä¸­é¢„æµ‹çš„ç†µï¼ˆçº¢è‰²ï¼‰å’Œæ‰€æœ‰ç¥ç»å…ƒç‹¬ç«‹çš„è¿‘ä¼¼ï¼ˆç°è‰²ï¼‰ã€‚æ¨¡å‹æ˜¯ä»æ€»äººå£ $N_{\\text{max}} = 160$ ä¸­é€‰æ‹©çš„è®¸å¤šå¤§å°ä¸º $N$ çš„å­ç»„ç‹¬ç«‹æ„å»ºçš„ï¼Œè¯¯å·®æ¡åŒ…æ‹¬è¿™äº›ç»„ä¹‹é—´çš„æ–¹å·®ã€‚ï¼ˆBï¼‰éšæœºé€‰æ‹©çš„ç½‘ç»œçŠ¶æ€ç›¸åŒçš„æ¦‚ç‡ï¼ŒåŒæ ·é€‚ç”¨äºè®¸å¤šå¤§å°ä¸º $N$ çš„å­ç»„ã€‚çœŸå®æ•°æ®çš„ç»“æœï¼ˆé»‘è‰²ï¼‰ã€æ´—ç‰Œæ•°æ®ï¼ˆç°è‰²ï¼‰å’Œ $K$â€“æˆå¯¹æ¨¡å‹ï¼ˆçº¢è‰²ï¼‰ã€‚\nThe $\\sim 25%$ reduction in entropy is significant, but more dramatic (and testable) is the prediction that the distribution over states is extremely inhomogeneous. Recall that if the distribution is uniform over some effective number of states $\\Omega_{\\text{eff}}$ then the entropy is $S = \\log{\\Omega_{\\text{eff}}}$ and the probability that two states chosen at random will be the same is $P_{c} = 1/\\Omega_{\\text{eff}}$ ; for nonâ€“uniform distributions we have $S\\geq âˆ’ \\log(P_{c})$. If neurons were independent then with $N$ cells we would have $P_{c}\\propto e^{âˆ’\\alpha N}$, and this is what we see in the data once they are shuffled to remove correlations (Fig. 12B). But the real data show a much more gradual decay with $N$ , and this is captured perfectly by the $K$â€“pairwise maximum entropy models.\n$\\sim 25%$ çš„ç†µå‡å°‘æ˜¯æ˜¾è‘—çš„ï¼Œä½†æ›´æˆå‰§æ€§ï¼ˆä¸”å¯æµ‹è¯•ï¼‰çš„é¢„æµ‹æ˜¯çŠ¶æ€åˆ†å¸ƒæä¸å‡åŒ€ã€‚å›æƒ³ä¸€ä¸‹ï¼Œå¦‚æœåˆ†å¸ƒåœ¨æŸä¸ªæœ‰æ•ˆçŠ¶æ€æ•° Î©eff ä¸Šæ˜¯å‡åŒ€çš„ï¼Œé‚£ä¹ˆç†µä¸º $S = \\log{\\Omega_{\\text{eff}}}$ï¼Œéšæœºé€‰æ‹©ä¸¤ä¸ªçŠ¶æ€ç›¸åŒçš„æ¦‚ç‡ä¸º $P_{c} = 1/\\Omega_{\\text{eff}}$ï¼›å¯¹äºéå‡åŒ€åˆ†å¸ƒï¼Œæˆ‘ä»¬æœ‰ $S\\geq âˆ’ \\log(P_{c})$ã€‚å¦‚æœç¥ç»å…ƒæ˜¯ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆå¯¹äº $N$ ä¸ªç»†èƒï¼Œæˆ‘ä»¬å°†æœ‰ $P_{c}\\propto e^{âˆ’\\alpha N}$ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬åœ¨æ•°æ®ä¸­çœ‹åˆ°çš„ï¼Œä¸€æ—¦å®ƒä»¬è¢«æ´—ç‰Œä»¥å»é™¤ç›¸å…³æ€§ï¼ˆå›¾ 12Bï¼‰ã€‚ä½†çœŸå®æ•°æ®éšç€ $N$ æ˜¾ç¤ºå‡ºæ›´æ¸è¿›çš„è¡°å‡ï¼Œè€Œè¿™æ­£è¢« $K$â€“æˆå¯¹æœ€å¤§ç†µæ¨¡å‹å®Œç¾æ•æ‰ã€‚\nAt $N = 120$ the logarithm of the coincidence probability (both measured and predicted) is an order of magnitude smaller than the entropy predicted by the model. Perhaps related is that the free energy per neuronâ€”which, as discussed above, can be obtained directly from the probability of the fully silent stateâ€”also decreases dramatically as $N$ increases. At $N = 120$ the free energy is just a few percent of the either the entropy or the mean energy, reflecting near perfect cancelation between these terms; one can see this also in a much simpler model that only matches $P_{N}(K)$ and not the individual means or pairwise correlations (TkacË‡ik et al., 2013). Importantly, these behaviors are captured by the Kâ€“pairwise model smoothly from $N \u003c 40$ through $N \u003e 100$, indicating that what we learned at more modest $N$ really does extrapolate up a scale comparable to the whole population of cells in a patch of the retina. We will have to work harder to decide if we can see the emergence of a true thermodynamic limit.\nåœ¨ $N = 120$ æ—¶ï¼Œå·§åˆæ¦‚ç‡çš„å¯¹æ•°ï¼ˆæµ‹é‡å’Œé¢„æµ‹ï¼‰æ¯”æ¨¡å‹é¢„æµ‹çš„ç†µå°ä¸€ä¸ªæ•°é‡çº§ã€‚ä¹Ÿè®¸ç›¸å…³çš„æ˜¯ï¼Œæ¯ä¸ªç¥ç»å…ƒçš„è‡ªç”±èƒ½â€”â€”æ­£å¦‚ä¸Šé¢è®¨è®ºçš„é‚£æ ·ï¼Œå¯ä»¥ç›´æ¥ä»å®Œå…¨é™æ­¢çŠ¶æ€çš„æ¦‚ç‡ä¸­è·å¾—â€”â€”éšç€ $N$ çš„å¢åŠ ä¹Ÿæ˜¾è‘—é™ä½ã€‚åœ¨ $N = 120$ æ—¶ï¼Œè‡ªç”±èƒ½ä»…å ç†µæˆ–å¹³å‡èƒ½é‡çš„ç™¾åˆ†ä¹‹å‡ ï¼Œåæ˜ äº†è¿™äº›é¡¹ä¹‹é—´å‡ ä¹å®Œç¾çš„æŠµæ¶ˆï¼›åœ¨ä¸€ä¸ªåªåŒ¹é… $P_{N}(K)$ è€Œä¸åŒ¹é…å•ä¸ªå‡å€¼æˆ–æˆå¯¹ç›¸å…³æ€§çš„æ›´ç®€å•æ¨¡å‹ä¸­ä¹Ÿå¯ä»¥çœ‹åˆ°è¿™ä¸€ç‚¹ï¼ˆTkacË‡ik ç­‰äººï¼Œ2013ï¼‰ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›è¡Œä¸ºé€šè¿‡ Kâ€“æˆå¯¹æ¨¡å‹ä» $N \u003c 40$ å¹³æ»‘åœ°æ•æ‰åˆ° $N \u003e 100$ï¼Œè¡¨æ˜æˆ‘ä»¬åœ¨è¾ƒé€‚åº¦çš„ $N$ ä¸Šå­¦åˆ°çš„ä¸œè¥¿ç¡®å®å¯ä»¥å¤–æ¨åˆ°ä¸è§†ç½‘è†œä¸€å—åŒºåŸŸå†…ç»†èƒçš„æ•´ä¸ªç¾¤ä½“ç›¸å½“çš„è§„æ¨¡ã€‚æˆ‘ä»¬å°†ä¸å¾—ä¸æ›´åŠ åŠªåŠ›åœ°å·¥ä½œï¼Œä»¥å†³å®šæˆ‘ä»¬æ˜¯å¦èƒ½å¤Ÿçœ‹åˆ°çœŸæ­£çƒ­åŠ›å­¦æé™çš„å‡ºç°ã€‚\nFinally, we should address the question of whether these results can be recovered as perturbations to a model of independent neurons. At lowest order in perturbation theory, there is a simple relationship between the observed correlations and the inferred interactions $J_{ij}$ in the pairwise model (Sessak and Monasson, 2009), and we can check this relationship against the values of $J_{ij}$ inferred from correctly matching the observed correlations. In the retina, large deviations from lowest order perturbation theory are visible already at $N = 15$, and correspondingly models built from the perturbative estimates of $J_{ij}$ are orders of magnitude further away from the data than the full model (TkacË‡ik et al., 2014). Higher order perturbative contributions to the entropy would be comparable to one another for $N = 20$ retinal neurons even in a hypothetical network where all correlations were scaled down by a factor of two from the real data (Azhar and Bialek, 2010). We conclude that the success of maximum entropy models in describing networks of real neurons is not something we can understand in low order perturbation theory. Interestingly, simulations of models with pure 3â€“ and 4-spin interactions at $N\\sim 20$ show that pairwise maximum entropy models typically are good approximations to the real distribution both in the weak correlation limit and in the limit of strong, dense interactions (Merchan and Nemenman, 2016).\næœ€åï¼Œæˆ‘ä»¬åº”è¯¥è§£å†³è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼šè¿™äº›ç»“æœæ˜¯å¦å¯ä»¥ä½œä¸ºç‹¬ç«‹ç¥ç»å…ƒæ¨¡å‹çš„å¾®æ‰°æ¥æ¢å¤ã€‚åœ¨å¾®æ‰°ç†è®ºçš„æœ€ä½é˜¶ä¸­ï¼Œè§‚å¯Ÿåˆ°çš„ç›¸å…³æ€§ä¸æˆå¯¹æ¨¡å‹ä¸­æ¨æ–­çš„ç›¸äº’ä½œç”¨ $J_{ij}$ ä¹‹é—´å­˜åœ¨ç®€å•å…³ç³»ï¼ˆSessak å’Œ Monassonï¼Œ2009ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®æ­£ç¡®åŒ¹é…è§‚å¯Ÿåˆ°çš„ç›¸å…³æ€§æ¥æ£€æŸ¥è¿™ç§å…³ç³»ä¸æ¨æ–­çš„ $J_{ij}$ å€¼ä¹‹é—´çš„å…³ç³»ã€‚åœ¨è§†ç½‘è†œä¸­ï¼Œåœ¨ $N = 15$ æ—¶å·²ç»å¯ä»¥çœ‹åˆ°ä¸æœ€ä½é˜¶å¾®æ‰°ç†è®ºçš„å·¨å¤§åå·®ï¼Œå› æ­¤ä»å¾®æ‰°ä¼°è®¡çš„ $J_{ij}$ æ„å»ºçš„æ¨¡å‹ä¸å®Œæ•´æ¨¡å‹ç›¸æ¯”ï¼Œä¸æ•°æ®ç›¸å·®å‡ ä¸ªæ•°é‡çº§ï¼ˆTkacË‡ik ç­‰äººï¼Œ2014ï¼‰ã€‚å³ä½¿åœ¨ä¸€ä¸ªå‡è®¾ç½‘ç»œä¸­ï¼Œæ‰€æœ‰ç›¸å…³æ€§éƒ½æ¯”çœŸå®æ•°æ®ç¼©å°äº†ä¸¤å€ï¼Œå¯¹äº $N = 20$ ä¸ªè§†ç½‘è†œç¥ç»å…ƒï¼Œé«˜é˜¶å¾®æ‰°å¯¹ç†µçš„è´¡çŒ®ä¹Ÿå°†å½¼æ­¤ç›¸å½“ï¼ˆAzhar å’Œ Bialekï¼Œ2010ï¼‰ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œæœ€å¤§ç†µæ¨¡å‹åœ¨æè¿°çœŸå®ç¥ç»å…ƒç½‘ç»œæ–¹é¢çš„æˆåŠŸä¸æ˜¯æˆ‘ä»¬å¯ä»¥åœ¨ä½é˜¶å¾®æ‰°ç†è®ºä¸­ç†è§£çš„ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨ $N\\sim 20$ æ—¶å…·æœ‰çº¯ 3â€“å’Œ 4â€“è‡ªæ—‹ç›¸äº’ä½œç”¨çš„æ¨¡å‹æ¨¡æ‹Ÿè¡¨æ˜ï¼Œæˆå¯¹æœ€å¤§ç†µæ¨¡å‹é€šå¸¸æ˜¯å®é™…åˆ†å¸ƒçš„è‰¯å¥½è¿‘ä¼¼ï¼Œæ— è®ºæ˜¯åœ¨å¼±ç›¸å…³æé™è¿˜æ˜¯åœ¨å¼ºçƒˆã€å¯†é›†ç›¸äº’ä½œç”¨çš„æé™ä¸‹ï¼ˆMerchan å’Œ Nemenmanï¼Œ2016ï¼‰ã€‚\nThe retina is a very special part of the brain, and one might worry that the success of maximum entropy models is somehow tied to these special features. It thus is important that the same methods work in capturing the collective behavior of neurons in very distant parts of the brain. An example is in prefrontal cortex, which is involved in a wide range of higher cognitive functions.\nè§†ç½‘è†œæ˜¯å¤§è„‘ä¸­ä¸€ä¸ªéå¸¸ç‰¹æ®Šçš„éƒ¨åˆ†ï¼Œäººä»¬å¯èƒ½ä¼šæ‹…å¿ƒæœ€å¤§ç†µæ¨¡å‹çš„æˆåŠŸä¸è¿™äº›ç‰¹æ®Šç‰¹å¾æœ‰å…³ã€‚å› æ­¤ï¼ŒåŒæ ·çš„æ–¹æ³•åœ¨æ•æ‰å¤§è„‘éå¸¸é¥è¿œéƒ¨åˆ†ç¥ç»å…ƒçš„é›†ä½“è¡Œä¸ºæ–¹é¢ä¹Ÿæœ‰æ•ˆï¼Œè¿™ä¸€ç‚¹å¾ˆé‡è¦ã€‚ä¸€ä¸ªä¾‹å­æ˜¯åœ¨å‰é¢å¶çš®å±‚ï¼Œå®ƒå‚ä¸äº†å¹¿æ³›çš„é«˜çº§è®¤çŸ¥åŠŸèƒ½ã€‚\nExperiments recording simultaneous activity from several tens of neurons in prefrontal cortex were analyzed with maximum entropy methods, and an example of the results is shown in Fig. 13 (Tavoni et al., 2017). We see that these models pass the same tests as in the retina, correctly predicting triplet correlations, the probability of $K$ out of $N$ cells being active simultaneously, and the probabilities for particular patterns of activity in subgroups of $N = 10$ cells. Extending this analysis across multiple experimental sessions it was possible to detect changes in the coupling matrix $J_{ij}$ as the animal learned to engage in different tasks. These changes were concentrated in subsets of cells which also were preferentially reâ€“activated during sleep between sessions. One should be careful about giving too mechanistic an interpretation of the Ising models that emerge from these analyses, but it is exciting to see the structure of the models connect to independently measurable functional dynamics in the network. This is true even in the farthest reaches of the cortex, the regions of the brain that we use for thinking, planning, and deciding.\nåœ¨å‰é¢å¶çš®å±‚è®°å½•åŒæ—¶æ´»åŠ¨çš„å‡ åä¸ªç¥ç»å…ƒçš„å®éªŒä½¿ç”¨æœ€å¤§ç†µæ–¹æ³•è¿›è¡Œäº†åˆ†æï¼Œç»“æœçš„ä¸€ä¸ªä¾‹å­å¦‚å›¾ 13 æ‰€ç¤ºï¼ˆTavoni ç­‰äººï¼Œ2017ï¼‰ã€‚æˆ‘ä»¬çœ‹åˆ°è¿™äº›æ¨¡å‹é€šè¿‡äº†ä¸è§†ç½‘è†œç›¸åŒçš„æµ‹è¯•ï¼Œæ­£ç¡®åœ°é¢„æµ‹äº†ä¸‰å…ƒç»„ç›¸å…³æ€§ã€$N$ ä¸ªç»†èƒä¸­æœ‰ $K$ ä¸ªåŒæ—¶æ´»è·ƒçš„æ¦‚ç‡ï¼Œä»¥åŠ $N = 10$ ä¸ªç»†èƒå­ç»„ä¸­ç‰¹å®šæ´»åŠ¨æ¨¡å¼çš„æ¦‚ç‡ã€‚é€šè¿‡è·¨å¤šä¸ªå®éªŒä¼šè¯æ‰©å±•æ­¤åˆ†æï¼Œå¯ä»¥æ£€æµ‹åˆ°åŠ¨ç‰©å­¦ä¹ å‚ä¸ä¸åŒä»»åŠ¡æ—¶è€¦åˆçŸ©é˜µ $J_{ij}$ çš„å˜åŒ–ã€‚è¿™äº›å˜åŒ–é›†ä¸­åœ¨ç»†èƒå­é›†ä¸­ï¼Œè¿™äº›ç»†èƒåœ¨ä¼šè¯ä¹‹é—´çš„ç¡çœ æœŸé—´ä¹Ÿè¢«ä¼˜å…ˆé‡æ–°æ¿€æ´»ã€‚æˆ‘ä»¬åº”è¯¥å°å¿ƒä¸è¦å¯¹ä»è¿™äº›åˆ†æä¸­å‡ºç°çš„ä¼Šè¾›æ¨¡å‹ç»™äºˆè¿‡äºæœºæ¢°çš„è§£é‡Šï¼Œä½†ä»¤äººå…´å¥‹çš„æ˜¯çœ‹åˆ°æ¨¡å‹çš„ç»“æ„ä¸ç½‘ç»œä¸­ç‹¬ç«‹å¯æµ‹é‡çš„åŠŸèƒ½åŠ¨æ€ç›¸è¿æ¥ã€‚å³ä½¿åœ¨å¤§è„‘çš®å±‚æœ€è¿œçš„åŒºåŸŸï¼Œæˆ‘ä»¬ç”¨äºæ€è€ƒã€è®¡åˆ’å’Œå†³ç­–çš„å¤§è„‘åŒºåŸŸä¹Ÿæ˜¯å¦‚æ­¤ã€‚\nFIG. 13 Pairwise maximum entropy models describe collective behavior of $N = 37$ neurons in prefrontal cortex (Tavoni et al., 2017). (A) Observed vs predicted triplet correlations among all neurons. Training results (blue) are predictions from the same segment of the experiment where the pairwise correlations were measured; test results (red) are in a different segment of the experiment. (B) Probability that $K$ out of $N$ neurons are active simultaneously, comparing predictions of the model with data in training and test segments. (C) Rate at which patterns of spiking and silence appear in a subset of ten neurons, comparing predicted vs observed rates in an independent model (cyan) and in the pairwise model (blue).\nå›¾ 13 æˆå¯¹æœ€å¤§ç†µæ¨¡å‹æè¿°å‰é¢å¶çš®å±‚ä¸­ $N = 37$ ä¸ªç¥ç»å…ƒçš„é›†ä½“è¡Œä¸ºï¼ˆTavoni ç­‰äººï¼Œ2017ï¼‰ã€‚ï¼ˆAï¼‰æ‰€æœ‰ç¥ç»å…ƒä¹‹é—´è§‚å¯Ÿåˆ°çš„ä¸é¢„æµ‹çš„ä¸‰å…ƒç»„ç›¸å…³æ€§ã€‚è®­ç»ƒç»“æœï¼ˆè“è‰²ï¼‰æ˜¯ä»å®éªŒçš„åŒä¸€æ®µæµ‹é‡æˆå¯¹ç›¸å…³æ€§çš„é¢„æµ‹ï¼›æµ‹è¯•ç»“æœï¼ˆçº¢è‰²ï¼‰åœ¨å®éªŒçš„ä¸åŒéƒ¨åˆ†ã€‚ï¼ˆBï¼‰$N$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªåŒæ—¶æ´»è·ƒçš„æ¦‚ç‡ï¼Œå°†æ¨¡å‹çš„é¢„æµ‹ä¸è®­ç»ƒå’Œæµ‹è¯•éƒ¨åˆ†çš„æ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚ï¼ˆCï¼‰åœ¨åä¸ªç¥ç»å…ƒå­é›†ä¸­å°–å³°å’Œé™é»˜æ¨¡å¼å‡ºç°çš„é€Ÿç‡ï¼Œåœ¨ç‹¬ç«‹æ¨¡å‹ï¼ˆé’è‰²ï¼‰å’Œæˆå¯¹æ¨¡å‹ï¼ˆè“è‰²ï¼‰ä¸­æ¯”è¾ƒé¢„æµ‹ä¸è§‚å¯Ÿåˆ°çš„é€Ÿç‡ã€‚\nThe Ising model also gives us a way of exploring how the network would respond to hypothetical perturbations (Tavoni et al., 2016). If we increase the magnetic field uniformly across all the cells in the population of prefrontal neurons, the predicted changes in activity are far from uniform. For some cells the response and the derivative of the response (susceptibility) are on a scale expected if neurons respond independently to applied fields, but there are groups of cells that coâ€“activate much more, with susceptibilities peaking at intermediate fields. It is tempting to think that these groups of cells have some functional significance, and this is supported by the fact that in the real data (with no fictitious fields) the groups of cells identified in this way remain coâ€“activated over relatively long periods of time.\nIsing æ¨¡å‹è¿˜ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§æ¢ç´¢ç½‘ç»œå¦‚ä½•å“åº”å‡è®¾æ‰°åŠ¨çš„æ–¹æ³•ï¼ˆTavoni ç­‰äººï¼Œ2016ï¼‰ã€‚å¦‚æœæˆ‘ä»¬åœ¨å‰é¢å¶ç¥ç»å…ƒç¾¤ä½“ä¸­çš„æ‰€æœ‰ç»†èƒä¸Šå‡åŒ€å¢åŠ ç£åœºï¼Œé¢„æµ‹çš„æ´»åŠ¨å˜åŒ–è¿œéå‡åŒ€ã€‚å¯¹äºæŸäº›ç»†èƒï¼Œå“åº”åŠå…¶å“åº”çš„å¯¼æ•°ï¼ˆæ˜“æ„Ÿæ€§ï¼‰å¤„äºé¢„æœŸçš„å°ºåº¦ä¸Šï¼Œå¦‚æœç¥ç»å…ƒç‹¬ç«‹åœ°å“åº”æ–½åŠ çš„åœºï¼Œä½†æœ‰ä¸€äº›ç»†èƒç¾¤ä½“å…±åŒæ¿€æ´»å¾—æ›´å¤šï¼Œåœ¨ä¸­é—´åœºå¤„æ˜“æ„Ÿæ€§è¾¾åˆ°å³°å€¼ã€‚ä»¤äººæƒ³èµ·çš„æ˜¯ï¼Œè¿™äº›ç»†èƒç¾¤ä½“å…·æœ‰æŸç§åŠŸèƒ½æ„ä¹‰ï¼Œè¿™å¾—åˆ°äº†è¿™æ ·ä¸€ä¸ªäº‹å®çš„æ”¯æŒï¼šåœ¨çœŸå®æ•°æ®ä¸­ï¼ˆæ²¡æœ‰è™šæ„åœºï¼‰ï¼Œä»¥è¿™ç§æ–¹å¼è¯†åˆ«çš„ç»†èƒç¾¤ä½“åœ¨ç›¸å¯¹è¾ƒé•¿çš„æ—¶é—´å†…ä¿æŒå…±åŒæ¿€æ´»ã€‚\nAt the opposite extreme of organismal complexity, the worm C. elegans is an attractive target for these analyses because one can record not just from a large number of cells but from a large fraction of the entire brain at single cell resolution (Â§III.C). A major challenge is that these neurons do not generate discrete action potentials or bursts, so the signal are not naturally binary. A first step was to discretize the continuous fluorescence signals into three levels, and construct a Pottsâ€“like model that matched the population of each state and the probabilities that pairs of neurons are in the same state (Chen et al., 2019). Although these early data sets were limited, this simple model succeeded in predicting offâ€“diagonal elements of the correlation matrix that were unconstrained, the probability that $K$ of $N$ neurons are in the same state, and the relative probabilities of different states in relation to the effective fields generated by the rest of the network. The fact that the same statistical physics approaches work in worms and in mammalian cortex is encouraging, though we should see more compelling tests with the next generation of experiments.\nåœ¨æœ‰æœºä½“å¤æ‚æ€§çš„ç›¸åæç«¯ï¼Œçº¿è™« C. elegans æ˜¯è¿™äº›åˆ†æçš„ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„ç›®æ ‡ï¼Œå› ä¸ºäººä»¬ä¸ä»…å¯ä»¥ä»å¤§é‡ç»†èƒä¸­è®°å½•ï¼Œè¿˜å¯ä»¥ä»æ•´ä¸ªå¤§è„‘çš„å¤§éƒ¨åˆ†åŒºåŸŸä»¥å•ç»†èƒåˆ†è¾¨ç‡è¿›è¡Œè®°å½•ï¼ˆÂ§III.Cï¼‰ã€‚ä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜æ˜¯è¿™äº›ç¥ç»å…ƒä¸ä¼šäº§ç”Ÿç¦»æ•£çš„åŠ¨ä½œç”µä½æˆ–çˆ†å‘ï¼Œå› æ­¤ä¿¡å·ä¸æ˜¯è‡ªç„¶äºŒè¿›åˆ¶çš„ã€‚ç¬¬ä¸€æ­¥æ˜¯å°†è¿ç»­çš„è§å…‰ä¿¡å·ç¦»æ•£åŒ–ä¸ºä¸‰ä¸ªæ°´å¹³ï¼Œå¹¶æ„å»ºä¸€ä¸ª ç±» Potts çš„æ¨¡å‹ï¼Œä»¥åŒ¹é…æ¯ä¸ªçŠ¶æ€çš„äººå£ä»¥åŠæˆå¯¹ç¥ç»å…ƒå¤„äºåŒä¸€çŠ¶æ€çš„æ¦‚ç‡ï¼ˆChen ç­‰äººï¼Œ2019ï¼‰ã€‚å°½ç®¡è¿™äº›æ—©æœŸæ•°æ®é›†æœ‰é™ï¼Œä½†è¿™ä¸ªç®€å•çš„æ¨¡å‹æˆåŠŸåœ°é¢„æµ‹äº†æœªå—çº¦æŸçš„ç›¸å…³çŸ©é˜µçš„éå¯¹è§’å…ƒç´ ï¼Œ$N$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªå¤„äºåŒä¸€çŠ¶æ€çš„æ¦‚ç‡ï¼Œä»¥åŠä¸ç½‘ç»œå…¶ä½™éƒ¨åˆ†äº§ç”Ÿçš„æœ‰æ•ˆåœºç›¸å…³çš„ä¸åŒçŠ¶æ€çš„ç›¸å¯¹æ¦‚ç‡ã€‚åŒæ ·çš„ç»Ÿè®¡ç‰©ç†æ–¹æ³•åœ¨è •è™«å’Œå“ºä¹³åŠ¨ç‰©çš®å±‚ä¸­éƒ½æœ‰æ•ˆï¼Œè¿™ä»¤äººé¼“èˆï¼Œå°½ç®¡æˆ‘ä»¬åº”è¯¥é€šè¿‡ä¸‹ä¸€ä»£å®éªŒçœ‹åˆ°æ›´æœ‰è¯´æœåŠ›çš„æµ‹è¯•ã€‚\nA very different approach is to study networks of neurons that have been removed from the animal and kept alive in a dish. There is a long history of work on these â€œcultured networks,â€ and as noted above (Â§III.A) some of the earliest experiments recording from many neurons were done with networks that had been grown onto an array of electrodes (Pine and Gilbert, 1982). Considerable interest was generated by the observation that patterns of activity in cultured networks of cortical neurons consist of â€œavalanchesâ€ that exhibit at least some degree of scale invariance (Â§VI.A). Recent work returns to these data and shows that detailed patterns of spiking and silence are well described by pairwise maximum entropy models, reproducing triplet correlations and the probability that $K$ out of $N = 60$ neurons are active simultaneously (Sampaio Filho et al., 2024).\nä¸€ç§éå¸¸ä¸åŒçš„æ–¹æ³•æ˜¯ç ”ç©¶ä»åŠ¨ç‰©ä½“å†…ç§»é™¤å¹¶åœ¨åŸ¹å…»çš¿ä¸­ä¿æŒæ´»åŠ›çš„ç¥ç»å…ƒç½‘ç»œã€‚è¿™äº›â€œåŸ¹å…»ç½‘ç»œâ€æœ‰ç€æ‚ ä¹…çš„ç ”ç©¶å†å²ï¼Œå¦‚ä¸Šæ‰€è¿°ï¼ˆÂ§III.Aï¼‰ï¼Œä¸€äº›æœ€æ—©è®°å½•è®¸å¤šç¥ç»å…ƒçš„å®éªŒæ˜¯ä½¿ç”¨ç”Ÿé•¿åœ¨ç”µæé˜µåˆ—ä¸Šçš„ç½‘ç»œè¿›è¡Œçš„ï¼ˆPine å’Œ Gilbertï¼Œ1982ï¼‰ã€‚äººä»¬å¯¹è§‚å¯Ÿåˆ°çš®å±‚ç¥ç»å…ƒåŸ¹å…»ç½‘ç»œä¸­çš„æ´»åŠ¨æ¨¡å¼ç”±â€œé›ªå´©â€ç»„æˆäº§ç”Ÿäº†ç›¸å½“å¤§çš„å…´è¶£ï¼Œè¿™äº›é›ªå´©è‡³å°‘è¡¨ç°å‡ºæŸç§ç¨‹åº¦çš„å°ºåº¦ä¸å˜æ€§ï¼ˆÂ§VI.Aï¼‰ã€‚æœ€è¿‘çš„å·¥ä½œå›åˆ°äº†è¿™äº›æ•°æ®ï¼Œå¹¶æ˜¾ç¤ºå°–å³°å’Œé™é»˜çš„è¯¦ç»†æ¨¡å¼å¯ä»¥å¾ˆå¥½åœ°ç”¨æˆå¯¹æœ€å¤§ç†µæ¨¡å‹æ¥æè¿°ï¼Œé‡ç°äº†ä¸‰å…ƒç»„ç›¸å…³æ€§ä»¥åŠ $N = 60$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªåŒæ—¶æ´»è·ƒçš„æ¦‚ç‡ï¼ˆSampaio Filho ç­‰äººï¼Œ2024ï¼‰ã€‚\nAs a final example we consider populations of $N\\sim 100$ neurons in the mouse hippocampus (Meshulam et al., 2017). The hippocampus plays a central role in navigation and episodic memory, and is perhaps best known for its population of â€œplace cells,â€ neurons that are active only when the animal moves to a particular position in its environment. First discovered in rodents (Oâ€™Keefe and Dostrovsky, 1971), it is thought that the whole population of these cells together provides the animal with a cognitive map (Oâ€™Keefe and Nadel, 1978). More recent work shows how this structure extends to three dimensions, and across hundreds of meters in bats (Tsoar et al., 2011; Yartsev and Ulanovsky, 2013).\nä½œä¸ºæœ€åä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬è€ƒè™‘å°é¼ æµ·é©¬ä½“ä¸­ $N\\sim 100$ ä¸ªç¥ç»å…ƒçš„ç¾¤ä½“ï¼ˆMeshulam ç­‰äººï¼Œ2017ï¼‰ã€‚æµ·é©¬ä½“åœ¨å¯¼èˆªå’Œæƒ…èŠ‚è®°å¿†ä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ï¼Œæˆ–è®¸æœ€è‘—åçš„æ˜¯å…¶â€œä½ç½®ç»†èƒâ€ç¾¤ä½“ï¼Œè¿™äº›ç¥ç»å…ƒä»…åœ¨åŠ¨ç‰©ç§»åŠ¨åˆ°å…¶ç¯å¢ƒä¸­çš„ç‰¹å®šä½ç½®æ—¶æ‰æ´»è·ƒã€‚æœ€æ—©åœ¨å•®é½¿åŠ¨ç‰©ä¸­å‘ç°ï¼ˆOâ€™Keefe å’Œ Dostrovskyï¼Œ1971ï¼‰ï¼Œäººä»¬è®¤ä¸ºè¿™äº›ç»†èƒçš„æ•´ä¸ªç¾¤ä½“å…±åŒä¸ºåŠ¨ç‰©æä¾›äº†è®¤çŸ¥åœ°å›¾ï¼ˆOâ€™Keefe å’Œ Nadelï¼Œ1978ï¼‰ã€‚æœ€è¿‘çš„å·¥ä½œå±•ç¤ºäº†è¿™ç§ç»“æ„å¦‚ä½•æ‰©å±•åˆ°ä¸‰ç»´ï¼Œå¹¶è·¨è¶Šè™è ä¸­çš„æ•°ç™¾ç±³ï¼ˆTsoar ç­‰äººï¼Œ2011ï¼›Yartsev å’Œ Ulanovskyï¼Œ2013ï¼‰ã€‚\nAs the animal explores its environment, or runs along a virtual track, the mean activity of individual neurons is quite small, as in the examples above. Most pairs of neuron have negative correlations, as expected if activity is tied to the positionâ€”if each cell is active in a different place, then on average one cell being active means that other cells must be silent, generating anti- correlations. Indeed it is tempting to make a model of the hippocampus in which some positional signal is computed by the brain, with inputs from many regions, and each cell in the hippocampus is active or silent depending on the value of this positional signal. This model is specified by the â€œplace fieldsâ€ of each cell, the probability that a cell is active as a function of position, and these can be estimated directly from the data; given the place fields all other properties of the network are determined with no adjustable parameters.\nå½“åŠ¨ç‰©æ¢ç´¢å…¶ç¯å¢ƒæˆ–æ²¿ç€è™šæ‹Ÿè½¨é“è¿è¡Œæ—¶ï¼Œå•ä¸ªç¥ç»å…ƒçš„å¹³å‡æ´»åŠ¨ç›¸å½“å°ï¼Œå¦‚ä¸Šé¢çš„ä¾‹å­æ‰€ç¤ºã€‚å¤§å¤šæ•°ç¥ç»å…ƒå¯¹å…·æœ‰è´Ÿç›¸å…³æ€§ï¼Œè¿™ç¬¦åˆé¢„æœŸï¼Œå¦‚æœæ´»åŠ¨ä¸ä½ç½®ç›¸å…³â€”â€”å¦‚æœæ¯ä¸ªç»†èƒåœ¨ä¸åŒçš„ä½ç½®æ´»è·ƒï¼Œé‚£ä¹ˆå¹³å‡è€Œè¨€ï¼Œä¸€ä¸ªç»†èƒæ´»è·ƒæ„å‘³ç€å…¶ä»–ç»†èƒå¿…é¡»é™é»˜ï¼Œä»è€Œäº§ç”Ÿåç›¸å…³ã€‚å®é™…ä¸Šï¼Œæ„å»ºä¸€ä¸ªæµ·é©¬ä½“æ¨¡å‹æ˜¯å¾ˆè¯±äººçš„ï¼Œåœ¨è¯¥æ¨¡å‹ä¸­ï¼Œå¤§è„‘é€šè¿‡æ¥è‡ªè®¸å¤šåŒºåŸŸçš„è¾“å…¥è®¡ç®—æŸç§ä½ç½®ä¿¡å·ï¼Œæµ·é©¬ä½“ä¸­çš„æ¯ä¸ªç»†èƒæ ¹æ®è¯¥ä½ç½®ä¿¡å·çš„å€¼å¤„äºæ´»è·ƒæˆ–é™é»˜çŠ¶æ€ã€‚è¯¥æ¨¡å‹ç”±æ¯ä¸ªç»†èƒçš„â€œä½ç½®åœºâ€æŒ‡å®šï¼Œå³ç»†èƒä½œä¸ºä½ç½®å‡½æ•°æ´»è·ƒçš„æ¦‚ç‡ï¼Œè¿™äº›å¯ä»¥ç›´æ¥ä»æ•°æ®ä¸­ä¼°è®¡ï¼›ç»™å®šä½ç½®åœºï¼Œç½‘ç»œçš„æ‰€æœ‰å…¶ä»–å±æ€§éƒ½æ²¡æœ‰å¯è°ƒå‚æ•°åœ°ç¡®å®šã€‚\nThe place field of cell $i$ is defined by the average activity conditional on the position $x$ along a track,\n$$ \\langle\\sigma_{i}\\rangle_{x} = F_{i}(x) $$\nIf activity in each cell depends independently on position, then the pairwise correlations are driven by the fact that all cells experience the same x, drawn from some distribution $P (x)$ across the experiment. The quantitative prediction is that\n$$ \\begin{aligned} C_{ij} \u0026\\equiv \\langle\\sigma_{i}\\sigma_{j}\\rangle - \\langle\\sigma_{i}\\rangle\\langle\\sigma_{j}\\rangle \\\\ \u0026= \\int\\mathrm{d}x P(x)F_{i}(x)F_{j}(x) - \\left[\\int\\mathrm{d}x P(x)F_{i}(x)\\right]\\left[\\int\\mathrm{d}x P(x)F_{j}(x)\\right] \\end{aligned} $$\nThe covariance matrix elements $C_{ij}$ have a pattern that is qualitatively similar to the real data, but quantitatively very far off. In particular the eigenvalue spectrum of the matrix predicted in this way falls very rapidly, while the real spectrum has a slow, nearly powerâ€“law decay (Meshulam et al., 2017). This is a first hint that the neurons in the hippocampal network share information, and hence exhibit collective behavior, beyond just place.\nç»†èƒ $i$ çš„ä½ç½®åœºç”±æ²¿è½¨é“ä½ç½® $x$ æ¡ä»¶ä¸‹çš„å¹³å‡æ´»åŠ¨å®šä¹‰ï¼Œ\n$$ \\langle\\sigma_{i}\\rangle_{x} = F_{i}(x) $$\nå¦‚æœæ¯ä¸ªç»†èƒçš„æ´»åŠ¨ç‹¬ç«‹åœ°ä¾èµ–äºä½ç½®ï¼Œé‚£ä¹ˆæˆå¯¹ç›¸å…³æ€§æ˜¯ç”±æ‰€æœ‰ç»†èƒç»å†ç›¸åŒçš„ x é©±åŠ¨çš„ï¼Œè¿™äº› x æ˜¯ä»å®éªŒä¸­çš„æŸä¸ªåˆ†å¸ƒ $P (x)$ ä¸­æŠ½å–çš„ã€‚å®šé‡é¢„æµ‹æ˜¯\n$$ \\begin{aligned} C_{ij} \u0026\\equiv \\langle\\sigma_{i}\\sigma_{j}\\rangle - \\langle\\sigma_{i}\\rangle\\langle\\sigma_{j}\\rangle \\\\ \u0026= \\int\\mathrm{d}x P(x)F_{i}(x)F_{j}(x) - \\left[\\int\\mathrm{d}x P(x)F_{i}(x)\\right]\\left[\\int\\mathrm{d}x P(x)F_{j}(x)\\right] \\end{aligned} $$\nåæ–¹å·®çŸ©é˜µå…ƒç´  $C_{ij}$ å…·æœ‰ä¸çœŸå®æ•°æ®å®šæ€§ä¸Šç›¸ä¼¼çš„æ¨¡å¼ï¼Œä½†åœ¨å®šé‡ä¸Šéå¸¸åç¦»ã€‚ç‰¹åˆ«æ˜¯ï¼Œä»¥è¿™ç§æ–¹å¼é¢„æµ‹çš„çŸ©é˜µçš„ç‰¹å¾å€¼è°±ä¸‹é™å¾—éå¸¸å¿«ï¼Œè€ŒçœŸå®è°±å…·æœ‰ç¼“æ…¢ã€è¿‘ä¼¼å¹‚å¾‹è¡°å‡ï¼ˆMeshulam ç­‰äººï¼Œ2017ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªåˆæ­¥çš„æš—ç¤ºï¼Œè¡¨æ˜æµ·é©¬ä½“ç½‘ç»œä¸­çš„ç¥ç»å…ƒå…±äº«ä¿¡æ¯ï¼Œå› æ­¤è¡¨ç°å‡ºè¶…è¶Šä½ç½®çš„é›†ä½“è¡Œä¸ºã€‚\nA new generation of experiments monitoring 1000+ neurons in the hippocampus provides unique opportunities for theory, as discussed in Â§Â§V and VII below. Here we want to emphasize the way in which collective dynamics emerge from maximum entropy models of $N\\sim 100$ cells. Equations (49, 50) and Figure 11 remind us that models for the joint distribution of activity in a neural population also predict the probability for one neuron to be active given the state of the rest of the network. We can go through the same exercise for a population of cells in the hippocampus: construct the pairwise maximum entropy model, and for each neuron at each moment compute the probability that it will be active given the state of all the other neurons; results are shown in Fig 14A.\næ–°ä¸€ä»£ç›‘æµ‹æµ·é©¬ä½“ä¸­ 1000 å¤šä¸ªç¥ç»å…ƒçš„å®éªŒä¸ºç†è®ºæä¾›äº†ç‹¬ç‰¹çš„æœºä¼šï¼Œå¦‚ä¸‹æ–‡ Â§Â§V å’Œ VII æ‰€è®¨è®ºçš„é‚£æ ·ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æƒ³å¼ºè°ƒé›†ä½“åŠ¨åŠ›å­¦å¦‚ä½•ä» $N\\sim 100$ ä¸ªç»†èƒçš„æœ€å¤§ç†µæ¨¡å‹ä¸­å‡ºç°ã€‚æ–¹ç¨‹ï¼ˆ49ï¼Œ50ï¼‰å’Œå›¾ 11 æé†’æˆ‘ä»¬ï¼Œç¥ç»ç¾¤ä½“ä¸­æ´»åŠ¨è”åˆåˆ†å¸ƒçš„æ¨¡å‹è¿˜é¢„æµ‹äº†åœ¨ç½‘ç»œå…¶ä½™éƒ¨åˆ†çŠ¶æ€ç»™å®šçš„æƒ…å†µä¸‹ä¸€ä¸ªç¥ç»å…ƒæ´»è·ƒçš„æ¦‚ç‡ã€‚æˆ‘ä»¬å¯ä»¥å¯¹æµ·é©¬ä½“ä¸­çš„ç»†èƒç¾¤ä½“è¿›è¡ŒåŒæ ·çš„ç»ƒä¹ ï¼šæ„å»ºæˆå¯¹æœ€å¤§ç†µæ¨¡å‹ï¼Œå¹¶åœ¨æ¯ä¸ªæ—¶åˆ»ä¸ºæ¯ä¸ªç¥ç»å…ƒè®¡ç®—åœ¨æ‰€æœ‰å…¶ä»–ç¥ç»å…ƒçŠ¶æ€ç»™å®šçš„æƒ…å†µä¸‹å®ƒå°†æ´»è·ƒçš„æ¦‚ç‡ï¼›ç»“æœå¦‚å›¾ 14A æ‰€ç¤ºã€‚\nCollective behavior in the mouse hippocampus (Meshulam et al., 2017). (A) Predicted probability of activity for single neurons, computed from the effective field in the pairwise maximum entropy model. Focus is on 32 place cells that should be active in sequence as the mouse runs along a virtual track. During the first run, cells 21â€“25 are predicted to â€œmissâ€ their place fields, but all cells are predicted to be active in the second run. (B) Real data of place cell activity during two runs down the linear track, in the same time window as (A) and (C); note the missed events for cells 21â€“25 in the first run. (C) Predicted probability from the independent place cell model. There is no indication of when fields should be missed.\nå°é¼ æµ·é©¬ä½“ä¸­çš„é›†ä½“è¡Œä¸ºï¼ˆMeshulam ç­‰äººï¼Œ2017ï¼‰ã€‚ï¼ˆAï¼‰ä»æˆå¯¹æœ€å¤§ç†µæ¨¡å‹ä¸­çš„æœ‰æ•ˆåœºè®¡ç®—çš„å•ä¸ªç¥ç»å…ƒçš„é¢„æµ‹æ´»åŠ¨æ¦‚ç‡ã€‚é‡ç‚¹å…³æ³¨ 32 ä¸ªä½ç½®ç»†èƒï¼Œå½“å°é¼ æ²¿ç€è™šæ‹Ÿè½¨é“è¿è¡Œæ—¶ï¼Œè¿™äº›ç»†èƒåº”è¯¥æŒ‰é¡ºåºæ´»è·ƒã€‚åœ¨ç¬¬ä¸€æ¬¡è¿è¡ŒæœŸé—´ï¼Œé¢„æµ‹ç»†èƒ 21â€“25 ä¼šâ€œé”™è¿‡â€å®ƒä»¬çš„ä½ç½®åœºï¼Œä½†åœ¨ç¬¬äºŒæ¬¡è¿è¡Œä¸­é¢„æµ‹æ‰€æœ‰ç»†èƒéƒ½ä¼šæ´»è·ƒã€‚ï¼ˆBï¼‰ä¸ï¼ˆAï¼‰å’Œï¼ˆCï¼‰ç›¸åŒæ—¶é—´çª—å£å†…çº¿æ€§è½¨é“ä¸Šä¸¤æ¬¡è¿è¡ŒæœŸé—´ä½ç½®ç»†èƒæ´»åŠ¨çš„çœŸå®æ•°æ®ï¼›è¯·æ³¨æ„ç¬¬ä¸€æ¬¡è¿è¡Œä¸­ç»†èƒ 21â€“25 çš„é”™è¿‡äº‹ä»¶ã€‚ï¼ˆCï¼‰æ¥è‡ªç‹¬ç«‹ä½ç½®ç»†èƒæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ã€‚æ²¡æœ‰è¿¹è±¡è¡¨æ˜ä½•æ—¶åº”è¯¥é”™è¿‡åœºã€‚\nWe see in Figure 14A that, roughly speaking, cells are predicted to be active in sequence. This makes sense since these are place cells, and the mouse is running at nearly constant speed along a virtual track, so cells with place fields arrayed along the track should be activated one after the other. Interestingly the calculation leading to this prediction makes no reference to the (virtual) position of the mouse, or even to the idea of place fields, but only to the dependence of activity in one cell on the rest of the network. In this window of time the mouse actually makes two trips along the track, and perhaps surprisingly the predictions for the two trips are different. On the first trip it is predicted that several of the cells will â€œmissâ€ their place fields, while all cells should be active in sequence on the second trip. This is exactly what we see in the data (Fig 14B). If neurons were driven only by the animalâ€™s position this wouldnâ€™t happen (Fig 14C). Thus what might have seemed like unpredictable variation really reflects the collective behavior of the network, and is captured very well by the Ising model, with no additional parameters. We return to Ising models for the hippocampus in Â§V below.\næˆ‘ä»¬åœ¨å›¾ 14A ä¸­çœ‹åˆ°ï¼Œå¤§è‡´æ¥è¯´ï¼Œç»†èƒè¢«é¢„æµ‹ä¸ºæŒ‰é¡ºåºæ´»è·ƒã€‚è¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºè¿™äº›æ˜¯ä½ç½®ç»†èƒï¼Œå°é¼ ä»¥å‡ ä¹æ’å®šçš„é€Ÿåº¦æ²¿ç€è™šæ‹Ÿè½¨é“è¿è¡Œï¼Œå› æ­¤æ²¿è½¨é“æ’åˆ—çš„ä½ç½®åœºçš„ç»†èƒåº”è¯¥ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°è¢«æ¿€æ´»ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¯¼è‡´è¿™ä¸€é¢„æµ‹çš„è®¡ç®—æ²¡æœ‰å‚è€ƒå°é¼ çš„ï¼ˆè™šæ‹Ÿï¼‰ä½ç½®ï¼Œç”šè‡³æ²¡æœ‰å‚è€ƒä½ç½®åœºçš„æ¦‚å¿µï¼Œè€Œåªæ˜¯å‚è€ƒä¸€ä¸ªç»†èƒå¯¹ç½‘ç»œå…¶ä½™éƒ¨åˆ†æ´»åŠ¨çš„ä¾èµ–ã€‚åœ¨è¿™æ®µæ—¶é—´å†…ï¼Œå°é¼ å®é™…ä¸Šæ²¿è½¨é“è¿›è¡Œäº†ä¸¤æ¬¡æ—…è¡Œï¼Œæˆ–è®¸ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä¸¤æ¬¡æ—…è¡Œçš„é¢„æµ‹æ˜¯ä¸åŒçš„ã€‚åœ¨ç¬¬ä¸€æ¬¡æ—…è¡Œä¸­ï¼Œé¢„æµ‹æœ‰å‡ ä¸ªç»†èƒä¼šâ€œé”™è¿‡â€å®ƒä»¬çš„ä½ç½®åœºï¼Œè€Œåœ¨ç¬¬äºŒæ¬¡æ—…è¡Œä¸­æ‰€æœ‰ç»†èƒéƒ½åº”è¯¥æŒ‰é¡ºåºæ´»è·ƒã€‚è¿™æ­£æ˜¯æˆ‘ä»¬åœ¨æ•°æ®ä¸­çœ‹åˆ°çš„ï¼ˆå›¾ 14Bï¼‰ã€‚å¦‚æœç¥ç»å…ƒä»…ç”±åŠ¨ç‰©çš„ä½ç½®é©±åŠ¨ï¼Œè¿™ç§æƒ…å†µå°±ä¸ä¼šå‘ç”Ÿï¼ˆå›¾ 14Cï¼‰ã€‚å› æ­¤ï¼Œçœ‹ä¼¼ä¸å¯é¢„æµ‹çš„å˜åŒ–å®é™…ä¸Šåæ˜ äº†ç½‘ç»œçš„é›†ä½“è¡Œä¸ºï¼Œå¹¶ä¸”è¢« Ising æ¨¡å‹å¾ˆå¥½åœ°æ•æ‰åˆ°äº†ï¼Œæ²¡æœ‰é¢å¤–çš„å‚æ•°ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„ Â§V ä¸­å›åˆ°æµ·é©¬ä½“çš„ Ising æ¨¡å‹ã€‚\nDoing more and doing less Is there any sense in which maximum entropy models are â€œbetterâ€ than alternative models? The pairwise maximum entropy models are singled out because they have the minimal structure needed to match the mean activity and twoâ€“point correlations in the network. But how different are they from other models that would also match these data? We could imagine, for example, that once we specify the full matrix of correlations then the set of allowed models is very tightly clustered in its predictions about higher order structure in the patterns of activity, in which case saying that these models â€œworkâ€ doesnâ€™t say much about the underlying physics.\næœ€å¤§ç†µæ¨¡å‹åœ¨æŸç§æ„ä¹‰ä¸Šæ˜¯å¦æ¯”æ›¿ä»£æ¨¡å‹â€œæ›´å¥½â€ï¼Ÿæˆå¯¹æœ€å¤§ç†µæ¨¡å‹ä¹‹æ‰€ä»¥è¢«å•ç‹¬æŒ‘é€‰å‡ºæ¥ï¼Œæ˜¯å› ä¸ºå®ƒä»¬å…·æœ‰åŒ¹é…ç½‘ç»œä¸­å¹³å‡æ´»åŠ¨å’Œä¸¤ç‚¹ç›¸å…³æ€§æ‰€éœ€çš„æœ€å°ç»“æ„ã€‚ä½†æ˜¯ï¼Œå®ƒä»¬ä¸å…¶ä»–ä¹Ÿèƒ½åŒ¹é…è¿™äº›æ•°æ®çš„æ¨¡å‹æœ‰å¤šå¤§ä¸åŒå‘¢ï¼Ÿä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡ï¼Œä¸€æ—¦æˆ‘ä»¬æŒ‡å®šäº†å®Œæ•´çš„ç›¸å…³çŸ©é˜µï¼Œé‚£ä¹ˆå…è®¸çš„æ¨¡å‹é›†åœ¨å…¶å¯¹æ´»åŠ¨æ¨¡å¼ä¸­æ›´é«˜é˜¶ç»“æ„çš„é¢„æµ‹ä¸­å°±ä¼šéå¸¸ç´§å¯†åœ°èšé›†ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯´è¿™äº›æ¨¡å‹â€œæœ‰æ•ˆâ€å¹¶ä¸èƒ½è¯´æ˜æ½œåœ¨çš„ç‰©ç†å­¦ã€‚\nOne can build a statistical mechanics on the space of probability distributions $p(\\vec{\\sigma})$, defining a â€œversion spaceâ€ by all the models that match a given set of pairwise correlations within some tolerance $\\epsilon$. We can construct a Boltzmann weight over this space in which the entropy of the underlying distribution plays the role of the (negative) energy,\n$$ Q[p(\\vec{\\sigma})]\\propto \\delta \\left[1-\\sum_{\\vec{\\sigma}}p(\\vec{\\sigma})\\right]\\mathbf{U}_{\\epsilon}[p(\\vec{\\sigma};\\{m_{i};C_{ij}\\})]\\times \\exp\\left[-\\beta\\sum_{\\vec{\\sigma}}p(\\vec{\\sigma})\\ln{p(\\vec{\\sigma})}\\right], $$\nwhere the first term in the product enforces normalization, the second term selects distributions that match expectation values within $\\epsilon$, and the last term is the Boltzmann weight (Obuchi et al., 2015). Note that this is the maximum entropy distribution of distributions (!) consistent with a particular mean value of the entropy and the measured expectation values. As $\\beta\\to\\infty$, $Q$ condenses around the maximum entropy distribution, while as $\\beta\\to 0$ all distributions consistent with the expectation values are given equal weight.\næˆ‘ä»¬å¯ä»¥åœ¨æ¦‚ç‡åˆ†å¸ƒ $p(\\vec{\\sigma})$ çš„ç©ºé—´ä¸Šæ„å»ºç»Ÿè®¡åŠ›å­¦ï¼Œé€šè¿‡åœ¨æŸä¸ªå®¹å·® $\\epsilon$ èŒƒå›´å†…åŒ¹é…ç»™å®šæˆå¯¹ç›¸å…³æ€§çš„ä¸€ç»„æ¨¡å‹æ¥å®šä¹‰â€œç‰ˆæœ¬ç©ºé—´â€ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ªç©ºé—´ä¸­æ„å»ºä¸€ä¸ªç»å°”å…¹æ›¼æƒé‡ï¼Œå…¶ä¸­åŸºç¡€åˆ†å¸ƒçš„ç†µèµ·ç€ï¼ˆè´Ÿï¼‰èƒ½é‡çš„ä½œç”¨ï¼Œ\n$$ Q[p(\\vec{\\sigma})]\\propto \\delta \\left[1-\\sum_{\\vec{\\sigma}}p(\\vec{\\sigma})\\right]\\mathbf{U}_{\\epsilon}[p(\\vec{\\sigma};\\{m_{i};C_{ij}\\})]\\times \\exp\\left[-\\beta\\sum_{\\vec{\\sigma}}p(\\vec{\\sigma})\\ln{p(\\vec{\\sigma})}\\right], $$\nå…¶ä¸­ä¹˜ç§¯ä¸­çš„ç¬¬ä¸€é¡¹å¼ºåˆ¶å½’ä¸€åŒ–ï¼Œç¬¬äºŒé¡¹é€‰æ‹©åœ¨ $\\epsilon$ èŒƒå›´å†…åŒ¹é…æœŸæœ›å€¼çš„åˆ†å¸ƒï¼Œæœ€åä¸€é¡¹æ˜¯ç»å°”å…¹æ›¼æƒé‡ï¼ˆObuchi ç­‰äººï¼Œ2015ï¼‰ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯ä¸ç†µçš„ç‰¹å®šå¹³å‡å€¼å’Œæµ‹é‡çš„æœŸæœ›å€¼ä¸€è‡´çš„åˆ†å¸ƒçš„æœ€å¤§ç†µåˆ†å¸ƒï¼ˆï¼ï¼‰ã€‚å½“ $\\beta\\to\\infty$ æ—¶ï¼Œ$Q$ åœ¨æœ€å¤§ç†µåˆ†å¸ƒå‘¨å›´å‡ç»“ï¼Œè€Œå½“ $\\beta\\to 0$ æ—¶ï¼Œä¸æœŸæœ›å€¼ä¸€è‡´çš„æ‰€æœ‰åˆ†å¸ƒéƒ½è¢«èµ‹äºˆç›¸ç­‰çš„æƒé‡ã€‚\nIf the matrix $C_{ij}$ is chosen at random then one can use methods from the statistical mechanics of disordered systems to develop an analytic theory that compares the similarity of the true distribution to those drawn at random from the ensembles of distributions at varying $\\beta$. In this random setting the maximum entropy models are not special, and in a rough sense all models that match the lowâ€“order correlations are equally good approximations (Obuchi et al., 2015). Importantly this is not true for the real data on retinal neurons, where the maximum entropy model gives a better description than the typical model that matches the pairwise correlations, and this advantage grows with $N$: â€œfor large networks it is better to pick the most random model than to pick a model at randomâ€ (Ferrari et al., 2017).\nå¦‚æœéšæœºé€‰æ‹©çŸ©é˜µ $C_{ij}$ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨æ— åºç³»ç»Ÿç»Ÿè®¡åŠ›å­¦çš„æ–¹æ³•æ¥å¼€å‘ä¸€ç§åˆ†æç†è®ºï¼Œå°†çœŸå®åˆ†å¸ƒä¸ä»ä¸åŒ $\\beta$ çš„åˆ†å¸ƒé›†åˆä¸­éšæœºæŠ½å–çš„åˆ†å¸ƒè¿›è¡Œæ¯”è¾ƒã€‚åœ¨è¿™ç§éšæœºè®¾ç½®ä¸­ï¼Œæœ€å¤§ç†µæ¨¡å‹å¹¶ä¸ç‰¹æ®Šï¼Œä»ç²—ç•¥æ„ä¹‰ä¸Šè®²ï¼Œæ‰€æœ‰åŒ¹é…ä½é˜¶ç›¸å…³æ€§çš„æ¨¡å‹éƒ½æ˜¯åŒæ ·å¥½çš„è¿‘ä¼¼ï¼ˆObuchi ç­‰äººï¼Œ2015ï¼‰ã€‚é‡è¦çš„æ˜¯ï¼Œå¯¹äºè§†ç½‘è†œç¥ç»å…ƒçš„çœŸå®æ•°æ®å¹¶éå¦‚æ­¤ï¼Œå…¶ä¸­æœ€å¤§ç†µæ¨¡å‹æ¯”åŒ¹é…æˆå¯¹ç›¸å…³æ€§çš„å…¸å‹æ¨¡å‹æä¾›äº†æ›´å¥½çš„æè¿°ï¼Œå¹¶ä¸”è¿™ç§ä¼˜åŠ¿éšç€ $N$ çš„å¢åŠ è€Œå¢é•¿ï¼šâ€œå¯¹äºå¤§å‹ç½‘ç»œæ¥è¯´ï¼Œé€‰æ‹©æœ€éšæœºçš„æ¨¡å‹æ¯”éšæœºé€‰æ‹©ä¸€ä¸ªæ¨¡å‹æ›´å¥½â€ï¼ˆFerrari ç­‰äººï¼Œ2017ï¼‰ã€‚\nOne way that we could do more in describing the patterns of neural activity is to address their time dependence more explicitly. In particular for the retina we know that the network is being driven by visual inputs. We can repeat the movie many times and ask about the mean activity of each cell at a given moment in the movie, $\\langle\\sigma_{i}(t)\\rangle$. In addition, as before, we can measure the correlations between neurons at the same moment in time, $\\langle\\sigma_{i}(t)\\sigma_{j}(t)\\rangle$. Thus we want to find a model for the distribution over sequences or trajetcories of network states $P_{\\text{traj}}[\\vec{\\sigma}(t)]$ that has maximal entropy and matches the timeâ€“dependent mean activity\n$$ \\begin{aligned} m_{i}(t) \u0026\\equiv \\langle\\sigma_{i}(t)\\rangle_{\\text{ext}}\\\\ \u0026= \\langle \\sigma_{i}(t)\\rangle_{P_{\\text{traj}}} \\end{aligned} $$\nas well as the time averaged equal time correlations\n$$ \\begin{aligned} C_{ij} \u0026\\equiv \\frac{1}{\\widetilde{T}}\\sum_{t}\\langle\\delta\\sigma_{i}(t)\\delta\\sigma_{j}(t)\\rangle_{\\text{ext}}\\\\ \u0026= \\frac{1}{\\widetilde{T}}\\sum_{t}\\langle\\delta\\sigma_{i}(t)\\delta\\sigma_{j}(t)\\rangle_{P_{\\text{traj}}} \\end{aligned} $$\nwhere $\\widetilde{T}$ is the duration of our observations in units of the time bin width $\\Delta\\tau$.\næˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´æ˜ç¡®åœ°è§£å†³å®ƒä»¬çš„æ—¶é—´ä¾èµ–æ€§æ¥æ›´å¥½åœ°æè¿°ç¥ç»æ´»åŠ¨æ¨¡å¼ã€‚ç‰¹åˆ«æ˜¯å¯¹äºè§†ç½‘è†œï¼Œæˆ‘ä»¬çŸ¥é“ç½‘ç»œæ­£åœ¨è¢«è§†è§‰è¾“å…¥é©±åŠ¨ã€‚æˆ‘ä»¬å¯ä»¥å¤šæ¬¡é‡å¤ç”µå½±ï¼Œå¹¶è¯¢é—®ç”µå½±ä¸­ç»™å®šæ—¶åˆ»æ¯ä¸ªç»†èƒçš„å¹³å‡æ´»åŠ¨ï¼Œ$\\langle\\sigma_{i}(t)\\rangle$ã€‚æ­¤å¤–ï¼Œå’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹é‡ç¥ç»å…ƒåœ¨åŒä¸€æ—¶é—´ç‚¹çš„ç›¸å…³æ€§ï¼Œ$\\langle\\sigma_{i}(t)\\sigma_{j}(t)\\rangle$ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æƒ³è¦æ‰¾åˆ°ä¸€ä¸ªå…³äºç½‘ç»œçŠ¶æ€åºåˆ—æˆ–è½¨è¿¹çš„åˆ†å¸ƒæ¨¡å‹ $P_{\\text{traj}}[\\vec{\\sigma}(t)]$ï¼Œè¯¥æ¨¡å‹å…·æœ‰æœ€å¤§ç†µå¹¶åŒ¹é…æ—¶é—´ä¾èµ–çš„å¹³å‡æ´»åŠ¨\n$$ \\begin{aligned} m_{i}(t) \u0026\\equiv \\langle\\sigma_{i}(t)\\rangle_{\\text{ext}}\\\\ \u0026= \\langle \\sigma_{i}(t)\\rangle_{P_{\\text{traj}}} \\end{aligned} $$\nä»¥åŠæ—¶é—´å¹³å‡çš„ç­‰æ—¶ç›¸å…³æ€§\n$$ \\begin{aligned} C_{ij} \u0026\\equiv \\frac{1}{\\widetilde{T}}\\sum_{t}\\langle\\delta\\sigma_{i}(t)\\delta\\sigma_{j}(t)\\rangle_{\\text{ext}}\\\\ \u0026= \\frac{1}{\\widetilde{T}}\\sum_{t}\\langle\\delta\\sigma_{i}(t)\\delta\\sigma_{j}(t)\\rangle_{P_{\\text{traj}}} \\end{aligned} $$\nå…¶ä¸­ $\\widetilde{T}$ æ˜¯æˆ‘ä»¬ä»¥æ—¶é—´ç®±å®½åº¦ $\\Delta\\tau$ ä¸ºå•ä½çš„è§‚å¯ŸæŒç»­æ—¶é—´ã€‚\nThis is an instance of the general structure presented in Â§IV.A, where the first set of observables is of the form\n$$ \\{f_{\\mu}\\}\\rightarrow\\{f_{i,t}\\} = \\{\\sigma_{i}(t)\\} $$\nTo constrain the expectation value of each of these terms we need a separate Lagrange multiplier, and as before we think of these as local field that now depend on time, $\\lambda_{i,t} = h_{i}(t)$. In addition we have observables of the form\n$$ \\{f_{\\mu}\\} \\rightarrow \\{f_{ij}\\} = \\left\\{\\sum_{t}\\sigma_{i}(t)\\sigma_{j}(t)\\right\\} $$\nand for each of these we again have a separate Lagrange multiplier that we think of as a spinâ€“spin coupling, $\\lambda_{ij} = J_{ij}$. The general Eqs (20, 21) now take the form\n$$ \\begin{aligned} P_{\\text{traj}}[\\vec{\\sigma}(t)] \u0026= \\frac{1}{Z_{\\text{traj}}}\\exp{(-E_{\\text{traj}}[\\vec{\\sigma}(t)])}\\\\ E_{\\text{traj}} \u0026= \\sum_{t}\\sum_{i}h_{i}(t)\\sigma_{i}(t) + \\frac{1}{2}\\sum_{t}\\sum_{ij}J_{ij}\\sigma_{i}(t)\\sigma_{j}(t) \\end{aligned} $$\nIn this class of models, correlations arise both because different neurons may be subject to correlated timedependent fields and because of effects intrinsic to the network. If all of the correlations were driven by visual inputs then matching the correlations would lead to $J_{ij} = 0$, but this never happens with real data.\nè¿™æ˜¯ Â§IV.A ä¸­ä»‹ç»çš„ä¸€èˆ¬ç»“æ„çš„ä¸€ä¸ªå®ä¾‹ï¼Œå…¶ä¸­ç¬¬ä¸€ç»„å¯è§‚å¯Ÿé‡çš„å½¢å¼ä¸º\n$$ \\{f_{\\mu}\\}\\rightarrow\\{f_{i,t}\\} = \\{\\sigma_{i}(t)\\} $$\nä¸ºäº†çº¦æŸè¿™äº›é¡¹çš„æœŸæœ›å€¼ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå•ç‹¬çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼Œå’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å°†è¿™äº›è§†ä¸ºç°åœ¨ä¾èµ–äºæ—¶é—´çš„å±€éƒ¨åœºï¼Œ$\\lambda_{i,t} = h_{i}(t)$ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰ä»¥ä¸‹å½¢å¼çš„å¯è§‚å¯Ÿé‡\n$$ \\{f_{\\mu}\\} \\rightarrow \\{f_{ij}\\} = \\left\\{\\sum_{t}\\sigma_{i}(t)\\sigma_{j}(t)\\right\\} $$\nå¯¹äºæ¯ä¸ªè¿™äº›ï¼Œæˆ‘ä»¬å†æ¬¡æœ‰ä¸€ä¸ªå•ç‹¬çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼Œæˆ‘ä»¬å°†å…¶è§†ä¸ºè‡ªæ—‹-è‡ªæ—‹è€¦åˆï¼Œ$\\lambda_{ij} = J_{ij}$ã€‚ä¸€èˆ¬æ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰ç°åœ¨é‡‡ç”¨ä»¥ä¸‹å½¢å¼\n$$ \\begin{aligned} P_{\\text{traj}}[\\vec{\\sigma}(t)] \u0026= \\frac{1}{Z_{\\text{traj}}}\\exp{(-E_{\\text{traj}}[\\vec{\\sigma}(t)])}\\\\ E_{\\text{traj}} \u0026= \\sum_{t}\\sum_{i}h_{i}(t)\\sigma_{i}(t) + \\frac{1}{2}\\sum_{t}\\sum_{ij}J_{ij}\\sigma_{i}(t)\\sigma_{j}(t) \\end{aligned} $$\nåœ¨è¿™ç±»æ¨¡å‹ä¸­ï¼Œç›¸å…³æ€§æ—¢æ˜¯å› ä¸ºä¸åŒçš„ç¥ç»å…ƒå¯èƒ½å—åˆ°ç›¸å…³çš„æ—¶é—´ä¾èµ–åœºçš„å½±å“ï¼Œä¹Ÿæ˜¯ç”±äºç½‘ç»œå†…åœ¨çš„å½±å“ã€‚å¦‚æœæ‰€æœ‰ç›¸å…³æ€§éƒ½æ˜¯ç”±è§†è§‰è¾“å…¥é©±åŠ¨çš„ï¼Œé‚£ä¹ˆåŒ¹é…ç›¸å…³æ€§å°†å¯¼è‡´ $J_{ij} = 0$ï¼Œä½†åœ¨çœŸå®æ•°æ®ä¸­è¿™ç§æƒ…å†µä»æœªå‘ç”Ÿè¿‡ã€‚\n",
  "wordCount" : "30459",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png","datePublished": "2025-11-12T00:18:23+08:00",
  "dateModified": "2025-11-12T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-4/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "æ— å¤„æƒ¹å°˜åŸƒ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿— (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿—</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/">ğŸ“• é˜…è¯»</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/">ğŸ“• æ–‡çŒ®</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/">ğŸ“• Statistical mechanics for networks of real neurons</a></div>
            <h1 class="post-title">
                Maximum entropy as a path to connect theory and experiment
            </h1>
            <div class="post-description">
                çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-11-12
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>30459å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>61åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#basics-of-maximum-entropy" aria-label="Basics of maximum entropy">Basics of maximum entropy</a></li>
                <li>
                    <a href="#first-connections-to-neurons" aria-label="First connections to neurons">First connections to neurons</a></li>
                <li>
                    <a href="#larger-networks-of-neurons" aria-label="Larger networks of neurons">Larger networks of neurons</a></li>
                <li>
                    <a href="#doing-more-and-doing-less" aria-label="Doing more and doing less">Doing more and doing less</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><blockquote>
<p>New experimental methods create new opportunities to test our theories. For neural networks, monitoring the electrical activity of tens, hundreds, or thousands of neurons simultaneously should allow us to test statistical approaches to these systems in detail. Doing this requires taking much more seriously the connection between our models and real neurons, a connection that sometimes has been tenuous. Can we really take the spins $\sigma_{i}$ in Eq (5) to represent the presence or absence of an action potential in cell $i$? We will indeed make this identification, and our goal will be an accurate description of the probability distribution out of which the â€œmicroscopicâ€ states of a large network are drawn. Note that, as in equilibrium statistical mechanics, this would be the beginning and not the end of our understanding.</p>
</blockquote>
<p>æ–°çš„å®éªŒæ–¹æ³•ä¸ºæµ‹è¯•æˆ‘ä»¬çš„ç†è®ºåˆ›é€ äº†æ–°çš„æœºä¼šã€‚å¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼ŒåŒæ—¶ç›‘æµ‹æ•°åã€æ•°ç™¾æˆ–æ•°åƒä¸ªç¥ç»å…ƒçš„ç”µæ´»åŠ¨åº”è¯¥å…è®¸æˆ‘ä»¬è¯¦ç»†æµ‹è¯•è¿™äº›ç³»ç»Ÿçš„ç»Ÿè®¡æ–¹æ³•ã€‚åšåˆ°è¿™ä¸€ç‚¹éœ€è¦æˆ‘ä»¬æ›´åŠ è®¤çœŸåœ°å¯¹å¾…æˆ‘ä»¬çš„æ¨¡å‹ä¸çœŸå®ç¥ç»å…ƒä¹‹é—´çš„è”ç³»ï¼Œè€Œè¿™ç§è”ç³»æœ‰æ—¶æ˜¯è„†å¼±çš„ã€‚æˆ‘ä»¬çœŸçš„å¯ä»¥å°†æ–¹ç¨‹ï¼ˆ5ï¼‰ä¸­çš„è‡ªæ—‹ $\sigma_{i}$ è§†ä¸ºç»†èƒ $i$ ä¸­åŠ¨ä½œç”µä½çš„å­˜åœ¨æˆ–ä¸å­˜åœ¨å—ï¼Ÿæˆ‘ä»¬ç¡®å®ä¼šåšå‡ºè¿™ç§è¯†åˆ«ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‡†ç¡®æè¿°ä»ä¸­æŠ½å–å¤§å‹ç½‘ç»œâ€œå¾®è§‚â€çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¯·æ³¨æ„ï¼Œä¸å¹³è¡¡ç»Ÿè®¡åŠ›å­¦ä¸€æ ·ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬ç†è§£çš„å¼€å§‹ï¼Œè€Œä¸æ˜¯ç»“æŸã€‚</p>
<blockquote>
<p>We will see that maximum entropy models provide a path that starts with data and constructs models that have a very direct connection to statistical physics. Our focus here is on networks of neurons, but it is important that the same concepts and methods are being used to study a much wider range of living systems, and there are important lessons to be drawn from seeing all these problems as part of the same project (Appendix A).</p>
</blockquote>
<p>æœ€å¤§ç†µæ¨¡å‹æä¾›äº†ä¸€æ¡ä»æ•°æ®å¼€å§‹å¹¶æ„å»ºä¸ç»Ÿè®¡ç‰©ç†æœ‰éå¸¸ç›´æ¥è”ç³»çš„æ¨¡å‹çš„è·¯å¾„ã€‚æˆ‘ä»¬è¿™é‡Œçš„é‡ç‚¹æ˜¯ç¥ç»å…ƒç½‘ç»œï¼Œä½†é‡è¦çš„æ˜¯ï¼ŒåŒæ ·çš„æ¦‚å¿µå’Œæ–¹æ³•æ­£åœ¨è¢«ç”¨æ¥ç ”ç©¶æ›´å¹¿æ³›çš„ç”Ÿç‰©ç³»ç»Ÿï¼Œå¹¶ä¸”ä»å°†æ‰€æœ‰è¿™äº›é—®é¢˜è§†ä¸ºåŒä¸€é¡¹ç›®çš„ä¸€éƒ¨åˆ†ä¸­å¯ä»¥å¾—å‡ºé‡è¦çš„æ•™è®­ï¼ˆé™„å½• Aï¼‰ã€‚</p>
<h1 id="basics-of-maximum-entropy">Basics of maximum entropy<a hidden class="anchor" aria-hidden="true" href="#basics-of-maximum-entropy">#</a></h1>
<blockquote>
<p>Consider a network of neurons, labelled by $i = 1, 2, \cdots , N$ , each with a state $\sigma_{i}$. In the simplest case where these states of individual neurons are binaryactive/inactive, or spiking/silentâ€”then the network as a whole has access to $\Omega = 2^{N}$ possible states</p>
<p>$$
\vec{\sigma} = \{\sigma_{1},\sigma_{2},\cdots,\sigma_{N}\}.
$$</p>
<p>These states mean something to the organism: they may represent sensory inputs, inferred features of the surrounding world, plans, motor commands, recalled memories, or internal thoughts. But before we can build a dictionary for these meanings we need a lexicon, describing which of the possible states actually occur, and how often. More formally, we would like to understand the probability distribution $P(\vec{\sigma})$. We might also be interested in sequences of states over time, $P [\{\vec{\sigma}(t_1), \vec{\sigma}(t_2),\cdots \}]$, but for simplicity we focus first on states at a single moment in time.</p>
</blockquote>
<p>è€ƒè™‘ä¸€ä¸ªç¥ç»å…ƒç½‘ç»œï¼Œæ ‡è®°ä¸º $i = 1, 2, \cdots , N$ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½æœ‰ä¸€ä¸ªçŠ¶æ€ $\sigma_{i}$ã€‚åœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œè¿™äº›å•ä¸ªç¥ç»å…ƒçš„çŠ¶æ€æ˜¯äºŒè¿›åˆ¶çš„â€”â€”æ´»è·ƒ/ä¸æ´»è·ƒï¼Œæˆ–å°–å³°/é™é»˜â€”â€”é‚£ä¹ˆæ•´ä¸ªç½‘ç»œå¯ä»¥è®¿é—® $\Omega = 2^{N}$ ä¸ªå¯èƒ½çš„çŠ¶æ€</p>
<p>$$
\vec{\sigma} = \{\sigma_{1},\sigma_{2},\cdots,\sigma_{N}\}.
$$</p>
<p>è¿™äº›çŠ¶æ€å¯¹æœ‰æœºä½“æ¥è¯´æ˜¯æœ‰æ„ä¹‰çš„ï¼šå®ƒä»¬å¯èƒ½ä»£è¡¨æ„Ÿå®˜è¾“å…¥ã€å‘¨å›´ä¸–ç•Œçš„æ¨æ–­ç‰¹å¾ã€è®¡åˆ’ã€è¿åŠ¨å‘½ä»¤ã€å›å¿†çš„è®°å¿†æˆ–å†…éƒ¨æ€ç»´ã€‚ä½†åœ¨æˆ‘ä»¬èƒ½å¤Ÿä¸ºè¿™äº›å«ä¹‰æ„å»ºè¯å…¸ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œæè¿°å“ªäº›å¯èƒ½çš„çŠ¶æ€å®é™…ä¸Šä¼šå‘ç”Ÿï¼Œä»¥åŠå®ƒä»¬å‘ç”Ÿçš„é¢‘ç‡ã€‚æ›´æ­£å¼åœ°è¯´ï¼Œæˆ‘ä»¬æƒ³è¦ç†è§£æ¦‚ç‡åˆ†å¸ƒ $P(\vec{\sigma})$ã€‚æˆ‘ä»¬ä¹Ÿå¯èƒ½å¯¹éšæ—¶é—´å˜åŒ–çš„çŠ¶æ€åºåˆ—æ„Ÿå…´è¶£ï¼Œ$P [\{\vec{\sigma}(t_1), \vec{\sigma}(t_2),\cdots \}]$ï¼Œä½†ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬é¦–å…ˆå…³æ³¨å•ä¸€æ—¶åˆ»çš„çŠ¶æ€ã€‚</p>
<blockquote>
<p>The distribution $P(\vec{\sigma})$ is a list of $\Omega$ numbers that sum to one. Even for modest size networks this is a very long list, $\Omega\sim 10^{30}$ for $N = 100$. To be clear, there is no way that we can measure all these numbers in any realistic experiment. More deeply, large networks could not visit all of their possible states in the age of the universe, let alone the lifetime of a single organism. This shouldnâ€™t bother us, since one can make similar observations about the states of molecules in the air around us, or the states of all the atoms in a tiny grain of sand. The fact that the number of possible states $\Omega$ is (beyond) astronomically large does not stop us from asking questions about the distribution from which these states are drawn.</p>
</blockquote>
<p>æ¦‚ç‡åˆ†å¸ƒ $P(\vec{\sigma})$ æ˜¯ä¸€ä¸ªåŒ…å« $\Omega$ ä¸ªæ•°å­—çš„åˆ—è¡¨ï¼Œè¿™äº›æ•°å­—çš„æ€»å’Œä¸ºä¸€ã€‚å³ä½¿å¯¹äºé€‚åº¦å¤§å°çš„ç½‘ç»œæ¥è¯´ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸é•¿çš„åˆ—è¡¨ï¼Œå¯¹äº $N = 100$ï¼Œ$\Omega\sim 10^{30}$ã€‚æ˜ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬æ— æ³•åœ¨ä»»ä½•ç°å®çš„å®éªŒä¸­æµ‹é‡å‡ºæ‰€æœ‰è¿™äº›æ•°å­—ã€‚æ›´æ·±å±‚æ¬¡çš„æ˜¯ï¼Œå¤§å‹ç½‘ç»œä¸å¯èƒ½åœ¨å®‡å®™çš„å¹´é¾„å†…è®¿é—®å®ƒä»¬æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€ï¼Œæ›´ä¸ç”¨è¯´å•ä¸ªæœ‰æœºä½“çš„å¯¿å‘½äº†ã€‚è¿™ä¸åº”è¯¥å›°æ‰°æˆ‘ä»¬ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥å¯¹æˆ‘ä»¬å‘¨å›´ç©ºæ°”ä¸­çš„åˆ†å­çŠ¶æ€ï¼Œæˆ–å¾®å°æ²™ç²’ä¸­æ‰€æœ‰åŸå­çš„çŠ¶æ€åšå‡ºç±»ä¼¼çš„è§‚å¯Ÿã€‚å¯èƒ½çŠ¶æ€çš„æ•°é‡ $\Omega$ï¼ˆè¶…å‡ºï¼‰å¤©æ–‡æ•°å­—çº§åˆ«ï¼Œå¹¶ä¸ä¼šé˜»æ­¢æˆ‘ä»¬å¯¹è¿™äº›çŠ¶æ€æ‰€æŠ½å–çš„åˆ†å¸ƒæå‡ºé—®é¢˜ã€‚</p>
<blockquote>
<p>The enormous value of $\Omega$ does mean, however, that answering questions about the distribution from which the states are drawn requires the answer to be, in some sense, simpler than it could be. If $P(\vec{\sigma})$ really were just a list of $\Omega$ numbers with no underlying structure, we could never make a meaningful experimental prediction. Progress in the description of manyâ€“body systems depends on the discovery of some regularity or simplicity, and without such simplifying hypotheses nothing can be inferred from any reasonable amount of data. The maximum entropy method is a way of being explicit about our simplifying hypotheses.</p>
</blockquote>
<p>ç„¶è€Œï¼Œ$\Omega$ çš„å·¨å¤§å€¼ç¡®å®æ„å‘³ç€ï¼Œè¦å›ç­”æœ‰å…³çŠ¶æ€æ‰€æŠ½å–çš„åˆ†å¸ƒçš„é—®é¢˜ï¼Œç­”æ¡ˆåœ¨æŸç§æ„ä¹‰ä¸Šå¿…é¡»æ¯”å®ƒå¯èƒ½çš„å½¢å¼æ›´ç®€å•ã€‚å¦‚æœ $P(\vec{\sigma})$ çœŸçš„æ˜¯ä¸€ä¸ªæ²¡æœ‰æ½œåœ¨ç»“æ„çš„ $\Omega$ ä¸ªæ•°å­—çš„åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†æ°¸è¿œæ— æ³•åšå‡ºæœ‰æ„ä¹‰çš„å®éªŒé¢„æµ‹ã€‚å¯¹å¤šä½“ç³»ç»Ÿæè¿°çš„è¿›å±•ä¾èµ–äºæŸç§è§„å¾‹æ€§æˆ–ç®€å•æ€§çš„å‘ç°ï¼Œå¦‚æœæ²¡æœ‰è¿™ç§ç®€åŒ–å‡è®¾ï¼Œä»ä»»ä½•åˆç†æ•°é‡çš„æ•°æ®ä¸­éƒ½æ— æ³•æ¨æ–­å‡ºä»»ä½•ä¸œè¥¿ã€‚æœ€å¤§ç†µæ–¹æ³•æ˜¯ä¸€ç§æ˜ç¡®è¡¨è¾¾æˆ‘ä»¬ç®€åŒ–å‡è®¾çš„æ–¹æ³•ã€‚</p>
<blockquote>
<p>We can imagine mapping each microscopic state $\vec{\sigma}$ into some perhaps more macroscopic observable $f(\vec{\sigma})$, and from reasonable experiments we should be able to estimate the average of this observable $\langle f(\vec{\sigma})\rangle_{\text{expt}}$. If we think this observable is an important and meaningful quantity, it makes sense to insist that any theory we write down for the distribution $P(\vec{\sigma})$ should predict this expectation value correctly,</p>
<p>$$
\langle f(\vec{\sigma})\rangle_{P} = \sum_{\vec{\sigma}} P(\vec{\sigma})f(\vec{\sigma}) = \langle f(\vec{\sigma})\rangle_{\text{expt}}.
$$</p>
<p>There might be several such meaningful observables, so we should have</p>
<p>$$
\langle f_{\mu}(\vec{\sigma})\rangle_{P} \equiv \sum_{\vec{\sigma}}P(\vec{\sigma})f_{\mu}(\vec{\sigma}) = \langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}
$$</p>
<p>for $\mu = 1, 2, \cdots , K$. These are strong constraints, but so long as the number of these observables $K \ll \Omega$ there are infinitely many distributions consistent with Eq (17). How do we choose among them?</p>
</blockquote>
<p>æˆ‘ä»¬å¯ä»¥æƒ³è±¡å°†æ¯ä¸ªå¾®è§‚çŠ¶æ€ $\vec{\sigma}$ æ˜ å°„åˆ°æŸä¸ªå¯èƒ½æ›´å®è§‚çš„å¯è§‚å¯Ÿé‡ $f(\vec{\sigma})$ï¼Œå¹¶ä¸”é€šè¿‡åˆç†çš„å®éªŒï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿä¼°è®¡å‡ºè¿™ä¸ªå¯è§‚å¯Ÿé‡çš„å¹³å‡å€¼ $\langle f(\vec{\sigma})\rangle_{\text{expt}}$ã€‚å¦‚æœæˆ‘ä»¬è®¤ä¸ºè¿™ä¸ªå¯è§‚å¯Ÿé‡æ˜¯ä¸€ä¸ªé‡è¦ä¸”æœ‰æ„ä¹‰çš„é‡ï¼Œé‚£ä¹ˆåšæŒä»»ä½•æˆ‘ä»¬ä¸ºåˆ†å¸ƒ $P(\vec{\sigma})$ å†™ä¸‹çš„ç†è®ºéƒ½åº”è¯¥æ­£ç¡®é¢„æµ‹è¿™ä¸ªæœŸæœ›å€¼æ˜¯æœ‰æ„ä¹‰çš„ï¼Œ</p>
<p>$$
\langle f(\vec{\sigma})\rangle_{P} = \sum_{\vec{\sigma}} P(\vec{\sigma})f(\vec{\sigma}) = \langle f(\vec{\sigma})\rangle_{\text{expt}}.
$$</p>
<p>å¯èƒ½ä¼šæœ‰å‡ ä¸ªè¿™æ ·çš„æœ‰æ„ä¹‰çš„å¯è§‚å¯Ÿé‡ï¼Œå› æ­¤æˆ‘ä»¬åº”è¯¥æœ‰</p>
<p>$$
\langle f_{\mu}(\vec{\sigma})\rangle_{P} \equiv \sum_{\vec{\sigma}}P(\vec{\sigma})f_{\mu}(\vec{\sigma}) = \langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}
$$</p>
<p>å¯¹äº $\mu = 1, 2, \cdots , K$ã€‚è¿™äº›æ˜¯å¼ºçº¦æŸï¼Œä½†åªè¦è¿™äº›å¯è§‚å¯Ÿé‡çš„æ•°é‡ $K \ll \Omega$ï¼Œå°±æœ‰æ— æ•°ä¸ªä¸æ–¹ç¨‹ï¼ˆ17ï¼‰ä¸€è‡´çš„åˆ†å¸ƒã€‚æˆ‘ä»¬å¦‚ä½•åœ¨å®ƒä»¬ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼Ÿ</p>
<blockquote>
<p>There are many ways of saying, in words, how we would like to make our choice among the $P (\sigma)$ that are consistent with the measured expectation values of observables. We would like to pick the simplest or least structured model. We would like not to inject into our model any information beyond what is given to us by the measurements $\{\langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}\}$. From a different point of view, we would like drawing states out of the distribution $P(\vec{\sigma})$ to generate samples that are as random as possible while still obeying the constraints in Eq (17). It might seem that each choice of words generates a new discussionâ€”what do we mean, mathematically, by â€œleast structured,â€ or â€œas random as possibleâ€?</p>
</blockquote>
<p>åœ¨ä¸è§‚æµ‹é‡çš„æµ‹é‡æœŸæœ›å€¼ä¸€è‡´çš„ $P (\sigma)$ ä¹‹é—´è¿›è¡Œé€‰æ‹©æ—¶ï¼Œæœ‰è®¸å¤šæ–¹æ³•å¯ä»¥ç”¨è¯­è¨€è¡¨è¾¾ã€‚æˆ‘ä»¬å¸Œæœ›é€‰æ‹©æœ€ç®€å•æˆ–ç»“æ„æœ€å°‘çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›ä¸è¦å°†ä»»ä½•è¶…å‡ºæµ‹é‡ $\{\langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}\}$ æ‰€æä¾›çš„ä¿¡æ¯æ³¨å…¥åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚ä»ä¸åŒçš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¸Œæœ›ä»åˆ†å¸ƒ $P(\vec{\sigma})$ ä¸­æŠ½å–çŠ¶æ€ï¼Œä»¥ç”Ÿæˆå°½å¯èƒ½éšæœºçš„æ ·æœ¬ï¼ŒåŒæ—¶ä»ç„¶éµå®ˆæ–¹ç¨‹ï¼ˆ17ï¼‰ä¸­çš„çº¦æŸã€‚ä¼¼ä¹æ¯ç§æªè¾éƒ½ä¼šå¼•å‘æ–°çš„è®¨è®ºâ€”â€”æˆ‘ä»¬åœ¨æ•°å­¦ä¸Šæ˜¯ä»€ä¹ˆæ„æ€ï¼Œâ€œç»“æ„æœ€å°‘â€æˆ–â€œå°½å¯èƒ½éšæœºâ€ï¼Ÿ</p>
<blockquote>
<p>Introductory courses in statistical mechanics make some remarks about entropy as a measure of our ignorance about the microscopic state of a system, but this connection often is left quite vague. In laying the foundations of information theory, Shannon made this connection precise (Shannon, 1948). If we ask a question, we have the intuition that we â€œgain informationâ€ when  we hear the answer. If we want to attach a number to this information gain, then the unique measure that is consistent with natural constraints is the entropy of the distribution out of which the answers are drawn. Thus, if we ask for the microscopic state of a system, the information we gain on hearing the answer is (on average) the entropy of the distribution over these microscopic states. Conversely, if the entropy is less than its maximum possible value, this reduction in entropy measures how much we already know about the microscopic state even before we see it. As a result, for states to be as random as possibleâ€”to be sure that we do not inject extra information about these statesâ€”we need to find the distribution that has the maximum entropy.</p>
</blockquote>
<p>ç»Ÿè®¡åŠ›å­¦çš„å…¥é—¨è¯¾ç¨‹å¯¹ç†µä½œä¸ºæˆ‘ä»¬å¯¹ç³»ç»Ÿå¾®è§‚çŠ¶æ€æ— çŸ¥çš„åº¦é‡åšäº†ä¸€äº›è¯„è®ºï¼Œä½†è¿™ç§è”ç³»é€šå¸¸ç›¸å½“æ¨¡ç³Šã€‚åœ¨å¥ å®šä¿¡æ¯ç†è®ºåŸºç¡€æ—¶ï¼Œé¦™å†œï¼ˆShannonï¼Œ1948ï¼‰ä½¿è¿™ç§è”ç³»å˜å¾—ç²¾ç¡®ã€‚å¦‚æœæˆ‘ä»¬æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æœ‰ä¸€ç§ç›´è§‰ï¼Œå½“æˆ‘ä»¬å¬åˆ°ç­”æ¡ˆæ—¶ï¼Œæˆ‘ä»¬â€œè·å¾—ä¿¡æ¯â€ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¸ºè¿™ç§ä¿¡æ¯å¢ç›Šé™„åŠ ä¸€ä¸ªæ•°å­—ï¼Œé‚£ä¹ˆä¸è‡ªç„¶çº¦æŸä¸€è‡´çš„å”¯ä¸€åº¦é‡å°±æ˜¯ä»ä¸­æŠ½å–ç­”æ¡ˆçš„åˆ†å¸ƒçš„ç†µã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬è¯¢é—®ç³»ç»Ÿçš„å¾®è§‚çŠ¶æ€ï¼Œé‚£ä¹ˆåœ¨å¬åˆ°ç­”æ¡ˆæ—¶æˆ‘ä»¬è·å¾—çš„ä¿¡æ¯ï¼ˆå¹³å‡è€Œè¨€ï¼‰å°±æ˜¯è¿™äº›å¾®è§‚çŠ¶æ€åˆ†å¸ƒçš„ç†µã€‚ç›¸åï¼Œå¦‚æœç†µå°äºå…¶æœ€å¤§å¯èƒ½å€¼ï¼Œè¿™ç§ç†µçš„å‡å°‘è¡¡é‡äº†å³ä½¿åœ¨çœ‹åˆ°å®ƒä¹‹å‰æˆ‘ä»¬å·²ç»çŸ¥é“äº†å¤šå°‘å…³äºå¾®è§‚çŠ¶æ€çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿çŠ¶æ€å°½å¯èƒ½éšæœºâ€”â€”ç¡®ä¿æˆ‘ä»¬ä¸ä¼šæ³¨å…¥å…³äºè¿™äº›çŠ¶æ€çš„é¢å¤–ä¿¡æ¯â€”â€”æˆ‘ä»¬éœ€è¦æ‰¾åˆ°å…·æœ‰æœ€å¤§ç†µçš„åˆ†å¸ƒã€‚</p>
<blockquote>
<p>Maximizing the entropy subject to constraints defines a variational problem, maximizing</p>
<p>$$
\widetilde{S} = -\sum_{\vec{\sigma}}P(\vec{\sigma})\ln{P(\vec{\sigma})} - \sum_{\mu = 1}^{K}\left[\sum_{\sigma}P(\vec{\sigma})f_{\mu}(\vec{\sigma}) - \langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}\right] - \lambda_{0}\left[\sum_{\vec{\sigma}}P(\vec{\sigma}) - 1\right]
$$</p>
<p>where the $\lambda_{\mu}$ are Lagrange multipliers. We include an additional term ($\propto \lambda_{0}$) to constrain the normalization, so we can treat each entry in the distribution as an independent variable. Then</p>
<p>$$
\begin{aligned}
\frac{\delta \widetilde{S}}{\delta P(\vec{\sigma})} &amp;= 0\\
\Rightarrow P(\vec{\sigma}) &amp;= \frac{1}{Z(\{\lambda_{\mu}\})}\exp{[-E(\vec{\sigma})]}\\
E(\vec{\sigma}) &amp;= \sum_{\mu = 1}^{K}\lambda_{\mu}f_{\mu}(\vec{\sigma})
\end{aligned}
$$</p>
<p>Thus the model we are looking for is equivalent to an equilibrium statistical mechanics problem in which the â€œenergyâ€ is a sum of terms, one for each of the observables whose expectation values we constrain; the Lagrange multipliers become coupling constants in the effective energy. To finish the construction we need to adjust these couplings $\{\lambda_{\mu}\}$ to satisfy Eq (17), and in general this is a hard problem; see Appendix B. Importantly, if we have some set of expectation values that we are matching, and we want to add one more, this just adds one more term to the form of the energy function, but in general implementing this extra constraint requires adjusting all the coupling constants.</p>
</blockquote>
<p>æœ€å¤§åŒ–å—çº¦æŸçš„ç†µå®šä¹‰äº†ä¸€ä¸ªå˜åˆ†é—®é¢˜ï¼Œæœ€å¤§åŒ–</p>
<p>$$
\widetilde{S} = -\sum_{\vec{\sigma}}P(\vec{\sigma})\ln{P(\vec{\sigma})} - \sum_{\mu = 1}^{K}\left[\sum_{\sigma}P(\vec{\sigma})f_{\mu}(\vec{\sigma}) - \langle f_{\mu}(\vec{\sigma})\rangle_{\text{expt}}\right] - \lambda_{0}\left[\sum_{\vec{\sigma}}P(\vec{\sigma}) - 1\right]
$$</p>
<p>å…¶ä¸­ $\lambda_{\mu}$ æ˜¯æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ã€‚æˆ‘ä»¬åŒ…æ‹¬ä¸€ä¸ªé¢å¤–çš„é¡¹ï¼ˆ$\propto \lambda_{0}$ï¼‰æ¥çº¦æŸå½’ä¸€åŒ–ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†åˆ†å¸ƒä¸­çš„æ¯ä¸ªæ¡ç›®è§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„å˜é‡ã€‚ç„¶å</p>
<p>$$
\begin{aligned}
\frac{\delta \widetilde{S}}{\delta P(\vec{\sigma})} &amp;= 0\\
\Rightarrow P(\vec{\sigma}) &amp;= \frac{1}{Z(\{\lambda_{\mu}\})}\exp{[-E(\vec{\sigma})]}\\
E(\vec{\sigma}) &amp;= \sum_{\mu = 1}^{K}\lambda_{\mu}f_{\mu}(\vec{\sigma})
\end{aligned}
$$</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾çš„æ¨¡å‹ç­‰ä»·äºä¸€ä¸ªå¹³è¡¡ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œå…¶ä¸­â€œèƒ½é‡â€æ˜¯å„ä¸ªå¯è§‚å¯Ÿé‡çš„æœŸæœ›å€¼ä¹‹å’Œï¼›æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æˆä¸ºæœ‰æ•ˆèƒ½é‡ä¸­çš„è€¦åˆå¸¸æ•°ã€‚ä¸ºäº†å®Œæˆæ„å»ºï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´è¿™äº›è€¦åˆ $\{\lambda_{\mu}\}$ ä»¥æ»¡è¶³æ–¹ç¨‹ï¼ˆ17ï¼‰ï¼Œé€šå¸¸è¿™æ˜¯ä¸€ä¸ªå›°éš¾çš„é—®é¢˜ï¼›è§é™„å½• Bã€‚é‡è¦çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ç»„æˆ‘ä»¬æ­£åœ¨åŒ¹é…çš„æœŸæœ›å€¼ï¼Œå¹¶ä¸”æˆ‘ä»¬æƒ³å†æ·»åŠ ä¸€ä¸ªï¼Œè¿™åªä¼šåœ¨èƒ½é‡å‡½æ•°çš„å½¢å¼ä¸­æ·»åŠ ä¸€ä¸ªé¢å¤–çš„é¡¹ï¼Œä½†é€šå¸¸å®ç°è¿™ä¸ªé¢å¤–çš„çº¦æŸéœ€è¦è°ƒæ•´æ‰€æœ‰çš„è€¦åˆå¸¸æ•°ã€‚</p>
<blockquote>
<p>To make the connections explicit, recall that we can define thermodynamic equilibrium as the state of maximum entropy given the constraint of fixed mean energy. This optimization problem is solved by the Boltzmann distribution. In this view the (inverse) temperature is a Lagrange multiplier that enforces the energy constraint, opposite to usual view of controlling the temperature and predicting the energy. The Boltzmann distribution generalizes if other expectation values are constrained (Landau and Lifshitz, 1977).</p>
</blockquote>
<p>ä¸ºäº†æ˜ç¡®è¿æ¥ï¼Œå›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†çƒ­åŠ›å­¦å¹³è¡¡å®šä¹‰ä¸ºåœ¨å›ºå®šå¹³å‡èƒ½é‡çº¦æŸä¸‹çš„æœ€å¤§ç†µçŠ¶æ€ã€‚è¿™ä¸ªä¼˜åŒ–é—®é¢˜ç”± Boltzmann åˆ†å¸ƒè§£å†³ã€‚åœ¨è¿™ç§è§‚ç‚¹ä¸­ï¼Œï¼ˆåï¼‰æ¸©åº¦æ˜¯ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œç”¨äºå¼ºåˆ¶æ‰§è¡Œèƒ½é‡çº¦æŸï¼Œè¿™ä¸é€šå¸¸æ§åˆ¶æ¸©åº¦å¹¶é¢„æµ‹èƒ½é‡çš„è§‚ç‚¹ç›¸åã€‚å¦‚æœçº¦æŸäº†å…¶ä»–æœŸæœ›å€¼ï¼ŒBoltzmann åˆ†å¸ƒä¼šè¿›è¡Œæ¨å¹¿ï¼ˆLandau å’Œ Lifshitzï¼Œ1977ï¼‰ã€‚</p>
<blockquote>
<p>The maximum entropy argument gives us the form of the probability distribution, but we also need the coupling constants. We can think of this as being an â€œinverse statistical mechanicsâ€ problem, since we are given expectation values or correlation functions and need to find the couplings, rather than the other way around. Different formulations of this problem have a long history in the mathematical physics community (Chayes et al., 1984; Keller and Zumino, 1959; Kunkin and Firsch, 1969). An early application to living systems involved reconstructing the forces that hold together the array of gap junction proteins which bridge the membranes of two cells in contact (Braun et al., 1984). As attention focused on networks of neurons, finding the relevant coupling constants came to be described as the â€œinverse Isingâ€ problem, as will become clear below.</p>
</blockquote>
<p>æœ€å¤§ç†µè®ºç»™äº†æˆ‘ä»¬æ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œä½†æˆ‘ä»¬ä¹Ÿéœ€è¦è€¦åˆå¸¸æ•°ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸º â€œé€†ç»Ÿè®¡åŠ›å­¦â€ é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬ç»™å‡ºäº†æœŸæœ›å€¼æˆ–ç›¸å…³å‡½æ•°ï¼Œå¹¶ä¸”éœ€è¦æ‰¾åˆ°è€¦åˆï¼Œè€Œä¸æ˜¯ç›¸åã€‚è¿™ä¸ªé—®é¢˜çš„ä¸åŒè¡¨è¿°åœ¨æ•°å­¦ç‰©ç†å­¦ç•Œæœ‰ç€æ‚ ä¹…çš„å†å²ï¼ˆChayes ç­‰äººï¼Œ1984ï¼›Keller å’Œ Zuminoï¼Œ1959ï¼›Kunkin å’Œ Firschï¼Œ1969ï¼‰ã€‚å¯¹ç”Ÿç‰©ç³»ç»Ÿçš„æ—©æœŸåº”ç”¨æ¶‰åŠé‡å»ºå°†ä¸¤ä¸ªæ¥è§¦ç»†èƒçš„è†œè¿æ¥åœ¨ä¸€èµ·çš„é—´éš™è¿æ¥è›‹ç™½é˜µåˆ—çš„åŠ›ï¼ˆBraun ç­‰äººï¼Œ1984ï¼‰ã€‚éšç€æ³¨æ„åŠ›é›†ä¸­åœ¨ç¥ç»å…ƒç½‘ç»œä¸Šï¼Œæ‰¾åˆ°ç›¸å…³çš„è€¦åˆå¸¸æ•°è¢«æè¿°ä¸º â€œé€† Isingâ€ é—®é¢˜ï¼Œæ­£å¦‚ä¸‹é¢å°†å˜å¾—æ¸…æ¥šçš„é‚£æ ·ã€‚</p>
<blockquote>
<p>In statistical physics there is in some sense a force driving systems toward equilibrium, as encapsulated in the Hâ€“theorem. In many cases this force triumphs, and what we see is a state with maximal entropy subject only to a very few constraints. In the networks of neurons that we study here, there is no Hâ€“theorem, and the list of constraints will be quite long compared to what we are used to in thermodynamics. This means that the probability distributions we write down will be mathematically equivalent to some equilibrium statistical mechanics problem, but they do not describe an equilibrium state of the system we are actually studying. This somewhat subtle relationship between maximum entropy as a description of thermal equilibrium and maximum entropy as a tool for inference was outlined long ago by Jaynes (1957, 1982).</p>
</blockquote>
<p>åœ¨ç»Ÿè®¡ç‰©ç†å­¦ä¸­ï¼Œåœ¨æŸç§æ„ä¹‰ä¸Šæœ‰ä¸€ç§é©±åŠ¨åŠ›å°†ç³»ç»Ÿæ¨å‘å¹³è¡¡ï¼Œæ­£å¦‚ H å®šç†æ‰€æ¦‚æ‹¬çš„é‚£æ ·ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¿™ç§åŠ›é‡å–å¾—äº†èƒœåˆ©ï¼Œæˆ‘ä»¬æ‰€çœ‹åˆ°çš„æ˜¯ä¸€ä¸ªä»…å—å¾ˆå°‘çº¦æŸçš„æœ€å¤§ç†µçŠ¶æ€ã€‚åœ¨æˆ‘ä»¬è¿™é‡Œç ”ç©¶çš„ç¥ç»å…ƒç½‘ç»œä¸­ï¼Œæ²¡æœ‰ H å®šç†ï¼Œå¹¶ä¸”ä¸æˆ‘ä»¬åœ¨çƒ­åŠ›å­¦ä¸­ä¹ æƒ¯çš„ç›¸æ¯”ï¼Œçº¦æŸåˆ—è¡¨å°†ç›¸å½“é•¿ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å†™ä¸‹çš„æ¦‚ç‡åˆ†å¸ƒåœ¨æ•°å­¦ä¸Šç­‰ä»·äºæŸä¸ªå¹³è¡¡ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œä½†å®ƒä»¬å¹¶ä¸æè¿°æˆ‘ä»¬å®é™…ç ”ç©¶çš„ç³»ç»Ÿçš„å¹³è¡¡çŠ¶æ€ã€‚Jaynesï¼ˆ1957ï¼Œ1982ï¼‰æ—©å·²æ¦‚è¿°äº†æœ€å¤§ç†µä½œä¸ºçƒ­å¹³è¡¡æè¿°å’Œæœ€å¤§ç†µä½œä¸ºæ¨ç†å·¥å…·ä¹‹é—´è¿™ç§å¾®å¦™çš„å…³ç³»ã€‚</p>
<blockquote>
<p>If we donâ€™t have any constraints then the maximum entropy distribution is uniform over all $\Omega$ states. Each observable whose expectation value we constrain lowers the maximum allowed value of the entropy, and if we add enough constraints we eventually reach the true entropy and hence the true distribution. Often it make sense to group the observables into oneâ€“body, twoâ€“body, threebody terms, etc.. Having constrained all the kâ€“body observables for $k\leq K$, the maximum entropy model makes parameterâ€“free predictions for correlations among groups of $k &gt; K$ variables. This provides a powerful path to testing the model, and defines a natural generalization of connected correlations (Schneidman et al., 2003).</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬æ²¡æœ‰ä»»ä½•çº¦æŸï¼Œé‚£ä¹ˆæœ€å¤§ç†µåˆ†å¸ƒåœ¨æ‰€æœ‰ $\Omega$ çŠ¶æ€ä¸Šæ˜¯å‡åŒ€çš„ã€‚æˆ‘ä»¬çº¦æŸçš„æ¯ä¸ªå¯è§‚å¯Ÿé‡çš„æœŸæœ›å€¼éƒ½ä¼šé™ä½å…è®¸çš„æœ€å¤§ç†µå€¼ï¼Œå¦‚æœæˆ‘ä»¬æ·»åŠ è¶³å¤Ÿçš„çº¦æŸï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šè¾¾åˆ°çœŸå®çš„ç†µå€¼ï¼Œä»è€Œå¾—åˆ°çœŸå®çš„åˆ†å¸ƒã€‚é€šå¸¸ï¼Œå°†å¯è§‚å¯Ÿé‡åˆ†ä¸ºå•ä½“ã€äºŒä½“ã€ä¸‰ä½“ç­‰é¡¹æ˜¯æœ‰æ„ä¹‰çš„ã€‚åœ¨çº¦æŸäº†æ‰€æœ‰ $k\leq K$ çš„ $k$ ä½“å¯è§‚å¯Ÿé‡ä¹‹åï¼Œæœ€å¤§ç†µæ¨¡å‹å¯¹ $k &gt; K$ å˜é‡ç»„ä¹‹é—´çš„ç›¸å…³æ€§åšå‡ºæ— å‚æ•°é¢„æµ‹ã€‚è¿™ä¸ºæµ‹è¯•æ¨¡å‹æä¾›äº†ä¸€æ¡å¼ºæœ‰åŠ›çš„è·¯å¾„ï¼Œå¹¶å®šä¹‰äº†è¿æ¥ç›¸å…³æ€§çš„è‡ªç„¶æ¨å¹¿ï¼ˆSchneidman ç­‰äººï¼Œ2003ï¼‰ã€‚</p>
<blockquote>
<p>The connection of maximum entropy models to the Boltzmann distribution gives us intuition and practical computational tools. It can also leave the impression that we are describing a system in equilibrium, which would be a disaster. In fact the maximum entropy distribution describes thermal equilibrium only if the observable that we constrain is the energy in the mechanical sense. There is no obstacle to building maximum entropy models for the distribution of states in a nonâ€“equilibrium system.</p>
</blockquote>
<p>æœ€å¤§ç†µæ¨¡å‹ä¸ Boltzmann åˆ†å¸ƒçš„è”ç³»ä¸ºæˆ‘ä»¬æä¾›äº†ç›´è§‰å’Œå®ç”¨çš„è®¡ç®—å·¥å…·ã€‚å®ƒä¹Ÿå¯èƒ½ç»™äººç•™ä¸‹æˆ‘ä»¬æ­£åœ¨æè¿°ä¸€ä¸ªå¹³è¡¡ç³»ç»Ÿçš„å°è±¡ï¼Œè¿™å°†æ˜¯ç¾éš¾æ€§çš„ã€‚äº‹å®ä¸Šï¼Œåªæœ‰å½“æˆ‘ä»¬çº¦æŸçš„å¯è§‚å¯Ÿé‡æ˜¯æœºæ¢°æ„ä¹‰ä¸Šçš„èƒ½é‡æ—¶ï¼Œæœ€å¤§ç†µåˆ†å¸ƒæ‰æè¿°çƒ­å¹³è¡¡ã€‚æ„å»ºéå¹³è¡¡ç³»ç»Ÿä¸­çŠ¶æ€åˆ†å¸ƒçš„æœ€å¤§ç†µæ¨¡å‹æ²¡æœ‰éšœç¢ã€‚</p>
<blockquote>
<p>Although we can usefully think of states distributed over an energy landscape, as we have formulated the maximum entropy construction this description works for states at one moment in time. Thus we cannot conclude that the dynamics by which the system moves from one state to another are analogous to Brownian motion on the effective energy surface. There are infinitely many models for the dynamics that are consistent with this description, and most of these will not obey detailed balance.
Recent work shows how to explore a large family of dynamical models consistent with the maximum entropy distribution, and applies these ideas to collective animal behavior (Chen et al., 2023). There also are generalizations of the maximum entropy method to describe distributions of trajectories, as we discuss below (Â§IV.D); maximum entropy models for trajectories sometimes are called <strong>maximum caliber</strong> (Ghosh et al., 2020; Press Ìe et al., 2013). Finally we note that, for better or worse, the symmetries that are central to many problems in statistical physics in general are absent from the systems we will be studying; flocks and swarms are an exception, as discussed in Â§A.2.</p>
</blockquote>
<p>å°½ç®¡æˆ‘ä»¬å¯ä»¥æœ‰ç”¨åœ°å°†çŠ¶æ€åˆ†å¸ƒè§†ä¸ºèƒ½é‡æ™¯è§‚ï¼Œä½†æ­£å¦‚æˆ‘ä»¬æ‰€åˆ¶å®šçš„æœ€å¤§ç†µæ„é€ ï¼Œè¿™ç§æè¿°é€‚ç”¨äºæŸä¸€æ—¶åˆ»çš„çŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½å¾—å‡ºç³»ç»Ÿä»ä¸€ä¸ªçŠ¶æ€ç§»åŠ¨åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„åŠ¨åŠ›å­¦ç±»ä¼¼äºæœ‰æ•ˆèƒ½é‡è¡¨é¢ä¸Šçš„å¸ƒæœ—è¿åŠ¨çš„ç»“è®ºã€‚æœ‰æ— æ•°ç§åŠ¨åŠ›å­¦æ¨¡å‹ä¸è¿™ç§æè¿°ä¸€è‡´ï¼Œå…¶ä¸­å¤§å¤šæ•°ä¸ä¼šéµå®ˆè¯¦ç»†å¹³è¡¡ã€‚
æœ€è¿‘çš„å·¥ä½œå±•ç¤ºäº†å¦‚ä½•æ¢ç´¢ä¸æœ€å¤§ç†µåˆ†å¸ƒä¸€è‡´çš„å¤§é‡åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œå¹¶å°†è¿™äº›æ€æƒ³åº”ç”¨äºé›†ä½“åŠ¨ç‰©è¡Œä¸ºï¼ˆChen ç­‰äººï¼Œ2023ï¼‰ã€‚æœ€å¤§ç†µæ–¹æ³•ä¹Ÿæœ‰æ¨å¹¿ï¼Œç”¨äºæè¿°è½¨è¿¹åˆ†å¸ƒï¼Œæ­£å¦‚æˆ‘ä»¬ä¸‹é¢è®¨è®ºçš„é‚£æ ·ï¼ˆÂ§IV.Dï¼‰ï¼›è½¨è¿¹çš„æœ€å¤§ç†µæ¨¡å‹æœ‰æ—¶è¢«ç§°ä¸º<strong>æœ€å¤§å£å¾„</strong>ï¼ˆGhosh ç­‰äººï¼Œ2020ï¼›Press Ìe ç­‰äººï¼Œ2013ï¼‰ã€‚æœ€åæˆ‘ä»¬æ³¨æ„åˆ°ï¼Œæ— è®ºå¥½åï¼Œè®¸å¤šç»Ÿè®¡ç‰©ç†å­¦é—®é¢˜ä¸­è‡³å…³é‡è¦çš„å¯¹ç§°æ€§åœ¨æˆ‘ä»¬å°†è¦ç ”ç©¶çš„ç³»ç»Ÿä¸­æ˜¯ç¼ºå¤±çš„ï¼›å¦‚ Â§A.2 æ‰€è®¨è®ºçš„ï¼Œé¸Ÿç¾¤å’Œè™«ç¾¤æ˜¯ä¸€ä¸ªä¾‹å¤–ã€‚</p>
<blockquote>
<p>To conclude this introduction, we emphasize that maximum entropy is unlike usual theories. We donâ€™t start with a theoretical principle or even a model. Rather, we start with some features of the data and test the hypothesis that these features alone encode everything we need to describe the system. Whenever we use this approach we are referring back to the basic structure of the optimization problem defined in Eq (18), and its formal solution in Eqs (20, 21), but there is no single maximum entropy model, and each time we need to be explicit: Which are the observables $f_{\mu}$ whose measured expectation values we want our model to reproduce? Can we find the corresponding Lagrange mutlipliers $\lambda_{mu}$? Do these parameters have a natural interpretation? Once we answer these questions, we can ask whether these relatively simple statistical physics descriptions make predictions that agree with experiment. There is an unusually clean separation between learning the model (matching observed expectation values) and testing the model (predicting new expectation values). In this sense we can think of maximum entropy as predicting a set of parameter free relations among different aspects of the data. Finally, we will have to think carefully about what it means for models to â€œwork.â€ We begin with early explorations at relatively small $N$ (Â§IV.B), then turn to a wide variety of larger networks (Â§IV.C), and finally address how these analyses can catch up to the experimental frontier (Â§IV.D).</p>
</blockquote>
<p>ä¸ºäº†ç»“æŸè¿™ä¸ªä»‹ç»ï¼Œæˆ‘ä»¬å¼ºè°ƒæœ€å¤§ç†µä¸åŒäºé€šå¸¸çš„ç†è®ºã€‚æˆ‘ä»¬ä¸æ˜¯ä»ä¸€ä¸ªç†è®ºåŸåˆ™ç”šè‡³ä¸€ä¸ªæ¨¡å‹å¼€å§‹ã€‚ç›¸åï¼Œæˆ‘ä»¬ä»æ•°æ®çš„ä¸€äº›ç‰¹å¾å¼€å§‹ï¼Œå¹¶æµ‹è¯•è¿™äº›ç‰¹å¾æ˜¯å¦ç¼–ç äº†æè¿°ç³»ç»Ÿæ‰€éœ€çš„ä¸€åˆ‡çš„å‡è®¾ã€‚æ¯å½“æˆ‘ä»¬ä½¿ç”¨è¿™ç§æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šå›åˆ°æ–¹ç¨‹ï¼ˆ18ï¼‰ä¸­å®šä¹‰çš„ä¼˜åŒ–é—®é¢˜çš„åŸºæœ¬ç»“æ„åŠå…¶åœ¨æ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰ä¸­çš„æ­£å¼è§£ï¼Œä½†æ²¡æœ‰å•ä¸€çš„æœ€å¤§ç†µæ¨¡å‹ï¼Œæ¯æ¬¡æˆ‘ä»¬éƒ½éœ€è¦æ˜ç¡®ï¼šå“ªäº›æ˜¯æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹é‡ç°å…¶æµ‹é‡æœŸæœ›å€¼çš„å¯è§‚å¯Ÿé‡ $f_{\mu}$ï¼Ÿæˆ‘ä»¬èƒ½æ‰¾åˆ°ç›¸åº”çš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•° $\lambda_{mu}$ å—ï¼Ÿè¿™äº›å‚æ•°æœ‰è‡ªç„¶çš„è§£é‡Šå—ï¼Ÿä¸€æ—¦æˆ‘ä»¬å›ç­”äº†è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å°±å¯ä»¥é—®è¿™äº›ç›¸å¯¹ç®€å•çš„ç»Ÿè®¡ç‰©ç†æè¿°æ˜¯å¦åšå‡ºäº†ä¸å®éªŒä¸€è‡´çš„é¢„æµ‹ã€‚åœ¨å­¦ä¹ æ¨¡å‹ï¼ˆåŒ¹é…è§‚å¯Ÿåˆ°çš„æœŸæœ›å€¼ï¼‰å’Œæµ‹è¯•æ¨¡å‹ï¼ˆé¢„æµ‹æ–°çš„æœŸæœ›å€¼ï¼‰ä¹‹é—´å­˜åœ¨ä¸€ç§å¼‚å¸¸æ¸…æ™°çš„åˆ†ç¦»ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè¯´ï¼Œæˆ‘ä»¬å¯ä»¥å°†æœ€å¤§ç†µè§†ä¸ºé¢„æµ‹æ•°æ®ä¸åŒæ–¹é¢ä¹‹é—´çš„ä¸€ç»„æ— å‚æ•°å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ä»”ç»†æ€è€ƒæ¨¡å‹â€œå·¥ä½œâ€çš„å«ä¹‰ã€‚æˆ‘ä»¬ä»ç›¸å¯¹è¾ƒå° $N$ çš„æ—©æœŸæ¢ç´¢å¼€å§‹ï¼ˆÂ§IV.Bï¼‰ï¼Œç„¶åè½¬å‘å„ç§æ›´å¤§çš„ç½‘ç»œï¼ˆÂ§IV.Cï¼‰ï¼Œæœ€åè§£å†³è¿™äº›åˆ†æå¦‚ä½•èµ¶ä¸Šå®éªŒå‰æ²¿çš„é—®é¢˜ï¼ˆÂ§IV.Dï¼‰ã€‚</p>
<h1 id="first-connections-to-neurons">First connections to neurons<a hidden class="anchor" aria-hidden="true" href="#first-connections-to-neurons">#</a></h1>
<blockquote>
<p>Suppose we observe three neurons, and measure their mean activity as well as their pairwise correlations. Given these measurements, should we be surprised by how often the three neurons are active together? Maximum entropy provides a way of answering this question, generating a â€œnull modelâ€ prediction assuming all the correlation structure is captured in the pairs, and this was appreciated âˆ¼2000 (Martignon et al., 2000). Over the next several years a more ambitious idea emerged: could we build maximum entropy models for patterns of activity in larger populations of neurons? The first target for this analysis was a population of neurons in the salamander retina, as it responds to naturalistic visual inputs (Schneidman et al., 2006).</p>
</blockquote>
<p>å‡è®¾æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸‰ä¸ªç¥ç»å…ƒï¼Œå¹¶æµ‹é‡å®ƒä»¬çš„å¹³å‡æ´»åŠ¨ä»¥åŠå®ƒä»¬çš„æˆå¯¹ç›¸å…³æ€§ã€‚é‰´äºè¿™äº›æµ‹é‡ç»“æœï¼Œæˆ‘ä»¬æ˜¯å¦åº”è¯¥å¯¹è¿™ä¸‰ä¸ªç¥ç»å…ƒä¸€èµ·æ´»è·ƒçš„é¢‘ç‡æ„Ÿåˆ°æƒŠè®¶ï¼Ÿæœ€å¤§ç†µæä¾›äº†ä¸€ç§å›ç­”è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•ï¼Œç”Ÿæˆä¸€ä¸ªâ€œé›¶æ¨¡å‹â€é¢„æµ‹ï¼Œå‡è®¾æ‰€æœ‰çš„ç›¸å…³ç»“æ„éƒ½åŒ…å«åœ¨å¯¹ä¸­ï¼Œè¿™åœ¨å¤§çº¦ 2000 å¹´è¢«è®¤è¯†åˆ°ï¼ˆMartignon ç­‰äººï¼Œ2000ï¼‰ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ å¹´é‡Œï¼Œä¸€ä¸ªæ›´é›„å¿ƒå‹ƒå‹ƒçš„æƒ³æ³•å‡ºç°äº†ï¼šæˆ‘ä»¬èƒ½å¦ä¸ºæ›´å¤§ç¾¤ä½“çš„ç¥ç»å…ƒæ´»åŠ¨æ¨¡å¼æ„å»ºæœ€å¤§ç†µæ¨¡å‹ï¼Ÿè¿™ç§åˆ†æçš„ç¬¬ä¸€ä¸ªç›®æ ‡æ˜¯è¾èˆè§†ç½‘è†œä¸­çš„ä¸€ç¾¤ç¥ç»å…ƒï¼Œå› ä¸ºå®ƒå¯¹è‡ªç„¶è§†è§‰è¾“å…¥åšå‡ºååº”ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚</p>
<blockquote>
<p>In response to natural movies, the output neurons of the retinaâ€”the â€œganglion cellsâ€ that carry visual signals from eye to brain, and which as a group form the optic nerveâ€”are sparsely activated, generating an average of just a few spikes per second each (Fig 7A, B). Those initial experiments monitored populations of up to forty neurons in a small patch of the retina, with recordings of up to one hour. Pairs of neurons have temporal correlations with a relatively sharp peak or trough on a broad background that tracks longer timescales in the visual input (Fig 7C). If we discretize time into bins of $\Delta\tau = 20$ ms then we capture most of the short time correlations but still have a very low probability of seeing two spikes in the same bin, so that responses of neuron i become binary, $\sigma_{i} = \{0, 1\}$.</p>
</blockquote>
<p>ä¸ºäº†å“åº”è‡ªç„¶æ´»åŠ¨ï¼Œè§†ç½‘è†œçš„è¾“å‡ºç¥ç»å…ƒâ€”â€”å°†è§†è§‰ä¿¡å·ä»çœ¼ç›ä¼ é€’åˆ°å¤§è„‘çš„â€œç¥ç»èŠ‚ç»†èƒâ€ï¼Œå®ƒä»¬ä½œä¸ºä¸€ä¸ªæ•´ä½“å½¢æˆè§†ç¥ç»â€”â€”è¢«ç¨€ç–æ¿€æ´»ï¼Œæ¯ä¸ªç¥ç»å…ƒå¹³å‡æ¯ç§’åªäº§ç”Ÿå‡ ä¸ªå°–å³°ï¼ˆå›¾ 7Aï¼ŒBï¼‰ã€‚é‚£äº›åˆå§‹å®éªŒç›‘æµ‹äº†è§†ç½‘è†œå°å—ä¸­å¤šè¾¾å››åä¸ªç¥ç»å…ƒçš„ç¾¤ä½“ï¼Œè®°å½•æ—¶é—´é•¿è¾¾ä¸€å°æ—¶ã€‚æˆå¯¹çš„ç¥ç»å…ƒå…·æœ‰æ—¶é—´ç›¸å…³æ€§ï¼Œåœ¨è§†è§‰è¾“å…¥çš„è¾ƒé•¿æ—¶é—´å°ºåº¦ä¸Šè·Ÿè¸ªè¾ƒå®½èƒŒæ™¯ä¸Šçš„ç›¸å¯¹å°–é”å³°å€¼æˆ–è°·å€¼ï¼ˆå›¾ 7Cï¼‰ã€‚å¦‚æœæˆ‘ä»¬å°†æ—¶é—´ç¦»æ•£åŒ–ä¸º $\Delta\tau = 20$ ms çš„æ—¶é—´æ®µï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ•æ‰åˆ°äº†å¤§éƒ¨åˆ†çŸ­æ—¶é—´ç›¸å…³æ€§ï¼Œä½†ä»ç„¶å¾ˆå°‘çœ‹åˆ°åŒä¸€æ—¶é—´æ®µå†…æœ‰ä¸¤ä¸ªå°–å³°ï¼Œå› æ­¤ç¥ç»å…ƒ i çš„å“åº”å˜ä¸ºäºŒè¿›åˆ¶ï¼Œ$\sigma_{i} = \{0, 1\}$ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/13/FgdE8S4rsWtiBja.png" alt=""  /></p>
<p>FIG. 7 Responses of the salamander retina to naturalistic movies (Schneidman et al., 2006). (A) Raster plot of the action potentials from $N = 40$ neurons. Each dot represents a spike from one cell. (B) Expanded view of the green box in (A), showing the discretization of time into bins of width $\Delta\tau = 20$ms. The result (bottom) is that the state of the network is a binary word $\{\sigma_{i}\}$. (C) Correlations between two neurons. Results are shown as the probability per unit time of a spike in cell $j$ (spike rate) given that there is a spike in cell $i$ at time $t = 0$; the plateau at long times should be the mean rate $r_{j} = \langle\sigma_{j}\rangle/\Delta\tau$. There a peak with a width ~ 100 ms, related to time scales in the visual input, and a peak with width ~ 20ms emphasizes in the inset; this motivates the choice of bins size. (D) Distribution of (off-diagonal) correlation coefficients, from Eq (24), across the population of $N = 40$ neurons. (E) Probability that $K$ out of the $N = 40$ neurons are active in the same time bin (red) compared with expectations if activity of each neuron were independent of all the others (blue). Dashed lines are exponential (red) and Poisson (blue), to guide the eye. (F) Predicted occurrence rates of different binary patterns vs the observed rates, for the independent model $P_{1}$ [Eqs (29, 30), blue] and the pairwise maximum entropy model $P_{2}$ [Eqs (35, 33), red].</p>
</blockquote>
<p>å›¾ 7 è¾èˆè§†ç½‘è†œå¯¹è‡ªç„¶ç”µå½±çš„å“åº”ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚(A) $N = 40$ ä¸ªç¥ç»å…ƒçš„åŠ¨ä½œç”µä½å…‰æ …å›¾ã€‚æ¯ä¸ªç‚¹ä»£è¡¨ä¸€ä¸ªç»†èƒçš„ä¸€ä¸ªå°–å³°ã€‚(B) (A) ä¸­ç»¿è‰²æ¡†çš„æ”¾å¤§è§†å›¾ï¼Œæ˜¾ç¤ºæ—¶é—´è¢«ç¦»æ•£åŒ–ä¸ºå®½åº¦ä¸º $\Delta\tau = 20$ms çš„æ—¶é—´æ®µã€‚ç»“æœï¼ˆåº•éƒ¨ï¼‰æ˜¯ç½‘ç»œçš„çŠ¶æ€æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶å­— $\{\sigma_{i}\}$ã€‚(C) ä¸¤ä¸ªç¥ç»å…ƒä¹‹é—´çš„ç›¸å…³æ€§ã€‚ç»“æœæ˜¾ç¤ºä¸ºåœ¨ç»†èƒ $i$ åœ¨æ—¶é—´ $t = 0$ å¤„æœ‰ä¸€ä¸ªå°–å³°çš„æƒ…å†µä¸‹ï¼Œç»†èƒ $j$ ä¸­å°–å³°çš„å•ä½æ—¶é—´æ¦‚ç‡ï¼ˆå°–å³°ç‡ï¼‰ï¼›é•¿æ—¶é—´å¤„çš„å¹³å°åº”è¯¥æ˜¯å¹³å‡ç‡ $r_{j} = \langle\sigma_{j}\rangle/\Delta\tau$ã€‚æœ‰ä¸€ä¸ªå®½åº¦çº¦ä¸º 100 ms çš„å³°å€¼ï¼Œä¸è§†è§‰è¾“å…¥ä¸­çš„æ—¶é—´å°ºåº¦æœ‰å…³ï¼Œæ’å›¾ä¸­å¼ºè°ƒäº†ä¸€ä¸ªå®½åº¦çº¦ä¸º 20ms çš„å³°å€¼ï¼›è¿™æ¿€å‘äº†é€‰æ‹©ç®±å­å¤§å°çš„åŠ¨æœºã€‚(D) è·¨è¶Š $N = 40$ ä¸ªç¥ç»å…ƒç¾¤ä½“çš„ï¼ˆéå¯¹è§’çº¿ï¼‰ç›¸å…³ç³»æ•°çš„åˆ†å¸ƒï¼Œæ¥è‡ªæ–¹ç¨‹ï¼ˆ24ï¼‰ã€‚(E) åœ¨åŒä¸€æ—¶é—´æ®µå†… $N = 40$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªæ´»è·ƒçš„æ¦‚ç‡ï¼ˆçº¢è‰²ï¼‰ä¸å¦‚æœæ¯ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–ç¥ç»å…ƒçš„é¢„æœŸï¼ˆè“è‰²ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚è™šçº¿åˆ†åˆ«ä¸ºæŒ‡æ•°ï¼ˆçº¢è‰²ï¼‰å’Œæ³Šæ¾ï¼ˆè“è‰²ï¼‰ï¼Œä»¥å¼•å¯¼çœ¼ç›ã€‚(F) ä¸åŒäºŒè¿›åˆ¶æ¨¡å¼çš„é¢„æµ‹å‘ç”Ÿç‡ä¸è§‚å¯Ÿåˆ°çš„å‘ç”Ÿç‡ï¼Œå¯¹äºç‹¬ç«‹æ¨¡å‹ $P_{1}$ [æ–¹ç¨‹ï¼ˆ29ï¼Œ30ï¼‰ï¼Œè“è‰²] å’Œæˆå¯¹æœ€å¤§ç†µæ¨¡å‹ $P_{2}$ [æ–¹ç¨‹ï¼ˆ35ï¼Œ33ï¼‰ï¼Œçº¢è‰²]ã€‚</p>
</blockquote>
<blockquote>
<p>If we define as usual the fluctuations around the mean,</p>
<p>$$
\delta\sigma_{i} = \sigma_{i} - \langle\sigma_{i}\rangle,
$$</p>
<p>then the data sets were large enough to get good estimates of the covariance</p>
<p>$$
C_{ij} = \langle\delta\sigma_{i}\delta\sigma_{j}\rangle = \langle\sigma_{i}\sigma_{j}\rangle_{c}
$$</p>
<p>where $\langle\cdots\rangle_{c}$ denotes the connected part of the correlations; in many cases we have more intuition about the correlation matrix</p>
<p>$$
\widetilde{C}_{ij} = \frac{C_{ij}}{\sqrt{C_{ii}C_{jj}}}
$$</p>
<p>Importantly, these pairwise correlations are weak: almost all of the $|\widetilde{C}_{i\neq j}|&lt;0.1$, and the bulk of these correlations are just a few percent (Fig 7D). The recordings are long enough that these weak correlations are statistically significant, and almost none of the matrix elements are zero within errors. Correlations thus are weak and widespread, which seems to be common across many different regions of the brain.</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·å®šä¹‰å›´ç»•å‡å€¼çš„æ³¢åŠ¨ï¼Œ</p>
<p>$$
\delta\sigma_{i} = \sigma_{i} - \langle\sigma_{i}\rangle,
$$</p>
<p>é‚£ä¹ˆæ•°æ®é›†è¶³å¤Ÿå¤§ï¼Œå¯ä»¥å¾ˆå¥½åœ°ä¼°è®¡åæ–¹å·®</p>
<p>$$
C_{ij} = \langle\delta\sigma_{i}\delta\sigma_{j}\rangle = \langle\sigma_{i}\sigma_{j}\rangle_{c}
$$</p>
<p>å…¶ä¸­ $\langle\cdots\rangle_{c}$ è¡¨ç¤ºç›¸å…³æ€§çš„è¿æ¥éƒ¨åˆ†ï¼›åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯¹ç›¸å…³çŸ©é˜µæœ‰æ›´å¤šçš„ç›´è§‰</p>
<p>$$
\widetilde{C}_{ij} = \frac{C_{ij}}{\sqrt{C_{ii}C_{jj}}}
$$</p>
<p>é‡è¦çš„æ˜¯ï¼Œè¿™äº›æˆå¯¹ç›¸å…³æ€§æ˜¯å¼±çš„ï¼šå‡ ä¹æ‰€æœ‰çš„ $|\widetilde{C}_{i\neq j}|&lt;0.1$ï¼Œè€Œä¸”è¿™äº›ç›¸å…³æ€§çš„ä¸»ä½“åªæœ‰å‡ ä¸ªç™¾åˆ†ç‚¹ï¼ˆå›¾ 7Dï¼‰ã€‚è®°å½•æ—¶é—´è¶³å¤Ÿé•¿ï¼Œè¿™äº›å¾®å¼±çš„ç›¸å…³æ€§åœ¨ç»Ÿè®¡ä¸Šæ˜¯æ˜¾è‘—çš„ï¼Œå¹¶ä¸”çŸ©é˜µå…ƒç´ å‡ ä¹æ²¡æœ‰åœ¨è¯¯å·®èŒƒå›´å†…ä¸ºé›¶ã€‚å› æ­¤ï¼Œç›¸å…³æ€§æ˜¯å¼±è€Œå¹¿æ³›çš„ï¼Œè¿™ä¼¼ä¹åœ¨å¤§è„‘çš„è®¸å¤šä¸åŒåŒºåŸŸä¸­å¾ˆå¸¸è§ã€‚</p>
<blockquote>
<p>If we look just at two neurons, the approximation that they are independent of one another is very good, because the correlations are so weak. But if we look more globally then the widespread correlations combine to have qualitative effects. As an example, we can ask for the probability that $K$ out of $N = 40$ neurons are active in the same time bin, $P_{N}(K)$, and we find that this has a much longer tail than expected if the cells were independent (Fig 7E); simultaneous activity of $K = 10$ neurons already is $\sim 10^{3}\times$ more likely than in the independent model.</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬åªçœ‹ä¸¤ä¸ªç¥ç»å…ƒï¼Œç”±äºç›¸å…³æ€§éå¸¸å¼±ï¼Œå®ƒä»¬ç›¸äº’ç‹¬ç«‹çš„è¿‘ä¼¼æ˜¯éå¸¸å¥½çš„ã€‚ä½†å¦‚æœæˆ‘ä»¬æ›´å…¨é¢åœ°è§‚å¯Ÿï¼Œé‚£ä¹ˆå¹¿æ³›çš„ç›¸å…³æ€§ä¼šç»“åˆèµ·æ¥äº§ç”Ÿå®šæ€§çš„å½±å“ã€‚ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è¯¢é—®åœ¨åŒä¸€æ—¶é—´æ®µå†… $N = 40$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªæ´»è·ƒçš„æ¦‚ç‡ $P_{N}(K)$ï¼Œæˆ‘ä»¬å‘ç°å¦‚æœç»†èƒæ˜¯ç‹¬ç«‹çš„ï¼Œè¿™ä¸ªæ¦‚ç‡çš„å°¾éƒ¨è¦é•¿å¾—å¤šï¼ˆå›¾ 7Eï¼‰ï¼›$K = 10$ ä¸ªç¥ç»å…ƒçš„åŒæ—¶æ´»åŠ¨å·²ç»æ¯”ç‹¬ç«‹æ¨¡å‹é«˜å‡ºçº¦ $10^{3}$ å€ã€‚</p>
<blockquote>
<p>If we focus on $N = 10$ neurons then the experiments are long enough to sample all $\Omega\sim 10^{3}$ states, and the probabilities of these different binary words depart dramatically from the predictions of an independent model (Fig 7F). If we group the different binary words by the total number of active neurons, then the predictions of the independent model actually are antiâ€“correlated with the real data. We emphasize that these failures occur despite the fact that pairwise correlations are weak, and that they are visible at a relatively modest $N = 10$.</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬å…³æ³¨ $N = 10$ ä¸ªç¥ç»å…ƒï¼Œé‚£ä¹ˆå®éªŒæ—¶é—´è¶³å¤Ÿé•¿ï¼Œå¯ä»¥å¯¹æ‰€æœ‰ $\Omega\sim 10^{3}$ çŠ¶æ€è¿›è¡Œé‡‡æ ·ï¼Œå¹¶ä¸”è¿™äº›ä¸åŒäºŒè¿›åˆ¶å­—çš„æ¦‚ç‡ä¸ç‹¬ç«‹æ¨¡å‹çš„é¢„æµ‹æœ‰æ˜¾è‘—åç¦»ï¼ˆå›¾ 7Fï¼‰ã€‚å¦‚æœæˆ‘ä»¬æŒ‰æ´»è·ƒç¥ç»å…ƒçš„æ€»æ•°å¯¹ä¸åŒçš„äºŒè¿›åˆ¶å­—è¿›è¡Œåˆ†ç»„ï¼Œé‚£ä¹ˆç‹¬ç«‹æ¨¡å‹çš„é¢„æµ‹å®é™…ä¸Šä¸çœŸå®æ•°æ®å‘ˆè´Ÿç›¸å…³ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œå°½ç®¡æˆå¯¹ç›¸å…³æ€§å¾ˆå¼±ï¼Œä½†è¿™äº›å¤±è´¥ä»ç„¶å‘ç”Ÿï¼Œå¹¶ä¸”å®ƒä»¬åœ¨ç›¸å¯¹é€‚åº¦çš„ $N = 10$ ä¸‹æ˜¯å¯è§çš„ã€‚</p>
<blockquote>
<p>If we want to build a model for the patterns of activity in networks of neurons it certainly makes sense to insist that we match the mean activity of each cell. At the risk of being pedantic, what this means explicitly is that we are looking for a probability distribution over network states, $P_{1}(\vec{\sigma})$ that has the maximum entropy while correctly predicting the expectation values</p>
<p>$$
m_{i} \equiv \langle\sigma_{i}\rangle_{\text{expt}} = \langle\sigma_{i}\rangle_{P_{1}}
$$</p>
<p>Referring back to Eq (17), the observables that we constrain become</p>
<p>$$
\{f_{\mu}^{(1)}\}\rightarrow \{\sigma_{i}\}
$$</p>
<p>note that $i = 1, 2,\cdots, N$, where $N$ is the number of neurons. To implement these constraints we need one Lagrange multiplier for each neuron, and it is convenient to write this multiplier as an â€œeffective fieldâ€ $h_{i}$, so that the general Eqs (20, 21) become</p>
<p>$$
\begin{aligned}
P_{1}(\vec{\sigma}) &amp;= \frac{1}{Z_{1}}\exp{[-E_{1}(\vec{\sigma})]}\\
E_{1}(\vec{\sigma}) &amp;= \sum_{\mu}\lambda_{\mu}^{(1)}f_{\mu}^{(1)}\\
&amp;= \sum_{i=1}^{N}h_{i}\sigma_{i}
\end{aligned}
$$</p>
<p>We notice that $E_1$ is the energy function for independent spins in local fields, and so the probability distribution over states factorizes,</p>
<p>$$
P_{1}(\vec{\sigma}) \propto \prod_{i=1}^{N}e^{-h_{i}\sigma_{i}}
$$</p>
<p>Thus a maximum entropy model which matches only the mean activities of individual neurons is a model in which the activity of each cell is independent of all the others. We have seen that this model is in dramatic disagreement with the data.</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬æƒ³ä¸ºç¥ç»å…ƒç½‘ç»œä¸­çš„æ´»åŠ¨æ¨¡å¼æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼ŒåšæŒåŒ¹é…æ¯ä¸ªç»†èƒçš„å¹³å‡æ´»åŠ¨å½“ç„¶æ˜¯æœ‰æ„ä¹‰çš„ã€‚å†’ç€å•°å—¦çš„é£é™©ï¼Œè¿™æ˜ç¡®æ„å‘³ç€æˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªç½‘ç»œçŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒ $P_{1}(\vec{\sigma})$ï¼Œè¯¥åˆ†å¸ƒå…·æœ‰æœ€å¤§ç†µï¼ŒåŒæ—¶æ­£ç¡®é¢„æµ‹æœŸæœ›å€¼</p>
<p>$$
m_{i} \equiv \langle\sigma_{i}\rangle_{\text{expt}} = \langle\sigma_{i}\rangle_{P_{1}}
$$</p>
<p>å›åˆ°æ–¹ç¨‹ï¼ˆ17ï¼‰ï¼Œæˆ‘ä»¬çº¦æŸçš„å¯è§‚å¯Ÿé‡å˜ä¸º</p>
<p>$$
\{f_{\mu}^{(1)}\}\rightarrow \{\sigma_{i}\}
$$</p>
<p>æ³¨æ„ $i = 1, 2,\cdots, N$ï¼Œå…¶ä¸­ $N$ æ˜¯ç¥ç»å…ƒçš„æ•°é‡ã€‚ä¸ºäº†å®ç°è¿™äº›çº¦æŸï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªç¥ç»å…ƒä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œå¹¶ä¸”å°†è¿™ä¸ªä¹˜æ•°å†™æˆâ€œæœ‰æ•ˆåœºâ€ $h_{i}$ æ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œå› æ­¤ä¸€èˆ¬çš„æ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰å˜ä¸º</p>
<p>$$
\begin{aligned}
P_{1}(\vec{\sigma}) &amp;= \frac{1}{Z_{1}}\exp{[-E_{1}(\vec{\sigma})]}\\
E_{1}(\vec{\sigma}) &amp;= \sum_{\mu}\lambda_{\mu}^{(1)}f_{\mu}^{(1)}\\
&amp;= \sum_{i=1}^{N}h_{i}\sigma_{i}
\end{aligned}
$$</p>
<p>æˆ‘ä»¬æ³¨æ„åˆ° $E_1$ æ˜¯å±€éƒ¨åœºä¸­ç‹¬ç«‹è‡ªæ—‹çš„èƒ½é‡å‡½æ•°ï¼Œå› æ­¤çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥åˆ†è§£ä¸ºï¼Œ</p>
<p>$$
P_{1}(\vec{\sigma}) \propto \prod_{i=1}^{N}e^{-h_{i}\sigma_{i}}
$$</p>
<p>å› æ­¤ï¼Œä¸€ä¸ªä»…åŒ¹é…å•ä¸ªç¥ç»å…ƒå¹³å‡æ´»åŠ¨çš„æœ€å¤§ç†µæ¨¡å‹æ˜¯ä¸€ä¸ªæ¯ä¸ªç»†èƒçš„æ´»åŠ¨éƒ½ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–ç»†èƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°è¿™ä¸ªæ¨¡å‹ä¸æ•°æ®æœ‰æ˜¾è‘—çš„ä¸ä¸€è‡´ã€‚</p>
<blockquote>
<p>A natural first step in trying to capture the nonindependence of neurons is to build a maximum entropy model that matches pairwise correlations. Thus, we are looking for a distribution $P_{2}(\sigma)$ that has maximum entropy while matching the mean activities as in Eq (25) and also the covariance of activity</p>
<p>$$
C_{ij}\equiv \langle\delta\sigma_{i}\delta\sigma_{j}\rangle_{\text{expt}} = \langle\delta\sigma_{i}\delta\sigma_{j}\rangle_{P_{2}}
$$</p>
<p>In the language of Eq (17) this means that we have a second set of relevant observables</p>
<p>$$
\{f_{\nu}^{(2)}\} \rightarrow \{\sigma_{i}\sigma_{j}\}
$$</p>
<p>As before we need one Lagrange multiplier for each constrained observable, and it is useful to think of the Lagrange multiplier that constrains $\sigma_{i}\sigma_{j}$ as being a â€œspinspinâ€ coupling $\lambda_{ij} = J_{ij}$. Recalling that each extra constraint adds a term to the effective energy function, Eqs (20, 21) become</p>
<p>$$
\begin{aligned}
P_{2}(\vec{\sigma}) &amp;= \frac{1}{Z_{2}(\{h_{i};J_{ij}\})}e^{-E_{2}(\vec{\sigma})}\\
E_{2}(\vec{\sigma}) &amp;= \sum_{\mu}\lambda_{\mu}^{(1)}f_{\mu}^{(1)} + \sum_{\mu}\lambda_{\mu}^{(2)}f_{\mu}^{(2)}\\
&amp;= \sum_{i=1}^{N}h_{i}\sigma_{i} + \frac{1}{2}\sum_{i\neq j}J_{ij}\sigma_{i}\sigma_{j}
\end{aligned}
$$</p>
<p>This is exactly an Ising model with pairwise interactions among the spinsâ€”not an analogy but a mathematical equivalence.</p>
</blockquote>
<p>æ•æ‰ç¥ç»å…ƒéç‹¬ç«‹æ€§çš„ä¸€ä¸ªè‡ªç„¶ç¬¬ä¸€æ­¥æ˜¯æ„å»ºä¸€ä¸ªåŒ¹é…æˆå¯¹ç›¸å…³æ€§çš„æœ€å¤§ç†µæ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªåˆ†å¸ƒ $P_{2}(\sigma)$ï¼Œè¯¥åˆ†å¸ƒå…·æœ‰æœ€å¤§ç†µï¼ŒåŒæ—¶åŒ¹é…æ–¹ç¨‹ï¼ˆ25ï¼‰ä¸­çš„å¹³å‡æ´»åŠ¨ä»¥åŠæ´»åŠ¨çš„åæ–¹å·®</p>
<p>$$
C_{ij}\equiv \langle\delta\sigma_{i}\delta\sigma_{j}\rangle_{\text{expt}} = \langle\delta\sigma_{i}\delta\sigma_{j}\rangle_{P_{2}}
$$</p>
<p>ç”¨æ–¹ç¨‹ï¼ˆ17ï¼‰çš„è¯­è¨€æ¥è¯´ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æœ‰ç¬¬äºŒç»„ç›¸å…³çš„å¯è§‚å¯Ÿé‡</p>
<p>$$
\{f_{\nu}^{(2)}\} \rightarrow \{\sigma_{i}\sigma_{j}\}
$$</p>
<p>åƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªå—çº¦æŸçš„å¯è§‚å¯Ÿé‡ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œå¹¶ä¸”å°†çº¦æŸ $\sigma_{i}\sigma_{j}$ çš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°è§†ä¸ºâ€œè‡ªæ—‹-è‡ªæ—‹â€è€¦åˆ $\lambda_{ij} = J_{ij}$ æ˜¯æœ‰ç”¨çš„ã€‚å›æƒ³ä¸€ä¸‹ï¼Œæ¯ä¸ªé¢å¤–çš„çº¦æŸéƒ½ä¼šå‘æœ‰æ•ˆèƒ½é‡å‡½æ•°æ·»åŠ ä¸€é¡¹ï¼Œæ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰å˜ä¸º</p>
<p>$$
\begin{aligned}
P_{2}(\vec{\sigma}) &amp;= \frac{1}{Z_{2}(\{h_{i};J_{ij}\})}e^{-E_{2}(\vec{\sigma})}\\
E_{2}(\vec{\sigma}) &amp;= \sum_{\mu}\lambda_{\mu}^{(1)}f_{\mu}^{(1)} + \sum_{\mu}\lambda_{\mu}^{(2)}f_{\mu}^{(2)}\\
&amp;= \sum_{i=1}^{N}h_{i}\sigma_{i} + \frac{1}{2}\sum_{i\neq j}J_{ij}\sigma_{i}\sigma_{j}
\end{aligned}
$$</p>
<p>è¿™æ­£æ˜¯å…·æœ‰è‡ªæ—‹ä¹‹é—´æˆå¯¹ç›¸äº’ä½œç”¨çš„ Ising æ¨¡å‹â€”â€”ä¸æ˜¯ç±»æ¯”ï¼Œè€Œæ˜¯æ•°å­¦ç­‰ä»·ã€‚</p>
<blockquote>
<p>Ising models for networks of neurons have a long history, as described in Â§II.C. In their earliest appearance, these models emerged from a hypothetical, simplified model of the underlying dynamics. Here they emerge as the least structured models consistent with measured properties of the network. As a result, we arrive not at some arbitrary Ising model, where we are free to choose the fields and couplings, but at a particular model that describes the actual network of neurons we are observing. To complete this construction we have to adjust the fields and couplings to match the observed mean activities and correlations. Concretely we have to solve Eqs (25, 31), which can be rewritten as</p>
<p>$$
\begin{aligned}
\langle \sigma_{i}\rangle_{\text{expt}} &amp;= \langle\sigma_{i}\rangle_{P_{2}} = \frac{\partial \ln{Z_{2}(\{h_{i};J_{ij}\})}}{\partial h_{i}}\\
\langle\sigma_{i}\sigma_{j}\rangle_{\text{expt}} &amp;= \langle\sigma_{i}\sigma_{j}\rangle_{P_{2}} = \frac{\partial \ln{Z_{2}(\{h_{i};J_{ij}\})}}{\partial J_{ij}}
\end{aligned}
$$</p>
<p>With $N = 10$ neurons this is challenging but can be done exactly, since the partition function is a sum over just $\Omega\sim 1000$ terms. Once we are done, the model is specified completely. Anything that we compute is a prediction, and there is no room to adjust parameters in search of better agreement with the data.</p>
</blockquote>
<p>ç¥ç»å…ƒç½‘ç»œçš„ Ising æ¨¡å‹æœ‰ç€æ‚ ä¹…çš„å†å²ï¼Œå¦‚ Â§II.C æ‰€è¿°ã€‚åœ¨å®ƒä»¬æœ€æ—©å‡ºç°æ—¶ï¼Œè¿™äº›æ¨¡å‹æºè‡ªå¯¹æ½œåœ¨åŠ¨åŠ›å­¦çš„å‡è®¾æ€§ç®€åŒ–æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œå®ƒä»¬ä½œä¸ºä¸ç½‘ç»œçš„æµ‹é‡å±æ€§ä¸€è‡´çš„æœ€å°‘ç»“æ„åŒ–æ¨¡å‹å‡ºç°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸æ˜¯å¾—åˆ°æŸä¸ªä»»æ„çš„ Ising æ¨¡å‹ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å¯ä»¥è‡ªç”±é€‰æ‹©åœºå’Œè€¦åˆï¼Œè€Œæ˜¯å¾—åˆ°ä¸€ä¸ªæè¿°æˆ‘ä»¬æ­£åœ¨è§‚å¯Ÿçš„å®é™…ç¥ç»å…ƒç½‘ç»œçš„ç‰¹å®šæ¨¡å‹ã€‚ä¸ºäº†å®Œæˆè¿™ä¸ªæ„å»ºï¼Œæˆ‘ä»¬å¿…é¡»è°ƒæ•´åœºå’Œè€¦åˆä»¥åŒ¹é…è§‚å¯Ÿåˆ°çš„å¹³å‡æ´»åŠ¨å’Œç›¸å…³æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¿…é¡»è§£å†³æ–¹ç¨‹ï¼ˆ25ï¼Œ31ï¼‰ï¼Œå®ƒä»¬å¯ä»¥é‡å†™ä¸º</p>
<p>$$
\begin{aligned}
\langle \sigma_{i}\rangle_{\text{expt}} &amp;= \langle\sigma_{i}\rangle_{P_{2}} = \frac{\partial \ln{Z_{2}(\{h_{i};J_{ij}\})}}{\partial h_{i}}\\
\langle\sigma_{i}\sigma_{j}\rangle_{\text{expt}} &amp;= \langle\sigma_{i}\sigma_{j}\rangle_{P_{2}} = \frac{\partial \ln{Z_{2}(\{h_{i};J_{ij}\})}}{\partial J_{ij}}
\end{aligned}
$$</p>
<p>å¯¹äº $N = 10$ ä¸ªç¥ç»å…ƒæ¥è¯´ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†å¯ä»¥ç²¾ç¡®å®Œæˆï¼Œå› ä¸ºé…åˆ†å‡½æ•°åªæ˜¯ $\Omega\sim 1000$ é¡¹çš„æ€»å’Œã€‚ä¸€æ—¦æˆ‘ä»¬å®Œæˆï¼Œæ¨¡å‹å°±å®Œå…¨æŒ‡å®šäº†ã€‚æˆ‘ä»¬è®¡ç®—çš„ä»»ä½•ä¸œè¥¿éƒ½æ˜¯ä¸€ä¸ªé¢„æµ‹ï¼Œæ²¡æœ‰è°ƒæ•´å‚æ•°ä»¥å¯»æ±‚ä¸æ•°æ®æ›´å¥½ä¸€è‡´çš„ä½™åœ°ã€‚</p>
<blockquote>
<p>As noted above, with $N = 10$ neurons the experiments are long enough to get a reasonably full sampling of the probability distribution over $\vec{\sigma}$. This provides the most detailed possible test of the model $P_{2}$, and in Fig 7F we see that the agreement between theory and experiment is excellent, except for very rare patterns where errors in the estimate of the probability are larger. Similar results are obtained for other groups of $N = 10$ cells drawn out of the full population of $N = 40$. Quantitatively we can measure the Jensenâ€“Shannon divergence between the estimated distribution $P_{\text{data}}(\sigma)$ and the model $P_{2}(\sigma)$; across multiple choices of ten cells this fluctuates by a factor of two around $D_{JS} = 0.001$ bits, which means that it takes thousands of independent observations to distinguish the model from the data.</p>
</blockquote>
<p>æ­£å¦‚ä¸Šé¢æ‰€æŒ‡å‡ºçš„ï¼Œå¯¹äº $N = 10$ ä¸ªç¥ç»å…ƒï¼Œå®éªŒæ—¶é—´è¶³å¤Ÿé•¿ï¼Œå¯ä»¥å¯¹ $\vec{\sigma}$ ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œåˆç†å®Œæ•´çš„é‡‡æ ·ã€‚è¿™ä¸ºæ¨¡å‹ $P_{2}$ æä¾›äº†æœ€è¯¦ç»†çš„å¯èƒ½æµ‹è¯•ï¼Œåœ¨å›¾ 7F ä¸­æˆ‘ä»¬çœ‹åˆ°ç†è®ºä¸å®éªŒä¹‹é—´çš„ä¸€è‡´æ€§éå¸¸å¥½ï¼Œé™¤äº†éå¸¸ç½•è§çš„æ¨¡å¼ï¼Œå…¶ä¸­æ¦‚ç‡ä¼°è®¡çš„è¯¯å·®è¾ƒå¤§ã€‚ä»å®Œæ•´çš„ $N = 40$ ç¾¤ä½“ä¸­æŠ½å–çš„å…¶ä»– $N = 10$ ä¸ªç»†èƒç»„ä¹Ÿå¾—åˆ°äº†ç±»ä¼¼çš„ç»“æœã€‚æˆ‘ä»¬å¯ä»¥å®šé‡åœ°æµ‹é‡ä¼°è®¡åˆ†å¸ƒ $P_{\text{data}}(\sigma)$ ä¸æ¨¡å‹ $P_{2}(\sigma)$ ä¹‹é—´çš„ Jensenâ€“Shannon æ•£åº¦ï¼›åœ¨å¤šä¸ªåä¸ªç»†èƒçš„é€‰æ‹©ä¸­ï¼Œè¿™ä¸ªå€¼å›´ç»• $D_{JS} = 0.001$ æ¯”ç‰¹æ³¢åŠ¨äº†ä¸¤å€ï¼Œè¿™æ„å‘³ç€éœ€è¦æ•°åƒæ¬¡ç‹¬ç«‹è§‚å¯Ÿæ‰èƒ½åŒºåˆ†æ¨¡å‹ä¸æ•°æ®ã€‚</p>
<blockquote>
<p>The architecture of the retina is such that many individual output neurons can be driven or inhibited by a single common neuron that is internal to the circuitry. This is one of many reasons that one might expect significant combinatorial regulation in the patterns of activity, and there were serious efforts to search for these effects (Schnitzer and Meister, 2003). The success of a pairwise model thus came as a considerable surprise.</p>
</blockquote>
<p>è§†ç½‘è†œçš„ç»“æ„ä½¿å¾—è®¸å¤šä¸ªä½“è¾“å‡ºç¥ç»å…ƒå¯ä»¥è¢«ç”µè·¯å†…éƒ¨çš„å•ä¸ªå…±åŒç¥ç»å…ƒé©±åŠ¨æˆ–æŠ‘åˆ¶ã€‚è¿™æ˜¯è®¸å¤šåŸå› ä¹‹ä¸€ï¼Œäººä»¬å¯èƒ½ä¼šæœŸæœ›åœ¨æ´»åŠ¨æ¨¡å¼ä¸­å­˜åœ¨æ˜¾è‘—çš„ç»„åˆè°ƒèŠ‚ï¼Œå¹¶ä¸”æ›¾ç»æœ‰è®¤çœŸåŠªåŠ›å»å¯»æ‰¾è¿™äº›æ•ˆåº”ï¼ˆSchnitzer å’Œ Meisterï¼Œ2003ï¼‰ã€‚å› æ­¤ï¼Œæˆå¯¹æ¨¡å‹çš„æˆåŠŸä»¤äººç›¸å½“æƒŠè®¶ã€‚</p>
<blockquote>
<p>The results in the salamander retina, with natural inputs, were quickly confirmed in the primate retina using simpler inputs (Shlens et al., 2006). Those experiments covered a larger area and thus could focus on subâ€“populations of neurons belonging to a single class, which are arrayed in a relatively regular lattice. In this case not only did the pairwise model work very well, but the effective interactions $J_{ij}$ were confined largely to nearest neighbors on this lattice.</p>
</blockquote>
<p>è¾èˆè§†ç½‘è†œä¸­ä½¿ç”¨è‡ªç„¶è¾“å…¥çš„ç»“æœå¾ˆå¿«åœ¨çµé•¿ç±»åŠ¨ç‰©è§†ç½‘è†œä¸­å¾—åˆ°äº†ç¡®è®¤ï¼Œä½¿ç”¨äº†æ›´ç®€å•çš„è¾“å…¥ï¼ˆShlens ç­‰äººï¼Œ2006ï¼‰ã€‚è¿™äº›å®éªŒè¦†ç›–äº†æ›´å¤§çš„åŒºåŸŸï¼Œå› æ­¤å¯ä»¥ä¸“æ³¨äºå±äºå•ä¸€ç±»åˆ«çš„ç¥ç»å…ƒäºšç¾¤ï¼Œè¿™äº›ç¥ç»å…ƒæ’åˆ—åœ¨ä¸€ä¸ªç›¸å¯¹è§„åˆ™çš„æ™¶æ ¼ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸ä»…æˆå¯¹æ¨¡å‹æ•ˆæœéå¸¸å¥½ï¼Œè€Œä¸”æœ‰æ•ˆç›¸äº’ä½œç”¨ $J_{ij}$ ä¸»è¦å±€é™äºè¯¥æ™¶æ ¼ä¸Šçš„æœ€è¿‘é‚»ã€‚</p>
<blockquote>
<p>Pairwise maximum entropy models also were reasonably successful in describing patterns of activity across $N\leq 10$ neurons sampled from a cluster of cortical neurons kept alive in a dish (Tang et al., 2008). This work also pointed to the fact that dynamics did not correspond to Brownian motion on the energy surface.</p>
</blockquote>
<p>æˆå¯¹æœ€å¤§ç†µæ¨¡å‹åœ¨æè¿°ä»åŸ¹å…»çš¿ä¸­ä¿æŒæ´»åŠ›çš„ä¸€ç°‡çš®å±‚ç¥ç»å…ƒä¸­é‡‡æ ·çš„ $N\leq 10$ ä¸ªç¥ç»å…ƒçš„æ´»åŠ¨æ¨¡å¼æ–¹é¢ä¹Ÿç›¸å½“æˆåŠŸï¼ˆTang ç­‰äººï¼Œ2008ï¼‰ã€‚è¿™é¡¹å·¥ä½œè¿˜æŒ‡å‡ºï¼ŒåŠ¨åŠ›å­¦å¹¶ä¸å¯¹åº”äºèƒ½é‡è¡¨é¢ä¸Šçš„å¸ƒæœ—è¿åŠ¨ã€‚</p>
<blockquote>
<p>These early successes with small numbers of neurons raised many questions. For example, the interaction matrix $J_{ij}$ contained a mix of positive and negative terms, suggesting that frustration could lead to many local minima of the energy function or equivalently local maxima of the probability $P(\vec{\sigma})$, as in the Hopfield model (Â§II.C); could these â€œattractorsâ€ have a function in representing the visual world? Relatedly, an important consequence of the collective behavior in the Ising model is that if we know that state of all neurons in the network but one, then we have a parameterâ€“free prediction for the probability that this last neuron will be active; does this allow for error correction? To address these and other issues one must go beyond $N\sim 10$ cells, which was already possible experimentally. But at larger $N$ one needs more powerful methods for solving the inverse problem that is at the heart of the maximum entropy construction, as described in Appendix B.</p>
</blockquote>
<p>è¿™äº›æ—©æœŸåœ¨å°‘é‡ç¥ç»å…ƒä¸Šçš„æˆåŠŸå¼•å‘äº†è®¸å¤šé—®é¢˜ã€‚ä¾‹å¦‚ï¼Œäº¤äº’çŸ©é˜µ $J_{ij}$ åŒ…å«æ­£è´Ÿæ··åˆé¡¹ï¼Œè¡¨æ˜é˜»æŒ«å¯èƒ½å¯¼è‡´èƒ½é‡å‡½æ•°çš„è®¸å¤šå±€éƒ¨æå°å€¼ï¼Œæˆ–è€…ç­‰ä»·åœ°ï¼Œæ¦‚ç‡ $P(\vec{\sigma})$ çš„å±€éƒ¨æå¤§å€¼ï¼Œå°±åƒ Hopfield æ¨¡å‹ï¼ˆÂ§II.Cï¼‰ä¸­ä¸€æ ·ï¼›è¿™äº›â€œå¸å¼•å­â€åœ¨è¡¨ç¤ºè§†è§‰ä¸–ç•Œæ–¹é¢æ˜¯å¦å…·æœ‰åŠŸèƒ½ï¼Ÿç›¸å…³åœ°ï¼ŒIsing æ¨¡å‹ä¸­é›†ä½“è¡Œä¸ºçš„ä¸€ä¸ªé‡è¦åæœæ˜¯ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“ç½‘ç»œä¸­é™¤ä¸€ä¸ªç¥ç»å…ƒå¤–æ‰€æœ‰ç¥ç»å…ƒçš„çŠ¶æ€ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥æ— å‚æ•°åœ°é¢„æµ‹è¿™ä¸ªæœ€åä¸€ä¸ªç¥ç»å…ƒæ˜¯å¦ä¼šæ´»è·ƒï¼›è¿™æ˜¯å¦å…è®¸çº é”™ï¼Ÿä¸ºäº†å¤„ç†è¿™äº›å’Œå…¶ä»–é—®é¢˜ï¼Œå¿…é¡»è¶…è¶Š $N\sim 10$ ä¸ªç»†èƒï¼Œè¿™åœ¨å®éªŒä¸Šå·²ç»æ˜¯å¯èƒ½çš„ã€‚ä½†åœ¨æ›´å¤§çš„ $N$ ä¸‹ï¼Œéœ€è¦æ›´å¼ºå¤§çš„æ–¹æ³•æ¥è§£å†³æœ€å¤§ç†µæ„é€ æ ¸å¿ƒçš„é€†é—®é¢˜ï¼Œå¦‚é™„å½• B æ‰€è¿°ã€‚</p>
<blockquote>
<p>The equivalence to equilibrium models entices us to describe the couplings $J_{ij}$ as â€œinteractions,â€ but there is no reason to think that these correspond to genuine connections between cells. In particular, $J_{ij}$ is symmetric because it is an effective interaction driving the equaltime correlations of activity in cells $i$ and $j$, and these correlations are symmetric by definition. If we go beyond single time slices to describe trajectories of activity over time, then with multiple cells the effective interactions can become asymmetric and break timereversal invariance.</p>
</blockquote>
<p>ä¸å¹³è¡¡æ¨¡å‹çš„ç­‰ä»·æ€§è¯±ä½¿æˆ‘ä»¬å°†è€¦åˆ $J_{ij}$ æè¿°ä¸ºâ€œç›¸äº’ä½œç”¨â€ï¼Œä½†æ²¡æœ‰ç†ç”±è®¤ä¸ºå®ƒä»¬å¯¹åº”äºç»†èƒä¹‹é—´çš„çœŸæ­£è¿æ¥ã€‚ç‰¹åˆ«æ˜¯ï¼Œ$J_{ij}$ æ˜¯å¯¹ç§°çš„ï¼Œå› ä¸ºå®ƒæ˜¯é©±åŠ¨ç»†èƒ $i$ å’Œ $j$ æ´»åŠ¨çš„åŒæ—¶ç›¸å…³æ€§çš„æœ‰æ•ˆç›¸äº’ä½œç”¨ï¼Œè€Œè¿™äº›ç›¸å…³æ€§æŒ‰å®šä¹‰æ˜¯å¯¹ç§°çš„ã€‚å¦‚æœæˆ‘ä»¬è¶…è¶Šå•ä¸ªæ—¶é—´ç‰‡æ¥æè¿°éšæ—¶é—´å˜åŒ–çš„æ´»åŠ¨è½¨è¿¹ï¼Œé‚£ä¹ˆå¯¹äºå¤šä¸ªç»†èƒï¼Œæœ‰æ•ˆç›¸äº’ä½œç”¨å¯ä»¥å˜å¾—ä¸å¯¹ç§°å¹¶æ‰“ç ´æ—¶é—´åæ¼”ä¸å˜æ€§ã€‚</p>
<blockquote>
<p>Before leaving the early work, it is useful to step back and ask about the goals and hopes from that time. As reviewed above, the use of statistical physics models for neural networks has a deep history. Saying that the brain is described by an Ising model captured both the optimism and (one must admit) the naÄ± Ìˆvet Ìe of the physics community in approaching the phenomena of life. One could balance optimism and naÄ± Ìˆvet Ìe by retreating to the position that these models are metaphors, illustrating what could happen rather than being theories of what actually happens. The success of maximum entropy models in the retina gave an example of how statistical physics ideas could provide a quantitative theory for networks of real neurons.</p>
</blockquote>
<p>åœ¨ç¦»å¼€æ—©æœŸå·¥ä½œä¹‹å‰ï¼Œé€€ä¸€æ­¥é—®ä¸€ä¸‹å½“æ—¶çš„ç›®æ ‡å’Œå¸Œæœ›æ˜¯æœ‰ç”¨çš„ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œç¥ç»ç½‘ç»œä½¿ç”¨ç»Ÿè®¡ç‰©ç†æ¨¡å‹æœ‰ç€æ·±åšçš„å†å²ã€‚è¯´å¤§è„‘ç”± Ising æ¨¡å‹æè¿°æ—¢æ•æ‰äº†ä¹è§‚ä¸»ä¹‰ï¼Œä¹Ÿæ•æ‰äº†ï¼ˆå¿…é¡»æ‰¿è®¤ï¼‰ç‰©ç†å­¦ç•Œåœ¨æ¥è¿‘ç”Ÿå‘½ç°è±¡æ—¶çš„å¤©çœŸã€‚é€šè¿‡é€€å›åˆ°è¿™äº›æ¨¡å‹æ˜¯éšå–»çš„ä½ç½®ï¼Œå¯ä»¥å¹³è¡¡ä¹è§‚ä¸»ä¹‰å’Œå¤©çœŸï¼Œè¯´æ˜å¯èƒ½ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œè€Œä¸æ˜¯å®é™…å‘ç”Ÿçš„ç†è®ºã€‚è§†ç½‘è†œä¸­æœ€å¤§ç†µæ¨¡å‹çš„æˆåŠŸæä¾›äº†ä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ç»Ÿè®¡ç‰©ç†æ€æƒ³å¦‚ä½•ä¸ºçœŸå®ç¥ç»å…ƒç½‘ç»œæä¾›å®šé‡ç†è®ºã€‚</p>
<h1 id="larger-networks-of-neurons">Larger networks of neurons<a hidden class="anchor" aria-hidden="true" href="#larger-networks-of-neurons">#</a></h1>
<blockquote>
<p>The use of maximum entropy for networks of real neurons quickly triggered almost all possible reactions: (a) It should never work, because systems are not in equilibrium, have combinational interactions, $\cdots$ . (b) It could work, but only under uninteresting conditions. (c) It should always work, since these models are very expressive. (d) It works at small $N$ , but this is a poor guide to what will happen at large $N$ . (e) Sure, but why not use [favorite alternative], for which we have efficient algorithms?</p>
</blockquote>
<p>å¯¹äºçœŸå®ç¥ç»å…ƒç½‘ç»œä½¿ç”¨æœ€å¤§ç†µè¿…é€Ÿå¼•å‘äº†å‡ ä¹æ‰€æœ‰å¯èƒ½çš„ååº”ï¼šï¼ˆaï¼‰å®ƒæ°¸è¿œä¸ä¼šèµ·ä½œç”¨ï¼Œå› ä¸ºç³»ç»Ÿä¸å¤„äºå¹³è¡¡çŠ¶æ€ï¼Œå…·æœ‰ç»„åˆç›¸äº’ä½œç”¨ï¼Œ$\cdots$ï¼ˆbï¼‰å®ƒå¯èƒ½èµ·ä½œç”¨ï¼Œä½†ä»…åœ¨æ— è¶£çš„æ¡ä»¶ä¸‹ã€‚ï¼ˆcï¼‰å®ƒåº”è¯¥æ€»æ˜¯æœ‰æ•ˆï¼Œå› ä¸ºè¿™äº›æ¨¡å‹éå¸¸æœ‰è¡¨ç°åŠ›ã€‚ï¼ˆdï¼‰å®ƒåœ¨å° $N$ ä¸‹æœ‰æ•ˆï¼Œä½†è¿™å¯¹å¤§ $N$ ä¼šå‘ç”Ÿä»€ä¹ˆæ²¡æœ‰å¾ˆå¥½çš„æŒ‡å¯¼æ„ä¹‰ã€‚ï¼ˆeï¼‰å½“ç„¶ï¼Œä½†ä¸ºä»€ä¹ˆä¸ä½¿ç”¨[æœ€å–œæ¬¢çš„æ›¿ä»£æ–¹æ¡ˆ]ï¼Œæˆ‘ä»¬æœ‰é«˜æ•ˆçš„ç®—æ³•ï¼Ÿ</p>
<blockquote>
<p>Perhaps the most concrete response to these issues is just to see what happens as we move to more examples, especially in larger networks. But we should do this with several questions in mind, some of which were very explicit in the early literature (Macke et al., 2011a; Roudi et al., 2009). First, finding the maximum entropy model that matches the desired constraintsâ€”that is, solving Eqs (17)â€”becomes more difficult at larger $N$ . Can we be sure that we are testing the maximum entropy idea, and our choice of constraints, rather than the efficacy of our algorithms for solving this problem?</p>
</blockquote>
<p>ä¹Ÿè®¸å¯¹è¿™äº›é—®é¢˜æœ€å…·ä½“çš„å›åº”å°±æ˜¯çœ‹çœ‹å½“æˆ‘ä»¬è½¬å‘æ›´å¤šä¾‹å­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´å¤§ç½‘ç»œä¸­ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ä½†æˆ‘ä»¬åº”è¯¥ç‰¢è®°å‡ ä¸ªé—®é¢˜ï¼Œå…¶ä¸­ä¸€äº›åœ¨æ—©æœŸæ–‡çŒ®ä¸­éå¸¸æ˜ç¡®ï¼ˆMacke ç­‰äººï¼Œ2011aï¼›Roudi ç­‰äººï¼Œ2009ï¼‰ã€‚é¦–å…ˆï¼Œæ‰¾åˆ°åŒ¹é…æ‰€éœ€çº¦æŸçš„æœ€å¤§ç†µæ¨¡å‹â€”â€”å³è§£å†³æ–¹ç¨‹ï¼ˆ17ï¼‰â€”â€”åœ¨æ›´å¤§çš„ $N$ ä¸‹å˜å¾—æ›´åŠ å›°éš¾ã€‚æˆ‘ä»¬èƒ½å¦ç¡®å®šæˆ‘ä»¬æ­£åœ¨æµ‹è¯•æœ€å¤§ç†µçš„æƒ³æ³•ï¼Œä»¥åŠæˆ‘ä»¬é€‰æ‹©çš„çº¦æŸï¼Œè€Œä¸æ˜¯æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜çš„ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Ÿ</p>
<blockquote>
<p>Second, as N increases the maximum entropy construction becomes very data hungry. This concern often is phrased as the usual problem of â€œoverâ€“fitting,â€ when the number of parameters in our model is too large to fully constrained by the data. But in the maximum entropy formulation the problem is even more fundamental. The maximum entropy construction builds the least structured model consistent with a set of known expectation values. With a finite amount of data, if our list of expectation values is too long then the claim that we â€œknowâ€ these features of the system just isnâ€™t true, and this problem arises even before we try to build the maximum entropy model.</p>
</blockquote>
<p>å…¶æ¬¡ï¼Œéšç€ N çš„å¢åŠ ï¼Œæœ€å¤§ç†µæ„é€ å˜å¾—éå¸¸éœ€è¦æ•°æ®ã€‚è¿™ä¸ªé—®é¢˜é€šå¸¸è¢«è¡¨è¿°ä¸ºâ€œè¿‡æ‹Ÿåˆâ€çš„å¸¸è§é—®é¢˜ï¼Œå½“æˆ‘ä»¬æ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡å¤ªå¤§è€Œæ— æ³•è¢«æ•°æ®å®Œå…¨çº¦æŸæ—¶ã€‚ä½†åœ¨æœ€å¤§ç†µå…¬å¼ä¸­ï¼Œè¿™ä¸ªé—®é¢˜ç”šè‡³æ›´ä¸ºæ ¹æœ¬ã€‚æœ€å¤§ç†µæ„é€ å»ºç«‹äº†ä¸€ä¸ªä¸ä¸€ç»„å·²çŸ¥æœŸæœ›å€¼ä¸€è‡´çš„æœ€å°‘ç»“æ„åŒ–æ¨¡å‹ã€‚å¯¹äºæœ‰é™çš„æ•°æ®é‡ï¼Œå¦‚æœæˆ‘ä»¬çš„æœŸæœ›å€¼åˆ—è¡¨å¤ªé•¿ï¼Œé‚£ä¹ˆæˆ‘ä»¬â€œçŸ¥é“â€ç³»ç»Ÿçš„è¿™äº›ç‰¹å¾çš„è¯´æ³•å°±ä¸æ˜¯çœŸçš„ï¼Œå³ä½¿åœ¨æˆ‘ä»¬å°è¯•æ„å»ºæœ€å¤§ç†µæ¨¡å‹ä¹‹å‰ï¼Œè¿™ä¸ªé—®é¢˜ä¹Ÿä¼šå‡ºç°ã€‚</p>
<blockquote>
<p>Third, because correlations are spread widely in these networks, if one develops a perturbation theory around the limit of independent neurons then factors of $N$ appear in the series, e.g. for the entropy per neuron. Success at modest $N$ might thus mean that we are in a perturbative regime, which would be much less interesting. The question of whether success is perturbative is subtle, since at finite $N$ all properties of the maximum entropy model are analytic functions of the correlations, and hence if we carry perturbation theory far enough we will get the right answer (Sessak and Monasson, 2009).</p>
</blockquote>
<p>ç¬¬ä¸‰ï¼Œå› ä¸ºç›¸å…³æ€§åœ¨è¿™äº›ç½‘ç»œä¸­å¹¿æ³›ä¼ æ’­ï¼Œå¦‚æœæˆ‘ä»¬å›´ç»•ç‹¬ç«‹ç¥ç»å…ƒçš„æé™å‘å±•å¾®æ‰°ç†è®ºï¼Œé‚£ä¹ˆ $N$ çš„å› ç´ ä¼šå‡ºç°åœ¨çº§æ•°ä¸­ï¼Œä¾‹å¦‚æ¯ä¸ªç¥ç»å…ƒçš„ç†µã€‚å› æ­¤ï¼Œåœ¨é€‚åº¦çš„ $N$ ä¸‹çš„æˆåŠŸå¯èƒ½æ„å‘³ç€æˆ‘ä»¬å¤„äºå¾®æ‰°èŒƒå›´å†…ï¼Œè¿™å°†ä¸é‚£ä¹ˆæœ‰è¶£ã€‚æˆåŠŸæ˜¯å¦æ˜¯å¾®æ‰°çš„é—®é¢˜æ˜¯å¾®å¦™çš„ï¼Œå› ä¸ºåœ¨æœ‰é™çš„ $N$ ä¸‹ï¼Œæœ€å¤§ç†µæ¨¡å‹çš„æ‰€æœ‰å±æ€§éƒ½æ˜¯ç›¸å…³æ€§çš„è§£æå‡½æ•°ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å°†å¾®æ‰°ç†è®ºè¿›è¡Œå¾—è¶³å¤Ÿè¿œï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ­£ç¡®çš„ç­”æ¡ˆï¼ˆSessak å’Œ Monassonï¼Œ2009ï¼‰ã€‚</p>
<blockquote>
<p>Finally, in statistical mechanics we are used to the idea of a large $N$, thermodynamic limit. Although this carries over to model networks (Amit, 1989), it is not obvious how to use this idea in thinking about networks of real neurons. Naive extrapolation of results from maximum entropy models of $N = 10 âˆ’ 20$ neurons in the retina indicated that something special had to happen by $N\sim 200$, or else the entropy would vanish; this was interesting because $N\sim 200$ is the number cells that are â€œlookingâ€ at overlapping regions of the visual world (Schneidman et al., 2006). A more sophisticated extrapolation imagines a large population of neurons in which mean activities and pairwise correlations are drawn at random from the same distribution as found in recordings from smaller numbers of neurons (TkaË‡cik et al., 2006, 2009). This sort of extrapolation is motivated in part by the observation that â€œthermodynamicâ€ properties of the maximum entropy models learned for $N = 20$ or $N = 40$ retinal neurons match the behavior of such random models at the same $N$. If we now extrapolate to $N = 120$ there are striking collective behaviors, and we will ask if these are seen in real data from $N &gt; 100$ cells.</p>
</blockquote>
<p>æœ€åï¼Œåœ¨ç»Ÿè®¡åŠ›å­¦ä¸­ï¼Œæˆ‘ä»¬ä¹ æƒ¯äºå¤§ $N$ã€çƒ­åŠ›å­¦æé™çš„æ¦‚å¿µã€‚å°½ç®¡è¿™å¯ä»¥è½¬ç§»åˆ°æ¨¡å‹ç½‘ç»œä¸­ï¼ˆAmitï¼Œ1989ï¼‰ï¼Œä½†åœ¨æ€è€ƒçœŸå®ç¥ç»å…ƒç½‘ç»œæ—¶å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæƒ³æ³•å¹¶ä¸æ˜æ˜¾ã€‚ä»è§†ç½‘è†œä¸­ $N = 10 âˆ’ 20$ ä¸ªç¥ç»å…ƒçš„æœ€å¤§ç†µæ¨¡å‹ç»“æœçš„å¤©çœŸå¤–æ¨è¡¨æ˜ï¼Œåˆ° $N\sim 200$ æ—¶å¿…é¡»å‘ç”Ÿä¸€äº›ç‰¹æ®Šçš„äº‹æƒ…ï¼Œå¦åˆ™ç†µå°†æ¶ˆå¤±ï¼›è¿™æ˜¯æœ‰è¶£çš„ï¼Œå› ä¸º $N\sim 200$ æ˜¯â€œè§‚å¯Ÿâ€è§†è§‰ä¸–ç•Œé‡å åŒºåŸŸçš„ç»†èƒæ•°é‡ï¼ˆSchneidman ç­‰äººï¼Œ2006ï¼‰ã€‚æ›´å¤æ‚çš„å¤–æ¨è®¾æƒ³äº†ä¸€ä¸ªå¤§å‹ç¥ç»å…ƒç¾¤ä½“ï¼Œå…¶ä¸­å¹³å‡æ´»åŠ¨å’Œæˆå¯¹ç›¸å…³æ€§æ˜¯ä»ä¸è¾ƒå°‘ç¥ç»å…ƒè®°å½•ä¸­å‘ç°çš„ç›¸åŒåˆ†å¸ƒä¸­éšæœºæŠ½å–çš„ï¼ˆTkaË‡cik ç­‰äººï¼Œ2006ï¼Œ2009ï¼‰ã€‚è¿™ç§å¤–æ¨éƒ¨åˆ†æ˜¯ç”±è§‚å¯Ÿåˆ°ä¸º $N = 20$ æˆ– $N = 40$ è§†ç½‘è†œç¥ç»å…ƒå­¦ä¹ çš„æœ€å¤§ç†µæ¨¡å‹çš„â€œçƒ­åŠ›å­¦â€å±æ€§ä¸åŒä¸€ $N$ ä¸‹æ­¤ç±»éšæœºæ¨¡å‹çš„è¡Œä¸ºç›¸åŒ¹é…æ‰€æ¿€å‘çš„ã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨å¤–æ¨åˆ° $N = 120$ï¼Œä¼šå‡ºç°æ˜¾è‘—çš„é›†ä½“è¡Œä¸ºï¼Œæˆ‘ä»¬å°†è¯¢é—®åœ¨æ¥è‡ª $N &gt; 100$ ä¸ªç»†èƒçš„çœŸå®æ•°æ®ä¸­æ˜¯å¦çœ‹åˆ°äº†è¿™äº›è¡Œä¸ºã€‚</p>
<blockquote>
<p>Early experiments in the retina already were monitoring $N = 40$ cells, and the development of numerical methods described in Appendix B quickly allowed analysis of these larger data sets (TkaË‡cik et al., 2006, 2009). With $N = 40$ cells one cannot check the predictions for probabilities of individual patterns $P(\vec{\sigma})$, but one can check the probability that $K$ out of $N$ cells are active in the same small time bin, as in Fig. 7E, or the correlations among triplets of neurons. At $N = 40$ we see the first hints that constraining pairwise correlations is not quite enough to capture the full structure of the network. There are disagreements between theory and experiment in the tails of the distribution $P_{N}(K)$, and more importantly a few percent disagreement at $K = 0$. This may not seem like much, but since the network is completely silent in roughly half of the $\Delta\tau = 20$ ms time bins, the data determine $P_{N}(K = 0)$ very precisely, and a one percent discrepancy is hugely significant.</p>
</blockquote>
<p>è§†ç½‘è†œä¸­çš„æ—©æœŸå®éªŒå·²ç»ç›‘æµ‹äº† $N = 40$ ä¸ªç»†èƒï¼Œå¹¶ä¸”é™„å½• B ä¸­æè¿°çš„æ•°å€¼æ–¹æ³•çš„å‘å±•å¾ˆå¿«å…è®¸åˆ†æè¿™äº›æ›´å¤§çš„æ•°æ®é›†ï¼ˆTkaË‡cik ç­‰äººï¼Œ2006ï¼Œ2009ï¼‰ã€‚å¯¹äº $N = 40$ ä¸ªç»†èƒï¼Œæˆ‘ä»¬æ— æ³•æ£€æŸ¥å•ä¸ªæ¨¡å¼ $P(\vec{\sigma})$ æ¦‚ç‡çš„é¢„æµ‹ï¼Œä½†æˆ‘ä»¬å¯ä»¥æ£€æŸ¥åœ¨åŒä¸€å°æ—¶é—´æ®µå†… $N$ ä¸ªç»†èƒä¸­æœ‰ $K$ ä¸ªæ´»è·ƒçš„æ¦‚ç‡ï¼Œå¦‚å›¾ 7E æ‰€ç¤ºï¼Œæˆ–ç¥ç»å…ƒä¸‰å…ƒç»„ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åœ¨ $N = 40$ æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›è¿¹è±¡è¡¨æ˜ï¼Œä»…çº¦æŸæˆå¯¹ç›¸å…³æ€§è¿˜ä¸è¶³ä»¥æ•æ‰ç½‘ç»œçš„å®Œæ•´ç»“æ„ã€‚åˆ†å¸ƒ $P_{N}(K)$ çš„å°¾éƒ¨å­˜åœ¨ç†è®ºä¸å®éªŒä¹‹é—´çš„ä¸ä¸€è‡´ï¼Œæ›´é‡è¦çš„æ˜¯åœ¨ $K = 0$ å¤„å­˜åœ¨å‡ ä¸ªç™¾åˆ†ç‚¹çš„ä¸ä¸€è‡´ã€‚è¿™ä¼¼ä¹ä¸å¤šï¼Œä½†ç”±äºç½‘ç»œåœ¨å¤§çº¦ä¸€åŠçš„ $\Delta\tau = 20$ æ¯«ç§’æ—¶é—´æ®µå†…å®Œå…¨é™é»˜ï¼Œæ•°æ®éå¸¸ç²¾ç¡®åœ°ç¡®å®šäº† $P_{N}(K = 0)$ï¼Œè€Œä¸”ç™¾åˆ†ä¹‹ä¸€çš„å·®å¼‚æ˜¯éå¸¸æ˜¾è‘—çš„ã€‚</p>
<blockquote>
<p>A new generation of electrode arrays made it possible to record $N = 100 âˆ’ 200$ cells, densely sampling a small patch of the retina (Â§III.A). As an example, these experiments could capture the signals from $N_{\text{max}} = 160$  ganglion cells in a $(450 \mu\text{m})^2$ area of the salamander retina that contains a total of $N\sim 200$ cells, and these recordings are stable for $\sim 1.5$ hr.</p>
</blockquote>
<p>æ–°ä¸€ä»£ç”µæé˜µåˆ—ä½¿å¾—è®°å½• $N = 100 âˆ’ 200$ ä¸ªç»†èƒæˆä¸ºå¯èƒ½ï¼Œå¯†é›†é‡‡æ ·è§†ç½‘è†œçš„å°å—ï¼ˆÂ§III.Aï¼‰ã€‚ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œè¿™äº›å®éªŒå¯ä»¥æ•æ‰åˆ°æ¥è‡ªè¾èˆè§†ç½‘è†œä¸­ $(450 \mu\text{m})^2$ åŒºåŸŸçš„ $N_{\text{max}} = 160$ ä¸ªç¥ç»èŠ‚ç»†èƒçš„ä¿¡å·ï¼Œè¯¥åŒºåŸŸæ€»å…±æœ‰ $N\sim 200$ ä¸ªç»†èƒï¼Œå¹¶ä¸”è¿™äº›è®°å½•åœ¨ $\sim 1.5$ å°æ—¶å†…æ˜¯ç¨³å®šçš„ã€‚</p>
<blockquote>
<p>As explained in Appendix B, we can build maximum entropy models at larger $N$ by using Monte Carlo simulation to estimate expectation values in the model, comparing with the measured expectation values, and then adjusting the coupling constants to improve the agreement. Necessarily this doesnâ€™t yield an exact solution to the constraint Eqs (17), but this seems acceptable since we are trying to match expectation values that are estimated from experiment and these have errors. Figure 8A shows that with $N = 100$ we can match the observed pairwise correlations within experimental error (TkaË‡cik et al., 2014). More precisely the errors in predicting the elements of the covariance matrix $C_{ij}$ [Eq (23)] are nearly Gaussian, with a variance equal to the variance of the measurement errors. This suggests, strongly, that one can successfully fit, but not overâ€“fit, a maximum entropy model to these data.</p>
</blockquote>
<p>å¦‚é™„å½• B æ‰€è¿°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿæ¥ä¼°è®¡æ¨¡å‹ä¸­çš„æœŸæœ›å€¼ï¼Œå°†å…¶ä¸æµ‹é‡çš„æœŸæœ›å€¼è¿›è¡Œæ¯”è¾ƒï¼Œç„¶åè°ƒæ•´è€¦åˆå¸¸æ•°ä»¥æ”¹å–„ä¸€è‡´æ€§ï¼Œä»è€Œåœ¨æ›´å¤§çš„ $N$ ä¸‹æ„å»ºæœ€å¤§ç†µæ¨¡å‹ã€‚å¿…ç„¶åœ°ï¼Œè¿™ä¸ä¼šäº§ç”Ÿå¯¹çº¦æŸæ–¹ç¨‹ï¼ˆ17ï¼‰çš„ç²¾ç¡®è§£ï¼Œä½†è¿™ä¼¼ä¹æ˜¯å¯ä»¥æ¥å—çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨å°è¯•åŒ¹é…ä»å®éªŒä¸­ä¼°è®¡çš„æœŸæœ›å€¼ï¼Œè€Œè¿™äº›æœŸæœ›å€¼å­˜åœ¨è¯¯å·®ã€‚å›¾ 8A æ˜¾ç¤ºï¼Œåœ¨ $N = 100$ æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å®éªŒè¯¯å·®èŒƒå›´å†…åŒ¹é…è§‚å¯Ÿåˆ°çš„æˆå¯¹ç›¸å…³æ€§ï¼ˆTkaË‡cik ç­‰äººï¼Œ2014ï¼‰ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œé¢„æµ‹åæ–¹å·®çŸ©é˜µ $C_{ij}$ [æ–¹ç¨‹ï¼ˆ23ï¼‰] å…ƒç´ çš„è¯¯å·®å‡ ä¹æ˜¯é«˜æ–¯åˆ†å¸ƒçš„ï¼Œå…¶æ–¹å·®ç­‰äºæµ‹é‡è¯¯å·®çš„æ–¹å·®ã€‚è¿™å¼ºçƒˆè¡¨æ˜ï¼Œå¯ä»¥æˆåŠŸåœ°æ‹Ÿåˆï¼Œä½†ä¸ä¼šè¿‡æ‹Ÿåˆè¿™äº›æ•°æ®çš„æœ€å¤§ç†µæ¨¡å‹ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/14/zSXyfvV76EQYeJ1.png" alt=""  /></p>
<p>FIG. 8 Fitting, but not over-fitting, with $N\sim 100$ neurons (TkaÃ©ik et al., 2014). (A) Distribution of errors in the prediction of pairwise correlations, after adjusting the parameters $\{h_{i}; J_{ij}\}$, for $N = 100$. Prediction errors are in units of the measurement error $\Delta C_{ij}$ for each element of the covariance matrix. Red line shows a Gaussian with zero mean and unit variance. (B) Logâ€”likelihood [Eq (38)] of test data not used in constructing the maximum entropy model, in units of the result for the training data. At $N = 10$ it is not surprising that these agree, since the number of parameters $\{h_{i}; J_{ij}\}$ is small. But we see this agreement persists at the $\sim 1%$ level out to $N = 120$, showing that even models for relatively large networks are not overfit.</p>
</blockquote>
<p>å›¾ 8 ä½¿ç”¨ $N\sim 100$ ä¸ªç¥ç»å…ƒè¿›è¡Œæ‹Ÿåˆï¼Œä½†æ²¡æœ‰è¿‡æ‹Ÿåˆï¼ˆTkaÃ©ik ç­‰äººï¼Œ2014ï¼‰ã€‚ï¼ˆAï¼‰åœ¨è°ƒæ•´å‚æ•° $\{h_{i}; J_{ij}\}$ åï¼Œå¯¹äº $N = 100$ï¼Œé¢„æµ‹æˆå¯¹ç›¸å…³æ€§çš„è¯¯å·®åˆ†å¸ƒã€‚é¢„æµ‹è¯¯å·®ä»¥åæ–¹å·®çŸ©é˜µæ¯ä¸ªå…ƒç´ çš„æµ‹é‡è¯¯å·® $\Delta C_{ij}$ ä¸ºå•ä½ã€‚çº¢çº¿æ˜¾ç¤ºäº†å‡å€¼ä¸ºé›¶ã€æ–¹å·®ä¸ºä¸€çš„é«˜æ–¯åˆ†å¸ƒã€‚ï¼ˆBï¼‰æµ‹è¯•æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ [æ–¹ç¨‹ï¼ˆ38ï¼‰]ï¼Œä»¥è®­ç»ƒæ•°æ®ç»“æœä¸ºå•ä½ã€‚åœ¨ $N = 10$ æ—¶ï¼Œè¿™äº›ç»“æœä¸€è‡´å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œå› ä¸ºå‚æ•°æ•°é‡ $\{h_{i}; J_{ij}\}$ å¾ˆå°‘ã€‚ä½†æˆ‘ä»¬çœ‹åˆ°è¿™ç§ä¸€è‡´æ€§åœ¨ $N = 120$ æ—¶ä»ç„¶ä¿æŒåœ¨çº¦ $1%$ çš„æ°´å¹³ï¼Œè¡¨æ˜å³ä½¿æ˜¯ç›¸å¯¹è¾ƒå¤§ç½‘ç»œçš„æ¨¡å‹ä¹Ÿæ²¡æœ‰è¿‡æ‹Ÿåˆã€‚</p>
</blockquote>
<blockquote>
<p>The test for fitting vs overâ€“fitting in Fig 8A looks at each pair of cells individually, but part of the worry is that at large N we can have accurate estimates of individual elements Cij while underâ€“determining the global properties of the matrix. We can take a familiar empirical approach, measuring the means âŸ¨ÏƒiâŸ© and covariances âŸ¨Î´ÏƒiÎ´ÏƒjâŸ©c in 90% of the data, using these to infer the parameters {hi; Jij} in a maximum entropy model, and then testing the predictions of the model [Eqs (35, 33)] on the remaining 10%. The fundamental measure of model quality is the logâ€“likelihood of the data, which we can normalize per sample and per neuron</p>
<p>$$
\mathcal{L} = \frac{1}{N}\langle \log{P(\vec{\sigma})}\rangle_{\text{expt}}
$$</p>
<p>Figure 8B shows that $\mathcal{L}$ is the same, to better than one percent, whether we evaluate it over the training data or over the test data. This is true at $N = 10$, where surely there can be no question that we have enough samples, and it is true at $N = 120$.</p>
</blockquote>
<p>æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆçš„æµ‹è¯•å¦‚å›¾ 8A æ‰€ç¤ºï¼Œå•ç‹¬æŸ¥çœ‹æ¯å¯¹ç»†èƒï¼Œä½†éƒ¨åˆ†æ‹…å¿§æ˜¯ï¼Œåœ¨å¤§ N ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å‡†ç¡®ä¼°è®¡å•ä¸ªå…ƒç´  Cijï¼ŒåŒæ—¶æœªç¡®å®šçŸ©é˜µçš„å…¨å±€å±æ€§ã€‚æˆ‘ä»¬å¯ä»¥é‡‡ç”¨ç†Ÿæ‚‰çš„ç»éªŒæ–¹æ³•ï¼Œåœ¨ 90% çš„æ•°æ®ä¸­æµ‹é‡å‡å€¼ âŸ¨ÏƒiâŸ© å’Œåæ–¹å·® âŸ¨Î´ÏƒiÎ´ÏƒjâŸ©cï¼Œä½¿ç”¨è¿™äº›æ¥æ¨æ–­æœ€å¤§ç†µæ¨¡å‹ä¸­çš„å‚æ•° {hi; Jij}ï¼Œç„¶ååœ¨å‰©ä½™çš„ 10% ä¸Šæµ‹è¯•æ¨¡å‹çš„é¢„æµ‹ [æ–¹ç¨‹ï¼ˆ35ï¼Œ33ï¼‰]ã€‚æ¨¡å‹è´¨é‡çš„åŸºæœ¬åº¦é‡æ˜¯æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰æ ·æœ¬å’Œæ¯ä¸ªç¥ç»å…ƒè¿›è¡Œå½’ä¸€åŒ–</p>
<p>$$
\mathcal{L} = \frac{1}{N}\langle \log{P(\vec{\sigma})}\rangle_{\text{expt}}
$$</p>
<p>å›¾ 8B æ˜¾ç¤ºï¼Œæ— è®ºæˆ‘ä»¬æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸Šè¿˜æ˜¯åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼° $\mathcal{L}$ï¼Œå…¶å€¼éƒ½ç›¸åŒï¼Œè¯¯å·®å°äºç™¾åˆ†ä¹‹ä¸€ã€‚è¿™åœ¨ $N = 10$ æ—¶æ˜¯çœŸçš„ï¼Œåœ¨é‚£é‡Œè‚¯å®šæ²¡æœ‰é—®é¢˜ï¼Œæˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œå¹¶ä¸”åœ¨ $N = 120$ æ—¶ä¹Ÿæ˜¯çœŸçš„ã€‚</p>
<blockquote>
<p>Different networks of neurons, in different organisms and different regions of the brain, have different correlation structures. One should thus be wary of generalizations such as â€œan hour is enough data for one hundred neurons.â€ But at least in the context of experiments on the retina, there is no question that maximum entropy models can be learned reliably from the available data, and that there is no overâ€“fitting. Said another way, the models really are the solutions to the mathematical problem that we set out to solve (Â§IV.A): What is the minimal model consistent with a set of expectation values measured in experiment? These models do not carry signatures of the algorithm that we used to find them, nor are they systematically perturbed by the finiteness of the data on which they are based. This answers the first two questions formulated above.</p>
</blockquote>
<p>ä¸åŒç”Ÿç‰©ä½“å’Œå¤§è„‘ä¸åŒåŒºåŸŸçš„ç¥ç»å…ƒç½‘ç»œå…·æœ‰ä¸åŒçš„ç›¸å…³ç»“æ„ã€‚å› æ­¤ï¼Œäººä»¬åº”è¯¥å¯¹â€œä¸€å°æ—¶è¶³å¤Ÿä¸€ç™¾ä¸ªç¥ç»å…ƒçš„æ•°æ®â€ä¹‹ç±»çš„æ¦‚æ‹¬æŒè°¨æ…æ€åº¦ã€‚ä½†è‡³å°‘åœ¨è§†ç½‘è†œå®éªŒçš„èƒŒæ™¯ä¸‹ï¼Œæ¯«æ— ç–‘é—®ï¼Œæœ€å¤§ç†µæ¨¡å‹å¯ä»¥ä»å¯ç”¨æ•°æ®ä¸­å¯é åœ°å­¦ä¹ ï¼Œå¹¶ä¸”æ²¡æœ‰è¿‡æ‹Ÿåˆã€‚æ¢å¥è¯è¯´ï¼Œè¿™äº›æ¨¡å‹ç¡®å®æ˜¯æˆ‘ä»¬ç€æ‰‹è§£å†³çš„æ•°å­¦é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼ˆÂ§IV.Aï¼‰ï¼šä¸å®éªŒä¸­æµ‹é‡çš„ä¸€ç»„æœŸæœ›å€¼ä¸€è‡´çš„æœ€å°æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿè¿™äº›æ¨¡å‹ä¸æºå¸¦æˆ‘ä»¬ç”¨æ¥æ‰¾åˆ°å®ƒä»¬çš„ç®—æ³•çš„ç‰¹å¾ï¼Œä¹Ÿä¸ä¼šè¢«å®ƒä»¬æ‰€åŸºäºçš„æ•°æ®çš„æœ‰é™æ€§ç³»ç»Ÿåœ°æ‰°åŠ¨ã€‚è¿™å›ç­”äº†ä¸Šé¢æå‡ºçš„å‰ä¸¤ä¸ªé—®é¢˜ã€‚</p>
<blockquote>
<p>Given that we can construct the maximum entropy models reliably, what do we learn? To begin, the small discrepancies in predicting the probability that $K$ out $N$ neurons are active simultaneously, $P_{N}(K)$, become larger as $N$ increases. The simplest solution to this problem is to add one more constraint, insisting that the maximum entropy model match the observed $P_{N}(K)$ exactly. This adds only $\sim N$ constraints to a problem in which we already have $N(N + 1)/2$, so the resulting â€œ$K$â€“pairwiseâ€ models are not significantly more complex.</p>
</blockquote>
<p>é‰´äºæˆ‘ä»¬å¯ä»¥å¯é åœ°æ„å»ºæœ€å¤§ç†µæ¨¡å‹ï¼Œæˆ‘ä»¬å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿé¦–å…ˆï¼Œé¢„æµ‹ $K$ ä¸ªç¥ç»å…ƒåŒæ—¶æ´»è·ƒçš„æ¦‚ç‡ $P_{N}(K)$ æ—¶çš„å°å·®å¼‚éšç€ $N$ çš„å¢åŠ è€Œå˜å¤§ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„æœ€ç®€å•æ–¹æ³•æ˜¯æ·»åŠ ä¸€ä¸ªé¢å¤–çš„çº¦æŸï¼ŒåšæŒè®©æœ€å¤§ç†µæ¨¡å‹å‡†ç¡®åŒ¹é…è§‚å¯Ÿåˆ°çš„ $P_{N}(K)$ã€‚è¿™åœ¨æˆ‘ä»¬å·²ç»æœ‰ $N(N + 1)/2$ ä¸ªçº¦æŸçš„é—®é¢˜ä¸­åªå¢åŠ äº† $\sim N$ ä¸ªçº¦æŸï¼Œå› æ­¤å¾—åˆ°çš„â€œ$K$â€“æˆå¯¹â€æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾è‘—æ›´å¤æ‚ã€‚</p>
<blockquote>
<p>Again, at the risk of being pedantic letâ€™s formulate matching of the observed $P_{N}(K)$ as constraining expectation values. If we introduce the Kronecker delta for integers $n$ and $m$,</p>
<p>$$
\begin{aligned}
\delta &amp;= 1\quad n=m\\
&amp;= 0\quad n\neq m
\end{aligned}
$$</p>
<p>then</p>
<p>$$
P_{N}(K) = \left\langle \delta\left(
K,\sum_{i}^{N}\sigma_{i}
\right)\right\rangle
$$</p>
<p>Thus to match $P_{N}(K)$ we want to enlarge our set of observables to include</p>
<p>$$
\{f_{\mu}^{(\text{counts})}\} \rightarrow \left\{\delta\left(K,\sum_{i}^{N}\sigma_{i}\right)\right\}
$$</p>
<p>As before, each new constraint adds a term to the effective energy,</p>
<p>$$
E(\vec{\sigma}) = \sum_{\mu}\lambda_{\mu}^{(\text{counts})}f_{\mu}^{(\text{counts})} = \sum_{K=0}^{N}\lambda_{K}\delta\left(K, \sum_{i}^{N}\sigma_{i}\right)
$$</p>
<p>It is useful to think of this as an effective potential that acts on the summed activity,</p>
<p>$$
\sum_{K=0}^{N}\lambda_{K}\delta\left(K,\sum_{i}^{N}\sigma_{i}\right) = V\left(\sum_{i=1}^{N}\sigma_{i}\right)
$$</p>
</blockquote>
<p>å†æ¬¡ï¼Œä¸ºäº†é¿å…è¿‡äºè¿‚è…ï¼Œè®©æˆ‘ä»¬å°†åŒ¹é…è§‚å¯Ÿåˆ°çš„ $P_{N}(K)$ è¡¨è¿°ä¸ºçº¦æŸæœŸæœ›å€¼ã€‚å¦‚æœæˆ‘ä»¬å¼•å…¥æ•´æ•° $n$ å’Œ $m$ çš„ Kronecker deltaï¼Œ</p>
<p>$$
\begin{aligned}
\delta &amp;= 1\quad n=m\\
&amp;= 0\quad n\neq m
\end{aligned}
$$</p>
<p>é‚£ä¹ˆ</p>
<p>$$
P_{N}(K) = \left\langle \delta\left(
K,\sum_{i}^{N}\sigma_{i}
\right)\right\rangle
$$</p>
<p>å› æ­¤ï¼Œä¸ºäº†åŒ¹é… $P_{N}(K)$ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æˆ‘ä»¬çš„å¯è§‚å¯Ÿé‡é›†åˆæ‰©å¤§åˆ°åŒ…æ‹¬</p>
<p>$$
\{f_{\mu}^{(\text{counts})}\} \rightarrow \left\{\delta\left(K,\sum_{i}^{N}\sigma_{i}\right)\right\}
$$</p>
<p>å¦‚å‰æ‰€è¿°ï¼Œæ¯ä¸ªæ–°çº¦æŸéƒ½ä¼šå‘æœ‰æ•ˆèƒ½é‡æ·»åŠ ä¸€é¡¹ï¼Œ</p>
<p>$$
E(\vec{\sigma}) = \sum_{\mu}\lambda_{\mu}^{(\text{counts})}f_{\mu}^{(\text{counts})} = \sum_{K=0}^{N}\lambda_{K}\delta\left(K, \sum_{i}^{N}\sigma_{i}\right)
$$</p>
<p>å°†å…¶è§†ä¸ºä½œç”¨äºæ€»æ´»åŠ¨çš„æœ‰æ•ˆåŠ¿æ˜¯æœ‰ç”¨çš„ï¼Œ</p>
<p>$$
\sum_{K=0}^{N}\lambda_{K}\delta\left(K,\sum_{i}^{N}\sigma_{i}\right) = V\left(\sum_{i=1}^{N}\sigma_{i}\right)
$$</p>
<blockquote>
<p>Putting the pieces together, the maximum entropy model that matches the mean activity of individual neurons, the correlations between pairs of neurons, and the probability that $K$ out of $N$ are active simultaneously takes the form</p>
<p>$$
\begin{aligned}
P_{2k}(\vec{\sigma}) &amp;= \frac{1}{Z_{2k}}e^{-E_{2k}(\vec{\sigma})}\\
E_{2k}(\vec{\sigma}) &amp;= \sum_{i=1}^{N}h_{i}\sigma_{i} + \frac{1}{2}\sum_{i\neq j}J_{ij}\sigma_{i}\sigma_{j} + V\left(\sum_{i=1}^{N}\sigma_{i}\right)
\end{aligned}
$$</p>
<p>We refer to this as the â€œ$K$â€“pairwiseâ€ model (TkaË‡cik et al., 2014).</p>
</blockquote>
<p>å°†å„ä¸ªéƒ¨åˆ†ç»„åˆåœ¨ä¸€èµ·ï¼ŒåŒ¹é…å•ä¸ªç¥ç»å…ƒçš„å¹³å‡æ´»åŠ¨ã€ç¥ç»å…ƒå¯¹ä¹‹é—´çš„ç›¸å…³æ€§ä»¥åŠ $N$ ä¸ªä¸­æœ‰ $K$ ä¸ªåŒæ—¶æ´»è·ƒçš„æ¦‚ç‡çš„æœ€å¤§ç†µæ¨¡å‹å½¢å¼ä¸º</p>
<p>$$
\begin{aligned}
P_{2k}(\vec{\sigma}) &amp;= \frac{1}{Z_{2k}}e^{-E_{2k}(\vec{\sigma})}\\
E_{2k}(\vec{\sigma}) &amp;= \sum_{i=1}^{N}h_{i}\sigma_{i} + \frac{1}{2}\sum_{i\neq j}J_{ij}\sigma_{i}\sigma_{j} + V\left(\sum_{i=1}^{N}\sigma_{i}\right)
\end{aligned}
$$</p>
<blockquote>
<p>We can test this model immediately by estimating the correlations among triplets of neurons,</p>
<p>$$
C_{ijk} = \langle (\sigma_{i}-\langle\sigma_{i}\rangle)(\sigma_{j}-\langle\sigma_{j}\rangle)(\sigma_{k}-\langle\sigma_{k}\rangle)\rangle
$$</p>
<p>Figure 9 shows the results with averages computed in both the pairwise and Kâ€“pairwise models, plotted vs. the experimental values. The discrepancies are very small, although still roughly three times larger than the experimental errors in the estimates of the correlations themselves (TkacË‡ik et al., 2014); we will see that one can sometimes get even better agreement (Â§V). Note that the potential $V$ which we add to match the constraint on $P_{N}(K)$ does not carry any information about the identities of the individual neurons. It thus is interesting that including this term improves the prediction of all the triplet correlations, which do depend on neural identity.</p>
</blockquote>
<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼°è®¡ç¥ç»å…ƒä¸‰å…ƒç»„ä¹‹é—´çš„ç›¸å…³æ€§æ¥ç«‹å³æµ‹è¯•è¿™ä¸ªæ¨¡å‹ï¼Œ</p>
<p>$$
C_{ijk} = \langle (\sigma_{i}-\langle\sigma_{i}\rangle)(\sigma_{j}-\langle\sigma_{j}\rangle)(\sigma_{k}-\langle\sigma_{k}\rangle)\rangle
$$</p>
<p>å›¾ 9 æ˜¾ç¤ºäº†åœ¨æˆå¯¹å’Œ Kâ€“æˆå¯¹æ¨¡å‹ä¸­è®¡ç®—çš„å¹³å‡å€¼ä¸å®éªŒå€¼çš„ç»“æœã€‚å°½ç®¡ä¸ç›¸å…³æ€§æœ¬èº«ä¼°è®¡çš„å®éªŒè¯¯å·®ç›¸æ¯”ä»ç„¶å¤§çº¦å¤§ä¸‰å€ï¼Œä½†å·®å¼‚éå¸¸å°ï¼ˆTkacË‡ik ç­‰äººï¼Œ2014ï¼‰ï¼›æˆ‘ä»¬å°†çœ‹åˆ°æœ‰æ—¶å¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼ˆÂ§Vï¼‰ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ ä»¥åŒ¹é… $P_{N}(K)$ çº¦æŸçš„åŠ¿ $V$ ä¸åŒ…å«æœ‰å…³å•ä¸ªç¥ç»å…ƒèº«ä»½çš„ä»»ä½•ä¿¡æ¯ã€‚å› æ­¤ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŒ…æ‹¬è¿™ä¸€é¡¹æ”¹å–„äº†å¯¹æ‰€æœ‰ä¸‰å…ƒç»„ç›¸å…³æ€§çš„é¢„æµ‹ï¼Œè€Œè¿™äº›ç›¸å…³æ€§ç¡®å®å–å†³äºç¥ç»å…ƒçš„èº«ä»½ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/14/8wJUahY93KbcznW.png" alt=""  /></p>
<p>Triplet correlations for $N = 100$ cells in the retina (TkaË‡cik et al., 2014). Measured $C_{ijk}$ (x-axis) vs predicted by the model (y-axis), shown for a single subgroup. The $\sim 1.6 \times 10^{5}$ distinct triplets are grouped into 1000 equally populated bins; error bars in x are s.d. across the bin. The corresponding values for the predictions are grouped together, yielding the mean and the s.d. of the prediction (y-axis). Inset zooms in on the bulk of the predictions at small correlation, for the $K$â€“pairwise model. The original reference used $\sigma_{i} = \pm 1$, so that all the $C_{ijk}$ shown here are $8\times$ larger than they would be in the $\sigma_{i} = {0, 1}$ representation.</p>
</blockquote>
<p>å›¾ 9 è§†ç½‘è†œä¸­ $N = 100$ ä¸ªç»†èƒçš„ä¸‰å…ƒç»„ç›¸å…³æ€§ï¼ˆTkaË‡cik ç­‰äººï¼Œ2014ï¼‰ã€‚æµ‹é‡çš„ $C_{ijk}$ï¼ˆx è½´ï¼‰ä¸æ¨¡å‹é¢„æµ‹çš„å€¼ï¼ˆy è½´ï¼‰ï¼Œæ˜¾ç¤ºä¸ºå•ä¸ªå­ç»„ã€‚å¤§çº¦ $1.6 \times 10^{5}$ ä¸ªä¸åŒçš„ä¸‰å…ƒç»„è¢«åˆ†æˆ 1000 ä¸ªåŒç­‰äººå£çš„ç®±ï¼›x è½´ä¸Šçš„è¯¯å·®æ¡æ˜¯è·¨ç®±çš„æ ‡å‡†å·®ã€‚é¢„æµ‹çš„ç›¸åº”å€¼è¢«åˆ†ç»„åœ¨ä¸€èµ·ï¼Œäº§ç”Ÿé¢„æµ‹çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆy è½´ï¼‰ã€‚æ’å›¾æ”¾å¤§äº† $K$â€“æˆå¯¹æ¨¡å‹ä¸­å°ç›¸å…³æ€§çš„é¢„æµ‹ä¸»ä½“ã€‚åŸå§‹å‚è€ƒä½¿ç”¨ $\sigma_{i} = \pm 1$ï¼Œå› æ­¤è¿™é‡Œæ˜¾ç¤ºçš„æ‰€æœ‰ $C_{ijk}$ éƒ½æ¯” $\sigma_{i} = {0, 1}$ è¡¨ç¤ºæ³•ä¸­çš„å€¼å¤§ $8\times$ã€‚</p>
</blockquote>
<blockquote>
<p>With $N = 100$ cells we cannot check, as in Fig 7F, the probability of every state of the network. But the model assigns to every state an energy $E_{2k}(\vec{\sigma})$, and we can ask about the distribution of this energy over the states that we see in the experiment vs. the expectation if states are drawn out of the model. To emphasize the extremes we look at the high energy tail,</p>
<p>$$
\Phi(E) = \langle\Theta[E - E_{2k}(\vec{\sigma})]\rangle
$$</p>
<p>where $\Theta(x)$ is the unit step function and the expectation value can be taken over the data or the theory. Figure 10 shows the comparison between theory and experiment. Note that the plot extends far past the point where individual states are predicted to occur once over the duration of the experiment, but we can make meaningful statements in this regime because there are (exponentially) many such states. Close agreement between theory and experiment extends out to $E\sim 25$, corresponding to states that are predicted to occur roughly once per fifty years.</p>
</blockquote>
<p>å¯¹äº $N = 100$ ä¸ªç»†èƒï¼Œæˆ‘ä»¬æ— æ³•åƒå›¾ 7F é‚£æ ·æ£€æŸ¥ç½‘ç»œæ¯ä¸ªçŠ¶æ€çš„æ¦‚ç‡ã€‚ä½†æ¨¡å‹ä¸ºæ¯ä¸ªçŠ¶æ€åˆ†é…äº†ä¸€ä¸ªèƒ½é‡ $E_{2k}(\vec{\sigma})$ï¼Œæˆ‘ä»¬å¯ä»¥è¯¢é—®å®éªŒä¸­çœ‹åˆ°çš„çŠ¶æ€çš„èƒ½é‡åˆ†å¸ƒä¸ä»æ¨¡å‹ä¸­æŠ½å–çŠ¶æ€æ—¶çš„æœŸæœ›å€¼ã€‚ä¸ºäº†å¼ºè°ƒæç«¯æƒ…å†µï¼Œæˆ‘ä»¬æŸ¥çœ‹é«˜èƒ½å°¾éƒ¨ï¼Œ</p>
<p>$$
\Phi(E) = \langle\Theta[E - E_{2k}(\vec{\sigma})]\rangle
$$</p>
<p>å…¶ä¸­ $\Theta(x)$ æ˜¯å•ä½é˜¶è·ƒå‡½æ•°ï¼ŒæœŸæœ›å€¼å¯ä»¥åœ¨æ•°æ®æˆ–ç†è®ºä¸Šè¿›è¡Œã€‚å›¾ 10 æ˜¾ç¤ºäº†ç†è®ºä¸å®éªŒä¹‹é—´çš„æ¯”è¾ƒã€‚è¯·æ³¨æ„ï¼Œè¯¥å›¾è¿œè¿œè¶…å‡ºäº†é¢„æµ‹åœ¨å®éªŒæŒç»­æ—¶é—´å†…å•ç‹¬çŠ¶æ€å‡ºç°ä¸€æ¬¡çš„ç‚¹ï¼Œä½†æˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ªèŒƒå›´å†…åšå‡ºæœ‰æ„ä¹‰çš„é™ˆè¿°ï¼Œå› ä¸ºå­˜åœ¨ï¼ˆæŒ‡æ•°çº§ï¼‰è®¸å¤šè¿™æ ·çš„çŠ¶æ€ã€‚ç†è®ºä¸å®éªŒä¹‹é—´çš„å¯†åˆ‡ä¸€è‡´å»¶ä¼¸åˆ° $E\sim 25$ï¼Œå¯¹åº”äºå¤§çº¦æ¯äº”åå¹´é¢„æµ‹å‡ºç°ä¸€æ¬¡çš„çŠ¶æ€ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/14/TXMtxWrEdNYqa8s.png" alt=""  /></p>
<p>FIG. 10 The cumulative distribution of energies for $N = 120$ neurons (TkacË‡ik et al., 2014). $\Phi(E)$ is defined in Eq (48), and averages are over data (black) or the theory (red). Dashed vertical line denotes an energy $E_{2k}(\sigma)$ such that the particular state $\sigma$ should occur on average once during the duration of the experiment.</p>
</blockquote>
<p>å›¾ 10 $N = 120$ ä¸ªç¥ç»å…ƒçš„èƒ½é‡ç´¯ç§¯åˆ†å¸ƒï¼ˆTkacË‡ik ç­‰äººï¼Œ2014ï¼‰ã€‚$\Phi(E)$ åœ¨æ–¹ç¨‹ï¼ˆ48ï¼‰ä¸­å®šä¹‰ï¼Œå¹³å‡å€¼æ˜¯åŸºäºæ•°æ®ï¼ˆé»‘è‰²ï¼‰æˆ–ç†è®ºï¼ˆçº¢è‰²ï¼‰ã€‚è™šçº¿å‚ç›´çº¿è¡¨ç¤ºèƒ½é‡ $E_{2k}(\sigma)$ï¼Œä½¿å¾—ç‰¹å®šçŠ¶æ€ $\sigma$ åœ¨å®éªŒæŒç»­æ—¶é—´å†…å¹³å‡å‡ºç°ä¸€æ¬¡ã€‚</p>
</blockquote>
<blockquote>
<p>This class of models predicts that neural activity is collective. Thus in a population of $N$ cells, if we know the state of $N âˆ’1$ we can make a prediction of the probability that the last cell will be active,</p>
<p>$$
P(\sigma_{i}=1|\{\sigma_{i\neq j}\}) = \frac{1}{1 + \exp{[-h_{i}^{\text{eff}}(\{\sigma_{i\neq j}\})]}}
$$</p>
<p>where we can think of the other neurons as applying an effective field to the one neuron that we focus on,</p>
<p>$$
\begin{aligned}
h_{i}^{\text{eff}}(\{\sigma_{i\neq j}\}) = &amp;E(\sigma_{1},\sigma_{2},\cdots,\sigma_{i}=1,\cdots,\sigma_{N}) \\
-&amp;E(\sigma_{1},\sigma_{2},\cdots,\sigma_{i}=0,\cdots,\sigma_{N})
\end{aligned}
$$</p>
<p>For each neuron and for each moment in time we can calculate the effective field predicted by the theory, with no free parameters, and we can group together all instances in which this field is in some narrow range and ask if the probability of the cell being active agrees with Eq (B8). Results are shown in Fig. 11A.</p>
</blockquote>
<p>è¿™ç§æ¨¡å‹ç±»é¢„æµ‹ç¥ç»æ´»åŠ¨æ˜¯é›†ä½“çš„ã€‚å› æ­¤ï¼Œåœ¨ $N$ ä¸ªç»†èƒçš„ç¾¤ä½“ä¸­ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“ $N âˆ’1$ çš„çŠ¶æ€ï¼Œæˆ‘ä»¬å¯ä»¥é¢„æµ‹æœ€åä¸€ä¸ªç»†èƒæ´»è·ƒçš„æ¦‚ç‡ï¼Œ</p>
<p>$$
P(\sigma_{i}=1|\{\sigma_{i\neq j}\}) = \frac{1}{1 + \exp{[-h_{i}^{\text{eff}}(\{\sigma_{i\neq j}\})]}}
$$</p>
<p>å…¶ä¸­æˆ‘ä»¬å¯ä»¥å°†å…¶ä»–ç¥ç»å…ƒè§†ä¸ºå¯¹æˆ‘ä»¬å…³æ³¨çš„ä¸€ä¸ªç¥ç»å…ƒæ–½åŠ æœ‰æ•ˆåœºï¼Œ</p>
<p>$$
\begin{aligned}
h_{i}^{\text{eff}}(\{\sigma_{i\neq j}\}) = &amp;E(\sigma_{1},\sigma_{2},\cdots,\sigma_{i}=1,\cdots,\sigma_{N}) \\
-&amp;E(\sigma_{1},\sigma_{2},\cdots,\sigma_{i}=0,\cdots,\sigma_{N})
\end{aligned}
$$</p>
<p>å¯¹äºæ¯ä¸ªç¥ç»å…ƒå’Œæ¯ä¸ªæ—¶é—´ç‚¹ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥è®¡ç®—ç†è®ºé¢„æµ‹çš„æœ‰æ•ˆåœºï¼Œæ²¡æœ‰è‡ªç”±å‚æ•°ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰å®ä¾‹åˆ†ç»„åœ¨ä¸€èµ·ï¼Œå…¶ä¸­è¯¥åœºå¤„äºæŸä¸ªç‹­çª„èŒƒå›´å†…ï¼Œå¹¶è¯¢é—®ç»†èƒæ´»è·ƒçš„æ¦‚ç‡æ˜¯å¦ä¸æ–¹ç¨‹ï¼ˆB8ï¼‰ä¸€è‡´ã€‚ç»“æœå¦‚å›¾ 11A æ‰€ç¤ºã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/14/3d1eJp5CURALMSN.png" alt=""  /></p>
<p>FIG. 11 Effective fields and the collective character of neural activity in the retina (TkacË‡ik et al., 2014). (A) The probability that a single neuron is active given the state of the rest of the network, with $N = 120$. Points with error bars are the data, with the effective field computed from the model as in Eq (50). Red line is the prediction from Eq (49), and grey points are results with the purely pairwise rather than â€œ$K$â€“pairwiseâ€ model. Shaded grey region shows the distribution of fields across the experiment, emphasizing that the errors at large positive field are in the tail of the distribution. Inset shows the same results on a logarithmic scale for probability. (B) Probability of a single neuron being active as a function of time in a repeated naturalistic movie, normalized as the probability per unit time of an action potential (spikes/s). Top, in red, experimental data. Lower traces, in black, predictions based on states of other neurons in an $N$â€“cell group, based on Eqs (B8, 50). Solid lines are the mean prediction across all repetitions of the movie, and thin lines are the envelope $\pm$ one standard deviation.</p>
</blockquote>
<p>å›¾ 11 è§†ç½‘è†œä¸­ç¥ç»æ´»åŠ¨çš„æœ‰æ•ˆåœºå’Œé›†ä½“ç‰¹å¾ï¼ˆTkacË‡ik ç­‰äººï¼Œ2014ï¼‰ã€‚ï¼ˆAï¼‰åœ¨ $N = 120$ çš„æƒ…å†µä¸‹ï¼Œç»™å®šç½‘ç»œå…¶ä½™éƒ¨åˆ†çŠ¶æ€æ—¶å•ä¸ªç¥ç»å…ƒæ´»è·ƒçš„æ¦‚ç‡ã€‚å¸¦è¯¯å·®æ¡çš„ç‚¹æ˜¯æ•°æ®ï¼Œæœ‰æ•ˆåœºæ˜¯æ ¹æ®æ–¹ç¨‹ï¼ˆ50ï¼‰ä»æ¨¡å‹ä¸­è®¡ç®—å¾—å‡ºçš„ã€‚çº¢çº¿æ˜¯æ–¹ç¨‹ï¼ˆ49ï¼‰çš„é¢„æµ‹ï¼Œç°è‰²ç‚¹æ˜¯çº¯æˆå¯¹è€Œä¸æ˜¯â€œ$K$â€“æˆå¯¹â€æ¨¡å‹çš„ç»“æœã€‚é˜´å½±ç°è‰²åŒºåŸŸæ˜¾ç¤ºäº†å®éªŒä¸­åœºçš„åˆ†å¸ƒï¼Œå¼ºè°ƒäº†åœ¨å¤§æ­£åœºå¤„çš„è¯¯å·®å¤„äºåˆ†å¸ƒçš„å°¾éƒ¨ã€‚æ’å›¾ä»¥æ¦‚ç‡çš„å¯¹æ•°åˆ»åº¦æ˜¾ç¤ºç›¸åŒçš„ç»“æœã€‚ï¼ˆBï¼‰åœ¨é‡å¤çš„è‡ªç„¶ç”µå½±ä¸­ï¼Œå•ä¸ªç¥ç»å…ƒæ´»è·ƒçš„æ¦‚ç‡ï¼Œå½’ä¸€åŒ–ä¸ºæ¯å•ä½æ—¶é—´å†…åŠ¨ä½œç”µä½ï¼ˆå°–å³°/ç§’ï¼‰çš„æ¦‚ç‡ã€‚é¡¶éƒ¨ä¸ºçº¢è‰²ï¼Œè¡¨ç¤ºå®éªŒæ•°æ®ã€‚ä¸‹æ–¹è½¨è¿¹ä¸ºé»‘è‰²ï¼ŒåŸºäº $N$ ä¸ªç»†èƒç»„ä¸­å…¶ä»–ç¥ç»å…ƒçŠ¶æ€çš„é¢„æµ‹ï¼ŒåŸºäºæ–¹ç¨‹ï¼ˆB8ï¼Œ50ï¼‰ã€‚å®çº¿æ˜¯ç”µå½±æ‰€æœ‰é‡å¤çš„å¹³å‡é¢„æµ‹ï¼Œç»†çº¿æ˜¯åŒ…ç»œçº¿ $\pm$ ä¸€ä¸ªæ ‡å‡†å·®ã€‚</p>
</blockquote>
<blockquote>
<p>We see that the predictions of Eqs (B8) and (50) in the $K$â€“pairwise model agree well with experiment throughout the bulk of the distribution of effective fields, but that discrepancies arise in the tails. These deviations are $\sim 1.5\times$ the error bars of the measurement, but have some systematic structure, suggesting that we are capturing much but not quite all of the collective behavior under conditions where neurons are driven most strongly.</p>
</blockquote>
<p>æˆ‘ä»¬çœ‹åˆ° $K$â€“æˆå¯¹æ¨¡å‹ä¸­æ–¹ç¨‹ï¼ˆB8ï¼‰å’Œï¼ˆ50ï¼‰çš„é¢„æµ‹ä¸å®éªŒåœ¨æœ‰æ•ˆåœºåˆ†å¸ƒçš„ä¸»ä½“éƒ¨åˆ†å¾ˆå¥½åœ°ä¸€è‡´ï¼Œä½†åœ¨å°¾éƒ¨å‡ºç°äº†åå·®ã€‚è¿™äº›åå·®çº¦ä¸ºæµ‹é‡è¯¯å·®æ¡çš„ $\sim 1.5\times$ï¼Œä½†å…·æœ‰ä¸€äº›ç³»ç»Ÿç»“æ„ï¼Œè¡¨æ˜æˆ‘ä»¬æ­£åœ¨æ•æ‰å¤§éƒ¨åˆ†ä½†å¹¶éå…¨éƒ¨çš„é›†ä½“è¡Œä¸ºï¼Œåœ¨ç¥ç»å…ƒå—åˆ°æœ€å¼ºé©±åŠ¨çš„æ¡ä»¶ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<blockquote>
<p>The results in Fig. 11A combine data across all times to estimate the probability of activity in one cell given the state of the rest of the network. It is interesting to unfold these results in time. In particular, the structure of the experiment was such that the retina saw the same movie many times, and so we can condition on a particular moment in the movie, as shown for one neuron in Fig 11B. It is conventional to plot not the probability of being active in a small bin but the corresponding â€œrateâ€ (Rieke et al., 1997)</p>
<p>$$
r_{i}(t) = \langle\sigma_{i}(t)\rangle/\Delta\tau
$$</p>
<p>where $\sigma_{i}(t)$ denotes the state of neuron $i$ at time $t$ relative to (in this case) the visual inputs. We see in the top trace of Fig. 11B that single neurons are active very rarely, with essentially zero probability of spiking between brief transients that generate on average one or a few spikes. This pattern is common in response to naturalistic stimuli, and very difficult to reproduce in models (Maheswaranathan et al., 2023).</p>
</blockquote>
<p>ç¿»è¯‘</p>
<p>å›¾ 11A ä¸­çš„ç»“æœç»“åˆäº†æ‰€æœ‰æ—¶é—´çš„æ•°æ®ï¼Œä»¥ä¼°è®¡åœ¨ç»™å®šç½‘ç»œå…¶ä½™éƒ¨åˆ†çŠ¶æ€æ—¶ä¸€ä¸ªç»†èƒçš„æ´»åŠ¨æ¦‚ç‡ã€‚æœ‰è¶£çš„æ˜¯å°†è¿™äº›ç»“æœå±•å¼€åœ¨æ—¶é—´ä¸Šã€‚ç‰¹åˆ«æ˜¯ï¼Œå®éªŒçš„ç»“æ„æ˜¯è§†ç½‘è†œå¤šæ¬¡çœ‹åˆ°åŒä¸€éƒ¨ç”µå½±ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä»¥ç”µå½±ä¸­çš„ç‰¹å®šæ—¶åˆ»ä¸ºæ¡ä»¶ï¼Œå¦‚å›¾ 11B ä¸­æ‰€ç¤ºçš„ä¸€ä¸ªç¥ç»å…ƒã€‚é€šå¸¸ä¸ç»˜åˆ¶åœ¨å°ç®±ä¸­æ´»è·ƒçš„æ¦‚ç‡ï¼Œè€Œæ˜¯ç»˜åˆ¶ç›¸åº”çš„â€œé€Ÿç‡â€ï¼ˆRieke ç­‰äººï¼Œ1997ï¼‰</p>
<p>$$
r_{i}(t) = \langle\sigma_{i}(t)\rangle/\Delta\tau
$$</p>
<p>å…¶ä¸­ $\sigma_{i}(t)$ è¡¨ç¤ºç›¸å¯¹äºï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼‰è§†è§‰è¾“å…¥çš„æ—¶é—´ $t$ ç¥ç»å…ƒ $i$ çš„çŠ¶æ€ã€‚æˆ‘ä»¬åœ¨å›¾ 11B çš„é¡¶éƒ¨è½¨è¿¹ä¸­çœ‹åˆ°ï¼Œå•ä¸ªç¥ç»å…ƒéå¸¸ç½•è§åœ°æ´»è·ƒï¼Œåœ¨äº§ç”Ÿå¹³å‡ä¸€ä¸ªæˆ–å‡ ä¸ªå°–å³°çš„çŸ­æš‚ç¬å˜ä¹‹é—´å‡ ä¹æ²¡æœ‰å°–å³°çš„æ¦‚ç‡ã€‚è¿™ç§æ¨¡å¼åœ¨å¯¹è‡ªç„¶åˆºæ¿€çš„å“åº”ä¸­å¾ˆå¸¸è§ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹ä¸­å¾ˆéš¾å†ç°ï¼ˆMaheswaranathan ç­‰äººï¼Œ2023ï¼‰ã€‚</p>
<blockquote>
<p>The maximum entropy models provide an extreme opposite point of view, making no reference to the visual inputs; instead activity is determined by the state of the rest of the network. We see that this approach correctly predicts sparse activity, with near zero rate between transients that are timed correctly relative to the input. Although here we see just one cell, the average neuron exhibits an $r_{i}(t)$ that has $\sim 80%$ correlation with the theoretical predictions at $N = 120$. There is no sign of saturation, and it seems likely we would make even more precise predictions from models based on all $N\sim 200$ cells in this small patch of the retina. The possibility of predicting activity without reference to the visual input suggests that the â€œvocabularyâ€ of the retinaâ€™s output is restricted, and that as with spelling rules this should allow for error-correction(Loback et al., 2017).</p>
</blockquote>
<p>æœ€å¤§ç†µæ¨¡å‹æä¾›äº†ä¸€ä¸ªæç«¯ç›¸åçš„è§‚ç‚¹ï¼Œä¸å‚è€ƒè§†è§‰è¾“å…¥ï¼›ç›¸åï¼Œæ´»åŠ¨ç”±ç½‘ç»œå…¶ä½™éƒ¨åˆ†çš„çŠ¶æ€å†³å®šã€‚æˆ‘ä»¬çœ‹åˆ°è¿™ç§æ–¹æ³•æ­£ç¡®åœ°é¢„æµ‹äº†ç¨€ç–æ´»åŠ¨ï¼Œåœ¨ä¸è¾“å…¥ç›¸å¯¹æ­£ç¡®å®šæ—¶çš„ç¬å˜ä¹‹é—´é€Ÿç‡æ¥è¿‘é›¶ã€‚è™½ç„¶è¿™é‡Œæˆ‘ä»¬åªçœ‹åˆ°ä¸€ä¸ªç»†èƒï¼Œä½†å¹³å‡ç¥ç»å…ƒè¡¨ç°å‡ºä¸ç†è®ºé¢„æµ‹åœ¨ $N = 120$ æ—¶çº¦æœ‰ $\sim 80%$ çš„ç›¸å…³æ€§çš„ $r_{i}(t)$ã€‚æ²¡æœ‰é¥±å’Œçš„è¿¹è±¡ï¼Œå¹¶ä¸”ä¼¼ä¹æˆ‘ä»¬å¯ä»¥ä»åŸºäºè§†ç½‘è†œè¿™ä¸ªå°å—ä¸­æ‰€æœ‰ $N\sim 200$ ä¸ªç»†èƒçš„æ¨¡å‹ä¸­åšå‡ºæ›´ç²¾ç¡®çš„é¢„æµ‹ã€‚åœ¨ä¸å‚è€ƒè§†è§‰è¾“å…¥çš„æƒ…å†µä¸‹é¢„æµ‹æ´»åŠ¨çš„å¯èƒ½æ€§è¡¨æ˜ï¼Œè§†ç½‘è†œè¾“å‡ºçš„â€œè¯æ±‡â€æ˜¯å—é™çš„ï¼Œå¹¶ä¸”å°±åƒæ‹¼å†™è§„åˆ™ä¸€æ ·ï¼Œè¿™åº”è¯¥å…è®¸è¿›è¡Œçº é”™ï¼ˆLoback ç­‰äººï¼Œ2017ï¼‰ã€‚</p>
<blockquote>
<p>Perhaps the most basic prediction from maximum entropy models is the entropy itself. There are several ways that we can estimate the entropy. First, in the $K$-pairwise model we can see that the effective energy of the completely silent state, from Eq (46), is zero, which means that the probability of this state is just the inverse of the partition function. Further, in this model, the probability of complete silence matches what we observe experimentally. Thus we can estimate the free energy of the model from the data, and then we can estimate the mean energy of the model from Monte Carlo, giving us an estimate of the entropy. An alternative is to generalize the model by introducing a fictitious temperature, as will be discussed in Â§VI.B. Then at $T = 0$ the entropy must be zero and at $T\to\infty$ the entropy must be $N\log{2}$, while the derivative of the entropy is related as always to the heat capacity. Thus the entropy of our model for the real system at $T = 1$ becomes</p>
<p>$$
S_{N}(T=1) = \int_{0}^{1}\mathrm{d}T\frac{C_{v}(T)}{T} = N\log{2} - \int_{1}^{\infty}\mathrm{d}T\frac{C_{v}(T)}{T}
$$</p>
<p>where the heat capacity is related as usual to the variance of the energy, $\begin{aligned}C_{v} = \frac{\langle (\delta E)^{2}\rangle}{T^{2}}\end{aligned}$, that we can estimate from Monte Carlo simulations at each $T$. There is also a check that the two estimates in Eq (52) should agree. All of these methods agree with one another at the percent level, with results shown in Fig. 12A.</p>
</blockquote>
<p>ä¹Ÿè®¸æœ€å¤§ç†µæ¨¡å‹çš„æœ€åŸºæœ¬é¢„æµ‹æ˜¯ç†µæœ¬èº«ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å‡ ç§æ–¹å¼æ¥ä¼°è®¡ç†µã€‚é¦–å…ˆï¼Œåœ¨ $K$-æˆå¯¹æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®Œå…¨é™æ­¢çŠ¶æ€çš„æœ‰æ•ˆèƒ½é‡ï¼Œä»æ–¹ç¨‹ï¼ˆ46ï¼‰æ¥çœ‹ä¸ºé›¶ï¼Œè¿™æ„å‘³ç€è¯¥çŠ¶æ€çš„æ¦‚ç‡åªæ˜¯é…åˆ†å‡½æ•°çš„å€’æ•°ã€‚æ­¤å¤–ï¼Œåœ¨è¯¥æ¨¡å‹ä¸­ï¼Œå®Œå…¨é™æ­¢çš„æ¦‚ç‡ä¸æˆ‘ä»¬åœ¨å®éªŒä¸­è§‚å¯Ÿåˆ°çš„ç›¸åŒ¹é…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ•°æ®ä¸­ä¼°è®¡æ¨¡å‹çš„è‡ªç”±èƒ½ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä»è’™ç‰¹å¡ç½—ä¸­ä¼°è®¡æ¨¡å‹çš„å¹³å‡èƒ½é‡ï¼Œä»è€Œç»™å‡ºç†µçš„ä¼°è®¡ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡å¼•å…¥è™šæ„æ¸©åº¦æ¥æ¨å¹¿æ¨¡å‹ï¼Œå¦‚ Â§VI.B ä¸­å°†è®¨è®ºçš„é‚£æ ·ã€‚é‚£ä¹ˆåœ¨ $T = 0$ æ—¶ç†µå¿…é¡»ä¸ºé›¶ï¼Œè€Œåœ¨ $T\to\infty$ æ—¶ç†µå¿…é¡»ä¸º $N\log{2}$ï¼Œè€Œç†µçš„å¯¼æ•°ä¸€å¦‚æ—¢å¾€åœ°ä¸çƒ­å®¹æœ‰å…³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹çœŸå®ç³»ç»Ÿåœ¨ $T = 1$ æ—¶æ¨¡å‹çš„ç†µå˜ä¸º</p>
<p>$$
S_{N}(T=1) = \int_{0}^{1}\mathrm{d}T\frac{C_{v}(T)}{T} = N\log{2} - \int_{1}^{\infty}\mathrm{d}T\frac{C_{v}(T)}{T}
$$</p>
<p>å…¶ä¸­çƒ­å®¹ä¸èƒ½é‡çš„æ–¹å·®æœ‰å…³ï¼Œ$\begin{aligned}C_{v} = \frac{\langle (\delta E)^{2}\rangle}{T^{2}}\end{aligned}$ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ¯ä¸ª $T$ çš„è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿä¸­ä¼°è®¡å®ƒã€‚è¿˜æœ‰ä¸€ä¸ªæ£€æŸ¥ï¼Œå³æ–¹ç¨‹ï¼ˆ52ï¼‰ä¸­çš„ä¸¤ä¸ªä¼°è®¡åº”è¯¥æ˜¯ä¸€è‡´çš„ã€‚æ‰€æœ‰è¿™äº›æ–¹æ³•åœ¨ç™¾åˆ†æ¯”æ°´å¹³ä¸Šå½¼æ­¤ä¸€è‡´ï¼Œç»“æœå¦‚å›¾ 12A æ‰€ç¤ºã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/14/5Wifk4mLnFRebt7.png" alt=""  /></p>
<p>FIG. 12 Entropy and coincidences in the activity of the retinal network (TkaË‡cik et al., 2014). (A) Entropy predicted in Kâ€“pairwise models (red) and in the approximation that all neurons are independent (grey). Models are constructed independently for many subgroups of size $N$ chosen out of the total population $N_{\text{max}} = 160$, and error bars include the variance across these groups. (B) Probability that two randomly chosen states of the network are the same, again for many subgroups of size $N$. Results for real data (black), shuffled data (grey), and the $K$â€“pairwise models (red).</p>
</blockquote>
<p>å›¾ 12 è§†ç½‘è†œç½‘ç»œæ´»åŠ¨ä¸­çš„ç†µå’Œå·§åˆï¼ˆTkaË‡cik ç­‰äººï¼Œ2014ï¼‰ã€‚ï¼ˆAï¼‰Kâ€“æˆå¯¹æ¨¡å‹ä¸­é¢„æµ‹çš„ç†µï¼ˆçº¢è‰²ï¼‰å’Œæ‰€æœ‰ç¥ç»å…ƒç‹¬ç«‹çš„è¿‘ä¼¼ï¼ˆç°è‰²ï¼‰ã€‚æ¨¡å‹æ˜¯ä»æ€»äººå£ $N_{\text{max}} = 160$ ä¸­é€‰æ‹©çš„è®¸å¤šå¤§å°ä¸º $N$ çš„å­ç»„ç‹¬ç«‹æ„å»ºçš„ï¼Œè¯¯å·®æ¡åŒ…æ‹¬è¿™äº›ç»„ä¹‹é—´çš„æ–¹å·®ã€‚ï¼ˆBï¼‰éšæœºé€‰æ‹©çš„ç½‘ç»œçŠ¶æ€ç›¸åŒçš„æ¦‚ç‡ï¼ŒåŒæ ·é€‚ç”¨äºè®¸å¤šå¤§å°ä¸º $N$ çš„å­ç»„ã€‚çœŸå®æ•°æ®çš„ç»“æœï¼ˆé»‘è‰²ï¼‰ã€æ´—ç‰Œæ•°æ®ï¼ˆç°è‰²ï¼‰å’Œ $K$â€“æˆå¯¹æ¨¡å‹ï¼ˆçº¢è‰²ï¼‰ã€‚</p>
</blockquote>
<blockquote>
<p>The $\sim 25%$ reduction in entropy is significant, but more dramatic (and testable) is the prediction that the distribution over states is extremely inhomogeneous. Recall that if the distribution is uniform over some effective number of states $\Omega_{\text{eff}}$ then the entropy is $S = \log{\Omega_{\text{eff}}}$ and the probability that two states chosen at random will be the same is $P_{c} = 1/\Omega_{\text{eff}}$ ; for nonâ€“uniform distributions we have $S\geq âˆ’ \log(P_{c})$. If neurons were independent then with $N$ cells we would have $P_{c}\propto e^{âˆ’\alpha N}$, and this is what we see in the data once they are shuffled to remove correlations (Fig. 12B). But the real data show a much more gradual decay with $N$ , and this is captured perfectly by the $K$â€“pairwise maximum entropy models.</p>
</blockquote>
<p>$\sim 25%$ çš„ç†µå‡å°‘æ˜¯æ˜¾è‘—çš„ï¼Œä½†æ›´æˆå‰§æ€§ï¼ˆä¸”å¯æµ‹è¯•ï¼‰çš„é¢„æµ‹æ˜¯çŠ¶æ€åˆ†å¸ƒæä¸å‡åŒ€ã€‚å›æƒ³ä¸€ä¸‹ï¼Œå¦‚æœåˆ†å¸ƒåœ¨æŸä¸ªæœ‰æ•ˆçŠ¶æ€æ•° Î©eff ä¸Šæ˜¯å‡åŒ€çš„ï¼Œé‚£ä¹ˆç†µä¸º $S = \log{\Omega_{\text{eff}}}$ï¼Œéšæœºé€‰æ‹©ä¸¤ä¸ªçŠ¶æ€ç›¸åŒçš„æ¦‚ç‡ä¸º $P_{c} = 1/\Omega_{\text{eff}}$ï¼›å¯¹äºéå‡åŒ€åˆ†å¸ƒï¼Œæˆ‘ä»¬æœ‰ $S\geq âˆ’ \log(P_{c})$ã€‚å¦‚æœç¥ç»å…ƒæ˜¯ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆå¯¹äº $N$ ä¸ªç»†èƒï¼Œæˆ‘ä»¬å°†æœ‰ $P_{c}\propto e^{âˆ’\alpha N}$ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬åœ¨æ•°æ®ä¸­çœ‹åˆ°çš„ï¼Œä¸€æ—¦å®ƒä»¬è¢«æ´—ç‰Œä»¥å»é™¤ç›¸å…³æ€§ï¼ˆå›¾ 12Bï¼‰ã€‚ä½†çœŸå®æ•°æ®éšç€ $N$ æ˜¾ç¤ºå‡ºæ›´æ¸è¿›çš„è¡°å‡ï¼Œè€Œè¿™æ­£è¢« $K$â€“æˆå¯¹æœ€å¤§ç†µæ¨¡å‹å®Œç¾æ•æ‰ã€‚</p>
<blockquote>
<p>At $N = 120$ the logarithm of the coincidence probability (both measured and predicted) is an order of magnitude smaller than the entropy predicted by the model. Perhaps related is that the free energy per neuronâ€”which, as discussed above, can be obtained directly from the probability of the fully silent stateâ€”also decreases dramatically as $N$ increases. At $N = 120$ the free energy is just a few percent of the either the entropy or the mean energy, reflecting near perfect cancelation between these terms; one can see this also in a much simpler model that only matches $P_{N}(K)$ and not the individual means or pairwise correlations (TkacË‡ik et al., 2013). Importantly, these behaviors are captured by the Kâ€“pairwise model smoothly from $N &lt; 40$ through $N &gt; 100$, indicating that what we learned at more modest $N$ really does extrapolate up a scale comparable to the whole population of cells in a patch of the retina. We will have to work harder to decide if we can see the emergence of a true thermodynamic limit.</p>
</blockquote>
<p>åœ¨ $N = 120$ æ—¶ï¼Œå·§åˆæ¦‚ç‡çš„å¯¹æ•°ï¼ˆæµ‹é‡å’Œé¢„æµ‹ï¼‰æ¯”æ¨¡å‹é¢„æµ‹çš„ç†µå°ä¸€ä¸ªæ•°é‡çº§ã€‚ä¹Ÿè®¸ç›¸å…³çš„æ˜¯ï¼Œæ¯ä¸ªç¥ç»å…ƒçš„è‡ªç”±èƒ½â€”â€”æ­£å¦‚ä¸Šé¢è®¨è®ºçš„é‚£æ ·ï¼Œå¯ä»¥ç›´æ¥ä»å®Œå…¨é™æ­¢çŠ¶æ€çš„æ¦‚ç‡ä¸­è·å¾—â€”â€”éšç€ $N$ çš„å¢åŠ ä¹Ÿæ˜¾è‘—é™ä½ã€‚åœ¨ $N = 120$ æ—¶ï¼Œè‡ªç”±èƒ½ä»…å ç†µæˆ–å¹³å‡èƒ½é‡çš„ç™¾åˆ†ä¹‹å‡ ï¼Œåæ˜ äº†è¿™äº›é¡¹ä¹‹é—´å‡ ä¹å®Œç¾çš„æŠµæ¶ˆï¼›åœ¨ä¸€ä¸ªåªåŒ¹é… $P_{N}(K)$ è€Œä¸åŒ¹é…å•ä¸ªå‡å€¼æˆ–æˆå¯¹ç›¸å…³æ€§çš„æ›´ç®€å•æ¨¡å‹ä¸­ä¹Ÿå¯ä»¥çœ‹åˆ°è¿™ä¸€ç‚¹ï¼ˆTkacË‡ik ç­‰äººï¼Œ2013ï¼‰ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›è¡Œä¸ºé€šè¿‡ Kâ€“æˆå¯¹æ¨¡å‹ä» $N &lt; 40$ å¹³æ»‘åœ°æ•æ‰åˆ° $N &gt; 100$ï¼Œè¡¨æ˜æˆ‘ä»¬åœ¨è¾ƒé€‚åº¦çš„ $N$ ä¸Šå­¦åˆ°çš„ä¸œè¥¿ç¡®å®å¯ä»¥å¤–æ¨åˆ°ä¸è§†ç½‘è†œä¸€å—åŒºåŸŸå†…ç»†èƒçš„æ•´ä¸ªç¾¤ä½“ç›¸å½“çš„è§„æ¨¡ã€‚æˆ‘ä»¬å°†ä¸å¾—ä¸æ›´åŠ åŠªåŠ›åœ°å·¥ä½œï¼Œä»¥å†³å®šæˆ‘ä»¬æ˜¯å¦èƒ½å¤Ÿçœ‹åˆ°çœŸæ­£çƒ­åŠ›å­¦æé™çš„å‡ºç°ã€‚</p>
<blockquote>
<p>Finally, we should address the question of whether these results can be recovered as perturbations to a model of independent neurons. At lowest order in perturbation theory, there is a simple relationship between the observed correlations and the inferred interactions $J_{ij}$ in the pairwise model (Sessak and Monasson, 2009), and we can check this relationship against the values of $J_{ij}$ inferred from correctly matching the observed correlations. In the retina, large deviations from lowest order perturbation theory are visible already at $N = 15$, and correspondingly models built from the perturbative estimates of $J_{ij}$ are orders of magnitude further away from the data than the full model (TkacË‡ik et al., 2014). Higher order perturbative contributions to the entropy would be comparable to one another for $N = 20$ retinal neurons even in a hypothetical network where all correlations were scaled down by a factor of two from the real data (Azhar and Bialek, 2010). We conclude that the success of maximum entropy models in describing networks of real neurons is not something we can understand in low order perturbation theory. Interestingly, simulations of models with pure 3â€“ and 4-spin interactions at $N\sim 20$ show that pairwise maximum entropy models typically are good approximations to the real distribution both in the weak correlation limit and in the limit of strong, dense interactions (Merchan and Nemenman, 2016).</p>
</blockquote>
<p>æœ€åï¼Œæˆ‘ä»¬åº”è¯¥è§£å†³è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼šè¿™äº›ç»“æœæ˜¯å¦å¯ä»¥ä½œä¸ºç‹¬ç«‹ç¥ç»å…ƒæ¨¡å‹çš„å¾®æ‰°æ¥æ¢å¤ã€‚åœ¨å¾®æ‰°ç†è®ºçš„æœ€ä½é˜¶ä¸­ï¼Œè§‚å¯Ÿåˆ°çš„ç›¸å…³æ€§ä¸æˆå¯¹æ¨¡å‹ä¸­æ¨æ–­çš„ç›¸äº’ä½œç”¨ $J_{ij}$ ä¹‹é—´å­˜åœ¨ç®€å•å…³ç³»ï¼ˆSessak å’Œ Monassonï¼Œ2009ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®æ­£ç¡®åŒ¹é…è§‚å¯Ÿåˆ°çš„ç›¸å…³æ€§æ¥æ£€æŸ¥è¿™ç§å…³ç³»ä¸æ¨æ–­çš„ $J_{ij}$ å€¼ä¹‹é—´çš„å…³ç³»ã€‚åœ¨è§†ç½‘è†œä¸­ï¼Œåœ¨ $N = 15$ æ—¶å·²ç»å¯ä»¥çœ‹åˆ°ä¸æœ€ä½é˜¶å¾®æ‰°ç†è®ºçš„å·¨å¤§åå·®ï¼Œå› æ­¤ä»å¾®æ‰°ä¼°è®¡çš„ $J_{ij}$ æ„å»ºçš„æ¨¡å‹ä¸å®Œæ•´æ¨¡å‹ç›¸æ¯”ï¼Œä¸æ•°æ®ç›¸å·®å‡ ä¸ªæ•°é‡çº§ï¼ˆTkacË‡ik ç­‰äººï¼Œ2014ï¼‰ã€‚å³ä½¿åœ¨ä¸€ä¸ªå‡è®¾ç½‘ç»œä¸­ï¼Œæ‰€æœ‰ç›¸å…³æ€§éƒ½æ¯”çœŸå®æ•°æ®ç¼©å°äº†ä¸¤å€ï¼Œå¯¹äº $N = 20$ ä¸ªè§†ç½‘è†œç¥ç»å…ƒï¼Œé«˜é˜¶å¾®æ‰°å¯¹ç†µçš„è´¡çŒ®ä¹Ÿå°†å½¼æ­¤ç›¸å½“ï¼ˆAzhar å’Œ Bialekï¼Œ2010ï¼‰ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œæœ€å¤§ç†µæ¨¡å‹åœ¨æè¿°çœŸå®ç¥ç»å…ƒç½‘ç»œæ–¹é¢çš„æˆåŠŸä¸æ˜¯æˆ‘ä»¬å¯ä»¥åœ¨ä½é˜¶å¾®æ‰°ç†è®ºä¸­ç†è§£çš„ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨ $N\sim 20$ æ—¶å…·æœ‰çº¯ 3â€“å’Œ 4â€“è‡ªæ—‹ç›¸äº’ä½œç”¨çš„æ¨¡å‹æ¨¡æ‹Ÿè¡¨æ˜ï¼Œæˆå¯¹æœ€å¤§ç†µæ¨¡å‹é€šå¸¸æ˜¯å®é™…åˆ†å¸ƒçš„è‰¯å¥½è¿‘ä¼¼ï¼Œæ— è®ºæ˜¯åœ¨å¼±ç›¸å…³æé™è¿˜æ˜¯åœ¨å¼ºçƒˆã€å¯†é›†ç›¸äº’ä½œç”¨çš„æé™ä¸‹ï¼ˆMerchan å’Œ Nemenmanï¼Œ2016ï¼‰ã€‚</p>
<blockquote>
<p>The retina is a very special part of the brain, and one might worry that the success of maximum entropy models is somehow tied to these special features. It thus is important that the same methods work in capturing the collective behavior of neurons in very distant parts of the brain. An example is in prefrontal cortex, which is involved in a wide range of higher cognitive functions.</p>
</blockquote>
<p>è§†ç½‘è†œæ˜¯å¤§è„‘ä¸­ä¸€ä¸ªéå¸¸ç‰¹æ®Šçš„éƒ¨åˆ†ï¼Œäººä»¬å¯èƒ½ä¼šæ‹…å¿ƒæœ€å¤§ç†µæ¨¡å‹çš„æˆåŠŸä¸è¿™äº›ç‰¹æ®Šç‰¹å¾æœ‰å…³ã€‚å› æ­¤ï¼ŒåŒæ ·çš„æ–¹æ³•åœ¨æ•æ‰å¤§è„‘éå¸¸é¥è¿œéƒ¨åˆ†ç¥ç»å…ƒçš„é›†ä½“è¡Œä¸ºæ–¹é¢ä¹Ÿæœ‰æ•ˆï¼Œè¿™ä¸€ç‚¹å¾ˆé‡è¦ã€‚ä¸€ä¸ªä¾‹å­æ˜¯åœ¨å‰é¢å¶çš®å±‚ï¼Œå®ƒå‚ä¸äº†å¹¿æ³›çš„é«˜çº§è®¤çŸ¥åŠŸèƒ½ã€‚</p>
<blockquote>
<p>Experiments recording simultaneous activity from several tens of neurons in prefrontal cortex were analyzed with maximum entropy methods, and an example of the results is shown in Fig. 13 (Tavoni et al., 2017). We see that these models pass the same tests as in the retina, correctly predicting triplet correlations, the probability of $K$ out of $N$ cells being active simultaneously, and the probabilities for particular patterns of activity in subgroups of $N = 10$ cells. Extending this analysis across multiple experimental sessions it was possible to detect changes in the coupling matrix $J_{ij}$ as the animal learned to engage in different tasks. These changes were concentrated in subsets of cells which also were preferentially reâ€“activated during sleep between sessions. One should be careful about giving too mechanistic an interpretation of the Ising models that emerge from these analyses, but it is exciting to see the structure of the models connect to independently measurable functional dynamics in the network. This is true even in the farthest reaches of the cortex, the regions of the brain that we use for thinking, planning, and deciding.</p>
</blockquote>
<p>åœ¨å‰é¢å¶çš®å±‚è®°å½•åŒæ—¶æ´»åŠ¨çš„å‡ åä¸ªç¥ç»å…ƒçš„å®éªŒä½¿ç”¨æœ€å¤§ç†µæ–¹æ³•è¿›è¡Œäº†åˆ†æï¼Œç»“æœçš„ä¸€ä¸ªä¾‹å­å¦‚å›¾ 13 æ‰€ç¤ºï¼ˆTavoni ç­‰äººï¼Œ2017ï¼‰ã€‚æˆ‘ä»¬çœ‹åˆ°è¿™äº›æ¨¡å‹é€šè¿‡äº†ä¸è§†ç½‘è†œç›¸åŒçš„æµ‹è¯•ï¼Œæ­£ç¡®åœ°é¢„æµ‹äº†ä¸‰å…ƒç»„ç›¸å…³æ€§ã€$N$ ä¸ªç»†èƒä¸­æœ‰ $K$ ä¸ªåŒæ—¶æ´»è·ƒçš„æ¦‚ç‡ï¼Œä»¥åŠ $N = 10$ ä¸ªç»†èƒå­ç»„ä¸­ç‰¹å®šæ´»åŠ¨æ¨¡å¼çš„æ¦‚ç‡ã€‚é€šè¿‡è·¨å¤šä¸ªå®éªŒä¼šè¯æ‰©å±•æ­¤åˆ†æï¼Œå¯ä»¥æ£€æµ‹åˆ°åŠ¨ç‰©å­¦ä¹ å‚ä¸ä¸åŒä»»åŠ¡æ—¶è€¦åˆçŸ©é˜µ $J_{ij}$ çš„å˜åŒ–ã€‚è¿™äº›å˜åŒ–é›†ä¸­åœ¨ç»†èƒå­é›†ä¸­ï¼Œè¿™äº›ç»†èƒåœ¨ä¼šè¯ä¹‹é—´çš„ç¡çœ æœŸé—´ä¹Ÿè¢«ä¼˜å…ˆé‡æ–°æ¿€æ´»ã€‚æˆ‘ä»¬åº”è¯¥å°å¿ƒä¸è¦å¯¹ä»è¿™äº›åˆ†æä¸­å‡ºç°çš„ä¼Šè¾›æ¨¡å‹ç»™äºˆè¿‡äºæœºæ¢°çš„è§£é‡Šï¼Œä½†ä»¤äººå…´å¥‹çš„æ˜¯çœ‹åˆ°æ¨¡å‹çš„ç»“æ„ä¸ç½‘ç»œä¸­ç‹¬ç«‹å¯æµ‹é‡çš„åŠŸèƒ½åŠ¨æ€ç›¸è¿æ¥ã€‚å³ä½¿åœ¨å¤§è„‘çš®å±‚æœ€è¿œçš„åŒºåŸŸï¼Œæˆ‘ä»¬ç”¨äºæ€è€ƒã€è®¡åˆ’å’Œå†³ç­–çš„å¤§è„‘åŒºåŸŸä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/14/M3pUjR95fwxKvIW.png" alt=""  /></p>
<p>FIG. 13 Pairwise maximum entropy models describe collective behavior of $N = 37$ neurons in prefrontal cortex (Tavoni et al., 2017). (A) Observed vs predicted triplet correlations among all neurons. Training results (blue) are predictions from the same segment of the experiment where the pairwise correlations were measured; test results (red) are in a different segment of the experiment. (B) Probability that $K$ out of $N$ neurons are active simultaneously, comparing predictions of the model with data in training and test segments. (C) Rate at which patterns of spiking and silence appear in a subset of ten neurons, comparing predicted vs observed rates in an independent model (cyan) and in the pairwise model (blue).</p>
</blockquote>
<p>å›¾ 13 æˆå¯¹æœ€å¤§ç†µæ¨¡å‹æè¿°å‰é¢å¶çš®å±‚ä¸­ $N = 37$ ä¸ªç¥ç»å…ƒçš„é›†ä½“è¡Œä¸ºï¼ˆTavoni ç­‰äººï¼Œ2017ï¼‰ã€‚ï¼ˆAï¼‰æ‰€æœ‰ç¥ç»å…ƒä¹‹é—´è§‚å¯Ÿåˆ°çš„ä¸é¢„æµ‹çš„ä¸‰å…ƒç»„ç›¸å…³æ€§ã€‚è®­ç»ƒç»“æœï¼ˆè“è‰²ï¼‰æ˜¯ä»å®éªŒçš„åŒä¸€æ®µæµ‹é‡æˆå¯¹ç›¸å…³æ€§çš„é¢„æµ‹ï¼›æµ‹è¯•ç»“æœï¼ˆçº¢è‰²ï¼‰åœ¨å®éªŒçš„ä¸åŒéƒ¨åˆ†ã€‚ï¼ˆBï¼‰$N$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªåŒæ—¶æ´»è·ƒçš„æ¦‚ç‡ï¼Œå°†æ¨¡å‹çš„é¢„æµ‹ä¸è®­ç»ƒå’Œæµ‹è¯•éƒ¨åˆ†çš„æ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚ï¼ˆCï¼‰åœ¨åä¸ªç¥ç»å…ƒå­é›†ä¸­å°–å³°å’Œé™é»˜æ¨¡å¼å‡ºç°çš„é€Ÿç‡ï¼Œåœ¨ç‹¬ç«‹æ¨¡å‹ï¼ˆé’è‰²ï¼‰å’Œæˆå¯¹æ¨¡å‹ï¼ˆè“è‰²ï¼‰ä¸­æ¯”è¾ƒé¢„æµ‹ä¸è§‚å¯Ÿåˆ°çš„é€Ÿç‡ã€‚</p>
</blockquote>
<blockquote>
<p>The Ising model also gives us a way of exploring how the network would respond to hypothetical perturbations (Tavoni et al., 2016). If we increase the magnetic field uniformly across all the cells in the population of prefrontal neurons, the predicted changes in activity are far from uniform. For some cells the response and the derivative of the response (susceptibility) are on a scale expected if neurons respond independently to applied fields, but there are groups of cells that coâ€“activate much more, with susceptibilities peaking at intermediate fields. It is tempting to think that these groups of cells have some functional significance, and this is supported by the fact that in the real data (with no fictitious fields) the groups of cells identified in this way remain coâ€“activated over relatively long periods of time.</p>
</blockquote>
<p>Ising æ¨¡å‹è¿˜ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§æ¢ç´¢ç½‘ç»œå¦‚ä½•å“åº”å‡è®¾æ‰°åŠ¨çš„æ–¹æ³•ï¼ˆTavoni ç­‰äººï¼Œ2016ï¼‰ã€‚å¦‚æœæˆ‘ä»¬åœ¨å‰é¢å¶ç¥ç»å…ƒç¾¤ä½“ä¸­çš„æ‰€æœ‰ç»†èƒä¸Šå‡åŒ€å¢åŠ ç£åœºï¼Œé¢„æµ‹çš„æ´»åŠ¨å˜åŒ–è¿œéå‡åŒ€ã€‚å¯¹äºæŸäº›ç»†èƒï¼Œå“åº”åŠå…¶å“åº”çš„å¯¼æ•°ï¼ˆæ˜“æ„Ÿæ€§ï¼‰å¤„äºé¢„æœŸçš„å°ºåº¦ä¸Šï¼Œå¦‚æœç¥ç»å…ƒç‹¬ç«‹åœ°å“åº”æ–½åŠ çš„åœºï¼Œä½†æœ‰ä¸€äº›ç»†èƒç¾¤ä½“å…±åŒæ¿€æ´»å¾—æ›´å¤šï¼Œåœ¨ä¸­é—´åœºå¤„æ˜“æ„Ÿæ€§è¾¾åˆ°å³°å€¼ã€‚ä»¤äººæƒ³èµ·çš„æ˜¯ï¼Œè¿™äº›ç»†èƒç¾¤ä½“å…·æœ‰æŸç§åŠŸèƒ½æ„ä¹‰ï¼Œè¿™å¾—åˆ°äº†è¿™æ ·ä¸€ä¸ªäº‹å®çš„æ”¯æŒï¼šåœ¨çœŸå®æ•°æ®ä¸­ï¼ˆæ²¡æœ‰è™šæ„åœºï¼‰ï¼Œä»¥è¿™ç§æ–¹å¼è¯†åˆ«çš„ç»†èƒç¾¤ä½“åœ¨ç›¸å¯¹è¾ƒé•¿çš„æ—¶é—´å†…ä¿æŒå…±åŒæ¿€æ´»ã€‚</p>
<blockquote>
<p>At the opposite extreme of organismal complexity, the worm <em>C. elegans</em> is an attractive target for these analyses because one can record not just from a large number of cells but from a large fraction of the entire brain at single cell resolution (Â§III.C). A major challenge is that these neurons do not generate discrete action potentials or bursts, so the signal are not naturally binary. A first step was to discretize the continuous fluorescence signals into three levels, and construct a Pottsâ€“like model that matched the population of each state and the probabilities that pairs of neurons are in the same state (Chen et al., 2019). Although these early data sets were limited, this simple model succeeded in predicting offâ€“diagonal elements of the correlation matrix that were unconstrained, the probability that $K$ of $N$ neurons are in the same state, and the relative probabilities of different states in relation to the effective fields generated by the rest of the network. The fact that the same statistical physics approaches work in worms and in mammalian cortex is encouraging, though we should see more compelling tests with the next generation of experiments.</p>
</blockquote>
<p>åœ¨æœ‰æœºä½“å¤æ‚æ€§çš„ç›¸åæç«¯ï¼Œçº¿è™« <em>C. elegans</em> æ˜¯è¿™äº›åˆ†æçš„ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„ç›®æ ‡ï¼Œå› ä¸ºäººä»¬ä¸ä»…å¯ä»¥ä»å¤§é‡ç»†èƒä¸­è®°å½•ï¼Œè¿˜å¯ä»¥ä»æ•´ä¸ªå¤§è„‘çš„å¤§éƒ¨åˆ†åŒºåŸŸä»¥å•ç»†èƒåˆ†è¾¨ç‡è¿›è¡Œè®°å½•ï¼ˆÂ§III.Cï¼‰ã€‚ä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜æ˜¯è¿™äº›ç¥ç»å…ƒä¸ä¼šäº§ç”Ÿç¦»æ•£çš„åŠ¨ä½œç”µä½æˆ–çˆ†å‘ï¼Œå› æ­¤ä¿¡å·ä¸æ˜¯è‡ªç„¶äºŒè¿›åˆ¶çš„ã€‚ç¬¬ä¸€æ­¥æ˜¯å°†è¿ç»­çš„è§å…‰ä¿¡å·ç¦»æ•£åŒ–ä¸ºä¸‰ä¸ªæ°´å¹³ï¼Œå¹¶æ„å»ºä¸€ä¸ª ç±» Potts çš„æ¨¡å‹ï¼Œä»¥åŒ¹é…æ¯ä¸ªçŠ¶æ€çš„äººå£ä»¥åŠæˆå¯¹ç¥ç»å…ƒå¤„äºåŒä¸€çŠ¶æ€çš„æ¦‚ç‡ï¼ˆChen ç­‰äººï¼Œ2019ï¼‰ã€‚å°½ç®¡è¿™äº›æ—©æœŸæ•°æ®é›†æœ‰é™ï¼Œä½†è¿™ä¸ªç®€å•çš„æ¨¡å‹æˆåŠŸåœ°é¢„æµ‹äº†æœªå—çº¦æŸçš„ç›¸å…³çŸ©é˜µçš„éå¯¹è§’å…ƒç´ ï¼Œ$N$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªå¤„äºåŒä¸€çŠ¶æ€çš„æ¦‚ç‡ï¼Œä»¥åŠä¸ç½‘ç»œå…¶ä½™éƒ¨åˆ†äº§ç”Ÿçš„æœ‰æ•ˆåœºç›¸å…³çš„ä¸åŒçŠ¶æ€çš„ç›¸å¯¹æ¦‚ç‡ã€‚åŒæ ·çš„ç»Ÿè®¡ç‰©ç†æ–¹æ³•åœ¨è •è™«å’Œå“ºä¹³åŠ¨ç‰©çš®å±‚ä¸­éƒ½æœ‰æ•ˆï¼Œè¿™ä»¤äººé¼“èˆï¼Œå°½ç®¡æˆ‘ä»¬åº”è¯¥é€šè¿‡ä¸‹ä¸€ä»£å®éªŒçœ‹åˆ°æ›´æœ‰è¯´æœåŠ›çš„æµ‹è¯•ã€‚</p>
<blockquote>
<p>A very different approach is to study networks of neurons that have been removed from the animal and kept alive in a dish. There is a long history of work on these â€œcultured networks,â€ and as noted above (Â§III.A) some of the earliest experiments recording from many neurons were done with networks that had been grown onto an array of electrodes (Pine and Gilbert, 1982). Considerable interest was generated by the observation that patterns of activity in cultured networks of cortical neurons consist of â€œavalanchesâ€ that exhibit at least some degree of scale invariance (Â§VI.A). Recent work returns to these data and shows that detailed patterns of spiking and silence are well described by pairwise maximum entropy models, reproducing triplet correlations and the probability that $K$ out of $N = 60$ neurons are active simultaneously (Sampaio Filho et al., 2024).</p>
</blockquote>
<p>ä¸€ç§éå¸¸ä¸åŒçš„æ–¹æ³•æ˜¯ç ”ç©¶ä»åŠ¨ç‰©ä½“å†…ç§»é™¤å¹¶åœ¨åŸ¹å…»çš¿ä¸­ä¿æŒæ´»åŠ›çš„ç¥ç»å…ƒç½‘ç»œã€‚è¿™äº›â€œåŸ¹å…»ç½‘ç»œâ€æœ‰ç€æ‚ ä¹…çš„ç ”ç©¶å†å²ï¼Œå¦‚ä¸Šæ‰€è¿°ï¼ˆÂ§III.Aï¼‰ï¼Œä¸€äº›æœ€æ—©è®°å½•è®¸å¤šç¥ç»å…ƒçš„å®éªŒæ˜¯ä½¿ç”¨ç”Ÿé•¿åœ¨ç”µæé˜µåˆ—ä¸Šçš„ç½‘ç»œè¿›è¡Œçš„ï¼ˆPine å’Œ Gilbertï¼Œ1982ï¼‰ã€‚äººä»¬å¯¹è§‚å¯Ÿåˆ°çš®å±‚ç¥ç»å…ƒåŸ¹å…»ç½‘ç»œä¸­çš„æ´»åŠ¨æ¨¡å¼ç”±â€œé›ªå´©â€ç»„æˆäº§ç”Ÿäº†ç›¸å½“å¤§çš„å…´è¶£ï¼Œè¿™äº›é›ªå´©è‡³å°‘è¡¨ç°å‡ºæŸç§ç¨‹åº¦çš„å°ºåº¦ä¸å˜æ€§ï¼ˆÂ§VI.Aï¼‰ã€‚æœ€è¿‘çš„å·¥ä½œå›åˆ°äº†è¿™äº›æ•°æ®ï¼Œå¹¶æ˜¾ç¤ºå°–å³°å’Œé™é»˜çš„è¯¦ç»†æ¨¡å¼å¯ä»¥å¾ˆå¥½åœ°ç”¨æˆå¯¹æœ€å¤§ç†µæ¨¡å‹æ¥æè¿°ï¼Œé‡ç°äº†ä¸‰å…ƒç»„ç›¸å…³æ€§ä»¥åŠ $N = 60$ ä¸ªç¥ç»å…ƒä¸­æœ‰ $K$ ä¸ªåŒæ—¶æ´»è·ƒçš„æ¦‚ç‡ï¼ˆSampaio Filho ç­‰äººï¼Œ2024ï¼‰ã€‚</p>
<blockquote>
<p>As a final example we consider populations of $N\sim 100$ neurons in the mouse hippocampus (Meshulam et al., 2017). The hippocampus plays a central role in navigation and episodic memory, and is perhaps best known for its population of â€œplace cells,â€ neurons that are active only when the animal moves to a particular position in its environment. First discovered in rodents (Oâ€™Keefe and Dostrovsky, 1971), it is thought that the whole population of these cells together provides the animal with a cognitive map (Oâ€™Keefe and Nadel, 1978). More recent work shows how this structure extends to three dimensions, and across hundreds of meters in bats (Tsoar et al., 2011; Yartsev and Ulanovsky, 2013).</p>
</blockquote>
<p>ä½œä¸ºæœ€åä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬è€ƒè™‘å°é¼ æµ·é©¬ä½“ä¸­ $N\sim 100$ ä¸ªç¥ç»å…ƒçš„ç¾¤ä½“ï¼ˆMeshulam ç­‰äººï¼Œ2017ï¼‰ã€‚æµ·é©¬ä½“åœ¨å¯¼èˆªå’Œæƒ…èŠ‚è®°å¿†ä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ï¼Œæˆ–è®¸æœ€è‘—åçš„æ˜¯å…¶â€œä½ç½®ç»†èƒâ€ç¾¤ä½“ï¼Œè¿™äº›ç¥ç»å…ƒä»…åœ¨åŠ¨ç‰©ç§»åŠ¨åˆ°å…¶ç¯å¢ƒä¸­çš„ç‰¹å®šä½ç½®æ—¶æ‰æ´»è·ƒã€‚æœ€æ—©åœ¨å•®é½¿åŠ¨ç‰©ä¸­å‘ç°ï¼ˆOâ€™Keefe å’Œ Dostrovskyï¼Œ1971ï¼‰ï¼Œäººä»¬è®¤ä¸ºè¿™äº›ç»†èƒçš„æ•´ä¸ªç¾¤ä½“å…±åŒä¸ºåŠ¨ç‰©æä¾›äº†è®¤çŸ¥åœ°å›¾ï¼ˆOâ€™Keefe å’Œ Nadelï¼Œ1978ï¼‰ã€‚æœ€è¿‘çš„å·¥ä½œå±•ç¤ºäº†è¿™ç§ç»“æ„å¦‚ä½•æ‰©å±•åˆ°ä¸‰ç»´ï¼Œå¹¶è·¨è¶Šè™è ä¸­çš„æ•°ç™¾ç±³ï¼ˆTsoar ç­‰äººï¼Œ2011ï¼›Yartsev å’Œ Ulanovskyï¼Œ2013ï¼‰ã€‚</p>
<blockquote>
<p>As the animal explores its environment, or runs along a virtual track, the mean activity of individual neurons is quite small, as in the examples above. Most pairs of neuron have negative correlations, as expected if activity is tied to the positionâ€”if each cell is active in a different place, then on average one cell being active means that other cells must be silent, generating anti- correlations. Indeed it is tempting to make a model of the hippocampus in which some positional signal is computed by the brain, with inputs from many regions, and each cell in the hippocampus is active or silent depending on the value of this positional signal. This model is specified by the â€œplace fieldsâ€ of each cell, the probability that a cell is active as a function of position, and these can be estimated directly from the data; given the place fields all other properties of the network are determined with no adjustable parameters.</p>
</blockquote>
<p>å½“åŠ¨ç‰©æ¢ç´¢å…¶ç¯å¢ƒæˆ–æ²¿ç€è™šæ‹Ÿè½¨é“è¿è¡Œæ—¶ï¼Œå•ä¸ªç¥ç»å…ƒçš„å¹³å‡æ´»åŠ¨ç›¸å½“å°ï¼Œå¦‚ä¸Šé¢çš„ä¾‹å­æ‰€ç¤ºã€‚å¤§å¤šæ•°ç¥ç»å…ƒå¯¹å…·æœ‰è´Ÿç›¸å…³æ€§ï¼Œè¿™ç¬¦åˆé¢„æœŸï¼Œå¦‚æœæ´»åŠ¨ä¸ä½ç½®ç›¸å…³â€”â€”å¦‚æœæ¯ä¸ªç»†èƒåœ¨ä¸åŒçš„ä½ç½®æ´»è·ƒï¼Œé‚£ä¹ˆå¹³å‡è€Œè¨€ï¼Œä¸€ä¸ªç»†èƒæ´»è·ƒæ„å‘³ç€å…¶ä»–ç»†èƒå¿…é¡»é™é»˜ï¼Œä»è€Œäº§ç”Ÿåç›¸å…³ã€‚å®é™…ä¸Šï¼Œæ„å»ºä¸€ä¸ªæµ·é©¬ä½“æ¨¡å‹æ˜¯å¾ˆè¯±äººçš„ï¼Œåœ¨è¯¥æ¨¡å‹ä¸­ï¼Œå¤§è„‘é€šè¿‡æ¥è‡ªè®¸å¤šåŒºåŸŸçš„è¾“å…¥è®¡ç®—æŸç§ä½ç½®ä¿¡å·ï¼Œæµ·é©¬ä½“ä¸­çš„æ¯ä¸ªç»†èƒæ ¹æ®è¯¥ä½ç½®ä¿¡å·çš„å€¼å¤„äºæ´»è·ƒæˆ–é™é»˜çŠ¶æ€ã€‚è¯¥æ¨¡å‹ç”±æ¯ä¸ªç»†èƒçš„â€œä½ç½®åœºâ€æŒ‡å®šï¼Œå³ç»†èƒä½œä¸ºä½ç½®å‡½æ•°æ´»è·ƒçš„æ¦‚ç‡ï¼Œè¿™äº›å¯ä»¥ç›´æ¥ä»æ•°æ®ä¸­ä¼°è®¡ï¼›ç»™å®šä½ç½®åœºï¼Œç½‘ç»œçš„æ‰€æœ‰å…¶ä»–å±æ€§éƒ½æ²¡æœ‰å¯è°ƒå‚æ•°åœ°ç¡®å®šã€‚</p>
<blockquote>
<p>The place field of cell $i$ is defined by the average activity conditional on the position $x$ along a track,</p>
<p>$$
\langle\sigma_{i}\rangle_{x} = F_{i}(x)
$$</p>
<p>If activity in each cell depends independently on position, then the pairwise correlations are driven by the fact that all cells experience the same x, drawn from some distribution $P (x)$ across the experiment. The quantitative prediction is that</p>
<p>$$
\begin{aligned}
C_{ij} &amp;\equiv \langle\sigma_{i}\sigma_{j}\rangle - \langle\sigma_{i}\rangle\langle\sigma_{j}\rangle \\
&amp;= \int\mathrm{d}x P(x)F_{i}(x)F_{j}(x) - \left[\int\mathrm{d}x P(x)F_{i}(x)\right]\left[\int\mathrm{d}x P(x)F_{j}(x)\right]
\end{aligned}
$$</p>
<p>The covariance matrix elements $C_{ij}$ have a pattern that is qualitatively similar to the real data, but quantitatively very far off. In particular the eigenvalue spectrum of the matrix predicted in this way falls very rapidly, while the real spectrum has a slow, nearly powerâ€“law decay (Meshulam et al., 2017). This is a first hint that the neurons in the hippocampal network share information, and hence exhibit collective behavior, beyond just place.</p>
</blockquote>
<p>ç»†èƒ $i$ çš„ä½ç½®åœºç”±æ²¿è½¨é“ä½ç½® $x$ æ¡ä»¶ä¸‹çš„å¹³å‡æ´»åŠ¨å®šä¹‰ï¼Œ</p>
<p>$$
\langle\sigma_{i}\rangle_{x} = F_{i}(x)
$$</p>
<p>å¦‚æœæ¯ä¸ªç»†èƒçš„æ´»åŠ¨ç‹¬ç«‹åœ°ä¾èµ–äºä½ç½®ï¼Œé‚£ä¹ˆæˆå¯¹ç›¸å…³æ€§æ˜¯ç”±æ‰€æœ‰ç»†èƒç»å†ç›¸åŒçš„ x é©±åŠ¨çš„ï¼Œè¿™äº› x æ˜¯ä»å®éªŒä¸­çš„æŸä¸ªåˆ†å¸ƒ $P (x)$ ä¸­æŠ½å–çš„ã€‚å®šé‡é¢„æµ‹æ˜¯</p>
<p>$$
\begin{aligned}
C_{ij} &amp;\equiv \langle\sigma_{i}\sigma_{j}\rangle - \langle\sigma_{i}\rangle\langle\sigma_{j}\rangle \\
&amp;= \int\mathrm{d}x P(x)F_{i}(x)F_{j}(x) - \left[\int\mathrm{d}x P(x)F_{i}(x)\right]\left[\int\mathrm{d}x P(x)F_{j}(x)\right]
\end{aligned}
$$</p>
<p>åæ–¹å·®çŸ©é˜µå…ƒç´  $C_{ij}$ å…·æœ‰ä¸çœŸå®æ•°æ®å®šæ€§ä¸Šç›¸ä¼¼çš„æ¨¡å¼ï¼Œä½†åœ¨å®šé‡ä¸Šéå¸¸åç¦»ã€‚ç‰¹åˆ«æ˜¯ï¼Œä»¥è¿™ç§æ–¹å¼é¢„æµ‹çš„çŸ©é˜µçš„ç‰¹å¾å€¼è°±ä¸‹é™å¾—éå¸¸å¿«ï¼Œè€ŒçœŸå®è°±å…·æœ‰ç¼“æ…¢ã€è¿‘ä¼¼å¹‚å¾‹è¡°å‡ï¼ˆMeshulam ç­‰äººï¼Œ2017ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªåˆæ­¥çš„æš—ç¤ºï¼Œè¡¨æ˜æµ·é©¬ä½“ç½‘ç»œä¸­çš„ç¥ç»å…ƒå…±äº«ä¿¡æ¯ï¼Œå› æ­¤è¡¨ç°å‡ºè¶…è¶Šä½ç½®çš„é›†ä½“è¡Œä¸ºã€‚</p>
<blockquote>
<p>A new generation of experiments monitoring 1000+ neurons in the hippocampus provides unique opportunities for theory, as discussed in Â§Â§V and VII below. Here we want to emphasize the way in which collective dynamics emerge from maximum entropy models of $N\sim 100$ cells. Equations (49, 50) and Figure 11 remind us that models for the joint distribution of activity in a neural population also predict the probability for one neuron to be active given the state of the rest of the network. We can go through the same exercise for a population of cells in the hippocampus: construct the pairwise maximum entropy model, and for each neuron at each moment compute the probability that it will be active given the state of all the other neurons; results are shown in Fig 14A.</p>
</blockquote>
<p>æ–°ä¸€ä»£ç›‘æµ‹æµ·é©¬ä½“ä¸­ 1000 å¤šä¸ªç¥ç»å…ƒçš„å®éªŒä¸ºç†è®ºæä¾›äº†ç‹¬ç‰¹çš„æœºä¼šï¼Œå¦‚ä¸‹æ–‡ Â§Â§V å’Œ VII æ‰€è®¨è®ºçš„é‚£æ ·ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æƒ³å¼ºè°ƒé›†ä½“åŠ¨åŠ›å­¦å¦‚ä½•ä» $N\sim 100$ ä¸ªç»†èƒçš„æœ€å¤§ç†µæ¨¡å‹ä¸­å‡ºç°ã€‚æ–¹ç¨‹ï¼ˆ49ï¼Œ50ï¼‰å’Œå›¾ 11 æé†’æˆ‘ä»¬ï¼Œç¥ç»ç¾¤ä½“ä¸­æ´»åŠ¨è”åˆåˆ†å¸ƒçš„æ¨¡å‹è¿˜é¢„æµ‹äº†åœ¨ç½‘ç»œå…¶ä½™éƒ¨åˆ†çŠ¶æ€ç»™å®šçš„æƒ…å†µä¸‹ä¸€ä¸ªç¥ç»å…ƒæ´»è·ƒçš„æ¦‚ç‡ã€‚æˆ‘ä»¬å¯ä»¥å¯¹æµ·é©¬ä½“ä¸­çš„ç»†èƒç¾¤ä½“è¿›è¡ŒåŒæ ·çš„ç»ƒä¹ ï¼šæ„å»ºæˆå¯¹æœ€å¤§ç†µæ¨¡å‹ï¼Œå¹¶åœ¨æ¯ä¸ªæ—¶åˆ»ä¸ºæ¯ä¸ªç¥ç»å…ƒè®¡ç®—åœ¨æ‰€æœ‰å…¶ä»–ç¥ç»å…ƒçŠ¶æ€ç»™å®šçš„æƒ…å†µä¸‹å®ƒå°†æ´»è·ƒçš„æ¦‚ç‡ï¼›ç»“æœå¦‚å›¾ 14A æ‰€ç¤ºã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/14/4XqTZnKOtAxkeRC.png" alt=""  /></p>
<p>Collective behavior in the mouse hippocampus (Meshulam et al., 2017). (A) Predicted probability of activity for single neurons, computed from the effective field in the pairwise maximum entropy model. Focus is on 32 place cells that should be active in sequence as the mouse runs along a virtual track. During the first run, cells 21â€“25 are predicted to â€œmissâ€ their place fields, but all cells are predicted to be active in the second run. (B) Real data of place cell activity during two runs down the linear track, in the same time window as (A) and (C); note the missed events for cells 21â€“25 in the first run. (C) Predicted probability from the independent place cell model. There is no indication of when fields should be missed.</p>
</blockquote>
<p>å°é¼ æµ·é©¬ä½“ä¸­çš„é›†ä½“è¡Œä¸ºï¼ˆMeshulam ç­‰äººï¼Œ2017ï¼‰ã€‚ï¼ˆAï¼‰ä»æˆå¯¹æœ€å¤§ç†µæ¨¡å‹ä¸­çš„æœ‰æ•ˆåœºè®¡ç®—çš„å•ä¸ªç¥ç»å…ƒçš„é¢„æµ‹æ´»åŠ¨æ¦‚ç‡ã€‚é‡ç‚¹å…³æ³¨ 32 ä¸ªä½ç½®ç»†èƒï¼Œå½“å°é¼ æ²¿ç€è™šæ‹Ÿè½¨é“è¿è¡Œæ—¶ï¼Œè¿™äº›ç»†èƒåº”è¯¥æŒ‰é¡ºåºæ´»è·ƒã€‚åœ¨ç¬¬ä¸€æ¬¡è¿è¡ŒæœŸé—´ï¼Œé¢„æµ‹ç»†èƒ 21â€“25 ä¼šâ€œé”™è¿‡â€å®ƒä»¬çš„ä½ç½®åœºï¼Œä½†åœ¨ç¬¬äºŒæ¬¡è¿è¡Œä¸­é¢„æµ‹æ‰€æœ‰ç»†èƒéƒ½ä¼šæ´»è·ƒã€‚ï¼ˆBï¼‰ä¸ï¼ˆAï¼‰å’Œï¼ˆCï¼‰ç›¸åŒæ—¶é—´çª—å£å†…çº¿æ€§è½¨é“ä¸Šä¸¤æ¬¡è¿è¡ŒæœŸé—´ä½ç½®ç»†èƒæ´»åŠ¨çš„çœŸå®æ•°æ®ï¼›è¯·æ³¨æ„ç¬¬ä¸€æ¬¡è¿è¡Œä¸­ç»†èƒ 21â€“25 çš„é”™è¿‡äº‹ä»¶ã€‚ï¼ˆCï¼‰æ¥è‡ªç‹¬ç«‹ä½ç½®ç»†èƒæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ã€‚æ²¡æœ‰è¿¹è±¡è¡¨æ˜ä½•æ—¶åº”è¯¥é”™è¿‡åœºã€‚</p>
</blockquote>
<blockquote>
<p>We see in Figure 14A that, roughly speaking, cells are predicted to be active in sequence. This makes sense since these are place cells, and the mouse is running at nearly constant speed along a virtual track, so cells with place fields arrayed along the track should be activated one after the other. Interestingly the calculation leading to this prediction makes no reference to the (virtual) position of the mouse, or even to the idea of place fields, but only to the dependence of activity in one cell on the rest of the network. In this window of time the mouse actually makes two trips along the track, and perhaps surprisingly the predictions for the two trips are different. On the first trip it is predicted that several of the cells will â€œmissâ€ their place fields, while all cells should be active in sequence on the second trip. This is exactly what we see in the data (Fig 14B). If neurons were driven only by the animalâ€™s position this wouldnâ€™t happen (Fig 14C). Thus what might have seemed like unpredictable variation really reflects the collective behavior of the network, and is captured very well by the Ising model, with no additional parameters. We return to Ising models for the hippocampus in Â§V below.</p>
</blockquote>
<p>æˆ‘ä»¬åœ¨å›¾ 14A ä¸­çœ‹åˆ°ï¼Œå¤§è‡´æ¥è¯´ï¼Œç»†èƒè¢«é¢„æµ‹ä¸ºæŒ‰é¡ºåºæ´»è·ƒã€‚è¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºè¿™äº›æ˜¯ä½ç½®ç»†èƒï¼Œå°é¼ ä»¥å‡ ä¹æ’å®šçš„é€Ÿåº¦æ²¿ç€è™šæ‹Ÿè½¨é“è¿è¡Œï¼Œå› æ­¤æ²¿è½¨é“æ’åˆ—çš„ä½ç½®åœºçš„ç»†èƒåº”è¯¥ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°è¢«æ¿€æ´»ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¯¼è‡´è¿™ä¸€é¢„æµ‹çš„è®¡ç®—æ²¡æœ‰å‚è€ƒå°é¼ çš„ï¼ˆè™šæ‹Ÿï¼‰ä½ç½®ï¼Œç”šè‡³æ²¡æœ‰å‚è€ƒä½ç½®åœºçš„æ¦‚å¿µï¼Œè€Œåªæ˜¯å‚è€ƒä¸€ä¸ªç»†èƒå¯¹ç½‘ç»œå…¶ä½™éƒ¨åˆ†æ´»åŠ¨çš„ä¾èµ–ã€‚åœ¨è¿™æ®µæ—¶é—´å†…ï¼Œå°é¼ å®é™…ä¸Šæ²¿è½¨é“è¿›è¡Œäº†ä¸¤æ¬¡æ—…è¡Œï¼Œæˆ–è®¸ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä¸¤æ¬¡æ—…è¡Œçš„é¢„æµ‹æ˜¯ä¸åŒçš„ã€‚åœ¨ç¬¬ä¸€æ¬¡æ—…è¡Œä¸­ï¼Œé¢„æµ‹æœ‰å‡ ä¸ªç»†èƒä¼šâ€œé”™è¿‡â€å®ƒä»¬çš„ä½ç½®åœºï¼Œè€Œåœ¨ç¬¬äºŒæ¬¡æ—…è¡Œä¸­æ‰€æœ‰ç»†èƒéƒ½åº”è¯¥æŒ‰é¡ºåºæ´»è·ƒã€‚è¿™æ­£æ˜¯æˆ‘ä»¬åœ¨æ•°æ®ä¸­çœ‹åˆ°çš„ï¼ˆå›¾ 14Bï¼‰ã€‚å¦‚æœç¥ç»å…ƒä»…ç”±åŠ¨ç‰©çš„ä½ç½®é©±åŠ¨ï¼Œè¿™ç§æƒ…å†µå°±ä¸ä¼šå‘ç”Ÿï¼ˆå›¾ 14Cï¼‰ã€‚å› æ­¤ï¼Œçœ‹ä¼¼ä¸å¯é¢„æµ‹çš„å˜åŒ–å®é™…ä¸Šåæ˜ äº†ç½‘ç»œçš„é›†ä½“è¡Œä¸ºï¼Œå¹¶ä¸”è¢« Ising æ¨¡å‹å¾ˆå¥½åœ°æ•æ‰åˆ°äº†ï¼Œæ²¡æœ‰é¢å¤–çš„å‚æ•°ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„ Â§V ä¸­å›åˆ°æµ·é©¬ä½“çš„ Ising æ¨¡å‹ã€‚</p>
<h1 id="doing-more-and-doing-less">Doing more and doing less<a hidden class="anchor" aria-hidden="true" href="#doing-more-and-doing-less">#</a></h1>
<blockquote>
<p>Is there any sense in which maximum entropy models are â€œbetterâ€ than alternative models? The pairwise maximum entropy models are singled out because they have the minimal structure needed to match the mean activity and twoâ€“point correlations in the network. But how different are they from other models that would also match these data? We could imagine, for example, that once we specify the full matrix of correlations then the set of allowed models is very tightly clustered in its predictions about higher order structure in the patterns of activity, in which case saying that these models â€œworkâ€ doesnâ€™t say much about the underlying physics.</p>
</blockquote>
<p>æœ€å¤§ç†µæ¨¡å‹åœ¨æŸç§æ„ä¹‰ä¸Šæ˜¯å¦æ¯”æ›¿ä»£æ¨¡å‹â€œæ›´å¥½â€ï¼Ÿæˆå¯¹æœ€å¤§ç†µæ¨¡å‹ä¹‹æ‰€ä»¥è¢«å•ç‹¬æŒ‘é€‰å‡ºæ¥ï¼Œæ˜¯å› ä¸ºå®ƒä»¬å…·æœ‰åŒ¹é…ç½‘ç»œä¸­å¹³å‡æ´»åŠ¨å’Œä¸¤ç‚¹ç›¸å…³æ€§æ‰€éœ€çš„æœ€å°ç»“æ„ã€‚ä½†æ˜¯ï¼Œå®ƒä»¬ä¸å…¶ä»–ä¹Ÿèƒ½åŒ¹é…è¿™äº›æ•°æ®çš„æ¨¡å‹æœ‰å¤šå¤§ä¸åŒå‘¢ï¼Ÿä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡ï¼Œä¸€æ—¦æˆ‘ä»¬æŒ‡å®šäº†å®Œæ•´çš„ç›¸å…³çŸ©é˜µï¼Œé‚£ä¹ˆå…è®¸çš„æ¨¡å‹é›†åœ¨å…¶å¯¹æ´»åŠ¨æ¨¡å¼ä¸­æ›´é«˜é˜¶ç»“æ„çš„é¢„æµ‹ä¸­å°±ä¼šéå¸¸ç´§å¯†åœ°èšé›†ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯´è¿™äº›æ¨¡å‹â€œæœ‰æ•ˆâ€å¹¶ä¸èƒ½è¯´æ˜æ½œåœ¨çš„ç‰©ç†å­¦ã€‚</p>
<blockquote>
<p>One can build a statistical mechanics on the space of probability distributions $p(\vec{\sigma})$, defining a â€œversion spaceâ€ by all the models that match a given set of pairwise correlations within some tolerance $\epsilon$. We can construct a Boltzmann weight over this space in which the entropy of the underlying distribution plays the role of the (negative) energy,</p>
<p>$$
Q[p(\vec{\sigma})]\propto \delta \left[1-\sum_{\vec{\sigma}}p(\vec{\sigma})\right]\mathbf{U}_{\epsilon}[p(\vec{\sigma};\{m_{i};C_{ij}\})]\times \exp\left[-\beta\sum_{\vec{\sigma}}p(\vec{\sigma})\ln{p(\vec{\sigma})}\right],
$$</p>
<p>where the first term in the product enforces normalization, the second term selects distributions that match expectation values within $\epsilon$, and the last term is the Boltzmann weight (Obuchi et al., 2015). Note that this is the maximum entropy distribution of distributions (!) consistent with a particular mean value of the entropy and the measured expectation values. As $\beta\to\infty$, $Q$ condenses around the maximum entropy distribution, while as $\beta\to 0$ all distributions consistent with the expectation values are given equal weight.</p>
</blockquote>
<p>æˆ‘ä»¬å¯ä»¥åœ¨æ¦‚ç‡åˆ†å¸ƒ $p(\vec{\sigma})$ çš„ç©ºé—´ä¸Šæ„å»ºç»Ÿè®¡åŠ›å­¦ï¼Œé€šè¿‡åœ¨æŸä¸ªå®¹å·® $\epsilon$ èŒƒå›´å†…åŒ¹é…ç»™å®šæˆå¯¹ç›¸å…³æ€§çš„ä¸€ç»„æ¨¡å‹æ¥å®šä¹‰â€œç‰ˆæœ¬ç©ºé—´â€ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ªç©ºé—´ä¸­æ„å»ºä¸€ä¸ªç»å°”å…¹æ›¼æƒé‡ï¼Œå…¶ä¸­åŸºç¡€åˆ†å¸ƒçš„ç†µèµ·ç€ï¼ˆè´Ÿï¼‰èƒ½é‡çš„ä½œç”¨ï¼Œ</p>
<p>$$
Q[p(\vec{\sigma})]\propto \delta \left[1-\sum_{\vec{\sigma}}p(\vec{\sigma})\right]\mathbf{U}_{\epsilon}[p(\vec{\sigma};\{m_{i};C_{ij}\})]\times \exp\left[-\beta\sum_{\vec{\sigma}}p(\vec{\sigma})\ln{p(\vec{\sigma})}\right],
$$</p>
<p>å…¶ä¸­ä¹˜ç§¯ä¸­çš„ç¬¬ä¸€é¡¹å¼ºåˆ¶å½’ä¸€åŒ–ï¼Œç¬¬äºŒé¡¹é€‰æ‹©åœ¨ $\epsilon$ èŒƒå›´å†…åŒ¹é…æœŸæœ›å€¼çš„åˆ†å¸ƒï¼Œæœ€åä¸€é¡¹æ˜¯ç»å°”å…¹æ›¼æƒé‡ï¼ˆObuchi ç­‰äººï¼Œ2015ï¼‰ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯ä¸ç†µçš„ç‰¹å®šå¹³å‡å€¼å’Œæµ‹é‡çš„æœŸæœ›å€¼ä¸€è‡´çš„åˆ†å¸ƒçš„æœ€å¤§ç†µåˆ†å¸ƒï¼ˆï¼ï¼‰ã€‚å½“ $\beta\to\infty$ æ—¶ï¼Œ$Q$ åœ¨æœ€å¤§ç†µåˆ†å¸ƒå‘¨å›´å‡ç»“ï¼Œè€Œå½“ $\beta\to 0$ æ—¶ï¼Œä¸æœŸæœ›å€¼ä¸€è‡´çš„æ‰€æœ‰åˆ†å¸ƒéƒ½è¢«èµ‹äºˆç›¸ç­‰çš„æƒé‡ã€‚</p>
<blockquote>
<p>If the matrix $C_{ij}$ is chosen at random then one can use methods from the statistical mechanics of disordered systems to develop an analytic theory that compares the similarity of the true distribution to those drawn at random from the ensembles of distributions at varying $\beta$. In this random setting the maximum entropy models are not special, and in a rough sense all models that match the lowâ€“order correlations are equally good approximations (Obuchi et al., 2015). Importantly this is not true for the real data on retinal neurons, where the maximum entropy model gives a better description than the typical model that matches the pairwise correlations, and this advantage grows with $N$: â€œfor large networks it is better to pick the most random model than to pick a model at randomâ€ (Ferrari et al., 2017).</p>
</blockquote>
<p>å¦‚æœéšæœºé€‰æ‹©çŸ©é˜µ $C_{ij}$ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨æ— åºç³»ç»Ÿç»Ÿè®¡åŠ›å­¦çš„æ–¹æ³•æ¥å¼€å‘ä¸€ç§åˆ†æç†è®ºï¼Œå°†çœŸå®åˆ†å¸ƒä¸ä»ä¸åŒ $\beta$ çš„åˆ†å¸ƒé›†åˆä¸­éšæœºæŠ½å–çš„åˆ†å¸ƒè¿›è¡Œæ¯”è¾ƒã€‚åœ¨è¿™ç§éšæœºè®¾ç½®ä¸­ï¼Œæœ€å¤§ç†µæ¨¡å‹å¹¶ä¸ç‰¹æ®Šï¼Œä»ç²—ç•¥æ„ä¹‰ä¸Šè®²ï¼Œæ‰€æœ‰åŒ¹é…ä½é˜¶ç›¸å…³æ€§çš„æ¨¡å‹éƒ½æ˜¯åŒæ ·å¥½çš„è¿‘ä¼¼ï¼ˆObuchi ç­‰äººï¼Œ2015ï¼‰ã€‚é‡è¦çš„æ˜¯ï¼Œå¯¹äºè§†ç½‘è†œç¥ç»å…ƒçš„çœŸå®æ•°æ®å¹¶éå¦‚æ­¤ï¼Œå…¶ä¸­æœ€å¤§ç†µæ¨¡å‹æ¯”åŒ¹é…æˆå¯¹ç›¸å…³æ€§çš„å…¸å‹æ¨¡å‹æä¾›äº†æ›´å¥½çš„æè¿°ï¼Œå¹¶ä¸”è¿™ç§ä¼˜åŠ¿éšç€ $N$ çš„å¢åŠ è€Œå¢é•¿ï¼šâ€œå¯¹äºå¤§å‹ç½‘ç»œæ¥è¯´ï¼Œé€‰æ‹©æœ€éšæœºçš„æ¨¡å‹æ¯”éšæœºé€‰æ‹©ä¸€ä¸ªæ¨¡å‹æ›´å¥½â€ï¼ˆFerrari ç­‰äººï¼Œ2017ï¼‰ã€‚</p>
<blockquote>
<p>One way that we could do more in describing the patterns of neural activity is to address their time dependence more explicitly. In particular for the retina we know that the network is being driven by visual inputs. We can repeat the movie many times and ask about the mean activity of each cell at a given moment in the movie, $\langle\sigma_{i}(t)\rangle$. In addition, as before, we can measure the correlations between neurons at the same moment in time, $\langle\sigma_{i}(t)\sigma_{j}(t)\rangle$. Thus we want to find a model for the distribution over sequences or trajetcories of network states $P_{\text{traj}}[\vec{\sigma}(t)]$ that has maximal entropy and matches the timeâ€“dependent mean activity</p>
<p>$$
\begin{aligned}
m_{i}(t) &amp;\equiv \langle\sigma_{i}(t)\rangle_{\text{ext}}\\
&amp;= \langle \sigma_{i}(t)\rangle_{P_{\text{traj}}}
\end{aligned}
$$</p>
<p>as well as the time averaged equal time correlations</p>
<p>$$
\begin{aligned}
C_{ij} &amp;\equiv \frac{1}{\widetilde{T}}\sum_{t}\langle\delta\sigma_{i}(t)\delta\sigma_{j}(t)\rangle_{\text{ext}}\\
&amp;= \frac{1}{\widetilde{T}}\sum_{t}\langle\delta\sigma_{i}(t)\delta\sigma_{j}(t)\rangle_{P_{\text{traj}}}
\end{aligned}
$$</p>
<p>where $\widetilde{T}$ is the duration of our observations in units of the time bin width $\Delta\tau$.</p>
</blockquote>
<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´æ˜ç¡®åœ°è§£å†³å®ƒä»¬çš„æ—¶é—´ä¾èµ–æ€§æ¥æ›´å¥½åœ°æè¿°ç¥ç»æ´»åŠ¨æ¨¡å¼ã€‚ç‰¹åˆ«æ˜¯å¯¹äºè§†ç½‘è†œï¼Œæˆ‘ä»¬çŸ¥é“ç½‘ç»œæ­£åœ¨è¢«è§†è§‰è¾“å…¥é©±åŠ¨ã€‚æˆ‘ä»¬å¯ä»¥å¤šæ¬¡é‡å¤ç”µå½±ï¼Œå¹¶è¯¢é—®ç”µå½±ä¸­ç»™å®šæ—¶åˆ»æ¯ä¸ªç»†èƒçš„å¹³å‡æ´»åŠ¨ï¼Œ$\langle\sigma_{i}(t)\rangle$ã€‚æ­¤å¤–ï¼Œå’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹é‡ç¥ç»å…ƒåœ¨åŒä¸€æ—¶é—´ç‚¹çš„ç›¸å…³æ€§ï¼Œ$\langle\sigma_{i}(t)\sigma_{j}(t)\rangle$ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æƒ³è¦æ‰¾åˆ°ä¸€ä¸ªå…³äºç½‘ç»œçŠ¶æ€åºåˆ—æˆ–è½¨è¿¹çš„åˆ†å¸ƒæ¨¡å‹ $P_{\text{traj}}[\vec{\sigma}(t)]$ï¼Œè¯¥æ¨¡å‹å…·æœ‰æœ€å¤§ç†µå¹¶åŒ¹é…æ—¶é—´ä¾èµ–çš„å¹³å‡æ´»åŠ¨</p>
<p>$$
\begin{aligned}
m_{i}(t) &amp;\equiv \langle\sigma_{i}(t)\rangle_{\text{ext}}\\
&amp;= \langle \sigma_{i}(t)\rangle_{P_{\text{traj}}}
\end{aligned}
$$</p>
<p>ä»¥åŠæ—¶é—´å¹³å‡çš„ç­‰æ—¶ç›¸å…³æ€§</p>
<p>$$
\begin{aligned}
C_{ij} &amp;\equiv \frac{1}{\widetilde{T}}\sum_{t}\langle\delta\sigma_{i}(t)\delta\sigma_{j}(t)\rangle_{\text{ext}}\\
&amp;= \frac{1}{\widetilde{T}}\sum_{t}\langle\delta\sigma_{i}(t)\delta\sigma_{j}(t)\rangle_{P_{\text{traj}}}
\end{aligned}
$$</p>
<p>å…¶ä¸­ $\widetilde{T}$ æ˜¯æˆ‘ä»¬ä»¥æ—¶é—´ç®±å®½åº¦ $\Delta\tau$ ä¸ºå•ä½çš„è§‚å¯ŸæŒç»­æ—¶é—´ã€‚</p>
<blockquote>
<p>This is an instance of the general structure presented in Â§IV.A, where the first set of observables is of the form</p>
<p>$$
\{f_{\mu}\}\rightarrow\{f_{i,t}\} = \{\sigma_{i}(t)\}
$$</p>
<p>To constrain the expectation value of each of these terms we need a separate Lagrange multiplier, and as before we think of these as local field that now depend on time, $\lambda_{i,t} = h_{i}(t)$. In addition we have observables of the form</p>
<p>$$
\{f_{\mu}\} \rightarrow \{f_{ij}\} = \left\{\sum_{t}\sigma_{i}(t)\sigma_{j}(t)\right\}
$$</p>
<p>and for each of these we again have a separate Lagrange multiplier that we think of as a spinâ€“spin coupling, $\lambda_{ij} = J_{ij}$. The general Eqs (20, 21) now take the form</p>
<p>$$
\begin{aligned}
P_{\text{traj}}[\vec{\sigma}(t)] &amp;= \frac{1}{Z_{\text{traj}}}\exp{(-E_{\text{traj}}[\vec{\sigma}(t)])}\\
E_{\text{traj}} &amp;= \sum_{t}\sum_{i}h_{i}(t)\sigma_{i}(t) + \frac{1}{2}\sum_{t}\sum_{ij}J_{ij}\sigma_{i}(t)\sigma_{j}(t)
\end{aligned}
$$</p>
<p>In this class of models, correlations arise both because different neurons may be subject to correlated timedependent fields and because of effects intrinsic to the network. If all of the correlations were driven by visual inputs then matching the correlations would lead to $J_{ij} = 0$, but this never happens with real data.</p>
</blockquote>
<p>è¿™æ˜¯ Â§IV.A ä¸­ä»‹ç»çš„ä¸€èˆ¬ç»“æ„çš„ä¸€ä¸ªå®ä¾‹ï¼Œå…¶ä¸­ç¬¬ä¸€ç»„å¯è§‚å¯Ÿé‡çš„å½¢å¼ä¸º</p>
<p>$$
\{f_{\mu}\}\rightarrow\{f_{i,t}\} = \{\sigma_{i}(t)\}
$$</p>
<p>ä¸ºäº†çº¦æŸè¿™äº›é¡¹çš„æœŸæœ›å€¼ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå•ç‹¬çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼Œå’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å°†è¿™äº›è§†ä¸ºç°åœ¨ä¾èµ–äºæ—¶é—´çš„å±€éƒ¨åœºï¼Œ$\lambda_{i,t} = h_{i}(t)$ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰ä»¥ä¸‹å½¢å¼çš„å¯è§‚å¯Ÿé‡</p>
<p>$$
\{f_{\mu}\} \rightarrow \{f_{ij}\} = \left\{\sum_{t}\sigma_{i}(t)\sigma_{j}(t)\right\}
$$</p>
<p>å¯¹äºæ¯ä¸ªè¿™äº›ï¼Œæˆ‘ä»¬å†æ¬¡æœ‰ä¸€ä¸ªå•ç‹¬çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼Œæˆ‘ä»¬å°†å…¶è§†ä¸ºè‡ªæ—‹-è‡ªæ—‹è€¦åˆï¼Œ$\lambda_{ij} = J_{ij}$ã€‚ä¸€èˆ¬æ–¹ç¨‹ï¼ˆ20ï¼Œ21ï¼‰ç°åœ¨é‡‡ç”¨ä»¥ä¸‹å½¢å¼</p>
<p>$$
\begin{aligned}
P_{\text{traj}}[\vec{\sigma}(t)] &amp;= \frac{1}{Z_{\text{traj}}}\exp{(-E_{\text{traj}}[\vec{\sigma}(t)])}\\
E_{\text{traj}} &amp;= \sum_{t}\sum_{i}h_{i}(t)\sigma_{i}(t) + \frac{1}{2}\sum_{t}\sum_{ij}J_{ij}\sigma_{i}(t)\sigma_{j}(t)
\end{aligned}
$$</p>
<p>åœ¨è¿™ç±»æ¨¡å‹ä¸­ï¼Œç›¸å…³æ€§æ—¢æ˜¯å› ä¸ºä¸åŒçš„ç¥ç»å…ƒå¯èƒ½å—åˆ°ç›¸å…³çš„æ—¶é—´ä¾èµ–åœºçš„å½±å“ï¼Œä¹Ÿæ˜¯ç”±äºç½‘ç»œå†…åœ¨çš„å½±å“ã€‚å¦‚æœæ‰€æœ‰ç›¸å…³æ€§éƒ½æ˜¯ç”±è§†è§‰è¾“å…¥é©±åŠ¨çš„ï¼Œé‚£ä¹ˆåŒ¹é…ç›¸å…³æ€§å°†å¯¼è‡´ $J_{ij} = 0$ï¼Œä½†åœ¨çœŸå®æ•°æ®ä¸­è¿™ç§æƒ…å†µä»æœªå‘ç”Ÿè¿‡ã€‚</p>
<blockquote>
</blockquote>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-3/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>New experimental methods</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-5/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>A unique test</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  
</body>







<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
