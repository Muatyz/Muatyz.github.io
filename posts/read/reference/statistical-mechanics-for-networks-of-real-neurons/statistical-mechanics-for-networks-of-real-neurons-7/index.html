<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Renormalization group for neurons | æ— å¤„æƒ¹å°˜åŸƒ</title>
<meta name="keywords" content="">
<meta name="description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>







<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="Renormalization group for neurons" />
<meta property="og:description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/" />
<meta property="og:image" content="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-12T00:18:23+08:00" />
<meta property="article:modified_time" content="2025-11-12T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" />
<meta name="twitter:title" content="Renormalization group for neurons"/>
<meta name="twitter:description" content="çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ“• é˜…è¯»",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "ğŸ“• æ–‡çŒ®",
          "item": "https://Muatyz.github.io/posts/read/reference/"
        },

        {
          "@type": "ListItem",
          "position":  4 ,
          "name": "ğŸ“• Statistical mechanics for networks of real neurons",
          "item": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/"
        }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "Renormalization group for neurons",
      "item": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Renormalization group for neurons",
  "name": "Renormalization group for neurons",
  "description": "çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦",
  "keywords": [
    ""
  ],
  "articleBody": " Physicists are known for our appreciation of simplified models, perhaps even to the point of overâ€“simplification (Devine and Cohen, 1992). The complexity of living systems is in obvious tension with this drive for simplification; we can perhaps sympathize with biologists who worry that our theoretical impulses may be mismatched to the richness of lifeâ€™s molecular details. A useful response is that there is nothing special about biology: in condensed matter physics and statistical mechanics we routinely describe the macroscopic behavior of materials using models that are much simpler than the underlying microscopic mechanisms. These simplified models succeed, not because we are lucky but because of the renormalization group (Wilson, 1979, 1983).\nç‰©ç†å­¦å®¶ä»¥æ¬£èµç®€åŒ–æ¨¡å‹è€Œé—»åï¼Œç”šè‡³å¯èƒ½è¿‡äºç®€åŒ–ï¼ˆDevine å’Œ Cohenï¼Œ1992ï¼‰ã€‚ç”Ÿå‘½ç³»ç»Ÿçš„å¤æ‚æ€§ä¸è¿™ç§ç®€åŒ–çš„é©±åŠ¨åŠ›æ˜æ˜¾å­˜åœ¨çŸ›ç›¾ï¼›æˆ‘ä»¬æˆ–è®¸å¯ä»¥åŒæƒ…é‚£äº›æ‹…å¿ƒæˆ‘ä»¬çš„ç†è®ºå†²åŠ¨å¯èƒ½ä¸ç”Ÿå‘½ä¸°å¯Œçš„åˆ†å­ç»†èŠ‚ä¸åŒ¹é…çš„ç”Ÿç‰©å­¦å®¶ã€‚ä¸€ç§æœ‰ç”¨çš„å›åº”æ˜¯ï¼Œç”Ÿç‰©å­¦å¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ï¼šåœ¨å‡èšæ€ç‰©ç†å­¦å’Œç»Ÿè®¡åŠ›å­¦ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨æ¯”åº•å±‚å¾®è§‚æœºåˆ¶ç®€å•å¾—å¤šçš„æ¨¡å‹æ¥æè¿°ææ–™çš„å®è§‚è¡Œä¸ºã€‚è¿™äº›ç®€åŒ–æ¨¡å‹ä¹‹æ‰€ä»¥æˆåŠŸï¼Œä¸æ˜¯å› ä¸ºæˆ‘ä»¬å¹¸è¿ï¼Œè€Œæ˜¯å› ä¸ºé‡æ•´åŒ–ç¾¤ï¼ˆWilsonï¼Œ1979ï¼Œ1983ï¼‰ã€‚\nThe central idea of the renormalization group (RG) is to ask how our description of a system changes, systematically, as we change the scale on which we look. The crucial qualitative result is that many different microscopic mechanisms flow toward the same macroscopic behavior as we â€œzoom outâ€ to look at longer length scales. This means that we can understand large scale phenomena quantitatively if we can assign them to the correct universality class, even if we canâ€™t get all the small scale details right, and this gives us license to write relatively simple models of complex systems (Anderson, 1984). We would like to exercise this license in the context of the brain. To do this we need to understand how to implement the RG when many of our usual guides (locality, symmetry, $\\cdots$) are absent. We then can ask whether there is any sign that simplification emerges from the data as we zoom out from individual neurons to more coarseâ€“grained variables.\né‡æ•´åŒ–ç¾¤ï¼ˆRGï¼‰çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå½“æˆ‘ä»¬æ”¹å˜è§‚å¯Ÿç³»ç»Ÿçš„å°ºåº¦æ—¶ï¼Œæˆ‘ä»¬å¯¹ç³»ç»Ÿçš„æè¿°å¦‚ä½•ç³»ç»Ÿåœ°å˜åŒ–ã€‚ä¸€ä¸ªå…³é”®çš„å®šæ€§ç»“æœæ˜¯ï¼Œéšç€æˆ‘ä»¬â€œæ”¾å¤§â€ä»¥è§‚å¯Ÿæ›´é•¿çš„é•¿åº¦å°ºåº¦ï¼Œè®¸å¤šä¸åŒçš„å¾®è§‚æœºåˆ¶ä¼šè¶‹å‘äºç›¸åŒçš„å®è§‚è¡Œä¸ºã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿå°†å¤§è§„æ¨¡ç°è±¡å½’ç±»åˆ°æ­£ç¡®çš„æ™®é€‚ç±»ä¸­ï¼Œå³ä½¿æˆ‘ä»¬æ— æ³•æ­£ç¡®å¤„ç†æ‰€æœ‰å°è§„æ¨¡ç»†èŠ‚ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å®šé‡åœ°ç†è§£å¤§è§„æ¨¡ç°è±¡ï¼Œè¿™èµ‹äºˆäº†æˆ‘ä»¬ç¼–å†™å¤æ‚ç³»ç»Ÿç›¸å¯¹ç®€å•æ¨¡å‹çš„è®¸å¯ï¼ˆAndersonï¼Œ1984ï¼‰ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨å¤§è„‘çš„èƒŒæ™¯ä¸‹è¡Œä½¿è¿™ç§è®¸å¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£å¦‚ä½•åœ¨ç¼ºå°‘è®¸å¤šé€šå¸¸æŒ‡å¯¼ï¼ˆå±€éƒ¨æ€§ã€å¯¹ç§°æ€§ç­‰ï¼‰çš„æƒ…å†µä¸‹å®ç° RGã€‚ç„¶åæˆ‘ä»¬å¯ä»¥é—®ï¼Œå½“æˆ‘ä»¬ä»å•ä¸ªç¥ç»å…ƒæ”¾å¤§åˆ°æ›´ç²—ç²’åº¦çš„å˜é‡æ—¶ï¼Œæ•°æ®ä¸­æ˜¯å¦æœ‰ä»»ä½•è¿¹è±¡è¡¨æ˜ç®€åŒ–å‡ºç°äº†ã€‚\nTaking inspiration from the RG The development of the renormalization group is one the great chapters of theoretical physics from the second half of the twentieth century, with origins in efforts to understand matter at both short and long distances (Gell-Mann and Low, 1954; Kadanoff, 1966). These ideas crystallized in the early 1970s and played a central role in revolutionizing our understanding of the strong interaction among elementary particles, critical phenomena at second order phase transitions, the transition to chaos, and more (Wilson, 1983). How can these ideas help us to think about networks of neurons?\né‡æ•´åŒ–ç¾¤çš„å‘å±•æ˜¯äºŒåä¸–çºªä¸‹åŠå¶ç†è®ºç‰©ç†å­¦çš„ä¼Ÿå¤§ç¯‡ç« ä¹‹ä¸€ï¼Œå…¶èµ·æºåœ¨äºç†è§£ç‰©è´¨åœ¨çŸ­è·ç¦»å’Œé•¿è·ç¦»ä¸Šçš„è¡Œä¸ºï¼ˆGell-Mann å’Œ Lowï¼Œ1954ï¼›Kadanoffï¼Œ1966ï¼‰ã€‚è¿™äº›æ€æƒ³åœ¨1970å¹´ä»£åˆæœŸå¾—ä»¥ç»“æ™¶ï¼Œå¹¶åœ¨å½»åº•æ”¹å˜æˆ‘ä»¬å¯¹åŸºæœ¬ç²’å­ä¹‹é—´å¼ºç›¸äº’ä½œç”¨ã€äºŒé˜¶ç›¸å˜çš„ä¸´ç•Œç°è±¡ã€å‘æ··æ²Œçš„è¿‡æ¸¡ç­‰æ–¹é¢çš„ç†è§£ä¸­å‘æŒ¥äº†æ ¸å¿ƒä½œç”¨ï¼ˆWilsonï¼Œ1983ï¼‰ã€‚è¿™äº›æ€æƒ³å¦‚ä½•å¸®åŠ©æˆ‘ä»¬æ€è€ƒç¥ç»å…ƒç½‘ç»œï¼Ÿ\nIn the standard formulation of the RG for statistical physics we start with a set of variables $z_{l_{0}}\\equiv \\{z_{i}(l_{0})\\}$ defined on some microscopic length scale $l_{0}$. Our description of these variables is given by a Hamiltonian that in turns specifies the Boltzmann distribution $P_{l_{0}}(z)$, or perhaps we will be interested in the dynamics generated by this Hamiltonian. We then imagine â€œcoarseâ€“grainingâ€ the variables to average out the details on length scales below some $l \u003e l_{0}$. The result is a new set of variables $z_{l}$, and we can ask for the effective Hamiltonian that governs these variables. If we think of the Hamiltonian as being built from different kinds of interactions, it becomes natural to say that the effective strengths of these interactions has changed as change scale from $l_{0}$ to $l$, and the RG invites us to follow this flow as we change $l$. Although this flow of interaction strengths or running of coupling constants often is the goal an RG analysis, it was emphasized early on by Jona-Lasinio (1975) that we can think more generally about flow in the space of probability distributions $P_{l}(z)$, leaving aside any reference to Hamiltonians.\nåœ¨ç»Ÿè®¡ç‰©ç†å­¦çš„ RG çš„æ ‡å‡†è¡¨è¿°ä¸­ï¼Œæˆ‘ä»¬ä»ä¸€ç»„å®šä¹‰åœ¨æŸä¸ªå¾®è§‚é•¿åº¦å°ºåº¦ $l_{0}$ ä¸Šçš„å˜é‡ $z_{l_{0}}\\equiv \\{z_{i}(l_{0})\\}$ å¼€å§‹ã€‚è¿™äº›å˜é‡çš„æè¿°ç”±ä¸€ä¸ªå“ˆå¯†é¡¿é‡ç»™å‡ºï¼Œè¯¥å“ˆå¯†é¡¿é‡åè¿‡æ¥æŒ‡å®šäº†ç»å°”å…¹æ›¼åˆ†å¸ƒ $P_{l_{0}}(z)$ï¼Œæˆ–è€…æˆ‘ä»¬å¯èƒ½ä¼šå¯¹è¯¥å“ˆå¯†é¡¿é‡ç”Ÿæˆçš„åŠ¨åŠ›å­¦æ„Ÿå…´è¶£ã€‚ç„¶åæˆ‘ä»¬æƒ³è±¡â€œç²—ç²’åŒ–â€è¿™äº›å˜é‡ï¼Œä»¥å¹³å‡æ‰æŸä¸ª $l \u003e l_{0}$ ä»¥ä¸‹é•¿åº¦å°ºåº¦çš„ç»†èŠ‚ã€‚ç»“æœæ˜¯ä¸€ç»„æ–°çš„å˜é‡ $z_{l}$ï¼Œæˆ‘ä»¬å¯ä»¥è¯¢é—®æ”¯é…è¿™äº›å˜é‡çš„ç­‰æ•ˆå“ˆå¯†é¡¿é‡ã€‚å¦‚æœæˆ‘ä»¬è®¤ä¸ºå“ˆå¯†é¡¿é‡æ˜¯ç”±ä¸åŒç±»å‹çš„ç›¸äº’ä½œç”¨æ„å»ºçš„ï¼Œé‚£ä¹ˆå½“æˆ‘ä»¬å°†å°ºåº¦ä» $l_{0}$ æ”¹å˜åˆ° $l$ æ—¶ï¼Œè¿™äº›ç›¸äº’ä½œç”¨çš„æœ‰æ•ˆå¼ºåº¦å‘ç”Ÿäº†å˜åŒ–ï¼Œè¿™å°±å¾ˆè‡ªç„¶åœ°è¯´ï¼ŒRG å¼•å¯¼æˆ‘ä»¬åœ¨æ”¹å˜ $l$ æ—¶è·Ÿè¸ªè¿™ç§æµåŠ¨ã€‚å°½ç®¡è¿™ç§ç›¸äº’ä½œç”¨å¼ºåº¦çš„æµåŠ¨æˆ–è€¦åˆå¸¸æ•°çš„è¿è¡Œé€šå¸¸æ˜¯ RG åˆ†æçš„ç›®æ ‡ï¼Œä½† Jona-Lasinioï¼ˆ1975ï¼‰æ—©æœŸå¼ºè°ƒï¼Œæˆ‘ä»¬å¯ä»¥æ›´ä¸€èˆ¬åœ°è€ƒè™‘æ¦‚ç‡åˆ†å¸ƒ $P_{l}(z)$ ç©ºé—´ä¸­çš„æµåŠ¨ï¼Œè€Œä¸å‚è€ƒä»»ä½•å“ˆå¯†é¡¿é‡ã€‚\nAn essential result of the renormalization group is that many different starting distributions $P_{l_{0}}(z)$ converge to the same $P_{l}(z)$ as $l$ becomes large. Along this trajectory parameters of the distribution exhibit simple scaling behaviors as a function of $l$. A familiar example is the central limit theorem, where if variables in $P_{l_{0}}(z)$ are sufficiently weakly correlated then $P_{l}(z)$ approaches a Gaussian as $l$ becomes large, and along the way the variances of the individual variables scale as $1/l$. The RG predicts that more interesting starting points can flow toward stable nonâ€“Gaussian distributions, with moments scaling as nonâ€“trivial powers of $l$.\né‡æ•´åŒ–ç¾¤çš„ä¸€ä¸ªåŸºæœ¬ç»“æœæ˜¯ï¼Œéšç€ $l$ å˜å¤§ï¼Œè®¸å¤šä¸åŒçš„èµ·å§‹åˆ†å¸ƒ $P_{l_{0}}(z)$ ä¼šæ”¶æ•›åˆ°ç›¸åŒçš„ $P_{l}(z)$ã€‚åœ¨è¿™æ¡è½¨è¿¹ä¸Šï¼Œåˆ†å¸ƒçš„å‚æ•°è¡¨ç°å‡ºä½œä¸º $l$ å‡½æ•°çš„ç®€å•ç¼©æ”¾è¡Œä¸ºã€‚ä¸€ä¸ªç†Ÿæ‚‰çš„ä¾‹å­æ˜¯ä¸­å¿ƒæé™å®šç†ï¼Œå¦‚æœ $P_{l_{0}}(z)$ ä¸­çš„å˜é‡ç›¸å…³æ€§è¶³å¤Ÿå¼±ï¼Œé‚£ä¹ˆå½“ $l$ å˜å¤§æ—¶ï¼Œ$P_{l}(z)$ ä¼šè¶‹è¿‘äºé«˜æ–¯åˆ†å¸ƒï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œå•ä¸ªå˜é‡çš„æ–¹å·®æŒ‰ $1/l$ ç¼©æ”¾ã€‚RG é¢„æµ‹ï¼Œæ›´æœ‰è¶£çš„èµ·ç‚¹å¯ä»¥æµå‘ç¨³å®šçš„éé«˜æ–¯åˆ†å¸ƒï¼Œå…¶çŸ©æŒ‰ $l$ çš„éå¹³å‡¡å¹‚ç¼©æ”¾ã€‚\nThe renormalization group approach provides a framework to understand how we can go from discrete Ising spins on a lattice to a description of smoothly varying local magnetization, or from the positions and momenta of individual molecules to the density of a fluid and the velocity of its flow. In these examples, the coarseâ€“graining operation is guided by symmetry and locality. Perhaps the most successful development of RG ideas in a biological context has been for flocks of birds and swarms of insects, where the ideas of symmetry and locality continue to be useful (Â§A.2). For networks of neurons, where connections can span distances encompassing thousands of cells, the principle of locality is less of a guide, and there are no obvious symmetries. How then do we choose a coarseâ€“graining strategy?\né‡æ•´åŒ–ç¾¤æ–¹æ³•æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£å¦‚ä½•ä»æ™¶æ ¼ä¸Šçš„ç¦»æ•£ Ising è‡ªæ—‹è½¬å˜ä¸ºå¹³æ»‘å˜åŒ–çš„å±€éƒ¨ç£åŒ–æè¿°ï¼Œæˆ–è€…ä»å•ä¸ªåˆ†å­çš„ä½ç§»å’ŒåŠ¨é‡è½¬å˜ä¸ºæµä½“çš„å¯†åº¦åŠå…¶æµåŠ¨é€Ÿåº¦ã€‚åœ¨è¿™äº›ä¾‹å­ä¸­ï¼Œç²—ç²’åŒ–æ“ä½œæ˜¯ç”±å¯¹ç§°æ€§å’Œå±€éƒ¨æ€§æŒ‡å¯¼çš„ã€‚åœ¨ç”Ÿç‰©å­¦èƒŒæ™¯ä¸‹ï¼ŒRG æ€æƒ³æœ€æˆåŠŸçš„å‘å±•å¯èƒ½æ˜¯å¯¹äºé¸Ÿç¾¤å’Œæ˜†è™«ç¾¤ä½“ï¼Œåœ¨é‚£é‡Œå¯¹ç§°æ€§å’Œå±€éƒ¨æ€§çš„æ€æƒ³ä»ç„¶æœ‰ç”¨ï¼ˆÂ§A.2ï¼‰ã€‚å¯¹äºç¥ç»å…ƒç½‘ç»œï¼Œè¿æ¥å¯ä»¥è·¨è¶ŠåŒ…å«æ•°åƒä¸ªç»†èƒçš„è·ç¦»ï¼Œå±€éƒ¨æ€§åŸåˆ™ä¸å†æ˜¯æŒ‡å¯¼ï¼Œå¹¶ä¸”æ²¡æœ‰æ˜æ˜¾çš„å¯¹ç§°æ€§ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•é€‰æ‹©ç²—ç²’åŒ–ç­–ç•¥å‘¢ï¼Ÿ\nPerhaps a more serious problem in taking inspiration from the renormalization group is that the RG is formulated as an approach to understanding theories or models, taming the complexities of interactions among degrees of freedom at many scales. These theories of course make quantitative predictions for experiment, but in the absence of a well defined model it is not clear how to proceed. There is a recent start on renormalization group analysis of models for a network of moderately realistic spiking neurons (Brinkman, 2023), and we hope there will be more of this. But, keeping to the spirit of the discussion thus far, we want to ask: How can we use the RG to guide the analysis of emerging data on large populations of real neurons?\nä¹Ÿè®¸ä»é‡æ•´åŒ–ç¾¤ä¸­è·å¾—çµæ„Ÿçš„ä¸€ä¸ªæ›´ä¸¥é‡çš„é—®é¢˜æ˜¯ï¼ŒRG è¢«è¡¨è¿°ä¸ºä¸€ç§ç†è§£ç†è®ºæˆ–æ¨¡å‹çš„æ–¹æ³•ï¼Œé©¯æœå¤šå°ºåº¦è‡ªç”±åº¦ä¹‹é—´ç›¸äº’ä½œç”¨çš„å¤æ‚æ€§ã€‚è¿™äº›ç†è®ºå½“ç„¶å¯¹å®éªŒåšå‡ºäº†å®šé‡é¢„æµ‹ï¼Œä½†åœ¨æ²¡æœ‰æ˜ç¡®å®šä¹‰æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä¸æ¸…æ¥šå¦‚ä½•ç»§ç»­ã€‚æœ€è¿‘å¼€å§‹å¯¹é€‚åº¦çœŸå®çš„å°–å³°ç¥ç»å…ƒç½‘ç»œæ¨¡å‹è¿›è¡Œé‡æ•´åŒ–ç¾¤åˆ†æï¼ˆBrinkmanï¼Œ2023ï¼‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä¼šæœ‰æ›´å¤šè¿™æ ·çš„å·¥ä½œã€‚ä½†ä¿æŒåˆ°ç›®å‰ä¸ºæ­¢è®¨è®ºçš„ç²¾ç¥ï¼Œæˆ‘ä»¬æƒ³é—®ï¼šæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ RG æ¥æŒ‡å¯¼å¯¹å¤§é‡çœŸå®ç¥ç»å…ƒç¾¤ä½“çš„æ–°å…´æ•°æ®çš„åˆ†æï¼Ÿ\nTo address these challenges we rely on two key ideas. First, as emphasized above, modern experiments on the electrical activity in networks of neurons give us access to something analogous to the trajectory of a Monte Carlo simulation on a statistical physics model, albeit a model that we donâ€™t know how to write down. Thus we can follow the approach used in now classical analysis of such simulations, for example by Binder (1981): We start with raw data on the most microscopic scale, construct coarseâ€“grained variables, and follow various features of the distribution of thee variables as we change the scale of coarseâ€“graining.\nä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¾èµ–ä¸¤ä¸ªå…³é”®æ€æƒ³ã€‚é¦–å…ˆï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œå…³äºç¥ç»å…ƒç½‘ç»œç”µæ´»åŠ¨çš„ç°ä»£å®éªŒä½¿æˆ‘ä»¬èƒ½å¤Ÿè®¿é—®ç±»ä¼¼äºç»Ÿè®¡ç‰©ç†æ¨¡å‹ä¸Š Monte Carlo æ¨¡æ‹Ÿè½¨è¿¹çš„ä¸œè¥¿ï¼Œå°½ç®¡è¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬ä¸çŸ¥é“å¦‚ä½•å†™ä¸‹çš„æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥éµå¾ªç°åœ¨ç»å…¸çš„æ­¤ç±»æ¨¡æ‹Ÿåˆ†ææ–¹æ³•ï¼Œä¾‹å¦‚ Binderï¼ˆ1981ï¼‰ï¼šæˆ‘ä»¬ä»æœ€å¾®è§‚å°ºåº¦çš„åŸå§‹æ•°æ®å¼€å§‹ï¼Œæ„å»ºç²—ç²’åŒ–å˜é‡ï¼Œå¹¶éšç€ç²—ç²’åŒ–å°ºåº¦çš„å˜åŒ–ï¼Œè·Ÿè¸ªè¿™äº›å˜é‡åˆ†å¸ƒçš„å„ç§ç‰¹å¾ã€‚\nSecond, we will use the measured pairwise correlations as guide to which neurons are â€œneighbors,â€ in the absence of locality (Bradde and Bialek, 2017). In one version (Â§VII.B), this involves averaging together the activities of the most correlated cells, building clusters of neurons that are analogous to block spins (Kadanoff, 1966). In another version (Â§VII.C), we successively filter out linear combinations of the population activity that make small contributions to the overall variance, and this is analogous to the momentum shell construction (Wilson, 1983). We will see that both these approaches uncover simple, precise, and reproducible scaling behaviors that now have been confirmed in multiple brain areas from multiple organisms. We then discuss the implications of these results and some future direction Â§VII.D.\nå…¶æ¬¡ï¼Œåœ¨ç¼ºä¹å±€éƒ¨æ€§çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ æµ‹é‡çš„æˆå¯¹ç›¸å…³æ€§ ä½œä¸ºæŒ‡å¯¼ï¼Œç¡®å®šå“ªäº›ç¥ç»å…ƒæ˜¯â€œé‚»å±…â€ï¼ˆBradde å’Œ Bialekï¼Œ2017ï¼‰ã€‚åœ¨ä¸€ç§ç‰ˆæœ¬ä¸­ï¼ˆÂ§VII.Bï¼‰ï¼Œè¿™æ¶‰åŠå°†æœ€ç›¸å…³çš„ç»†èƒçš„æ´»åŠ¨å¹³å‡åœ¨ä¸€èµ·ï¼Œæ„å»ºç±»ä¼¼äºå—è‡ªæ—‹çš„ç¥ç»å…ƒç°‡ï¼ˆKadanoffï¼Œ1966ï¼‰ã€‚åœ¨å¦ä¸€ç§ç‰ˆæœ¬ä¸­ï¼ˆÂ§VII.Cï¼‰ï¼Œæˆ‘ä»¬è¿ç»­æ»¤é™¤å¯¹æ•´ä½“æ–¹å·®è´¡çŒ®è¾ƒå°çš„äººå£æ´»åŠ¨çš„çº¿æ€§ç»„åˆï¼Œè¿™ç±»ä¼¼äºåŠ¨é‡å£³æ„é€ ï¼ˆWilsonï¼Œ1983ï¼‰ã€‚æˆ‘ä»¬å°†çœ‹åˆ°ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½æ­ç¤ºäº†ç®€å•ã€ç²¾ç¡®ä¸”å¯é‡å¤çš„ç¼©æ”¾è¡Œä¸ºï¼Œè¿™äº›è¡Œä¸ºç°åœ¨å·²åœ¨å¤šä¸ªæœ‰æœºä½“çš„å¤šä¸ªå¤§è„‘åŒºåŸŸå¾—åˆ°ç¡®è®¤ã€‚ç„¶åæˆ‘ä»¬è®¨è®ºè¿™äº›ç»“æœçš„æ„ä¹‰å’Œä¸€äº›æœªæ¥çš„æ–¹å‘ Â§VII.Dã€‚\nBy analogy with realâ€“space methods Renormalization group methods in statistical physics rest on a notion of coarseâ€“graining, averaging over microscopic details. If we start with variables $\\{z_{i}\\}$ that live on a regular lattice, the it is natural to do this by combining variables with their neighbors, as in Fig 32. Formally we can write\n$$ z_{i}\\rightarrow \\widetilde{z}_{i} = f\\left(\\sum_{j\\in\\mathcal{N}_{i}}z_{j}\\right) $$\nwhere $\\mathcal{N}_{i}$ is a neighborhood surrounding site $i$. If the function $f(\\cdot)$ is linear then we are just averaging over a neighborhood, and for example this will lead from discrete Isingâ€“like variables to a more continuous local magnetization if we iterate. If $f(\\cdot)$ is a threshold function then we can implement majority rule, so that clusters of Isingâ€“like variables are mapped into Isingâ€“like variables on the sparser lattice, as in the original block spin construction (Kadanoff, 1966).\né‡æ•´åŒ–ç¾¤æ–¹æ³•åœ¨ç»Ÿè®¡ç‰©ç†å­¦ä¸­åŸºäºç²—ç²’åŒ–çš„æ¦‚å¿µï¼Œå³å¹³å‡å¾®è§‚ç»†èŠ‚ã€‚å¦‚æœæˆ‘ä»¬ä»ç”Ÿæ´»åœ¨è§„åˆ™æ™¶æ ¼ä¸Šçš„å˜é‡ $\\{z_{i}\\}$ å¼€å§‹ï¼Œé‚£ä¹ˆé€šè¿‡å°†å˜é‡ä¸å…¶é‚»å±…ç»“åˆèµ·æ¥è¿›è¡Œç²—ç²’åŒ–æ˜¯å¾ˆè‡ªç„¶çš„ï¼Œå¦‚å›¾32æ‰€ç¤ºã€‚å½¢å¼ä¸Šæˆ‘ä»¬å¯ä»¥å†™æˆ\n$$ z_{i}\\rightarrow \\widetilde{z}_{i} = f\\left(\\sum_{j\\in\\mathcal{N}_{i}}z_{j}\\right) $$\nå…¶ä¸­ $\\mathcal{N}_{i}$ æ˜¯å›´ç»•èŠ‚ç‚¹ $i$ çš„é‚»åŸŸã€‚å¦‚æœå‡½æ•° $f(\\cdot)$ æ˜¯çº¿æ€§çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªæ˜¯å¯¹ä¸€ä¸ªé‚»åŸŸè¿›è¡Œå¹³å‡ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è¿­ä»£ï¼Œè¿™å°†å¯¼è‡´ä»ç¦»æ•£çš„ç±»ä¼¼ Ising çš„å˜é‡åˆ°æ›´è¿ç»­çš„å±€éƒ¨ç£åŒ–ã€‚å¦‚æœ $f(\\cdot)$ æ˜¯ä¸€ä¸ªé˜ˆå€¼å‡½æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å®ç°å¤šæ•°è§„åˆ™ï¼Œè¿™æ ·ç±»ä¼¼ Ising çš„å˜é‡ç°‡å°±è¢«æ˜ å°„åˆ°æ›´ç¨€ç–æ™¶æ ¼ä¸Šçš„ç±»ä¼¼ Ising çš„å˜é‡ï¼Œå°±åƒåŸå§‹çš„å—è‡ªæ—‹æ„é€ ï¼ˆKadanoffï¼Œ1966ï¼‰ä¸€æ ·ã€‚\nCoarseâ€“graining on a regular lattice. We start with binary (black/white) variables $\\{z_{i}\\}$, and replace $2 \\times 2$ blocks with the average of these variables $\\{\\widetilde{z}_{i}\\}$, shown as grey levels. The interesting question is what happens to the joint distribution as we coarseâ€“grain, not just once but iteratively.\nåœ¨è§„åˆ™æ™¶æ ¼ä¸Šçš„ç²—ç²’åŒ–ã€‚æˆ‘ä»¬ä»äºŒåˆ†ï¼ˆé»‘/ç™½ï¼‰å˜é‡ $\\{z_{i}\\}$ å¼€å§‹ï¼Œå¹¶ç”¨è¿™äº›å˜é‡çš„å¹³å‡å€¼ $\\{\\widetilde{z}_{i}\\}$ æ›¿æ¢ $2 \\times 2$ å—ï¼Œå¦‚ç°åº¦æ‰€ç¤ºã€‚æœ‰è¶£çš„é—®é¢˜æ˜¯ï¼Œå½“æˆ‘ä»¬è¿›è¡Œç²—ç²’åŒ–æ—¶ï¼Œè”åˆåˆ†å¸ƒä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼Œä¸ä»…ä»…æ˜¯ä¸€æ¬¡ï¼Œè€Œæ˜¯è¿­ä»£åœ°ã€‚\nIn a system with local interactions, the variables in the neighborhood typically are the most strongly correlated with one another. This suggests that even if we donâ€™t have a notion of neighborhood, we can make progress by searching for the most correlated variables and using these to build the clusters that we use in coarseâ€“graining. A schematic of how this can work for neural activity is shown in Fig 33.\nåœ¨å…·æœ‰å±€éƒ¨ç›¸äº’ä½œç”¨çš„ç³»ç»Ÿä¸­ï¼Œé‚»åŸŸå†…çš„å˜é‡é€šå¸¸å½¼æ­¤ä¹‹é—´ç›¸å…³æ€§æœ€å¼ºã€‚è¿™è¡¨æ˜ï¼Œå³ä½¿æˆ‘ä»¬æ²¡æœ‰é‚»åŸŸçš„æ¦‚å¿µï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡æœç´¢æœ€ç›¸å…³çš„å˜é‡å¹¶ä½¿ç”¨å®ƒä»¬æ¥æ„å»ºæˆ‘ä»¬åœ¨ç²—ç²’åŒ–ä¸­ä½¿ç”¨çš„ç°‡æ¥å–å¾—è¿›å±•ã€‚å›¾33æ˜¾ç¤ºäº†è¿™å¦‚ä½•é€‚ç”¨äºç¥ç»æ´»åŠ¨çš„ç¤ºæ„å›¾ã€‚\nCoarseâ€“graining neural activity. (A) A small group of neurons with links indicating the most strongly correlated pairs, and the strength of these correlations. (B) Schematic sequence of action potentials from these cells. (C) Coarseâ€“graining by summing the activity in highly correlated pairs. (D) Finding the most strongly correlated pairs of coarseâ€“grained variables in (C) and coarseâ€“graining again by summing. The strengths of the correlations are color coded as in (A). (E) One more iteration of this â€œreal spaceâ€ coarsegraining.\nç¥ç»æ´»åŠ¨çš„ç²—ç²’åŒ–ã€‚ (A) ä¸€å°ç»„ç¥ç»å…ƒï¼Œé“¾æ¥è¡¨ç¤ºæœ€å¼ºç›¸å…³çš„å¯¹åŠå…¶ç›¸å…³å¼ºåº¦ã€‚ (B) è¿™äº›ç»†èƒçš„åŠ¨ä½œç”µä½ç¤ºæ„åºåˆ—ã€‚ (C) é€šè¿‡å¯¹é«˜åº¦ç›¸å…³çš„å¯¹çš„æ´»åŠ¨æ±‚å’Œè¿›è¡Œç²—ç²’åŒ–ã€‚ (D) åœ¨ (C) ä¸­æ‰¾åˆ°æœ€å¼ºç›¸å…³çš„ç²—ç²’åŒ–å˜é‡å¯¹ï¼Œå¹¶é€šè¿‡æ±‚å’Œå†æ¬¡è¿›è¡Œç²—ç²’åŒ–ã€‚ç›¸å…³å¼ºåº¦æŒ‰ (A) ä¸­çš„é¢œè‰²ç¼–ç ã€‚ (E) è¿™ç§â€œå®ç©ºé—´â€ç²—ç²’åŒ–çš„åˆä¸€æ¬¡è¿­ä»£ã€‚\nWe start with variables $\\{\\sigma_{i}\\}$, as before, describing the patterns of activity ($\\sigma_{i} = 1$) and silence ($\\sigma_{i} = 0$) across all the neurons $i = 1, 2,\\cdots,N$ in a small window of time. To emphasize that this is the most microscopic description we will write this as $\\sigma_{i} = \\sigma_{i}^{(1)}$ . Then as before we can compute the means, covariance, and correlation matrices:\n$$ \\begin{aligned} m_{i}^{(1)} \u0026= \\langle \\sigma_{i}^{(1)}\\rangle \\\\ C_{ij}^{(1)} \u0026= \\left\\langle \\left[\\sigma_{i}^{(1)}-m_{i}^{(1)}\\right] \\left[\\sigma_{j}^{(1)}-m_{j}^{(1)}\\right] \\right\\rangle\\\\ c_{ij}^{(1)} \u0026= \\frac{C_{ij}^{(1)}}{\\sqrt{C_{ii}^{(1)}C_{jj}^{(1)}}} \\end{aligned} $$\nNow we search for the maximal nonâ€“diagonal element in the matrix of correlation coefficients, then zero the rows and columns associated with this pair of cells $i, j_{*}(i)$, and repeat. The result is a set of maximally correlated pairs $\\{i, j_{*}(i)\\}$, and we then define coarseâ€“grained variables\n$$ \\sigma_{i}^{(2)} = \\sigma_{i}^{(1)} + \\sigma_{j_{*}(i)}^{(1)} $$\nwhere now $i = 1, 2, \\cdots , N/2$. Importantly, we can iterate this process across scales: we compute the correlation matrix of the variables $\\{\\sigma_{i}^{(2)}\\}$ and search again for the maximally correlated pairs $\\{i, j_{*}(i)\\}$, then define\n$$ \\sigma_{i}^{(3)} = \\sigma_{i}^{(2)} + \\sigma_{j_{*}(i)}^{(2)} $$\nand so on; at each stage we have $N_{k} = \\lfloor N/2^{kâˆ’1}\\rfloor$ variables remaining. This coarse graining produces clusters of $K = 2, 4,\\cdots , 2^{kâˆ’1}$ neurons, and the variable $\\sigma_{i}^{(k)}$ is the summed activity of cluster $i$.\næˆ‘ä»¬ä»å˜é‡ $\\{\\sigma_{i}\\}$ å¼€å§‹ï¼Œå¦‚å‰æ‰€è¿°ï¼Œæè¿°åœ¨ä¸€ä¸ªå°æ—¶é—´çª—å£å†…æ‰€æœ‰ç¥ç»å…ƒ $i = 1, 2,\\cdots,N$ çš„æ´»åŠ¨æ¨¡å¼ï¼ˆ$\\sigma_{i} = 1$ï¼‰å’Œé™é»˜ï¼ˆ$\\sigma_{i} = 0$ï¼‰ã€‚ä¸ºäº†å¼ºè°ƒè¿™æ˜¯æœ€å¾®è§‚çš„æè¿°ï¼Œæˆ‘ä»¬å°†å…¶å†™ä¸º $\\sigma_{i} = \\sigma_{i}^{(1)}$ã€‚ç„¶ååƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡å€¼ã€åæ–¹å·®å’Œç›¸å…³çŸ©é˜µï¼š\n$$ \\begin{aligned} m_{i}^{(1)} \u0026= \\langle \\sigma_{i}^{(1)}\\rangle \\\\ C_{ij}^{(1)} \u0026= \\left\\langle \\left[\\sigma_{i}^{(1)}-m_{i}^{(1)}\\right] \\left[\\sigma_{j}^{(1)}-m_{j}^{(1)}\\right] \\right\\rangle\\\\ c_{ij}^{(1)} \u0026= \\frac{C_{ij}^{(1)}}{\\sqrt{C_{ii}^{(1)}C_{jj}^{(1)}}} \\end{aligned} $$\nç°åœ¨æˆ‘ä»¬åœ¨ç›¸å…³ç³»æ•°çŸ©é˜µä¸­æœç´¢éå¯¹è§’å…ƒç´ çš„æœ€å¤§å€¼ï¼Œç„¶åå°†ä¸è¯¥ç»†èƒå¯¹ $i, j_{*}(i)$ ç›¸å…³çš„è¡Œå’Œåˆ—å½’é›¶ï¼Œå¹¶é‡å¤ã€‚ç»“æœæ˜¯ä¸€ç»„æœ€å¤§ç›¸å…³å¯¹ $\\{i, j_{*}(i)\\}$ï¼Œç„¶åæˆ‘ä»¬å®šä¹‰ç²—ç²’åŒ–å˜é‡\n$$ \\sigma_{i}^{(2)} = \\sigma_{i}^{(1)} + \\sigma_{j_{*}(i)}^{(1)} $$\nç°åœ¨ $i = 1, 2, \\cdots , N/2$ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥è·¨å°ºåº¦è¿­ä»£è¿™ä¸ªè¿‡ç¨‹ï¼šæˆ‘ä»¬è®¡ç®—å˜é‡ $\\{\\sigma_{i}^{(2)}\\}$ çš„ç›¸å…³çŸ©é˜µå¹¶å†æ¬¡æœç´¢æœ€å¤§ç›¸å…³å¯¹ $\\{i, j_{*}(i)\\}$ï¼Œç„¶åå®šä¹‰\n$$ \\sigma_{i}^{(3)} = \\sigma_{i}^{(2)} + \\sigma_{j_{*}(i)}^{(2)} $$\nä¾æ­¤ç±»æ¨ï¼›åœ¨æ¯ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å‰©ä¸‹ $N_{k} = \\lfloor N/2^{kâˆ’1}\\rfloor$ ä¸ªå˜é‡ã€‚è¿™ä¸ªç²—ç²’åŒ–äº§ç”Ÿäº† $K = 2, 4,\\cdots , 2^{kâˆ’1}$ ä¸ªç¥ç»å…ƒç°‡ï¼Œå˜é‡ $\\sigma_{i}^{(k)}$ æ˜¯ç°‡ $i$ çš„æ€»æ´»åŠ¨é‡ã€‚\nWe emphasize that one could have different criteria for coarseâ€“graining, and different ways of combing the variables. We return to some of these points below (Â§VII.D), but for now we explore what happens when we apply this simplest scheme to a network of real neurons. The first such example used the experiments on the activity of 1000+ neurons described in Â§V (Meshulam et al., 2018, 2019).\næˆ‘ä»¬å¼ºè°ƒï¼Œå¯ä»¥æœ‰ä¸åŒçš„ç²—ç²’åŒ–æ ‡å‡†å’Œä¸åŒçš„å˜é‡ç»„åˆæ–¹å¼ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„æŸäº›ç‚¹ä¸Šå›åˆ°è¿™äº›é—®é¢˜ï¼ˆÂ§VII.Dï¼‰ï¼Œä½†ç°åœ¨æˆ‘ä»¬æ¢ç´¢å½“æˆ‘ä»¬å°†è¿™ä¸ªæœ€ç®€å•çš„æ–¹æ¡ˆåº”ç”¨äºçœŸå®ç¥ç»å…ƒç½‘ç»œæ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ç¬¬ä¸€ä¸ªè¿™æ ·çš„ä¾‹å­ä½¿ç”¨äº†Â§Vä¸­æè¿°çš„ 1000 å¤šä¸ªç¥ç»å…ƒæ´»åŠ¨çš„å®éªŒï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼Œ2019ï¼‰ã€‚\nWe are interested in how the probability distributions transform and flow as we pass through successive scales of coarseâ€“graining. Of course looking at the joint distribution $P(\\{\\sigma_{i}^{(k)}\\})$ is essentially impossible. But much can be learned by looking at slices through this distribution, even the distribution of individual coarsegrained variables, as with the magnetization in the Ising model (Binder, 1981).\næˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯éšç€æˆ‘ä»¬é€šè¿‡è¿ç»­çš„ç²—ç²’åŒ–å°ºåº¦ï¼Œæ¦‚ç‡åˆ†å¸ƒå¦‚ä½•å˜æ¢å’ŒæµåŠ¨ã€‚å½“ç„¶ï¼ŒæŸ¥çœ‹è”åˆåˆ†å¸ƒ $P(\\{\\sigma_{i}^{(k)}\\})$ æœ¬è´¨ä¸Šæ˜¯ä¸å¯èƒ½çš„ã€‚ä½†æ˜¯ï¼Œé€šè¿‡æŸ¥çœ‹è¯¥åˆ†å¸ƒçš„åˆ‡ç‰‡ï¼Œç”šè‡³æ˜¯å•ä¸ªç²—ç²’åŒ–å˜é‡çš„åˆ†å¸ƒï¼Œå°±åƒ Ising æ¨¡å‹ä¸­çš„ç£åŒ–ä¸€æ ·ï¼Œå¯ä»¥å­¦åˆ°å¾ˆå¤šä¸œè¥¿ï¼ˆBinderï¼Œ1981ï¼‰ã€‚\nSince this coarseâ€“graining is based simply on adding the â€œneighboringâ€ variables, the first moment of the distribution of the individual variables must scale linearly,\n$$ M_{1}(k) \\equiv \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}\\langle\\sigma_{i}^{(k)}\\rangle = \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}m_{i}^{(k)} = KM_{1}(1) $$\nwhere after $k$ steps we have $N_{k}$ clusters each involving $K = 2^{kâˆ’1}$ of the original variables. The first nonâ€“trivial question is about the second moment, or the variance in activity,\n$$ \\begin{aligned} M_{2}(K) \\equiv \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}\\left\\langle \\left( \\sigma_{i}^{(k)} - m_{i}^{(k)} \\right)^{2}\\right\\rangle \\end{aligned} $$\nNote that if neurons are independent we expect $M_{2}(K)\\propto K$, and many weakly correlated populations should approach this behavior at large $K$. If neurons are perfectly correlated, on the other hand, we expect $M_{2}(K)\\propto K^{2}$. Looking at the data, in Fig 34A, we see that for neurons in the hippocampus $M_{2}\\propto K^{\\widetilde{\\alpha}}$, with $\\widetilde{\\alpha}= 1.4 \\pm 0.06$. This nonâ€“trivial scaling is visible over more than two decades.\nç”±äºè¿™ç§ç²—ç²’åŒ–ä»…åŸºäºæ·»åŠ â€œé‚»è¿‘â€å˜é‡ï¼Œå•ä¸ªå˜é‡åˆ†å¸ƒçš„ç¬¬ä¸€çŸ©å¿…é¡»çº¿æ€§ç¼©æ”¾ï¼Œ\n$$ M_{1}(k) \\equiv \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}\\langle\\sigma_{i}^{(k)}\\rangle = \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}m_{i}^{(k)} = KM_{1}(1) $$\nå…¶ä¸­ç»è¿‡ $k$ æ­¥åï¼Œæˆ‘ä»¬æœ‰ $N_{k}$ ä¸ªç°‡ï¼Œæ¯ä¸ªç°‡æ¶‰åŠåŸå§‹å˜é‡çš„ $K = 2^{kâˆ’1}$ã€‚ç¬¬ä¸€ä¸ªéå¹³å‡¡çš„é—®é¢˜æ˜¯å…³äºç¬¬äºŒçŸ©ï¼Œæˆ–æ´»åŠ¨çš„æ–¹å·®ï¼Œ\n$$ \\begin{aligned} M_{2}(K) \\equiv \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}\\left\\langle \\left( \\sigma_{i}^{(k)} - m_{i}^{(k)} \\right)^{2}\\right\\rangle \\end{aligned} $$\nè¯·æ³¨æ„ï¼Œå¦‚æœç¥ç»å…ƒæ˜¯ç‹¬ç«‹çš„ï¼Œæˆ‘ä»¬æœŸæœ› $M_{2}(K)\\propto K$ï¼Œå¹¶ä¸”è®¸å¤šå¼±ç›¸å…³çš„ç¾¤ä½“åº”è¯¥åœ¨å¤§ $K$ æ—¶æ¥è¿‘è¿™ç§è¡Œä¸ºã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœç¥ç»å…ƒæ˜¯å®Œå…¨ç›¸å…³çš„ï¼Œæˆ‘ä»¬æœŸæœ› $M_{2}(K)\\propto K^{2}$ã€‚æŸ¥çœ‹æ•°æ®ï¼Œåœ¨å›¾ 34A ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¯¹äºæµ·é©¬ä½“ä¸­çš„ç¥ç»å…ƒï¼Œ$M_{2}\\propto K^{\\widetilde{\\alpha}}$ï¼Œå…¶ä¸­ $\\widetilde{\\alpha}= 1.4 \\pm 0.06$ã€‚è¿™ç§éå¹³å‡¡çš„ç¼©æ”¾åœ¨ä¸¤ä¸ªæ•°é‡çº§ä»¥ä¸Šæ˜¯å¯è§çš„ã€‚\nThree slices through the distribution of coarsegrained variables (Meshulam et al., 2018, 2019). (A) Variance of the activity vs. the (real space) coarse graining scale, from Eq (151). Solid line is $M_{2}\\propto K^{\\widetilde{\\alpha}}$, $\\widetilde{\\alpha}= 1.4 \\pm 0.06$; dashed lines are predictions for independent ($\\widetilde{\\alpha}= 1$) or perfectly correlated ($\\widetilde{\\alpha}= 2$) neurons. (B) Probability of silence vs. the coarsegraining scale. Solid line is Eq (152) with $\\widetilde{\\beta}= 0.88 \\pm 0.01$; dashed line is the expectation for independent neurons, $\\widetilde{\\beta}= 1$. (C) Distribution of the normalized nonâ€“zero activity, as defined in Eq (153).\nä¸‰ä¸ªé€šè¿‡ç²—ç²’åŒ–å˜é‡åˆ†å¸ƒçš„åˆ‡ç‰‡ï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼Œ2019ï¼‰ã€‚(A) æ´»åŠ¨æ–¹å·®ä¸ï¼ˆå®ç©ºé—´ï¼‰ç²—ç²’åŒ–å°ºåº¦çš„å…³ç³»ï¼Œæ¥è‡ªæ–¹ç¨‹ï¼ˆ151ï¼‰ã€‚å®çº¿ä¸º $M_{2}\\propto K^{\\widetilde{\\alpha}}$ï¼Œ$\\widetilde{\\alpha}= 1.4 \\pm 0.06$ï¼›è™šçº¿ä¸ºç‹¬ç«‹ç¥ç»å…ƒï¼ˆ$\\widetilde{\\alpha}= 1$ï¼‰æˆ–å®Œå…¨ç›¸å…³ç¥ç»å…ƒï¼ˆ$\\widetilde{\\alpha}= 2$ï¼‰çš„é¢„æµ‹ã€‚(B) é™é»˜æ¦‚ç‡ä¸ç²—ç²’åŒ–å°ºåº¦çš„å…³ç³»ã€‚å®çº¿ä¸ºæ–¹ç¨‹ï¼ˆ152ï¼‰ï¼Œ$\\widetilde{\\beta}= 0.88 \\pm 0.01$ï¼›è™šçº¿ä¸ºç‹¬ç«‹ç¥ç»å…ƒçš„æœŸæœ›ï¼Œ$\\widetilde{\\beta}= 1$ã€‚(C) å½’ä¸€åŒ–éé›¶æ´»åŠ¨çš„åˆ†å¸ƒï¼Œå¦‚æ–¹ç¨‹ï¼ˆ153ï¼‰ä¸­å®šä¹‰ã€‚\nWe can take another slice through the distribution by asking for the probability $P_{k}(0)$ that the coarseâ€“grained variable $\\sigma_{i}^{(k)} = 0$. Since we started with variables $\\sigma_{i} = \\{0, 1\\}$, this is the same as asking for the probability that all of the neurons inside the cluster of size $K = 2^{kâˆ’1}$ are silent. If the neurons are independent we expect a simple scaling $P_{k}(0)\\propto \\exp{(âˆ’aK)}$, and once more expect to see this at large $K$ even if the cells are weakly correlated. Experimentally we see in Fig 34B that\n$$ P_{k}(0) = \\exp{(-a K^{\\widetilde{\\beta}})} $$\nwith the exponent $\\widetilde{\\beta}= 0.88 \\pm 0.01$. Again scaling is precise over more than two decades.\næˆ‘ä»¬å¯ä»¥é€šè¿‡è¯¢é—®ç²—ç²’åŒ–å˜é‡ $\\sigma_{i}^{(k)} = 0$ çš„æ¦‚ç‡ $P_{k}(0)$ æ¥è·å–åˆ†å¸ƒçš„å¦ä¸€ä¸ªåˆ‡ç‰‡ã€‚ç”±äºæˆ‘ä»¬ä»å˜é‡ $\\sigma_{i} = \\{0, 1\\}$ å¼€å§‹ï¼Œè¿™ä¸è¯¢é—®ç°‡å¤§å°ä¸º $K = 2^{kâˆ’1}$ çš„æ‰€æœ‰ç¥ç»å…ƒéƒ½å¤„äºé™é»˜çŠ¶æ€çš„æ¦‚ç‡ç›¸åŒã€‚å¦‚æœç¥ç»å…ƒæ˜¯ç‹¬ç«‹çš„ï¼Œæˆ‘ä»¬æœŸæœ›ä¸€ä¸ªç®€å•çš„ç¼©æ”¾ $P_{k}(0)\\propto \\exp{(âˆ’aK)}$ï¼Œå³ä½¿ç»†èƒæ˜¯å¼±ç›¸å…³çš„ï¼Œæˆ‘ä»¬ä¹ŸæœŸæœ›åœ¨å¤§ $K$ æ—¶çœ‹åˆ°è¿™ç§æƒ…å†µã€‚å®éªŒä¸Šæˆ‘ä»¬åœ¨å›¾ 34B ä¸­çœ‹åˆ°\n$$ P_{k}(0) = \\exp{(-a K^{\\widetilde{\\beta}})} $$\nå…¶ä¸­æŒ‡æ•° $\\widetilde{\\beta}= 0.88 \\pm 0.01$ã€‚åŒæ ·ï¼Œç¼©æ”¾åœ¨ä¸¤ä¸ªæ•°é‡çº§ä»¥ä¸Šæ˜¯ç²¾ç¡®çš„ã€‚\nIf we imagine making an explicit model for the joint activity of all the neurons inside one of the clusters, perhaps in the form of the pairwise models above [Eq (83)], then the probability of complete silence is dependent only on the partition function, $P_{k}(0) = 1/Z$. This generalizes if we include higherâ€“order terms, so that Fig 34B probes the effective free energy, which apparently behaves as $F(K) = -aK^{\\widetilde{\\beta}}$. Since $\\widetilde{\\beta}\u003c 1$, the free energy is subâ€“extensive, and hence the free energy per neuron will vanish in the thermodynamic limit. This is consistent with the equality of entropy and energy that we saw for retinal neurons in Â§VI.B (Fig 29).\nå¦‚æœæˆ‘ä»¬æƒ³è±¡ä¸ºç°‡å†…æ‰€æœ‰ç¥ç»å…ƒçš„è”åˆæ´»åŠ¨å»ºç«‹ä¸€ä¸ªæ˜¾å¼æ¨¡å‹ï¼Œå¯èƒ½é‡‡ç”¨ä¸Šè¿°æˆå¯¹æ¨¡å‹çš„å½¢å¼[æ–¹ç¨‹ï¼ˆ83ï¼‰]ï¼Œé‚£ä¹ˆå®Œå…¨é™é»˜çš„æ¦‚ç‡ä»…ä¾èµ–äºé…åˆ†å‡½æ•°ï¼Œ$P_{k}(0) = 1/Z$ã€‚å¦‚æœæˆ‘ä»¬åŒ…æ‹¬æ›´é«˜é˜¶é¡¹ï¼Œè¿™ç§æƒ…å†µä¼šè¢«æ¨å¹¿ï¼Œå› æ­¤å›¾ 34B æ¢æµ‹äº†æœ‰æ•ˆè‡ªç”±èƒ½ï¼Œæ˜¾ç„¶è¡¨ç°ä¸º $F(K) = -aK^{\\widetilde{\\beta}}$ã€‚ç”±äº $\\widetilde{\\beta}\u003c 1$ï¼Œè‡ªç”±èƒ½æ˜¯äºšå¹¿å»¶çš„ï¼Œå› æ­¤åœ¨çƒ­åŠ›å­¦æé™ä¸‹æ¯ä¸ªç¥ç»å…ƒçš„è‡ªç”±èƒ½å°†æ¶ˆå¤±ã€‚è¿™ä¸æˆ‘ä»¬åœ¨ Â§VI.Bï¼ˆå›¾29ï¼‰ä¸­çœ‹åˆ°çš„è§†ç½‘è†œç¥ç»å…ƒçš„ç†µå’Œèƒ½é‡ç›¸ç­‰æ˜¯ä¸€è‡´çš„ã€‚\nMore generally if we define the normalized variable $x = \\sigma^{(k)}/K$, then\n$$ P_{k}(x) = P_{k}(0)\\delta_{x,0} + [1-P_{k}(0)]Q_{k}(x) $$\nFigure 34C shows the evolution of $Q_{k}(x)$ as $K$ increases. We see that the tail of the distribution is gradually absorbed into the bulk, which seems to approach a fixed form $Q(x)\\sim e^{âˆ’x/x_{0}}$ . If the neurons were independent the central limit theorem would drive this distribution toward a Gaussian, but instead we see the emergence of a fixed nonâ€“Gaussian form.\næ›´ä¸€èˆ¬åœ°ï¼Œå¦‚æœæˆ‘ä»¬å®šä¹‰å½’ä¸€åŒ–å˜é‡ $x = \\sigma^{(k)}/K$ï¼Œé‚£ä¹ˆ\n$$ P_{k}(x) = P_{k}(0)\\delta_{x,0} + [1-P_{k}(0)]Q_{k}(x) $$\nå›¾ 34C æ˜¾ç¤ºäº†éšç€ $K$ å¢åŠ ï¼Œ$Q_{k}(x)$ çš„æ¼”å˜ã€‚æˆ‘ä»¬çœ‹åˆ°åˆ†å¸ƒçš„å°¾éƒ¨é€æ¸è¢«å¸æ”¶åˆ°ä¸»ä½“ä¸­ï¼Œä¼¼ä¹æ¥è¿‘ä¸€ä¸ªå›ºå®šå½¢å¼ $Q(x)\\sim e^{âˆ’x/x_{0}}$ã€‚å¦‚æœç¥ç»å…ƒæ˜¯ç‹¬ç«‹çš„ï¼Œä¸­å¿ƒæé™å®šç†ä¼šå°†è¯¥åˆ†å¸ƒé©±åŠ¨å‘é«˜æ–¯åˆ†å¸ƒï¼Œä½†ç›¸åï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€ä¸ªå›ºå®šçš„éé«˜æ–¯å½¢å¼çš„å‡ºç°ã€‚\nIn addition to looking at the distribution of single coarseâ€“grained variables we can look at the covariance matrix of the microscopic variables within each cluster of size $K$. The eigenvalue spectrum of this covariance matrix depends on the rank scaled by $K$, and there is a substantial region over which the spectrum is a power $\\lambda\\sim (K/\\text{rank})^{\\mu}$, with $Î¼ = 0.71 \\pm 0.06$, although this is less crisp than the other examples of scaling.\né™¤äº†æŸ¥çœ‹å•ä¸ªç²—ç²’åŒ–å˜é‡çš„åˆ†å¸ƒå¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹æ¯ä¸ªå¤§å°ä¸º $K$ çš„ç°‡å†…å¾®è§‚å˜é‡çš„åæ–¹å·®çŸ©é˜µã€‚è¯¥åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼è°±å–å†³äºæŒ‰ $K$ ç¼©æ”¾çš„ç§©ï¼Œå¹¶ä¸”åœ¨å¾ˆå¤§ä¸€éƒ¨åˆ†åŒºåŸŸå†…ï¼Œè°±æ˜¯ä¸€ä¸ªå¹‚ $\\lambda\\sim (K/\\text{rank})^{\\mu}$ï¼Œå…¶ä¸­ $Î¼ = 0.71 \\pm 0.06$ï¼Œå°½ç®¡è¿™ä¸å¦‚å…¶ä»–ç¼©æ”¾ç¤ºä¾‹é‚£ä¹ˆæ¸…æ™°ã€‚\nOur discussion of thus far has focused on the distribution of variables at a single moment in time. In the applications of the RG that we understand, however, we can often observe dynamic scaling (Hohenberg and Halperin, 1977). Intuitively, fluctuations on longer length scales take longer to relax because the underlying interactions are local. What is nonâ€“trivial is that correlation functions for variables coarseâ€“grained to different length scales collapse to a universal form if we measure time in units of the correlation time, and this correlation time itself varies as a power of the length scale. An elegant example of these ideas in a fully biological context is provided by dynamic scaling of the velocity fluctuations in natural swarms of insects (Cavagna et al., 2017).\nåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„è®¨è®ºé›†ä¸­åœ¨å•ä¸€æ—¶é—´ç‚¹ä¸Šå˜é‡çš„åˆ†å¸ƒã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬ç†è§£çš„ RG åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥è§‚å¯Ÿåˆ°åŠ¨æ€ç¼©æ”¾ï¼ˆHohenberg å’Œ Halperinï¼Œ1977ï¼‰ã€‚ç›´è§‚åœ°è¯´ï¼Œæ›´é•¿é•¿åº¦å°ºåº¦ä¸Šçš„æ³¢åŠ¨éœ€è¦æ›´é•¿æ—¶é—´æ‰èƒ½å¼›è±«ï¼Œå› ä¸ºåŸºç¡€ç›¸äº’ä½œç”¨æ˜¯å±€éƒ¨çš„ã€‚éå¹³å‡¡çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ä»¥ç›¸å…³æ—¶é—´ä¸ºå•ä½æµ‹é‡æ—¶é—´ï¼Œé‚£ä¹ˆå¯¹ä¸åŒé•¿åº¦å°ºåº¦ç²—ç²’åŒ–å˜é‡çš„ç›¸å…³å‡½æ•°ä¼šåç¼©ä¸ºä¸€ä¸ªé€šç”¨å½¢å¼ï¼Œå¹¶ä¸”è¯¥ç›¸å…³æ—¶é—´æœ¬èº«éšç€é•¿åº¦å°ºåº¦çš„å¹‚å˜åŒ–ã€‚åœ¨å®Œå…¨ç”Ÿç‰©å­¦èƒŒæ™¯ä¸‹ï¼Œè¿™äº›æ€æƒ³çš„ä¸€ä¸ªä¼˜é›…ä¾‹å­æ˜¯æ˜†è™«è‡ªç„¶ç¾¤ä½“ä¸­é€Ÿåº¦æ³¢åŠ¨çš„åŠ¨æ€ç¼©æ”¾ï¼ˆCavagna ç­‰äººï¼Œ2017ï¼‰ã€‚\nWith networks of neurons we donâ€™t expect locality to be a good guide, but it still is plausible that more strongly coarseâ€“grained variables will have slower dynamics, and we can search for dynamic scaling. Concretely we define the correlation function for individual variables at coarseâ€“graining scale $k$,\n$$ \\widetilde{C}_{i}^{(k)}(t) = \\left\\langle\\left[ \\sigma_{i}^{k}(t_{0})-m_{i}^{(k)} \\right]\\left[ \\sigma_{i}^{k}(t_{0}+t)-m_{i}^{(k)} \\right]\\right\\rangle $$\nand then we can normalize and average over the clusters to give\n$$ C^{(k)}(t) = \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}\\frac{\\widetilde{C}_{i}^{(k)}(t)}{\\widetilde{C}_{i}^{(k)}(0)} $$\nDynamic scaling is the hypothesis that the dependence on scale is captured by a single correlation time,\n$$ C^{(k)}(t) = C[t/\\tau_{c}(k)] $$\nwith $\\tau_{c}(k)\\propto K^{\\widetilde{z}}$. In Figure 35 we see that all of this works for the population of hippocampal neurons. We note that dynamic range of correlation times accessed in this experiment is limited, at short times by the dynamics of the indicator molecules and at long times by the small value of the exponent $\\widetilde{z}= 0.16 \\pm 0.02$.\nå¯¹äºç¥ç»å…ƒç½‘ç»œï¼Œæˆ‘ä»¬ä¸æœŸæœ›å±€éƒ¨æ€§æ˜¯ä¸€ä¸ªå¥½çš„æŒ‡å¯¼ï¼Œä½†æ›´å¼ºç²—ç²’åŒ–çš„å˜é‡ä»ç„¶å¯èƒ½å…·æœ‰è¾ƒæ…¢çš„åŠ¨æ€ï¼Œæˆ‘ä»¬å¯ä»¥æœç´¢åŠ¨æ€ç¼©æ”¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å®šä¹‰ç²—ç²’åŒ–å°ºåº¦ $k$ ä¸‹å•ä¸ªå˜é‡çš„ç›¸å…³å‡½æ•°ï¼Œ\n$$ \\widetilde{C}_{i}^{(k)}(t) = \\left\\langle\\left[ \\sigma_{i}^{k}(t_{0})-m_{i}^{(k)} \\right]\\left[ \\sigma_{i}^{k}(t_{0}+t)-m_{i}^{(k)} \\right]\\right\\rangle $$\nç„¶åæˆ‘ä»¬å¯ä»¥å½’ä¸€åŒ–å¹¶å¯¹ç°‡è¿›è¡Œå¹³å‡ï¼Œå¾—åˆ°\n$$ C^{(k)}(t) = \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}\\frac{\\widetilde{C}_{i}^{(k)}(t)}{\\widetilde{C}_{i}^{(k)}(0)} $$\nåŠ¨æ€ç¼©æ”¾æ˜¯å‡è®¾å°ºåº¦çš„ä¾èµ–æ€§ç”±å•ä¸€ç›¸å…³æ—¶é—´æ•è·ï¼Œ\n$$ C^{(k)}(t) = C[t/\\tau_{c}(k)] $$\nå…¶ä¸­ $\\tau_{c}(k)\\propto K^{\\widetilde{z}}$ã€‚åœ¨å›¾ 35 ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¯¹äºæµ·é©¬ä½“ç¥ç»å…ƒç¾¤ä½“ï¼Œæ‰€æœ‰è¿™äº›éƒ½æœ‰æ•ˆã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨è¿™ä¸ªå®éªŒä¸­è®¿é—®çš„ç›¸å…³æ—¶é—´çš„åŠ¨æ€èŒƒå›´æ˜¯æœ‰é™çš„ï¼Œåœ¨çŸ­æ—¶é—´å†…å—åˆ°æŒ‡ç¤ºå‰‚åˆ†å­åŠ¨åŠ›å­¦çš„é™åˆ¶ï¼Œåœ¨é•¿æ—¶é—´å†…å—åˆ°æŒ‡æ•° $\\widetilde{z}= 0.16 \\pm 0.02$ çš„å°å€¼çš„é™åˆ¶ã€‚\nDynamic scaling across 1000+ neurons in the hippocampus (Meshulam et al., 2018). (A) Mean correlation functions for coarseâ€“grained variables, Eq (155), in clusters of $K = 2,4,\\cdots, 256$ neurons (lightest orange corresponds to the largest cluster), with larger clusters exhibiting slower dynamics. In dashed gray, $\\pm$ one standard deviation across the K = 256 neuron clusters. (B) Collapse under scaling of the time axis, Eq (156). (C) Correlation time vs cluster size, fit to $\\tau_{c}\\propto K^{\\widetilde{z}}$, with $\\widetilde{z}= 0.16 \\pm 0.02$.\næµ·é©¬ä½“ä¸­ 1000 å¤šä¸ªç¥ç»å…ƒçš„åŠ¨æ€ç¼©æ”¾ï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼‰ã€‚(A) ç²—ç²’åŒ–å˜é‡çš„å¹³å‡ç›¸å…³å‡½æ•°ï¼Œæ–¹ç¨‹ï¼ˆ155ï¼‰ï¼Œåœ¨ $K = 2,4,\\cdots, 256$ ä¸ªç¥ç»å…ƒçš„ç°‡ä¸­ï¼ˆæœ€æµ…çš„æ©™è‰²å¯¹åº”äºæœ€å¤§çš„ç°‡ï¼‰ï¼Œè¾ƒå¤§çš„ç°‡è¡¨ç°å‡ºè¾ƒæ…¢çš„åŠ¨æ€ã€‚è™šçº¿ç°è‰²è¡¨ç¤º K = 256 ç¥ç»å…ƒç°‡çš„æ­£è´Ÿä¸€ä¸ªæ ‡å‡†å·®ã€‚(B) æ—¶é—´è½´çš„ç¼©æ”¾åç¼©ï¼Œæ–¹ç¨‹ï¼ˆ156ï¼‰ã€‚(C) ç°‡å¤§å°ä¸ç›¸å…³æ—¶é—´çš„å…³ç³»ï¼Œæ‹Ÿåˆä¸º $\\tau_{c}\\propto K^{\\widetilde{z}}$ï¼Œå…¶ä¸­ $\\widetilde{z}= 0.16 \\pm 0.02$ã€‚\nIt is important that these scaling behavior are not somehow driven by our choice to describe neural activity with binary variables. In these experiments, neural activity was recorded by imaging of fluorescence from indicator molecules that provide a continuous signal as in Figs 6 and 16. We can follow the same steps of coarsegraining for these continuous signals, and the results are the same (Meshulam et al., 2019).\né‡è¦çš„æ˜¯ï¼Œè¿™äº›ç¼©æ”¾è¡Œä¸ºå¹¶ä¸æ˜¯ç”±æˆ‘ä»¬é€‰æ‹©ç”¨äºŒè¿›åˆ¶å˜é‡æ¥æè¿°ç¥ç»æ´»åŠ¨æ‰€é©±åŠ¨çš„ã€‚åœ¨è¿™äº›å®éªŒä¸­ï¼Œç¥ç»æ´»åŠ¨æ˜¯é€šè¿‡æˆåƒæŒ‡ç¤ºå‰‚åˆ†å­çš„è§å…‰è®°å½•çš„ï¼Œè¿™äº›åˆ†å­æä¾›äº†ä¸€ä¸ªè¿ç»­çš„ä¿¡å·ï¼Œå¦‚å›¾ 6 å’Œ 16 æ‰€ç¤ºã€‚æˆ‘ä»¬å¯ä»¥å¯¹è¿™äº›è¿ç»­ä¿¡å·éµå¾ªç›¸åŒçš„ç²—ç²’åŒ–æ­¥éª¤ï¼Œç»“æœæ˜¯ç›¸åŒçš„ï¼ˆMeshulam ç­‰äººï¼Œ2019ï¼‰ã€‚\nIn the full theoretical structure of the RG, scaling exponents are signatures of universality classes. Before we can ask about universality we have to ask about reproducibility, especially in such complex systems. As a first step, the same analyses have been done with data from experiments on multiple mice. Because scaling is precise across more than two decades, the error bars in determining the exponents in individual mice are small, which sets a high standard for reproducibility. For example, the exponent describing the scaling of the free energy (Fig 34B) is $\\widetilde{\\beta}= 0.87Â±0.014Â±0.015$ for the mean, the rms error in single experiments, and the standard deviation across experiments in three mice. This holds out the hope that we have uncovered features of the emergent behavior that are reproducible in the second decimal place.\nåœ¨ RG çš„å®Œæ•´ç†è®ºç»“æ„ä¸­ï¼Œç¼©æ”¾æŒ‡æ•°æ˜¯æ™®é€‚ç±»çš„æ ‡å¿—ã€‚åœ¨æˆ‘ä»¬è¯¢é—®æ™®é€‚æ€§ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»è¯¢é—®å¯é‡å¤æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¦‚æ­¤å¤æ‚çš„ç³»ç»Ÿä¸­ã€‚ä½œä¸ºç¬¬ä¸€æ­¥ï¼Œå·²ç»ä½¿ç”¨æ¥è‡ªå¤šåªå°é¼ å®éªŒçš„æ•°æ®è¿›è¡Œäº†ç›¸åŒçš„åˆ†æã€‚ç”±äºç¼©æ”¾åœ¨ä¸¤ä¸ªæ•°é‡çº§ä»¥ä¸Šæ˜¯ç²¾ç¡®çš„ï¼Œå› æ­¤åœ¨å•åªå°é¼ ä¸­ç¡®å®šæŒ‡æ•°çš„è¯¯å·®æ¡å¾ˆå°ï¼Œè¿™ä¸ºå¯é‡å¤æ€§è®¾å®šäº†é«˜æ ‡å‡†ã€‚ä¾‹å¦‚ï¼Œæè¿°è‡ªç”±èƒ½ç¼©æ”¾çš„æŒ‡æ•°ï¼ˆå›¾ 34Bï¼‰ä¸º $\\widetilde{\\beta}= 0.87Â±0.014Â±0.015$ï¼Œåˆ†åˆ«è¡¨ç¤ºå•æ¬¡å®éªŒä¸­çš„å‡å€¼ã€å‡æ–¹æ ¹è¯¯å·®å’Œä¸‰åªå°é¼ å®éªŒä¸­çš„æ ‡å‡†åå·®ã€‚è¿™è®©äººå¸Œæœ›æˆ‘ä»¬å·²ç»å‘ç°äº†åœ¨ç¬¬äºŒä¸ªå°æ•°ä½ä¸Šå¯é‡å¤çš„æ¶Œç°è¡Œä¸ºç‰¹å¾ã€‚\nA more ambitious search for universality was undertaken by Morales et al. (2023). They analyzed experiments that are part of a large effort at the Allen Institute for Brain Science, in this case using multiple neuropixels probes (Fig 5) to record 100+ neurons from each of many different areas of the mouse brain, simultaneously. Note that in addition to exploring many different brain regions, the technique for recording activity is completely different than in the hippocampal imaging data analyzed in Figs 34 and 35. Nonetheless, all aspects of scaling are reproduced across all these brain areas; examples include the scaling of the variance in coarseâ€“grained activity (Fig 36A) and dynamic scaling (Fig 36B).\nMorales ç­‰äººï¼ˆ2023ï¼‰è¿›è¡Œäº†æ›´é›„å¿ƒå‹ƒå‹ƒçš„æ™®é€‚æ€§æœç´¢ã€‚ä»–ä»¬åˆ†æäº†å±äº Allen Institute for Brain Science å¤§å‹é¡¹ç›®çš„ä¸€äº›å®éªŒï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤šä¸ªç¥ç»åƒç´ æ¢é’ˆï¼ˆå›¾ 5ï¼‰åŒæ—¶è®°å½•æ¥è‡ªå°é¼ å¤§è„‘è®¸å¤šä¸åŒåŒºåŸŸçš„ 100 å¤šä¸ªç¥ç»å…ƒã€‚è¯·æ³¨æ„ï¼Œé™¤äº†æ¢ç´¢è®¸å¤šä¸åŒçš„å¤§è„‘åŒºåŸŸå¤–ï¼Œè®°å½•æ´»åŠ¨çš„æŠ€æœ¯ä¸å›¾ 34 å’Œ 35 ä¸­åˆ†æçš„æµ·é©¬ä½“æˆåƒæ•°æ®å®Œå…¨ä¸åŒã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ‰€æœ‰è¿™äº›å¤§è„‘åŒºåŸŸéƒ½é‡ç°äº†ç¼©æ”¾çš„å„ä¸ªæ–¹é¢ï¼›ç¤ºä¾‹åŒ…æ‹¬ç²—ç²’åŒ–æ´»åŠ¨ä¸­æ–¹å·®çš„ç¼©æ”¾ï¼ˆå›¾ 36Aï¼‰å’ŒåŠ¨æ€ç¼©æ”¾ï¼ˆå›¾ 36Bï¼‰ã€‚\nFIG. 36 Scaling in mutliple distinct areas of the mouse brain (MoralÃ©s et al., 2023). neuronal data after the â€œreal spaceâ€ (direct correlations) coarse-graining procedure. (A) Variance of the coarse-grained activity vs cluster size for neurons in sixteen different brain regions (depicted as different markers), comparable to Fig 34A. (B) Dynamic scaling for the same brain areas. Correlation time vs cluster size, comparable to Fig 35. Inset: Decay of the autocorrelation function for the neurons in one brain region (primary motor cortex) showing the collapse once time is rescaled.\nå›¾ 36 å°é¼ å¤§è„‘å¤šä¸ªä¸åŒåŒºåŸŸçš„ç¼©æ”¾ï¼ˆMoralÃ©s ç­‰äººï¼Œ2023ï¼‰ã€‚ç¥ç»å…ƒæ•°æ®ç»è¿‡â€œå®ç©ºé—´â€ï¼ˆç›´æ¥ç›¸å…³ï¼‰ç²—ç²’åŒ–ç¨‹åºã€‚(A) åå…­ä¸ªä¸åŒå¤§è„‘åŒºåŸŸä¸­ç²—ç²’åŒ–æ´»åŠ¨çš„æ–¹å·®ä¸ç°‡å¤§å°çš„å…³ç³»ï¼ˆä»¥ä¸åŒæ ‡è®°è¡¨ç¤ºï¼‰ï¼Œå¯ä¸å›¾ 34A ç›¸å¯¹æ¯”ã€‚(B) ç›¸åŒå¤§è„‘åŒºåŸŸçš„åŠ¨æ€ç¼©æ”¾ã€‚ç›¸å…³æ—¶é—´ä¸ç°‡å¤§å°çš„å…³ç³»ï¼Œå¯ä¸å›¾ 35 ç›¸å¯¹æ¯”ã€‚æ’å›¾ï¼šä¸€ä¸ªå¤§è„‘åŒºåŸŸï¼ˆåˆçº§è¿åŠ¨çš®å±‚ï¼‰ä¸­ç¥ç»å…ƒè‡ªç›¸å…³å‡½æ•°çš„è¡°å‡ï¼Œæ˜¾ç¤ºå‡ºä¸€æ—¦æ—¶é—´é‡æ–°ç¼©æ”¾åçš„åç¼©ã€‚\nAs we were completing this review a striking result was reported by Munn et al. (2024). Rather than looking at experiments across multiple brain areas in a single organism, they looked at experiments on many different organisms, from the tiny worm C. elegans to primates much like us. There are significant technical differences among these experiments, including differences in the calcium indicator proteins (Â§III.C) and differences in the sampling rate; complete resolution of individual neurons vs â€œregions of interest;â€ and recording from the entire brain is smaller model organisms vs. a single sensory or motor area in larger organisms. Many microscopic features of these networks also are very different, with the extreme being that C. elegans neurons generate slow, graded potentials instead of discrete action potentials or spikes. Despite these caveats, we can ask how the patterns of neural activity in these systems transform under coarseâ€“graining across a range from two to five decades. Results for the variance of the coarseâ€“grained activity, $M_{2}(k)$ from Eq (151), are shown in Fig 37. The apparent universality of these results is tantalizing.\nåœ¨æˆ‘ä»¬å®Œæˆè¿™ç¯‡ç»¼è¿°æ—¶ï¼ŒMunn ç­‰äººï¼ˆ2024ï¼‰æŠ¥é“äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„ç»“æœã€‚ä»–ä»¬æ²¡æœ‰æŸ¥çœ‹å•ä¸ªæœ‰æœºä½“ä¸­å¤šä¸ªå¤§è„‘åŒºåŸŸçš„å®éªŒï¼Œè€Œæ˜¯æŸ¥çœ‹äº†è®¸å¤šä¸åŒæœ‰æœºä½“çš„å®éªŒï¼Œä»å¾®å°çš„çº¿è™« C. elegans åˆ°ä¸æˆ‘ä»¬éå¸¸ç›¸ä¼¼çš„çµé•¿ç±»åŠ¨ç‰©ã€‚è¿™äº›å®éªŒä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æŠ€æœ¯å·®å¼‚ï¼ŒåŒ…æ‹¬é’™æŒ‡ç¤ºè›‹ç™½ï¼ˆÂ§III.Cï¼‰çš„å·®å¼‚å’Œé‡‡æ ·ç‡çš„å·®å¼‚ï¼›å®Œå…¨åˆ†è¾¨å•ä¸ªç¥ç»å…ƒä¸â€œæ„Ÿå…´è¶£åŒºåŸŸâ€ï¼›ä»¥åŠåœ¨è¾ƒå°æ¨¡å‹æœ‰æœºä½“ä¸­è®°å½•æ•´ä¸ªå¤§è„‘ä¸åœ¨è¾ƒå¤§æœ‰æœºä½“ä¸­è®°å½•å•ä¸ªæ„Ÿè§‰æˆ–è¿åŠ¨åŒºåŸŸã€‚ è¿™äº›ç½‘ç»œçš„è®¸å¤šå¾®è§‚ç‰¹å¾ä¹Ÿéå¸¸ä¸åŒï¼Œæç«¯æƒ…å†µæ˜¯ C. elegans ç¥ç»å…ƒäº§ç”Ÿç¼“æ…¢çš„æ¸å˜ç”µä½ï¼Œè€Œä¸æ˜¯ç¦»æ•£çš„åŠ¨ä½œç”µä½æˆ–å°–å³°ã€‚å°½ç®¡å­˜åœ¨è¿™äº›è­¦å‘Šï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥è¯¢é—®è¿™äº›ç³»ç»Ÿä¸­çš„ç¥ç»æ´»åŠ¨æ¨¡å¼å¦‚ä½•åœ¨ä»ä¸¤ä¸ªåˆ°äº”ä¸ªæ•°é‡çº§çš„èŒƒå›´å†…é€šè¿‡ç²—ç²’åŒ–è¿›è¡Œè½¬æ¢ã€‚å›¾ 37 æ˜¾ç¤ºäº†ç²—ç²’åŒ–æ´»åŠ¨æ–¹å·® $M_{2}(k)$ï¼ˆæ¥è‡ªæ–¹ç¨‹ï¼ˆ151ï¼‰ï¼‰çš„ç»“æœã€‚è¿™äº›ç»“æœçš„æ˜æ˜¾æ™®é€‚æ€§ä»¤äººç€è¿·ã€‚\nScaling in the variance of neural activity, Eq (151), as a function of scale across multiple species (Munn et al., 2024). (A) Zebrafish. (B) The worm C. elegans. (C) The fruit fly Drosophila melanogaster. (D) Mouse primary visual cortex. (E) Macaque primary visual and motor cortices. Grey lines are results from individual animals, red points with errors are means within species, and red lines are fits to $M_{2}\\propto K^{\\widetilde{\\alpha}}$, with exponents as shown. Expectations for independent (blue) and completely correlated (green) populations corresponding to the dashed lines in Fig 34A.\nç¥ç»æ´»åŠ¨æ–¹å·®çš„ç¼©æ”¾ï¼Œæ–¹ç¨‹ï¼ˆ151ï¼‰ï¼Œä½œä¸ºè·¨å¤šä¸ªç‰©ç§çš„å°ºåº¦å‡½æ•°ï¼ˆMunn ç­‰äººï¼Œ2024ï¼‰ã€‚(A) æ–‘é©¬é±¼ã€‚(B) çº¿è™« C. elegansã€‚(C) æœè‡ Drosophila melanogasterã€‚(D) å°é¼ åˆçº§è§†è§‰çš®å±‚ã€‚(E) çŒ•çŒ´åˆçº§è§†è§‰å’Œè¿åŠ¨çš®å±‚ã€‚ç°çº¿æ˜¯å•ä¸ªåŠ¨ç‰©çš„ç»“æœï¼Œå¸¦è¯¯å·®çš„çº¢ç‚¹æ˜¯ç‰©ç§å†…çš„å‡å€¼ï¼Œçº¢çº¿æ˜¯å¯¹ $M_{2}\\propto K^{\\widetilde{\\alpha}}$ çš„æ‹Ÿåˆï¼ŒæŒ‡æ•°å¦‚å›¾æ‰€ç¤ºã€‚ç‹¬ç«‹ï¼ˆè“è‰²ï¼‰å’Œå®Œå…¨ç›¸å…³ï¼ˆç»¿è‰²ï¼‰ç¾¤ä½“çš„æœŸæœ›å¯¹åº”äºå›¾ 34A ä¸­çš„è™šçº¿ã€‚\nBy analogy with momentum shell methods In problems where â€œscaleâ€ really is a length scale, coarseâ€“graining is a gradual blurring out of spatial detail much as what happens when we look through a microscope and defocus. In that analogy, the spatial pattern is Fourier transformed and then reconstructed using only a limited range of wavelengths. Concretely, if we start with variables $\\phi(\\vec{x})$ in a $d$â€“dimensional space with coordinates $\\vec{x}$, the coarseâ€“graining operation becomes\n$$ \\begin{aligned} \\phi(\\vec{x}) \u0026\\rightarrow \\phi_{\\Lambda}(\\vec{x}) = z_{\\Lambda}\\int_{|\\vec{k}|\u003c\\Lambda}\\frac{\\mathrm{d}^{d}k}{(2\\pi)^{d}}e^{i\\vec{k}\\cdot\\vec{x}}\\widetilde{\\phi}(\\vec{k})\\\\ \\widetilde{\\phi}(\\vec{k}) \u0026= \\int \\mathrm{d}^{d}x e^{-i\\vec{k}\\cdot\\vec{x}}\\phi(\\vec{x}) \\end{aligned} $$\nwhere $\\Lambda = \\pi/l$ cuts off contributions below a length scale $l$ and $z_{\\Lambda}$ serves to (re)normalize the variables; in the microscopic analogy this compensates for the loss of contrast as we defocus. As in real space we are interested in how the probability distribution $P_{\\lambda}[\\phi_{\\Lambda}]$ evolves as a function of the cutoff $\\Lambda$. Since the Fourier variables are continuous (in the limit of a large system) we can make infinitesimal changes $\\Lambda\\to\\Lambda-\\mathrm{d}\\Lambda$. In quantum mechanics wave with wavevector $\\vec{k}$ describe particles with momentum $\\vec{p}=\\hbar\\vec{k}$, so that average over the details in a range $\\Lambda âˆ’ \\mathrm{d}\\Lambda \u003c |\\vec{k}| \u003c \\Lambda$ is equivalent to integrating out a â€œmomentum shellâ€ (Wilson and Kogut, 1974).\nåœ¨â€œå°ºåº¦â€çœŸæ­£æ˜¯é•¿åº¦å°ºåº¦çš„é—®é¢˜ä¸­ï¼Œç²—ç²’åŒ–æ˜¯ç©ºé—´ç»†èŠ‚çš„é€æ¸æ¨¡ç³Šï¼Œå°±åƒæˆ‘ä»¬é€šè¿‡æ˜¾å¾®é•œè§‚å¯Ÿå¹¶å¤±ç„¦æ—¶å‘ç”Ÿçš„æƒ…å†µä¸€æ ·ã€‚åœ¨è¿™ä¸ªç±»æ¯”ä¸­ï¼Œç©ºé—´æ¨¡å¼è¢«å‚…é‡Œå¶å˜æ¢ï¼Œç„¶åä»…ä½¿ç”¨æœ‰é™èŒƒå›´çš„æ³¢é•¿è¿›è¡Œé‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬ä» $d$ ç»´ç©ºé—´ä¸­åæ ‡ä¸º $\\vec{x}$ çš„å˜é‡ $\\phi(\\vec{x})$ å¼€å§‹ï¼Œç²—ç²’åŒ–æ“ä½œå˜ä¸º\n$$ \\begin{aligned} \\phi(\\vec{x}) \u0026\\rightarrow \\phi_{\\Lambda}(\\vec{x}) = z_{\\Lambda}\\int_{|\\vec{k}|\u003c\\Lambda}\\frac{\\mathrm{d}^{d}k}{(2\\pi)^{d}}e^{i\\vec{k}\\cdot\\vec{x}}\\widetilde{\\phi}(\\vec{k})\\\\ \\widetilde{\\phi}(\\vec{k}) \u0026= \\int \\mathrm{d}^{d}x e^{-i\\vec{k}\\cdot\\vec{x}}\\phi(\\vec{x}) \\end{aligned} $$\nå…¶ä¸­ $\\Lambda = \\pi/l$ æˆªæ–­äº†ä½äºé•¿åº¦å°ºåº¦ $l$ çš„è´¡çŒ®ï¼Œ$z_{\\Lambda}$ ç”¨äºï¼ˆé‡æ–°ï¼‰å½’ä¸€åŒ–å˜é‡ï¼›åœ¨å¾®è§‚ç±»æ¯”ä¸­ï¼Œè¿™è¡¥å¿äº†æˆ‘ä»¬å¤±ç„¦æ—¶å¯¹æ¯”åº¦çš„æŸå¤±ã€‚ä¸å®ç©ºé—´ä¸€æ ·ï¼Œæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯æ¦‚ç‡åˆ†å¸ƒ $P_{\\lambda}[\\phi_{\\Lambda}]$ å¦‚ä½•éšç€æˆªæ­¢å€¼ $\\Lambda$ çš„å˜åŒ–è€Œæ¼”å˜ã€‚ç”±äºå‚…é‡Œå¶å˜é‡æ˜¯è¿ç»­çš„ï¼ˆåœ¨å¤§ç³»ç»Ÿçš„æé™ä¸‹ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œæ— ç©·å°å˜åŒ– $\\Lambda\\to\\Lambda-\\mathrm{d}\\Lambda$ã€‚åœ¨é‡å­åŠ›å­¦ä¸­ï¼Œæ³¢çŸ¢ä¸º $\\vec{k}$ çš„æ³¢æè¿°åŠ¨é‡ä¸º $\\vec{p}=\\hbar\\vec{k}$ çš„ç²’å­ï¼Œå› æ­¤åœ¨èŒƒå›´ $\\Lambda âˆ’ \\mathrm{d}\\Lambda \u003c |\\vec{k}| \u003c \\Lambda$ å†…å¯¹ç»†èŠ‚è¿›è¡Œå¹³å‡ç›¸å½“äºç§¯åˆ†å‡ºä¸€ä¸ªâ€œåŠ¨é‡å£³â€ï¼ˆWilson å’Œ Kogutï¼Œ1974ï¼‰ã€‚\nMomentum is conserved in systems with translation invariance. Independent of these physical principles, spatial translation invariance privileges the Fourier transform. As an example, if variables $z_{i}$ live on a lattice of points $\\vec{x}_{i}$, translation invariance means that the covariance matrix elements $C_{ij}$ can depend only on the difference in positions,\n$$ C_{ij} = C(\\vec{x}_{i} - \\vec{x}_{j}) $$\nthis matrix is diagonalized in a Fourier basis,\n$$ \\begin{aligned} \\sum_{j=1}^{N}C_{ij}u_{jr} \u0026= \\lambda_{r}u_{ir}\\\\ u_{jr} \u0026\\propto \\exp{(i\\vec{k}_{r}\\cdot\\vec{x}_{j})} \\end{aligned} $$\nwhere we can put the modes in order by the rank of the eigenvalue $r$.\nåœ¨å…·æœ‰å¹³ç§»ä¸å˜æ€§çš„ç³»ç»Ÿä¸­ï¼ŒåŠ¨é‡æ˜¯å®ˆæ’çš„ã€‚ç‹¬ç«‹äºè¿™äº›ç‰©ç†åŸç†ï¼Œç©ºé—´å¹³ç§»ä¸å˜æ€§ä½¿å‚…é‡Œå¶å˜æ¢å…·æœ‰ç‰¹æƒã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œå¦‚æœå˜é‡ $z_{i}$ ä½äºç‚¹ $\\vec{x}_{i}$ çš„æ™¶æ ¼ä¸Šï¼Œå¹³ç§»ä¸å˜æ€§æ„å‘³ç€åæ–¹å·®çŸ©é˜µå…ƒç´  $C_{ij}$ åªèƒ½ä¾èµ–äºä½ç½®çš„å·®å¼‚ï¼Œ\n$$ C_{ij} = C(\\vec{x}_{i} - \\vec{x}_{j}) $$\nè¯¥çŸ©é˜µåœ¨å‚…é‡Œå¶åŸºä¸­å¯¹è§’åŒ–ï¼Œ\n$$ \\begin{aligned} \\sum_{j=1}^{N}C_{ij}u_{jr} \u0026= \\lambda_{r}u_{ir}\\\\ u_{jr} \u0026\\propto \\exp{(i\\vec{k}_{r}\\cdot\\vec{x}_{j})} \\end{aligned} $$\næˆ‘ä»¬å¯ä»¥æ ¹æ®ç‰¹å¾å€¼çš„ç§© $r$ å¯¹æ¨¡å¼è¿›è¡Œæ’åºã€‚\nIn the usual applications of the RG, large momenta correspond to small eigenvalues of the covariance matrix. Thus suggests that we can construct coarseâ€“grained variables by filtering out the â€œmodesâ€ that correspond to small eigenvalues, without reference to space or momenta (Bradde and Bialek, 2017). This connects coarsegraining to a more familiar data analysis technique, principal components analysis (Shlens, 2014).\nåœ¨ RG çš„å¸¸è§„åº”ç”¨ä¸­ï¼Œå¤§åŠ¨é‡å¯¹åº”äºåæ–¹å·®çŸ©é˜µçš„å°ç‰¹å¾å€¼ã€‚è¿™è¡¨æ˜æˆ‘ä»¬å¯ä»¥é€šè¿‡æ»¤é™¤å¯¹åº”äºå°ç‰¹å¾å€¼çš„â€œæ¨¡å¼â€æ¥æ„å»ºç²—ç²’åŒ–å˜é‡ï¼Œè€Œä¸å‚è€ƒç©ºé—´æˆ–åŠ¨é‡ï¼ˆBradde å’Œ Bialekï¼Œ2017ï¼‰ã€‚è¿™å°†ç²—ç²’åŒ–ä¸æ›´ç†Ÿæ‚‰çš„æ•°æ®åˆ†ææŠ€æœ¯ä¸»æˆåˆ†åˆ†æï¼ˆShlensï¼Œ2014ï¼‰è”ç³»èµ·æ¥ã€‚\nConcretely, if we start with microscopic variables $\\{\\sigma_{i}\\}$, we can compute the covariance matrix as usual\n$$ C_{ij} = \\langle(\\sigma_{i}-\\langle\\sigma_{i}\\rangle)(\\sigma_{j}-\\langle\\sigma_{j}\\rangle)\\rangle $$\nand then we have eigenvalues and eigenvectors as in Eq (160). Letâ€™s choose the rank $r$ so that $\\lambda_{1}\\geq \\lambda_{2}\\cdots\\lambda_{N}$. We can define a projection onto the $\\hat{K}$ modes that make the largest contribution to the variance,\n$$ \\begin{aligned} \\hat{P}(\\hat{K}) \u0026= \\sum_{r=1}^{\\hat{K}} u_{ir}u_{jr}\\\\ \\phi_{\\hat{K}}(i) \u0026= z_{i}(\\hat{K})\\sum_{j}\\hat{P}_{ij}(\\hat{K})[\\sigma_{i}-\\langle\\sigma_{i}\\rangle] \\end{aligned} $$\nwith the normalization $z_{i}(\\hat{K})$ such that $\\langle[\\phi_{\\hat{K}}(i)]^{2}\\rangle = 1$.\nå…·ä½“æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬ä»å¾®è§‚å˜é‡ $\\{\\sigma_{i}\\}$ å¼€å§‹ï¼Œæˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·è®¡ç®—åæ–¹å·®çŸ©é˜µ\n$$ C_{ij} = \\langle(\\sigma_{i}-\\langle\\sigma_{i}\\rangle)(\\sigma_{j}-\\langle\\sigma_{j}\\rangle)\\rangle $$\nç„¶åæˆ‘ä»¬æœ‰å¦‚æ–¹ç¨‹ï¼ˆ160ï¼‰æ‰€ç¤ºçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚è®©æˆ‘ä»¬é€‰æ‹©ç§© $r$ï¼Œä½¿å¾— $\\lambda_{1}\\geq \\lambda_{2}\\cdots\\lambda_{N}$ã€‚æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªæŠ•å½±åˆ°å¯¹æ–¹å·®è´¡çŒ®æœ€å¤§çš„ $\\hat{K}$ ä¸ªæ¨¡å¼ä¸Šï¼Œ\n$$ \\begin{aligned} \\hat{P}(\\hat{K}) \u0026= \\sum_{r=1}^{\\hat{K}} u_{ir}u_{jr}\\\\ \\phi_{\\hat{K}}(i) \u0026= z_{i}(\\hat{K})\\sum_{j}\\hat{P}_{ij}(\\hat{K})[\\sigma_{i}-\\langle\\sigma_{i}\\rangle] \\end{aligned} $$\nå…¶ä¸­å½’ä¸€åŒ– $z_{i}(\\hat{K})$ ä½¿å¾— $\\langle[\\phi_{\\hat{K}}(i)]^{2}\\rangle = 1$ã€‚\nAs before, we want to follow the distribution of the individual coarseâ€“grained variables, $P_{\\hat{K}}(\\phi_{\\hat{K}})$; results are shown in Fig 38A. To be sure that we have control over the full matrix $C_{ij}$ we look at clusters of $N = 128$ neurons identified through the real space coarseâ€“graining above. We can then filter out half of the modes, so that $\\hat{K} = 32$, resulting in a distribution $P_{\\hat{K}}(\\phi_{\\hat{K}})$ that still has some fine structure. If we reduce to $\\hat{K} = 32$ these wiggles disappear but the distribution remains asymmetric with long tails. This pattern continues as we reduce to $\\hat{K} = 16$ and then $\\hat{K} = 8$, and in these last steps the distribution hardly changes. This suggests that as we coarseâ€“grain, the distribution flows toward a fixed form. Importantly this form is very different from the Gaussian that would be guaranteed by the central limit theorem if correlations were weak.\nå¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬æƒ³è¦è·Ÿè¸ªå•ä¸ªç²—ç²’åŒ–å˜é‡çš„åˆ†å¸ƒï¼Œ$P_{\\hat{K}}(\\phi_{\\hat{K}})$ï¼›ç»“æœæ˜¾ç¤ºåœ¨å›¾ 38A ä¸­ã€‚ä¸ºäº†ç¡®ä¿æˆ‘ä»¬å¯¹æ•´ä¸ªçŸ©é˜µ $C_{ij}$ æœ‰æ§åˆ¶ï¼Œæˆ‘ä»¬æŸ¥çœ‹äº†é€šè¿‡ä¸Šè¿°å®ç©ºé—´ç²—ç²’åŒ–è¯†åˆ«çš„ $N = 128$ ä¸ªç¥ç»å…ƒçš„ç°‡ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥æ»¤é™¤ä¸€åŠçš„æ¨¡å¼ï¼Œä½¿å¾— $\\hat{K} = 32$ï¼Œå¾—åˆ°çš„åˆ†å¸ƒ $P_{\\hat{K}}(\\phi_{\\hat{K}})$ ä»ç„¶å…·æœ‰ä¸€äº›ç»†å¾®ç»“æ„ã€‚å¦‚æœæˆ‘ä»¬å‡å°‘åˆ° $\\hat{K} = 32$ï¼Œè¿™äº›æ³¢åŠ¨æ¶ˆå¤±äº†ï¼Œä½†åˆ†å¸ƒä»ç„¶æ˜¯ä¸å¯¹ç§°çš„ï¼Œå¹¶ä¸”å…·æœ‰é•¿å°¾ã€‚è¿™ç§æ¨¡å¼åœ¨æˆ‘ä»¬å‡å°‘åˆ° $\\hat{K} = 16$ ç„¶å $\\hat{K} = 8$ æ—¶ç»§ç»­å­˜åœ¨ï¼Œåœ¨è¿™æœ€åå‡ ä¸ªæ­¥éª¤ä¸­ï¼Œåˆ†å¸ƒå‡ ä¹æ²¡æœ‰å˜åŒ–ã€‚è¿™è¡¨æ˜éšç€æˆ‘ä»¬è¿›è¡Œç²—ç²’åŒ–ï¼Œåˆ†å¸ƒè¶‹å‘äºä¸€ä¸ªå›ºå®šå½¢å¼ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ç§å½¢å¼ä¸å¦‚æœç›¸å…³æ€§è¾ƒå¼±ï¼Œä¸­å¿ƒæé™å®šç†æ‰€ä¿è¯çš„é«˜æ–¯å½¢å¼éå¸¸ä¸åŒã€‚\nCoarseâ€“graining in groups of $N = 128$ neurons via â€œmomentum shellsâ€ (Meshulam et al., 2018). (A) Following the distribution of individual coarseâ€“grained variables from Eq (164). Different colors correspond to keeping different numbers of modes $\\hat{K}$, as in inset; dashed line is a Gaussian for comparison. (B) Dynamic scaling of the correlation time for fluctuations in mode r, Eq (166), vs the associated eigenvalue of the covariance matrix, $\\tau_{c}(r)\\propto \\lambda_{r}^{\\widetilde{z}^{\\prime}}$ , $\\widetilde{z}^{\\prime} = 0.37 \\pm 0.04$.\né€šè¿‡â€œåŠ¨é‡å£³â€å¯¹ $N = 128$ ä¸ªç¥ç»å…ƒè¿›è¡Œç²—ç²’åŒ–ï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼‰ã€‚(A) è·Ÿè¸ªæ¥è‡ªæ–¹ç¨‹ï¼ˆ164ï¼‰çš„å•ä¸ªç²—ç²’åŒ–å˜é‡çš„åˆ†å¸ƒã€‚ä¸åŒé¢œè‰²å¯¹åº”äºä¿ç•™ä¸åŒæ•°é‡çš„æ¨¡å¼ $\\hat{K}$ï¼Œå¦‚æ’å›¾æ‰€ç¤ºï¼›è™šçº¿ä¸ºé«˜æ–¯åˆ†å¸ƒä»¥ä¾›æ¯”è¾ƒã€‚(B) æ¨¡å¼ r ä¸­æ³¢åŠ¨çš„ç›¸å…³æ—¶é—´çš„åŠ¨æ€ç¼©æ”¾ï¼Œæ–¹ç¨‹ï¼ˆ166ï¼‰ï¼Œä¸åæ–¹å·®çŸ©é˜µçš„ç›¸å…³ç‰¹å¾å€¼ $\\tau_{c}(r)\\propto \\lambda_{r}^{\\widetilde{z}^{\\prime}}$ï¼Œå…¶ä¸­ $\\widetilde{z}^{\\prime} = 0.37 \\pm 0.04$ã€‚\nThe intuition behind dynamic scaling is that fluctuations on larger length scales relax more slowly, and we have seen that this generalizes to a network of neurons even though the meaning of â€œscaleâ€ now if more abstract (Fig 35). By transforming to basis that diagonalizes the covariance matrix we have isolated the modes of fluctuation that are independent at second order, and it is natural to ask how these fluctuations along these modes relax. Variations along mode $r$ are define by\n$$ \\widetilde{\\phi}_{r} = \\sum_{i=1}^{N}[\\sigma_{i}-\\langle\\sigma_{i}\\rangle]u_{ir} $$\nand the correlation function is\n$$ C_{r}(t) = \\langle\\widetilde{\\phi}_{r}(t_{0})\\widetilde{\\phi}_{r}(t_{0}+t)\\rangle $$\nDynamic scaling is the statement that all these correlations collapse when time is scaled by a single correlation time, and that this correlation time itself has a powerâ€“law dependence of scale. In the usual examples this means $\\tau_{c}\\propto |\\vec{k}|^{z}$ (Hohenberg and Halperin, 1977), but near a critical point the eigenvalues of the covariance matrix also have a powerâ€“law dependence on $|\\vec{k}|$, so we can test directly for $\\tau_{c}\\propto \\lambda^{\\widetilde{z}^{\\prime}}$ as shown in Fig 38B. As before, the shortest correlation times are limited by the response time of the fluorescent proteins that report on electrical activity, and the longest times are limited by the magnitude of the dynamic scaling exponent; nonetheless we can observe reasonably precise scaling across two decades in $\\lambda$.\nåŠ¨æ€ç¼©æ”¾çš„ç›´è§‰æ˜¯ï¼Œæ›´å¤§é•¿åº¦å°ºåº¦ä¸Šçš„æ³¢åŠ¨å¼›è±«å¾—æ›´æ…¢ï¼Œå°½ç®¡â€œå°ºåº¦â€çš„å«ä¹‰ç°åœ¨æ›´æŠ½è±¡ï¼ˆå›¾ 35ï¼‰ï¼Œä½†æˆ‘ä»¬å·²ç»çœ‹åˆ°è¿™å¯ä»¥æ¨å¹¿åˆ°ç¥ç»å…ƒç½‘ç»œã€‚é€šè¿‡è½¬æ¢åˆ°å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œå¯¹è§’åŒ–çš„åŸºï¼Œæˆ‘ä»¬å·²ç»éš”ç¦»äº†åœ¨äºŒé˜¶ä¸Šç‹¬ç«‹çš„æ³¢åŠ¨æ¨¡å¼ï¼Œè‡ªç„¶ä¼šé—®è¿™äº›æ¨¡å¼ä¸Šçš„æ³¢åŠ¨æ˜¯å¦‚ä½•å¼›è±«çš„ã€‚æ²¿æ¨¡å¼ $r$ çš„å˜åŒ–å®šä¹‰ä¸º\n$$ \\widetilde{\\phi}_{r} = \\sum_{i=1}^{N}[\\sigma_{i}-\\langle\\sigma_{i}\\rangle]u_{ir} $$\nç›¸å…³å‡½æ•°ä¸º\n$$ C_{r}(t) = \\langle\\widetilde{\\phi}_{r}(t_{0})\\widetilde{\\phi}_{r}(t_{0}+t)\\rangle $$\nåŠ¨æ€ç¼©æ”¾æ˜¯è¿™æ ·ä¸€ç§è¯´æ³•ï¼šå½“æ—¶é—´æŒ‰å•ä¸€ç›¸å…³æ—¶é—´è¿›è¡Œç¼©æ”¾æ—¶ï¼Œæ‰€æœ‰è¿™äº›ç›¸å…³æ€§éƒ½ä¼šåç¼©ï¼Œå¹¶ä¸”è¯¥ç›¸å…³æ—¶é—´æœ¬èº«å…·æœ‰å°ºåº¦çš„å¹‚å¾‹ä¾èµ–ã€‚åœ¨é€šå¸¸çš„ä¾‹å­ä¸­ï¼Œè¿™æ„å‘³ç€ $\\tau_{c}\\propto |\\vec{k}|^{z}$ï¼ˆHohenberg å’Œ Halperinï¼Œ1977ï¼‰ï¼Œä½†åœ¨ä¸´ç•Œç‚¹é™„è¿‘ï¼Œåæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼ä¹Ÿå¯¹ $|\\vec{k}|$ å…·æœ‰å¹‚å¾‹ä¾èµ–ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›´æ¥æµ‹è¯• $\\tau_{c}\\propto \\lambda^{\\widetilde{z}^{\\prime}}$ï¼Œå¦‚å›¾ 38B æ‰€ç¤ºã€‚å¦‚å‰æ‰€è¿°ï¼Œæœ€çŸ­çš„ç›¸å…³æ—¶é—´å—åˆ°æŠ¥å‘Šç”µæ´»åŠ¨çš„è§å…‰è›‹ç™½å“åº”æ—¶é—´çš„é™åˆ¶ï¼Œæœ€é•¿çš„æ—¶é—´å—åˆ°åŠ¨æ€ç¼©æ”¾æŒ‡æ•°å¤§å°çš„é™åˆ¶ï¼›å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥è§‚å¯Ÿåˆ° $\\lambda$ ä¸Šä¸¤ä¸ªæ•°é‡çº§çš„ç›¸å½“ç²¾ç¡®çš„ç¼©æ”¾ã€‚\nThe dynamic exponent $\\widetilde{z}^{\\prime}$ that one finds by looking at the correlation times of the modes should be related to the one we see via coarseâ€“graining in real space, $\\widetilde{z}$(Fig 35C), through the exponent Î¼ that describes the decay of the eigenvalues of the covariance matrix, $\\widetilde{z} = \\mu\\widetilde{z}^{\\prime}$. This works, although error bars are large (Meshulam et al., 2018). More importantly, these results indicate that the network has no single characteristic time scale, but rather a continuum of time scales that can be accessed by probing on different scales.\né€šè¿‡æŸ¥çœ‹æ¨¡å¼çš„ç›¸å…³æ—¶é—´æ‰¾åˆ°çš„åŠ¨æ€æŒ‡æ•° $\\widetilde{z}^{\\prime}$ åº”è¯¥ä¸æˆ‘ä»¬é€šè¿‡å®ç©ºé—´ç²—ç²’åŒ–çœ‹åˆ°çš„æŒ‡æ•° $\\widetilde{z}$(å›¾ 35C) ç›¸å…³ï¼Œé€šè¿‡æè¿°åæ–¹å·®çŸ©é˜µç‰¹å¾å€¼è¡°å‡çš„æŒ‡æ•° Î¼ï¼Œ$\\widetilde{z} = \\mu\\widetilde{z}^{\\prime}$ã€‚è¿™ç¡®å®æœ‰æ•ˆï¼Œå°½ç®¡è¯¯å·®æ¡å¾ˆå¤§ï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™äº›ç»“æœè¡¨æ˜ç½‘ç»œæ²¡æœ‰å•ä¸€çš„ç‰¹å¾æ—¶é—´å°ºåº¦ï¼Œè€Œæ˜¯ä¸€ä¸ªè¿ç»­çš„æ—¶é—´å°ºåº¦ï¼Œå¯ä»¥é€šè¿‡åœ¨ä¸åŒå°ºåº¦ä¸Šæ¢æµ‹æ¥è®¿é—®ã€‚\nRG as a path to understanding If we believe there is an underlying simplicity to be found amidst the complexity of neural network function and activity, we might want to pause for a moment to convince ourselves that following the RG simplification can actually lead us there. This quest now feels attainable, given the explosive experimental progress in obtaining datasets with increasing number of neurons, as in the examples above. While we may not know how to manipulate â€œtemperatureâ€ or â€œmagnetizationâ€ in the brain, we are gaining decades in the sheer number of monitored neurons.\nå¦‚æœæˆ‘ä»¬ç›¸ä¿¡åœ¨ç¥ç»ç½‘ç»œåŠŸèƒ½å’Œæ´»åŠ¨çš„å¤æ‚æ€§ä¸­å¯ä»¥æ‰¾åˆ°æ½œåœ¨çš„ç®€å•æ€§ï¼Œæˆ‘ä»¬å¯èƒ½æƒ³æš‚åœç‰‡åˆ»æ¥è¯´æœè‡ªå·±ï¼Œéµå¾ª RG ç®€åŒ–å®é™…ä¸Šå¯ä»¥å¼•å¯¼æˆ‘ä»¬åˆ°è¾¾é‚£é‡Œã€‚é‰´äºåœ¨è·å¾—è¶Šæ¥è¶Šå¤šç¥ç»å…ƒæ•°æ®é›†æ–¹é¢çš„çˆ†ç‚¸æ€§å®éªŒè¿›å±•ï¼Œå¦‚ä¸Šé¢çš„ä¾‹å­æ‰€ç¤ºï¼Œè¿™ä¸€è¿½æ±‚ç°åœ¨æ„Ÿè§‰æ˜¯å¯ä»¥å®ç°çš„ã€‚è™½ç„¶æˆ‘ä»¬å¯èƒ½ä¸çŸ¥é“å¦‚ä½•æ“çºµå¤§è„‘ä¸­çš„â€œæ¸©åº¦â€æˆ–â€œç£åŒ–å¼ºåº¦â€ï¼Œä½†æˆ‘ä»¬æ­£åœ¨é€šè¿‡ç›‘æµ‹çš„ç¥ç»å…ƒæ•°é‡è·å¾—æ•°åå¹´çš„è¿›æ­¥ã€‚\nThe renormalization group is a powerful theoretical structure. Because we do not have a microscopic model for neural dynamics, we are not yet able to exploit this structure. What we have done instead is to adopt an RGâ€“inspired approach to data analysis, which has been described as a â€œphenomenological renormalization groupâ€ (Nicoletti et al., 2020) or â€œiterative coarsegrainingâ€ (Munn et al., 2024). If we apply these approaches to well understood equilibrium statistical mechanics problems, the most interesting outcome would be the flow of probability distributions toward some fixed, nonâ€“Gaussian form, and the appearance of powerâ€“law scaling along this trajectory, as would happen at a critical point. Remarkably, this is what has been found, both in the initial application to the hippocampus and now in many other systems; scaling exponents are reproducible and perhaps even universal. It is tempting to conclude that the underlying network dynamics must be described by a theory which is at a nonâ€“trivial fixed point of the renormalization group.\né‡æ•´åŒ–ç¾¤æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç†è®ºç»“æ„ã€‚ç”±äºæˆ‘ä»¬æ²¡æœ‰ç¥ç»åŠ¨åŠ›å­¦çš„å¾®è§‚æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜æ— æ³•åˆ©ç”¨è¿™ä¸ªç»“æ„ã€‚ç›¸åï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å— RG å¯å‘çš„æ•°æ®åˆ†ææ–¹æ³•ï¼Œè¿™è¢«æè¿°ä¸ºâ€œç°è±¡å­¦é‡æ•´åŒ–ç¾¤â€ï¼ˆNicoletti ç­‰äººï¼Œ2020ï¼‰æˆ–â€œè¿­ä»£ç²—ç²’åŒ–â€ï¼ˆMunn ç­‰äººï¼Œ2024ï¼‰ã€‚å¦‚æœæˆ‘ä»¬å°†è¿™äº›æ–¹æ³•åº”ç”¨äºç†è§£è‰¯å¥½çš„å¹³è¡¡ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œæœ€æœ‰è¶£çš„ç»“æœå°†æ˜¯æ¦‚ç‡åˆ†å¸ƒæœç€æŸç§å›ºå®šçš„éé«˜æ–¯å½¢å¼æµåŠ¨ï¼Œä»¥åŠæ²¿ç€è¿™æ¡è½¨è¿¹å‡ºç°å¹‚å¾‹ç¼©æ”¾ï¼Œå°±åƒåœ¨ä¸´ç•Œç‚¹å‘ç”Ÿçš„é‚£æ ·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ­£æ˜¯æ‰€å‘ç°çš„ï¼Œæ— è®ºæ˜¯åœ¨å¯¹æµ·é©¬ä½“çš„åˆå§‹åº”ç”¨ä¸­ï¼Œè¿˜æ˜¯ç°åœ¨åœ¨è®¸å¤šå…¶ä»–ç³»ç»Ÿä¸­ï¼›ç¼©æ”¾æŒ‡æ•°æ˜¯å¯é‡å¤çš„ï¼Œç”šè‡³å¯èƒ½æ˜¯æ™®é€‚çš„ã€‚å¾ˆå®¹æ˜“å¾—å‡ºç»“è®ºï¼ŒåŸºç¡€ç½‘ç»œåŠ¨åŠ›å­¦å¿…é¡»ç”±é‡æ•´åŒ–ç¾¤çš„éå¹³å‡¡ä¸åŠ¨ç‚¹æè¿°çš„ç†è®ºæ¥æè¿°ã€‚\nWe should be cautious. Is it possible that some of the behaviors under coarseâ€“graining that we associate with RG fixed points could emerge, more generically, in nonâ€“equilibrium systems? Nicoletti et al. (2020) addressed this by analyzing simulations of the contact process, in which binary variables are turned on with a probability per unit time proportional to the density of active variables at neighboring sites, and then deactivate with a fixed probability per unit time. This model has one parameter, the proportionality constant in the activation rate, and there is a critical value that depends on the geometry of the network (Marro and Dickman, 1999). Below the critical point the fully inactive state is absorbing, so the question is whether the phenomenological RG can distinguish the critical point from superâ€“critical behaviors.\næˆ‘ä»¬åº”è¯¥ä¿æŒè°¨æ…ã€‚æ˜¯å¦æœ‰å¯èƒ½æˆ‘ä»¬ä¸ RG ä¸åŠ¨ç‚¹ç›¸å…³è”çš„ä¸€äº›ç²—ç²’åŒ–è¡Œä¸ºå¯ä»¥æ›´æ™®éåœ°å‡ºç°åœ¨éå¹³è¡¡ç³»ç»Ÿä¸­ï¼ŸNicoletti ç­‰äººï¼ˆ2020ï¼‰é€šè¿‡åˆ†ææ¥è§¦è¿‡ç¨‹çš„æ¨¡æ‹Ÿæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨è¯¥è¿‡ç¨‹ä¸­ï¼ŒäºŒè¿›åˆ¶å˜é‡ä»¥ä¸é‚»è¿‘ä½ç½®æ´»è·ƒå˜é‡å¯†åº¦æˆæ­£æ¯”çš„å•ä½æ—¶é—´æ¦‚ç‡è¢«æ¿€æ´»ï¼Œç„¶åä»¥å›ºå®šçš„å•ä½æ—¶é—´æ¦‚ç‡è¢«åœç”¨ã€‚è¯¥æ¨¡å‹æœ‰ä¸€ä¸ªå‚æ•°ï¼Œå³æ¿€æ´»ç‡ä¸­çš„æ¯”ä¾‹å¸¸æ•°ï¼Œå¹¶ä¸”å­˜åœ¨ä¸€ä¸ªå–å†³äºç½‘ç»œå‡ ä½•å½¢çŠ¶çš„ä¸´ç•Œå€¼ï¼ˆMarro å’Œ Dickmanï¼Œ1999ï¼‰ã€‚åœ¨ä¸´ç•Œç‚¹ä»¥ä¸‹ï¼Œå®Œå…¨ä¸æ´»è·ƒçŠ¶æ€æ˜¯å¸æ”¶æ€ï¼Œå› æ­¤é—®é¢˜æ˜¯ç°è±¡å­¦ RG æ˜¯å¦å¯ä»¥åŒºåˆ†ä¸´ç•Œç‚¹ä¸è¶…ä¸´ç•Œè¡Œä¸ºã€‚\nPerhaps surprisingly, one can see (weakly) nonâ€“trivial scaling behavior in some quantities even away from the critical point, as with the variance in activity shown in Fig 39A. But other quantities show clear deviations from scaling, even very close to criticality, as with the correlation times in Fig 39B. What is unambiguous is that the probability distributions of coarseâ€“grained variables flow toward a nonâ€“trivial fixed form at the critical point, and toward a Gaussian otherwise. We can see this by coarseâ€“graining in real space (Fig 39C) or via momentum shells (Fig 39D). Nicoletti et al. (2020) emphasize that the phenomenological RG can identify critical points unambiguously, but only if we check the full range of behaviors.\nä¹Ÿè®¸ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿è¿œç¦»ä¸´ç•Œç‚¹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨æŸäº›é‡ä¸­çœ‹åˆ°ï¼ˆå¼±ï¼‰éå¹³å‡¡çš„ç¼©æ”¾è¡Œä¸ºï¼Œå¦‚å›¾ 39A æ‰€ç¤ºçš„æ´»åŠ¨æ–¹å·®ã€‚ä½†å…¶ä»–é‡å³ä½¿éå¸¸æ¥è¿‘ä¸´ç•Œæ€§ä¹Ÿæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„åç¦»ç¼©æ”¾è¡Œä¸ºï¼Œå¦‚å›¾ 39B æ‰€ç¤ºçš„ç›¸å…³æ—¶é—´ã€‚æ˜ç¡®æ— è¯¯çš„æ˜¯ï¼Œåœ¨ä¸´ç•Œç‚¹å¤„ï¼Œç²—ç²’åŒ–å˜é‡çš„æ¦‚ç‡åˆ†å¸ƒæœç€éå¹³å‡¡çš„å›ºå®šå½¢å¼æµåŠ¨ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹åˆ™æœç€é«˜æ–¯å½¢å¼æµåŠ¨ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å®ç©ºé—´ç²—ç²’åŒ–ï¼ˆå›¾ 39Cï¼‰æˆ–é€šè¿‡åŠ¨é‡å£³ï¼ˆå›¾ 39Dï¼‰æ¥çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚Nicoletti ç­‰äººï¼ˆ2020ï¼‰å¼ºè°ƒï¼Œç°è±¡å­¦ RG å¯ä»¥æ˜ç¡®åœ°è¯†åˆ«ä¸´ç•Œç‚¹ï¼Œä½†å‰ææ˜¯æˆ‘ä»¬æ£€æŸ¥äº†å…¨èŒƒå›´çš„è¡Œä¸ºã€‚\nFIG. 39 Coarse-graining of the contact process (Nicoletti et al., 2020). (A) Variance of activity vs. the scale of coarseâ€” graining in real space, as in Figs 34A and 37. Behavior at criticality (blue) is clearly different from the super-â€”critical case (red), which departs systematically but weakly from the expectations for independent variables (dashed lines). (B) Correlation time vs. the scale of coarse-graining in real space, as in Fig 35. The control parameter is set close to its critical value, and we see hints of scaling at small $K$ but clear departures at large $K$. (C) Distribution of individual coarseâ€” grained variables for $K = 32,64, 128, 256$ at criticality (blue) and away from criticality (red). In both cases we see flow toward a fixed distribution, but away from criticality this is Gaussian as expected from the central limit theorem. (D) As in (C), but with coarse-graning via momentum shells, keeping $N/8, N/16, N/32, N/64, N/128$ of the modes.\nå›¾ 39 æ¥è§¦è¿‡ç¨‹çš„ç²—ç²’åŒ–ï¼ˆNicoletti ç­‰äººï¼Œ2020ï¼‰ã€‚(A) å®ç©ºé—´ä¸­ç²—ç²’åŒ–å°ºåº¦ä¸æ´»åŠ¨æ–¹å·®çš„å…³ç³»ï¼Œå¦‚å›¾ 34A å’Œ 37 æ‰€ç¤ºã€‚ä¸´ç•Œæ€§ä¸‹çš„è¡Œä¸ºï¼ˆè“è‰²ï¼‰æ˜æ˜¾ä¸åŒäºè¶…ä¸´ç•Œæƒ…å†µï¼ˆçº¢è‰²ï¼‰ï¼Œåè€…ç³»ç»Ÿåœ°ä½†å¼±åœ°åç¦»äº†ç‹¬ç«‹å˜é‡çš„é¢„æœŸï¼ˆè™šçº¿ï¼‰ã€‚(B) å®ç©ºé—´ä¸­ç²—ç²’åŒ–å°ºåº¦ä¸ç›¸å…³æ—¶é—´çš„å…³ç³»ï¼Œå¦‚å›¾ 35 æ‰€ç¤ºã€‚æ§åˆ¶å‚æ•°è®¾ç½®æ¥è¿‘å…¶ä¸´ç•Œå€¼ï¼Œæˆ‘ä»¬åœ¨å° $K$ å¤„çœ‹åˆ°ç¼©æ”¾çš„è¿¹è±¡ï¼Œä½†åœ¨å¤§ $K$ å¤„æ˜æ˜¾åç¦»ã€‚(C) ä¸´ç•Œæ€§ä¸‹ï¼ˆè“è‰²ï¼‰å’Œè¿œç¦»ä¸´ç•Œæ€§ï¼ˆçº¢è‰²ï¼‰æ—¶ï¼Œ$K = 32,64, 128, 256$ çš„å•ä¸ªç²—ç²’åŒ–å˜é‡çš„åˆ†å¸ƒã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½çœ‹åˆ°äº†æœç€å›ºå®šåˆ†å¸ƒçš„æµåŠ¨ï¼Œä½†è¿œç¦»ä¸´ç•Œæ€§æ—¶ï¼Œè¿™ç¬¦åˆä¸­å¿ƒæé™å®šç†æ‰€é¢„æœŸçš„é«˜æ–¯åˆ†å¸ƒã€‚(D) ä¸ (C) ç±»ä¼¼ï¼Œä½†é€šè¿‡åŠ¨é‡å£³è¿›è¡Œç²—ç²’åŒ–ï¼Œä¿ç•™ $N/8, N/16, N/32, N/64, N/128$ ä¸ªæ¨¡å¼ã€‚\nAs with the (related) discussion of criticality in $VI.D, it has been suggested that some of the phenomena uncovered by iterative coarse-graining can be reproduced in a model where neurons respond independently to latent fields (Morrell et al., 2021). In this view, scaling and the flow toward fixed distributions are approximate, and it is not clear why scaling exponents should be reproducible across animals; a broader notion of universality, as in Fig 37, would be even more difhicult to understand.\næ­£å¦‚åœ¨ VI.D ä¸­ï¼ˆç›¸å…³ï¼‰å…³äºä¸´ç•Œæ€§çš„è®¨è®ºä¸­æ‰€æåˆ°çš„é‚£æ ·ï¼Œæœ‰äººå»ºè®®é€šè¿‡è¿­ä»£ç²—ç²’åŒ–å‘ç°çš„ä¸€äº›ç°è±¡å¯ä»¥åœ¨ç¥ç»å…ƒç‹¬ç«‹å“åº”æ½œåœ¨åœºçš„æ¨¡å‹ä¸­å†ç°ï¼ˆMorrell ç­‰äººï¼Œ2021ï¼‰ã€‚åœ¨è¿™ç§è§‚ç‚¹ä¸­ï¼Œç¼©æ”¾å’Œæœå‘å›ºå®šåˆ†å¸ƒçš„æµåŠ¨æ˜¯è¿‘ä¼¼çš„ï¼Œå¹¶ä¸”ä¸æ¸…æ¥šä¸ºä»€ä¹ˆç¼©æ”¾æŒ‡æ•°åº”è¯¥åœ¨åŠ¨ç‰©ä¹‹é—´æ˜¯å¯é‡å¤çš„ï¼›å¦‚å›¾ 37 æ‰€ç¤ºï¼Œæ›´å¹¿æ³›çš„æ™®é€‚æ€§æ¦‚å¿µå°†æ›´éš¾ç†è§£ã€‚\nCertainly the suggestion that scaling behaviors emerge generically from latent variable models is incorrect. Consider models in which the effective field acting on each neuron $i$ is a linear combination of $K$ latent variables drawn from a Gaussian distribution. If the fields are weak then the covariance matrix of neural activity has the same rank as the covariance matrix of the fields. This simple result breaks down at stronger fields, but even in the limit of infinitely strong fields there remains a gap in the eigenvalue spectrum of the covariance matrix, at least for typical choices of parameters, so that it is impossible to recover precise scaling behaviors.\næ¯«æ— ç–‘é—®ï¼Œç¼©æ”¾è¡Œä¸ºä»æ½œåœ¨å˜é‡æ¨¡å‹ä¸­æ™®éå‡ºç°çš„å»ºè®®æ˜¯é”™è¯¯çš„ã€‚è€ƒè™‘è¿™æ ·ä¸€ç§æ¨¡å‹ï¼Œå…¶ä¸­ä½œç”¨åœ¨æ¯ä¸ªç¥ç»å…ƒ $i$ ä¸Šçš„æœ‰æ•ˆåœºæ˜¯ä»é«˜æ–¯åˆ†å¸ƒä¸­æŠ½å–çš„ $K$ ä¸ªæ½œåœ¨å˜é‡çš„çº¿æ€§ç»„åˆã€‚å¦‚æœåœºæ˜¯å¼±çš„ï¼Œé‚£ä¹ˆç¥ç»æ´»åŠ¨çš„åæ–¹å·®çŸ©é˜µä¸åœºçš„åæ–¹å·®çŸ©é˜µå…·æœ‰ç›¸åŒçš„ç§©ã€‚è¿™ä¸ªç®€å•çš„ç»“æœåœ¨æ›´å¼ºçš„åœºä¸‹å¤±æ•ˆï¼Œä½†å³ä½¿åœ¨æ— é™å¼ºåœºçš„æé™ä¸‹ï¼Œåæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼è°±ä¸­ä»ç„¶å­˜åœ¨ä¸€ä¸ªé—´éš™ï¼Œè‡³å°‘å¯¹äºå…¸å‹å‚æ•°é€‰æ‹©æ¥è¯´æ˜¯è¿™æ ·ï¼Œå› æ­¤ä¸å¯èƒ½æ¢å¤ç²¾ç¡®çš„ç¼©æ”¾è¡Œä¸ºã€‚\nWe note that a concrete, biologically motivated model of latent fieldsâ€”the independent place cell model discussed in Â§VI.Dâ€”fails to exhibit scaling (Meshulam et al., 2018). This result perhaps should not be surprising. In a population of place cells, there are two length scales, the approximate width of the place fields and the mean distance between place field centers. In the oneâ€“dimensional (virtual) environment that provides the background for the hippocampal experiments analyzed here, the ratio of these lengths gives us a characteristic number of neurons, $K_{c}\\sim 18$. Indeed, analyses of the independent place cell model corresponding to Figs 34A, B show â€œbreaksâ€ at $K \\sim K_{c}$. While these are approximate statements, they highlight the fact that, in the presence of such obvious scales, the observation of rather precise powerâ€“law scaling in both static and dynamic quantities really is surprising.\næˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå…·ä½“çš„ã€ç”Ÿç‰©å­¦åŠ¨æœºçš„æ½œåœ¨åœºæ¨¡å‹â€”â€”Â§VI.D ä¸­è®¨è®ºçš„ç‹¬ç«‹ä½ç½®ç»†èƒæ¨¡å‹â€”â€”æœªèƒ½è¡¨ç°å‡ºç¼©æ”¾è¡Œä¸ºï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼‰ã€‚è¿™ä¸ªç»“æœæˆ–è®¸å¹¶ä¸ä»¤äººæƒŠè®¶ã€‚åœ¨ä½ç½®ç»†èƒç¾¤ä½“ä¸­ï¼Œæœ‰ä¸¤ä¸ªé•¿åº¦å°ºåº¦ï¼Œå³ä½ç½®åœºçš„è¿‘ä¼¼å®½åº¦å’Œä½ç½®åœºä¸­å¿ƒä¹‹é—´çš„å¹³å‡è·ç¦»ã€‚åœ¨ä¸ºæ­¤å¤„åˆ†æçš„æµ·é©¬ä½“å®éªŒæä¾›èƒŒæ™¯çš„ä¸€ç»´ï¼ˆè™šæ‹Ÿï¼‰ç¯å¢ƒä¸­ï¼Œè¿™äº›é•¿åº¦çš„æ¯”ç‡ç»™äº†æˆ‘ä»¬ä¸€ä¸ªç‰¹å¾ç¥ç»å…ƒæ•°ï¼Œ$K_{c}\\sim 18$ã€‚å®é™…ä¸Šï¼Œå¯¹åº”äºå›¾ 34Aã€B çš„ç‹¬ç«‹ä½ç½®ç»†èƒæ¨¡å‹çš„åˆ†ææ˜¾ç¤ºåœ¨ $K \\sim K_{c}$ å¤„æœ‰â€œæ–­ç‚¹â€ã€‚è™½ç„¶è¿™äº›éƒ½æ˜¯è¿‘ä¼¼é™ˆè¿°ï¼Œä½†å®ƒä»¬çªæ˜¾äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼šåœ¨å­˜åœ¨å¦‚æ­¤æ˜æ˜¾å°ºåº¦çš„æƒ…å†µä¸‹ï¼Œåœ¨é™æ€å’ŒåŠ¨æ€é‡ä¸­è§‚å¯Ÿåˆ°ç›¸å½“ç²¾ç¡®çš„å¹‚å¾‹ç¼©æ”¾ç¡®å®ä»¤äººæƒŠè®¶ã€‚\nFaced with highâ€“dimensional observations, a natural reaction is to search for a lower dimensional description. In some sense the renormalization group is the opposite approach (Bradde and Bialek, 2017). Rather than looking for the correct number of dimensions onto which to project the data, the RG invites us to examine how our description changes as we move the boundary between details that we ignore and features that we keep. Things simplify not because we have fewer degrees of freedom but because the model describing these degrees of freedom flows toward something simpler and more universal. The evidence thus far points toward the existence of such a simplified description. From the theoretical side, initial efforts at an RG analysis of models for networks of more realistic neurons suggest that these are described by new universality classes (Brinkman, 2023).\né¢å¯¹é«˜ç»´è§‚å¯Ÿï¼Œä¸€ä¸ªè‡ªç„¶çš„ååº”æ˜¯å¯»æ‰¾ä¸€ä¸ªä½ç»´æè¿°ã€‚åœ¨æŸç§æ„ä¹‰ä¸Šï¼Œé‡æ•´åŒ–ç¾¤æ˜¯ç›¸åçš„æ–¹æ³•ï¼ˆBradde å’Œ Bialekï¼Œ2017ï¼‰ã€‚ä¸å¯»æ‰¾æ­£ç¡®çš„ç»´æ•°ä»¥æŠ•å½±æ•°æ®ä¸åŒï¼ŒRG é‚€è¯·æˆ‘ä»¬æ£€æŸ¥å½“æˆ‘ä»¬ç§»åŠ¨å¿½ç•¥çš„ç»†èŠ‚å’Œä¿ç•™çš„ç‰¹å¾ä¹‹é—´çš„è¾¹ç•Œæ—¶ï¼Œæˆ‘ä»¬çš„æè¿°å¦‚ä½•å˜åŒ–ã€‚äº‹æƒ…ç®€åŒ–äº†ï¼Œä¸æ˜¯å› ä¸ºæˆ‘ä»¬æœ‰æ›´å°‘çš„è‡ªç”±åº¦ï¼Œè€Œæ˜¯å› ä¸ºæè¿°è¿™äº›è‡ªç”±åº¦çš„æ¨¡å‹æœç€æ›´ç®€å•ã€æ›´æ™®éçš„ä¸œè¥¿æµåŠ¨ã€‚åˆ°ç›®å‰ä¸ºæ­¢çš„è¯æ®æŒ‡å‘è¿™æ ·ä¸€ç§ç®€åŒ–æè¿°çš„å­˜åœ¨ã€‚ä»ç†è®ºæ–¹é¢æ¥çœ‹ï¼Œå¯¹æ›´ç°å®ç¥ç»å…ƒç½‘ç»œæ¨¡å‹è¿›è¡Œ RG åˆ†æçš„åˆæ­¥åŠªåŠ›è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹ç”±æ–°çš„æ™®é€‚ç±»æè¿°ï¼ˆBrinkmanï¼Œ2023ï¼‰ã€‚\nWhat we have not emphasized here is the connection of coarseâ€“graining to more functional behaviors. In the hippocampus, how is position represented in the coarsegrained variables? More generally, do fineâ€“grained and coarseâ€“grained variables implement different principles for the encoding of the sensory world (Munn et al., 2024)? Can local networks of neurons access different scaling trajectories as the brain switches among different global states (Castro et al., 2024)? As coarseâ€“graining becomes a more commonly used tool for the analysis of large scale neural recordings, we expect progress on these issues over the next years.\næˆ‘ä»¬åœ¨è¿™é‡Œæ²¡æœ‰å¼ºè°ƒçš„æ˜¯ç²—ç²’åŒ–ä¸æ›´åŠŸèƒ½æ€§è¡Œä¸ºçš„è”ç³»ã€‚åœ¨æµ·é©¬ä½“ä¸­ï¼Œä½ç½®å¦‚ä½•åœ¨ç²—ç²’åŒ–å˜é‡ä¸­è¡¨ç¤ºï¼Ÿæ›´ä¸€èˆ¬åœ°è¯´ï¼Œç»†ç²’åŒ–å’Œç²—ç²’åŒ–å˜é‡æ˜¯å¦å®ç°äº†ç¼–ç æ„Ÿå®˜ä¸–ç•Œçš„ä¸åŒåŸåˆ™ï¼ˆMunn ç­‰äººï¼Œ2024ï¼‰ï¼Ÿå½“å¤§è„‘åœ¨ä¸åŒçš„å…¨å±€çŠ¶æ€ä¹‹é—´åˆ‡æ¢æ—¶ï¼Œç¥ç»å…ƒçš„å±€éƒ¨ç½‘ç»œèƒ½å¦è®¿é—®ä¸åŒçš„ç¼©æ”¾è½¨è¿¹ï¼ˆCastro ç­‰äººï¼Œ2024ï¼‰ï¼Ÿéšç€ç²—ç²’åŒ–æˆä¸ºåˆ†æå¤§è§„æ¨¡ç¥ç»è®°å½•ä¸­æ›´å¸¸ç”¨çš„å·¥å…·ï¼Œæˆ‘ä»¬é¢„è®¡åœ¨æœªæ¥å‡ å¹´å†…åœ¨è¿™äº›é—®é¢˜ä¸Šä¼šå–å¾—è¿›å±•ã€‚\nThe most detailed tests of scaling in equilibrium critical phenomena span six decades with better than one percent precision (Lipa et al., 1996). As described in Â§Â§III.B and III.C, the experimental frontier is moving toward recording from $\\sim 10^{6}$ neurons simultaneously. This opens the possibility of following coarseâ€“graining trajectories across five decades with single cell resolution, and of driving error bars down to the one percent level across more limited ranges. The extension of existing tools to organisms with larger brains also means that we will see simultaneous recordings from more neurons in single brain areas, within which scaling seems more likely. We already see signs that quantities which emerge from these analyses can be reproducible in the second decimal place. One possibility is that new, larger experiments will reveal crossovers between different regimes on different scales. Alternatively, the scaling behaviors seen thus far might prove to be essentially exact. Whatever the outcome, it is extraordinary to think that experiments on real, functioning brains could soon reach a precision comparable to those on equilibrium critical phenomena. The corresponding challenge to theory should be clear.\nåœ¨å¹³è¡¡ä¸´ç•Œç°è±¡ä¸­ï¼Œå¯¹ç¼©æ”¾çš„æœ€è¯¦ç»†æµ‹è¯•è·¨è¶Šäº†å…­ä¸ªæ•°é‡çº§ï¼Œç²¾åº¦ä¼˜äºç™¾åˆ†ä¹‹ä¸€ï¼ˆLipa ç­‰äººï¼Œ1996ï¼‰ã€‚å¦‚ Â§Â§III.B å’Œ III.C æ‰€è¿°ï¼Œå®éªŒå‰æ²¿æ­£æœç€åŒæ—¶è®°å½• $\\sim 10^{6}$ ä¸ªç¥ç»å…ƒçš„æ–¹å‘å‘å±•ã€‚è¿™ä¸ºæˆ‘ä»¬æ‰“å¼€äº†ä¸€ä¸ªå¯èƒ½æ€§ï¼Œå¯ä»¥åœ¨å•ç»†èƒåˆ†è¾¨ç‡ä¸‹è·¨è¶Šäº”ä¸ªæ•°é‡çº§è·Ÿè¸ªç²—ç²’åŒ–è½¨è¿¹ï¼Œå¹¶å°†è¯¯å·®æ¡é™ä½åˆ°æ›´æœ‰é™èŒƒå›´å†…çš„ç™¾åˆ†ä¹‹ä¸€æ°´å¹³ã€‚å°†ç°æœ‰å·¥å…·æ‰©å±•åˆ°å…·æœ‰æ›´å¤§è„‘å®¹é‡çš„ç”Ÿç‰©ä½“ä¹Ÿæ„å‘³ç€æˆ‘ä»¬å°†åœ¨å•ä¸ªå¤§è„‘åŒºåŸŸä¸­çœ‹åˆ°æ›´å¤šç¥ç»å…ƒçš„åŒæ—¶è®°å½•ï¼Œåœ¨è¿™äº›åŒºåŸŸä¸­ç¼©æ”¾ä¼¼ä¹æ›´æœ‰å¯èƒ½ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°è¿™äº›åˆ†æä¸­å‡ºç°çš„é‡å¯ä»¥åœ¨å°æ•°ç‚¹åç¬¬äºŒä½ä¸Šé‡å¤çš„è¿¹è±¡ã€‚ä¸€ç§å¯èƒ½æ€§æ˜¯ï¼Œæ–°çš„ã€æ›´å¤§çš„å®éªŒå°†æ­ç¤ºä¸åŒå°ºåº¦ä¸Šä¸åŒæœºåˆ¶ä¹‹é—´çš„äº¤å‰ã€‚æˆ–è€…ï¼Œåˆ°ç›®å‰ä¸ºæ­¢çœ‹åˆ°çš„ç¼©æ”¾è¡Œä¸ºå¯èƒ½è¢«è¯æ˜æ˜¯åŸºæœ¬ä¸Šæ˜¯ç²¾ç¡®çš„ã€‚æ— è®ºç»“æœå¦‚ä½•ï¼Œæƒ³åˆ°å¯¹çœŸå®ã€åŠŸèƒ½æ€§å¤§è„‘çš„å®éªŒå¾ˆå¿«å°±èƒ½è¾¾åˆ°ä¸å¹³è¡¡ä¸´ç•Œç°è±¡ç›¸å½“çš„ç²¾åº¦ï¼Œè¿™éƒ½æ˜¯éå‡¡çš„ã€‚å¯¹ç†è®ºçš„ç›¸åº”æŒ‘æˆ˜åº”è¯¥æ˜¯æ˜ç¡®çš„ã€‚\n",
  "wordCount" : "18366",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png","datePublished": "2025-11-12T00:18:23+08:00",
  "dateModified": "2025-11-12T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "æ— å¤„æƒ¹å°˜åŸƒ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿— (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿—</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/">ğŸ“• é˜…è¯»</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/">ğŸ“• æ–‡çŒ®</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/">ğŸ“• Statistical mechanics for networks of real neurons</a></div>
            <h1 class="post-title">
                Renormalization group for neurons
            </h1>
            <div class="post-description">
                çœŸå®ç¥ç»å…ƒç½‘ç»œçš„ç»Ÿè®¡åŠ›å­¦
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-11-12
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>18366å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>37åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#taking-inspiration-from-the-rg" aria-label="Taking inspiration from the RG">Taking inspiration from the RG</a></li>
                <li>
                    <a href="#by-analogy-with-realspace-methods" aria-label="By analogy with realâ€“space methods">By analogy with realâ€“space methods</a></li>
                <li>
                    <a href="#by-analogy-with-momentum-shell-methods" aria-label="By analogy with momentum shell methods">By analogy with momentum shell methods</a></li>
                <li>
                    <a href="#rg-as-a-path-to-understanding" aria-label="RG as a path to understanding">RG as a path to understanding</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><blockquote>
<p>Physicists are known for our appreciation of simplified models, perhaps even to the point of overâ€“simplification (Devine and Cohen, 1992). The complexity of living systems is in obvious tension with this drive for simplification; we can perhaps sympathize with biologists who worry that our theoretical impulses may be mismatched to the richness of lifeâ€™s molecular details. A useful response is that there is nothing special about biology: in condensed matter physics and statistical mechanics we routinely describe the macroscopic behavior of materials using models that are much simpler than the underlying microscopic mechanisms. These simplified models succeed, not because we are lucky but because of the <strong>renormalization group</strong> (Wilson, 1979, 1983).</p>
</blockquote>
<p>ç‰©ç†å­¦å®¶ä»¥æ¬£èµç®€åŒ–æ¨¡å‹è€Œé—»åï¼Œç”šè‡³å¯èƒ½è¿‡äºç®€åŒ–ï¼ˆDevine å’Œ Cohenï¼Œ1992ï¼‰ã€‚ç”Ÿå‘½ç³»ç»Ÿçš„å¤æ‚æ€§ä¸è¿™ç§ç®€åŒ–çš„é©±åŠ¨åŠ›æ˜æ˜¾å­˜åœ¨çŸ›ç›¾ï¼›æˆ‘ä»¬æˆ–è®¸å¯ä»¥åŒæƒ…é‚£äº›æ‹…å¿ƒæˆ‘ä»¬çš„ç†è®ºå†²åŠ¨å¯èƒ½ä¸ç”Ÿå‘½ä¸°å¯Œçš„åˆ†å­ç»†èŠ‚ä¸åŒ¹é…çš„ç”Ÿç‰©å­¦å®¶ã€‚ä¸€ç§æœ‰ç”¨çš„å›åº”æ˜¯ï¼Œç”Ÿç‰©å­¦å¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ï¼šåœ¨å‡èšæ€ç‰©ç†å­¦å’Œç»Ÿè®¡åŠ›å­¦ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨æ¯”åº•å±‚å¾®è§‚æœºåˆ¶ç®€å•å¾—å¤šçš„æ¨¡å‹æ¥æè¿°ææ–™çš„å®è§‚è¡Œä¸ºã€‚è¿™äº›ç®€åŒ–æ¨¡å‹ä¹‹æ‰€ä»¥æˆåŠŸï¼Œä¸æ˜¯å› ä¸ºæˆ‘ä»¬å¹¸è¿ï¼Œè€Œæ˜¯å› ä¸º<strong>é‡æ•´åŒ–ç¾¤</strong>ï¼ˆWilsonï¼Œ1979ï¼Œ1983ï¼‰ã€‚</p>
<blockquote>
<p>The central idea of the renormalization group (RG) is to ask how our description of a system changes, systematically, as we change the scale on which we look. The crucial qualitative result is that many different microscopic mechanisms flow toward the same macroscopic behavior as we â€œzoom outâ€ to look at longer length scales. This means that we can understand large scale phenomena quantitatively if we can assign them to the correct universality class, even if we canâ€™t get all the small scale details right, and this gives us license to write relatively simple models of complex systems (Anderson, 1984). We would like to exercise this license in the context of the brain. To do this we need to understand how to implement the RG when many of our usual guides (locality, symmetry, $\cdots$) are absent. We then can ask whether there is any sign that simplification emerges from the data as we zoom out from individual neurons to more coarseâ€“grained variables.</p>
</blockquote>
<p>é‡æ•´åŒ–ç¾¤ï¼ˆRGï¼‰çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå½“æˆ‘ä»¬æ”¹å˜è§‚å¯Ÿç³»ç»Ÿçš„å°ºåº¦æ—¶ï¼Œæˆ‘ä»¬å¯¹ç³»ç»Ÿçš„æè¿°å¦‚ä½•ç³»ç»Ÿåœ°å˜åŒ–ã€‚ä¸€ä¸ªå…³é”®çš„å®šæ€§ç»“æœæ˜¯ï¼Œéšç€æˆ‘ä»¬â€œæ”¾å¤§â€ä»¥è§‚å¯Ÿæ›´é•¿çš„é•¿åº¦å°ºåº¦ï¼Œè®¸å¤šä¸åŒçš„å¾®è§‚æœºåˆ¶ä¼šè¶‹å‘äºç›¸åŒçš„å®è§‚è¡Œä¸ºã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿå°†å¤§è§„æ¨¡ç°è±¡å½’ç±»åˆ°æ­£ç¡®çš„æ™®é€‚ç±»ä¸­ï¼Œå³ä½¿æˆ‘ä»¬æ— æ³•æ­£ç¡®å¤„ç†æ‰€æœ‰å°è§„æ¨¡ç»†èŠ‚ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å®šé‡åœ°ç†è§£å¤§è§„æ¨¡ç°è±¡ï¼Œè¿™èµ‹äºˆäº†æˆ‘ä»¬ç¼–å†™å¤æ‚ç³»ç»Ÿç›¸å¯¹ç®€å•æ¨¡å‹çš„è®¸å¯ï¼ˆAndersonï¼Œ1984ï¼‰ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨å¤§è„‘çš„èƒŒæ™¯ä¸‹è¡Œä½¿è¿™ç§è®¸å¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£å¦‚ä½•åœ¨ç¼ºå°‘è®¸å¤šé€šå¸¸æŒ‡å¯¼ï¼ˆå±€éƒ¨æ€§ã€å¯¹ç§°æ€§ç­‰ï¼‰çš„æƒ…å†µä¸‹å®ç° RGã€‚ç„¶åæˆ‘ä»¬å¯ä»¥é—®ï¼Œå½“æˆ‘ä»¬ä»å•ä¸ªç¥ç»å…ƒæ”¾å¤§åˆ°æ›´ç²—ç²’åº¦çš„å˜é‡æ—¶ï¼Œæ•°æ®ä¸­æ˜¯å¦æœ‰ä»»ä½•è¿¹è±¡è¡¨æ˜ç®€åŒ–å‡ºç°äº†ã€‚</p>
<h1 id="taking-inspiration-from-the-rg">Taking inspiration from the RG<a hidden class="anchor" aria-hidden="true" href="#taking-inspiration-from-the-rg">#</a></h1>
<blockquote>
<p>The development of the renormalization group is one the great chapters of theoretical physics from the second half of the twentieth century, with origins in efforts to understand matter at both short and long distances (Gell-Mann and Low, 1954; Kadanoff, 1966). These ideas crystallized in the early 1970s and played a central role in revolutionizing our understanding of the strong interaction among elementary particles, critical phenomena at second order phase transitions, the transition to chaos, and more (Wilson, 1983). How can these ideas help us to think about networks of neurons?</p>
</blockquote>
<p>é‡æ•´åŒ–ç¾¤çš„å‘å±•æ˜¯äºŒåä¸–çºªä¸‹åŠå¶ç†è®ºç‰©ç†å­¦çš„ä¼Ÿå¤§ç¯‡ç« ä¹‹ä¸€ï¼Œå…¶èµ·æºåœ¨äºç†è§£ç‰©è´¨åœ¨çŸ­è·ç¦»å’Œé•¿è·ç¦»ä¸Šçš„è¡Œä¸ºï¼ˆGell-Mann å’Œ Lowï¼Œ1954ï¼›Kadanoffï¼Œ1966ï¼‰ã€‚è¿™äº›æ€æƒ³åœ¨1970å¹´ä»£åˆæœŸå¾—ä»¥ç»“æ™¶ï¼Œå¹¶åœ¨å½»åº•æ”¹å˜æˆ‘ä»¬å¯¹åŸºæœ¬ç²’å­ä¹‹é—´å¼ºç›¸äº’ä½œç”¨ã€äºŒé˜¶ç›¸å˜çš„ä¸´ç•Œç°è±¡ã€å‘æ··æ²Œçš„è¿‡æ¸¡ç­‰æ–¹é¢çš„ç†è§£ä¸­å‘æŒ¥äº†æ ¸å¿ƒä½œç”¨ï¼ˆWilsonï¼Œ1983ï¼‰ã€‚è¿™äº›æ€æƒ³å¦‚ä½•å¸®åŠ©æˆ‘ä»¬æ€è€ƒç¥ç»å…ƒç½‘ç»œï¼Ÿ</p>
<blockquote>
<p>In the standard formulation of the RG for statistical physics we start with a set of variables $z_{l_{0}}\equiv \{z_{i}(l_{0})\}$ defined on some microscopic length scale $l_{0}$. Our description of these variables is given by a Hamiltonian that in turns specifies the Boltzmann distribution $P_{l_{0}}(z)$, or perhaps we will be interested in the dynamics generated by this Hamiltonian. We then imagine â€œcoarseâ€“grainingâ€ the variables to average out the details on length scales below some $l &gt; l_{0}$. The result is a new set of variables $z_{l}$, and we can ask for the effective Hamiltonian that governs these variables. If we think of the Hamiltonian as being built from different kinds of interactions, it becomes natural to say that the effective strengths of these interactions has changed as change scale from $l_{0}$ to $l$, and the RG invites us to follow this flow as we change $l$. Although this flow of interaction strengths or running of coupling constants often is the goal an RG analysis, it was emphasized early on by Jona-Lasinio (1975) that we can think more generally about flow in the space of probability distributions $P_{l}(z)$, leaving aside any reference to Hamiltonians.</p>
</blockquote>
<p>åœ¨ç»Ÿè®¡ç‰©ç†å­¦çš„ RG çš„æ ‡å‡†è¡¨è¿°ä¸­ï¼Œæˆ‘ä»¬ä»ä¸€ç»„å®šä¹‰åœ¨æŸä¸ªå¾®è§‚é•¿åº¦å°ºåº¦ $l_{0}$ ä¸Šçš„å˜é‡ $z_{l_{0}}\equiv \{z_{i}(l_{0})\}$ å¼€å§‹ã€‚è¿™äº›å˜é‡çš„æè¿°ç”±ä¸€ä¸ªå“ˆå¯†é¡¿é‡ç»™å‡ºï¼Œè¯¥å“ˆå¯†é¡¿é‡åè¿‡æ¥æŒ‡å®šäº†ç»å°”å…¹æ›¼åˆ†å¸ƒ $P_{l_{0}}(z)$ï¼Œæˆ–è€…æˆ‘ä»¬å¯èƒ½ä¼šå¯¹è¯¥å“ˆå¯†é¡¿é‡ç”Ÿæˆçš„åŠ¨åŠ›å­¦æ„Ÿå…´è¶£ã€‚ç„¶åæˆ‘ä»¬æƒ³è±¡â€œç²—ç²’åŒ–â€è¿™äº›å˜é‡ï¼Œä»¥å¹³å‡æ‰æŸä¸ª $l &gt; l_{0}$ ä»¥ä¸‹é•¿åº¦å°ºåº¦çš„ç»†èŠ‚ã€‚ç»“æœæ˜¯ä¸€ç»„æ–°çš„å˜é‡ $z_{l}$ï¼Œæˆ‘ä»¬å¯ä»¥è¯¢é—®æ”¯é…è¿™äº›å˜é‡çš„ç­‰æ•ˆå“ˆå¯†é¡¿é‡ã€‚å¦‚æœæˆ‘ä»¬è®¤ä¸ºå“ˆå¯†é¡¿é‡æ˜¯ç”±ä¸åŒç±»å‹çš„ç›¸äº’ä½œç”¨æ„å»ºçš„ï¼Œé‚£ä¹ˆå½“æˆ‘ä»¬å°†å°ºåº¦ä» $l_{0}$ æ”¹å˜åˆ° $l$ æ—¶ï¼Œè¿™äº›ç›¸äº’ä½œç”¨çš„æœ‰æ•ˆå¼ºåº¦å‘ç”Ÿäº†å˜åŒ–ï¼Œè¿™å°±å¾ˆè‡ªç„¶åœ°è¯´ï¼ŒRG å¼•å¯¼æˆ‘ä»¬åœ¨æ”¹å˜ $l$ æ—¶è·Ÿè¸ªè¿™ç§æµåŠ¨ã€‚å°½ç®¡è¿™ç§ç›¸äº’ä½œç”¨å¼ºåº¦çš„æµåŠ¨æˆ–è€¦åˆå¸¸æ•°çš„è¿è¡Œé€šå¸¸æ˜¯ RG åˆ†æçš„ç›®æ ‡ï¼Œä½† Jona-Lasinioï¼ˆ1975ï¼‰æ—©æœŸå¼ºè°ƒï¼Œæˆ‘ä»¬å¯ä»¥æ›´ä¸€èˆ¬åœ°è€ƒè™‘æ¦‚ç‡åˆ†å¸ƒ $P_{l}(z)$ ç©ºé—´ä¸­çš„æµåŠ¨ï¼Œè€Œä¸å‚è€ƒä»»ä½•å“ˆå¯†é¡¿é‡ã€‚</p>
<blockquote>
<p>An essential result of the renormalization group is that many different starting distributions $P_{l_{0}}(z)$ converge to the same $P_{l}(z)$ as $l$ becomes large. Along this trajectory parameters of the distribution exhibit simple scaling behaviors as a function of $l$. A familiar example is the central limit theorem, where if variables in $P_{l_{0}}(z)$ are sufficiently weakly correlated then $P_{l}(z)$ approaches a Gaussian as $l$ becomes large, and along the way the variances of the individual variables scale as $1/l$. The RG predicts that more interesting starting points can flow toward stable nonâ€“Gaussian distributions, with moments scaling as nonâ€“trivial powers of $l$.</p>
</blockquote>
<p>é‡æ•´åŒ–ç¾¤çš„ä¸€ä¸ªåŸºæœ¬ç»“æœæ˜¯ï¼Œéšç€ $l$ å˜å¤§ï¼Œè®¸å¤šä¸åŒçš„èµ·å§‹åˆ†å¸ƒ $P_{l_{0}}(z)$ ä¼šæ”¶æ•›åˆ°ç›¸åŒçš„ $P_{l}(z)$ã€‚åœ¨è¿™æ¡è½¨è¿¹ä¸Šï¼Œåˆ†å¸ƒçš„å‚æ•°è¡¨ç°å‡ºä½œä¸º $l$ å‡½æ•°çš„ç®€å•ç¼©æ”¾è¡Œä¸ºã€‚ä¸€ä¸ªç†Ÿæ‚‰çš„ä¾‹å­æ˜¯ä¸­å¿ƒæé™å®šç†ï¼Œå¦‚æœ $P_{l_{0}}(z)$ ä¸­çš„å˜é‡ç›¸å…³æ€§è¶³å¤Ÿå¼±ï¼Œé‚£ä¹ˆå½“ $l$ å˜å¤§æ—¶ï¼Œ$P_{l}(z)$ ä¼šè¶‹è¿‘äºé«˜æ–¯åˆ†å¸ƒï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œå•ä¸ªå˜é‡çš„æ–¹å·®æŒ‰ $1/l$ ç¼©æ”¾ã€‚RG é¢„æµ‹ï¼Œæ›´æœ‰è¶£çš„èµ·ç‚¹å¯ä»¥æµå‘ç¨³å®šçš„éé«˜æ–¯åˆ†å¸ƒï¼Œå…¶çŸ©æŒ‰ $l$ çš„éå¹³å‡¡å¹‚ç¼©æ”¾ã€‚</p>
<blockquote>
<p>The renormalization group approach provides a framework to understand how we can go from discrete Ising spins on a lattice to a description of smoothly varying local magnetization, or from the positions and momenta of individual molecules to the density of a fluid and the velocity of its flow. In these examples, the coarseâ€“graining operation is guided by symmetry and locality. Perhaps the most successful development of RG ideas in a biological context has been for flocks of birds and swarms of insects, where the ideas of symmetry and locality continue to be useful (Â§A.2). For networks of neurons, where connections can span distances encompassing thousands of cells, the principle of locality is less of a guide, and there are no obvious symmetries. How then do we choose a coarseâ€“graining strategy?</p>
</blockquote>
<p>é‡æ•´åŒ–ç¾¤æ–¹æ³•æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£å¦‚ä½•ä»æ™¶æ ¼ä¸Šçš„ç¦»æ•£ Ising è‡ªæ—‹è½¬å˜ä¸ºå¹³æ»‘å˜åŒ–çš„å±€éƒ¨ç£åŒ–æè¿°ï¼Œæˆ–è€…ä»å•ä¸ªåˆ†å­çš„ä½ç§»å’ŒåŠ¨é‡è½¬å˜ä¸ºæµä½“çš„å¯†åº¦åŠå…¶æµåŠ¨é€Ÿåº¦ã€‚åœ¨è¿™äº›ä¾‹å­ä¸­ï¼Œç²—ç²’åŒ–æ“ä½œæ˜¯ç”±å¯¹ç§°æ€§å’Œå±€éƒ¨æ€§æŒ‡å¯¼çš„ã€‚åœ¨ç”Ÿç‰©å­¦èƒŒæ™¯ä¸‹ï¼ŒRG æ€æƒ³æœ€æˆåŠŸçš„å‘å±•å¯èƒ½æ˜¯å¯¹äºé¸Ÿç¾¤å’Œæ˜†è™«ç¾¤ä½“ï¼Œåœ¨é‚£é‡Œå¯¹ç§°æ€§å’Œå±€éƒ¨æ€§çš„æ€æƒ³ä»ç„¶æœ‰ç”¨ï¼ˆÂ§A.2ï¼‰ã€‚å¯¹äºç¥ç»å…ƒç½‘ç»œï¼Œè¿æ¥å¯ä»¥è·¨è¶ŠåŒ…å«æ•°åƒä¸ªç»†èƒçš„è·ç¦»ï¼Œå±€éƒ¨æ€§åŸåˆ™ä¸å†æ˜¯æŒ‡å¯¼ï¼Œå¹¶ä¸”æ²¡æœ‰æ˜æ˜¾çš„å¯¹ç§°æ€§ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•é€‰æ‹©ç²—ç²’åŒ–ç­–ç•¥å‘¢ï¼Ÿ</p>
<blockquote>
<p>Perhaps a more serious problem in taking inspiration from the renormalization group is that the RG is formulated as an approach to understanding theories or models, taming the complexities of interactions among degrees of freedom at many scales. These theories of course make quantitative predictions for experiment, but in the absence of a well defined model it is not clear how to proceed. There is a recent start on renormalization group analysis of models for a network of moderately realistic spiking neurons (Brinkman, 2023), and we hope there will be more of this. But, keeping to the spirit of the discussion thus far, we want to ask: How can we use the RG to guide the analysis of emerging data on large populations of real neurons?</p>
</blockquote>
<p>ä¹Ÿè®¸ä»é‡æ•´åŒ–ç¾¤ä¸­è·å¾—çµæ„Ÿçš„ä¸€ä¸ªæ›´ä¸¥é‡çš„é—®é¢˜æ˜¯ï¼ŒRG è¢«è¡¨è¿°ä¸ºä¸€ç§ç†è§£ç†è®ºæˆ–æ¨¡å‹çš„æ–¹æ³•ï¼Œé©¯æœå¤šå°ºåº¦è‡ªç”±åº¦ä¹‹é—´ç›¸äº’ä½œç”¨çš„å¤æ‚æ€§ã€‚è¿™äº›ç†è®ºå½“ç„¶å¯¹å®éªŒåšå‡ºäº†å®šé‡é¢„æµ‹ï¼Œä½†åœ¨æ²¡æœ‰æ˜ç¡®å®šä¹‰æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä¸æ¸…æ¥šå¦‚ä½•ç»§ç»­ã€‚æœ€è¿‘å¼€å§‹å¯¹é€‚åº¦çœŸå®çš„å°–å³°ç¥ç»å…ƒç½‘ç»œæ¨¡å‹è¿›è¡Œé‡æ•´åŒ–ç¾¤åˆ†æï¼ˆBrinkmanï¼Œ2023ï¼‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä¼šæœ‰æ›´å¤šè¿™æ ·çš„å·¥ä½œã€‚ä½†ä¿æŒåˆ°ç›®å‰ä¸ºæ­¢è®¨è®ºçš„ç²¾ç¥ï¼Œæˆ‘ä»¬æƒ³é—®ï¼šæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ RG æ¥æŒ‡å¯¼å¯¹å¤§é‡çœŸå®ç¥ç»å…ƒç¾¤ä½“çš„æ–°å…´æ•°æ®çš„åˆ†æï¼Ÿ</p>
<blockquote>
<p>To address these challenges we rely on two key ideas. First, as emphasized above, modern experiments on the electrical activity in networks of neurons give us access to something analogous to the trajectory of a Monte Carlo simulation on a statistical physics model, albeit a model that we donâ€™t know how to write down. Thus we can follow the approach used in now classical analysis of such simulations, for example by Binder (1981): We start with raw data on the most microscopic scale, construct coarseâ€“grained variables, and follow various features of the distribution of thee variables as we change the scale of coarseâ€“graining.</p>
</blockquote>
<p>ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¾èµ–ä¸¤ä¸ªå…³é”®æ€æƒ³ã€‚é¦–å…ˆï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œå…³äºç¥ç»å…ƒç½‘ç»œç”µæ´»åŠ¨çš„ç°ä»£å®éªŒä½¿æˆ‘ä»¬èƒ½å¤Ÿè®¿é—®ç±»ä¼¼äºç»Ÿè®¡ç‰©ç†æ¨¡å‹ä¸Š Monte Carlo æ¨¡æ‹Ÿè½¨è¿¹çš„ä¸œè¥¿ï¼Œå°½ç®¡è¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬ä¸çŸ¥é“å¦‚ä½•å†™ä¸‹çš„æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥éµå¾ªç°åœ¨ç»å…¸çš„æ­¤ç±»æ¨¡æ‹Ÿåˆ†ææ–¹æ³•ï¼Œä¾‹å¦‚ Binderï¼ˆ1981ï¼‰ï¼šæˆ‘ä»¬ä»æœ€å¾®è§‚å°ºåº¦çš„åŸå§‹æ•°æ®å¼€å§‹ï¼Œæ„å»ºç²—ç²’åŒ–å˜é‡ï¼Œå¹¶éšç€ç²—ç²’åŒ–å°ºåº¦çš„å˜åŒ–ï¼Œè·Ÿè¸ªè¿™äº›å˜é‡åˆ†å¸ƒçš„å„ç§ç‰¹å¾ã€‚</p>
<blockquote>
<p>Second, we will use <strong>the measured pairwise correlations</strong> as guide to which neurons are â€œneighbors,â€ in the absence of locality (Bradde and Bialek, 2017). In one version (Â§VII.B), this involves averaging together the activities of the most correlated cells, building clusters of neurons that are analogous to block spins (Kadanoff, 1966). In another version (Â§VII.C), we successively filter out linear combinations of the population activity that make small contributions to the overall variance, and this is analogous to the momentum shell construction (Wilson, 1983). We will see that both these approaches uncover simple, precise, and reproducible scaling behaviors that now have been confirmed in multiple brain areas from multiple organisms. We then discuss the implications of these results and some future direction Â§VII.D.</p>
</blockquote>
<p>å…¶æ¬¡ï¼Œåœ¨ç¼ºä¹å±€éƒ¨æ€§çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ <strong>æµ‹é‡çš„æˆå¯¹ç›¸å…³æ€§</strong> ä½œä¸ºæŒ‡å¯¼ï¼Œç¡®å®šå“ªäº›ç¥ç»å…ƒæ˜¯â€œé‚»å±…â€ï¼ˆBradde å’Œ Bialekï¼Œ2017ï¼‰ã€‚åœ¨ä¸€ç§ç‰ˆæœ¬ä¸­ï¼ˆÂ§VII.Bï¼‰ï¼Œè¿™æ¶‰åŠå°†æœ€ç›¸å…³çš„ç»†èƒçš„æ´»åŠ¨å¹³å‡åœ¨ä¸€èµ·ï¼Œæ„å»ºç±»ä¼¼äºå—è‡ªæ—‹çš„ç¥ç»å…ƒç°‡ï¼ˆKadanoffï¼Œ1966ï¼‰ã€‚åœ¨å¦ä¸€ç§ç‰ˆæœ¬ä¸­ï¼ˆÂ§VII.Cï¼‰ï¼Œæˆ‘ä»¬è¿ç»­æ»¤é™¤å¯¹æ•´ä½“æ–¹å·®è´¡çŒ®è¾ƒå°çš„äººå£æ´»åŠ¨çš„çº¿æ€§ç»„åˆï¼Œè¿™ç±»ä¼¼äºåŠ¨é‡å£³æ„é€ ï¼ˆWilsonï¼Œ1983ï¼‰ã€‚æˆ‘ä»¬å°†çœ‹åˆ°ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½æ­ç¤ºäº†ç®€å•ã€ç²¾ç¡®ä¸”å¯é‡å¤çš„ç¼©æ”¾è¡Œä¸ºï¼Œè¿™äº›è¡Œä¸ºç°åœ¨å·²åœ¨å¤šä¸ªæœ‰æœºä½“çš„å¤šä¸ªå¤§è„‘åŒºåŸŸå¾—åˆ°ç¡®è®¤ã€‚ç„¶åæˆ‘ä»¬è®¨è®ºè¿™äº›ç»“æœçš„æ„ä¹‰å’Œä¸€äº›æœªæ¥çš„æ–¹å‘ Â§VII.Dã€‚</p>
<h1 id="by-analogy-with-realspace-methods">By analogy with realâ€“space methods<a hidden class="anchor" aria-hidden="true" href="#by-analogy-with-realspace-methods">#</a></h1>
<blockquote>
<p>Renormalization group methods in statistical physics rest on a notion of coarseâ€“graining, averaging over microscopic details. If we start with variables $\{z_{i}\}$ that live on a regular lattice, the it is natural to do this by combining variables with their neighbors, as in Fig 32. Formally we can write</p>
<p>$$
z_{i}\rightarrow \widetilde{z}_{i} = f\left(\sum_{j\in\mathcal{N}_{i}}z_{j}\right)
$$</p>
<p>where $\mathcal{N}_{i}$ is a neighborhood surrounding site $i$. If the function $f(\cdot)$ is linear then we are just averaging over a neighborhood, and for example this will lead from discrete Isingâ€“like variables to a more continuous local magnetization if we iterate. If $f(\cdot)$ is a threshold function then we can implement majority rule, so that clusters of Isingâ€“like variables are mapped into Isingâ€“like variables on the sparser lattice, as in the original block spin construction (Kadanoff, 1966).</p>
</blockquote>
<p>é‡æ•´åŒ–ç¾¤æ–¹æ³•åœ¨ç»Ÿè®¡ç‰©ç†å­¦ä¸­åŸºäºç²—ç²’åŒ–çš„æ¦‚å¿µï¼Œå³å¹³å‡å¾®è§‚ç»†èŠ‚ã€‚å¦‚æœæˆ‘ä»¬ä»ç”Ÿæ´»åœ¨è§„åˆ™æ™¶æ ¼ä¸Šçš„å˜é‡ $\{z_{i}\}$ å¼€å§‹ï¼Œé‚£ä¹ˆé€šè¿‡å°†å˜é‡ä¸å…¶é‚»å±…ç»“åˆèµ·æ¥è¿›è¡Œç²—ç²’åŒ–æ˜¯å¾ˆè‡ªç„¶çš„ï¼Œå¦‚å›¾32æ‰€ç¤ºã€‚å½¢å¼ä¸Šæˆ‘ä»¬å¯ä»¥å†™æˆ</p>
<p>$$
z_{i}\rightarrow \widetilde{z}_{i} = f\left(\sum_{j\in\mathcal{N}_{i}}z_{j}\right)
$$</p>
<p>å…¶ä¸­ $\mathcal{N}_{i}$ æ˜¯å›´ç»•èŠ‚ç‚¹ $i$ çš„é‚»åŸŸã€‚å¦‚æœå‡½æ•° $f(\cdot)$ æ˜¯çº¿æ€§çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªæ˜¯å¯¹ä¸€ä¸ªé‚»åŸŸè¿›è¡Œå¹³å‡ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è¿­ä»£ï¼Œè¿™å°†å¯¼è‡´ä»ç¦»æ•£çš„ç±»ä¼¼ Ising çš„å˜é‡åˆ°æ›´è¿ç»­çš„å±€éƒ¨ç£åŒ–ã€‚å¦‚æœ $f(\cdot)$ æ˜¯ä¸€ä¸ªé˜ˆå€¼å‡½æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å®ç°å¤šæ•°è§„åˆ™ï¼Œè¿™æ ·ç±»ä¼¼ Ising çš„å˜é‡ç°‡å°±è¢«æ˜ å°„åˆ°æ›´ç¨€ç–æ™¶æ ¼ä¸Šçš„ç±»ä¼¼ Ising çš„å˜é‡ï¼Œå°±åƒåŸå§‹çš„å—è‡ªæ—‹æ„é€ ï¼ˆKadanoffï¼Œ1966ï¼‰ä¸€æ ·ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/LfiEyK2De6NWbIH.png" alt=""  /></p>
<p>Coarseâ€“graining on a regular lattice. We start with binary (black/white) variables $\{z_{i}\}$, and replace $2 \times 2$ blocks with the average of these variables $\{\widetilde{z}_{i}\}$, shown as grey levels. The interesting question is what happens to the joint distribution as we coarseâ€“grain, not just once but iteratively.</p>
</blockquote>
<p>åœ¨è§„åˆ™æ™¶æ ¼ä¸Šçš„ç²—ç²’åŒ–ã€‚æˆ‘ä»¬ä»äºŒåˆ†ï¼ˆé»‘/ç™½ï¼‰å˜é‡ $\{z_{i}\}$ å¼€å§‹ï¼Œå¹¶ç”¨è¿™äº›å˜é‡çš„å¹³å‡å€¼ $\{\widetilde{z}_{i}\}$ æ›¿æ¢ $2 \times 2$ å—ï¼Œå¦‚ç°åº¦æ‰€ç¤ºã€‚æœ‰è¶£çš„é—®é¢˜æ˜¯ï¼Œå½“æˆ‘ä»¬è¿›è¡Œç²—ç²’åŒ–æ—¶ï¼Œè”åˆåˆ†å¸ƒä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼Œä¸ä»…ä»…æ˜¯ä¸€æ¬¡ï¼Œè€Œæ˜¯è¿­ä»£åœ°ã€‚</p>
</blockquote>
<blockquote>
<p>In a system with local interactions, the variables in the neighborhood typically are the most strongly correlated with one another. This suggests that even if we donâ€™t have a notion of neighborhood, we can make progress by searching for the most correlated variables and using these to build the clusters that we use in coarseâ€“graining. A schematic of how this can work for neural activity is shown in Fig 33.</p>
</blockquote>
<p>åœ¨å…·æœ‰å±€éƒ¨ç›¸äº’ä½œç”¨çš„ç³»ç»Ÿä¸­ï¼Œé‚»åŸŸå†…çš„å˜é‡é€šå¸¸å½¼æ­¤ä¹‹é—´ç›¸å…³æ€§æœ€å¼ºã€‚è¿™è¡¨æ˜ï¼Œå³ä½¿æˆ‘ä»¬æ²¡æœ‰é‚»åŸŸçš„æ¦‚å¿µï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡æœç´¢æœ€ç›¸å…³çš„å˜é‡å¹¶ä½¿ç”¨å®ƒä»¬æ¥æ„å»ºæˆ‘ä»¬åœ¨ç²—ç²’åŒ–ä¸­ä½¿ç”¨çš„ç°‡æ¥å–å¾—è¿›å±•ã€‚å›¾33æ˜¾ç¤ºäº†è¿™å¦‚ä½•é€‚ç”¨äºç¥ç»æ´»åŠ¨çš„ç¤ºæ„å›¾ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/bZcoh18zGEvpUQT.png" alt=""  /></p>
<p>Coarseâ€“graining neural activity.
(A) A small group of neurons with links indicating the most strongly correlated pairs, and the strength of these correlations.
(B) Schematic sequence of action potentials from these cells.
(C) Coarseâ€“graining by summing the activity in highly correlated pairs.
(D) Finding the most strongly correlated pairs of coarseâ€“grained variables in (C) and coarseâ€“graining again by summing. The strengths of the correlations are color coded as in (A).
(E) One more iteration of this â€œreal spaceâ€ coarsegraining.</p>
</blockquote>
<p>ç¥ç»æ´»åŠ¨çš„ç²—ç²’åŒ–ã€‚
(A) ä¸€å°ç»„ç¥ç»å…ƒï¼Œé“¾æ¥è¡¨ç¤ºæœ€å¼ºç›¸å…³çš„å¯¹åŠå…¶ç›¸å…³å¼ºåº¦ã€‚
(B) è¿™äº›ç»†èƒçš„åŠ¨ä½œç”µä½ç¤ºæ„åºåˆ—ã€‚
(C) é€šè¿‡å¯¹é«˜åº¦ç›¸å…³çš„å¯¹çš„æ´»åŠ¨æ±‚å’Œè¿›è¡Œç²—ç²’åŒ–ã€‚
(D) åœ¨ (C) ä¸­æ‰¾åˆ°æœ€å¼ºç›¸å…³çš„ç²—ç²’åŒ–å˜é‡å¯¹ï¼Œå¹¶é€šè¿‡æ±‚å’Œå†æ¬¡è¿›è¡Œç²—ç²’åŒ–ã€‚ç›¸å…³å¼ºåº¦æŒ‰ (A) ä¸­çš„é¢œè‰²ç¼–ç ã€‚
(E) è¿™ç§â€œå®ç©ºé—´â€ç²—ç²’åŒ–çš„åˆä¸€æ¬¡è¿­ä»£ã€‚</p>
</blockquote>
<blockquote>
<p>We start with variables $\{\sigma_{i}\}$, as before, describing the patterns of activity ($\sigma_{i} = 1$) and silence ($\sigma_{i} = 0$) across all the neurons $i = 1, 2,\cdots,N$ in a small window of time. To emphasize that this is the most microscopic description we will write this as $\sigma_{i} = \sigma_{i}^{(1)}$ . Then as before we can compute the means, covariance, and <strong>correlation matrices</strong>:</p>
<p>$$
\begin{aligned}
m_{i}^{(1)} &amp;= \langle \sigma_{i}^{(1)}\rangle \\
C_{ij}^{(1)} &amp;= \left\langle \left[\sigma_{i}^{(1)}-m_{i}^{(1)}\right] \left[\sigma_{j}^{(1)}-m_{j}^{(1)}\right] \right\rangle\\
c_{ij}^{(1)} &amp;= \frac{C_{ij}^{(1)}}{\sqrt{C_{ii}^{(1)}C_{jj}^{(1)}}}
\end{aligned}
$$</p>
<p>Now we search for the maximal nonâ€“diagonal element in the matrix of correlation coefficients, then zero the rows and columns associated with this pair of cells $i, j_{*}(i)$, and repeat. The result is a set of maximally correlated pairs $\{i, j_{*}(i)\}$, and we then define coarseâ€“grained variables</p>
<p>$$
\sigma_{i}^{(2)} = \sigma_{i}^{(1)} + \sigma_{j_{*}(i)}^{(1)}
$$</p>
<p>where now $i = 1, 2, \cdots , N/2$. Importantly, we can iterate this process across scales: we compute the correlation matrix of the variables $\{\sigma_{i}^{(2)}\}$ and search  again for the maximally correlated pairs $\{i, j_{*}(i)\}$, then define</p>
<p>$$
\sigma_{i}^{(3)} = \sigma_{i}^{(2)} + \sigma_{j_{*}(i)}^{(2)}
$$</p>
<p>and so on; at each stage we have $N_{k} = \lfloor N/2^{kâˆ’1}\rfloor$ variables remaining. This coarse graining produces  clusters of $K = 2, 4,\cdots , 2^{kâˆ’1}$ neurons, and the variable $\sigma_{i}^{(k)}$ is the summed activity of cluster $i$.</p>
</blockquote>
<p>æˆ‘ä»¬ä»å˜é‡ $\{\sigma_{i}\}$ å¼€å§‹ï¼Œå¦‚å‰æ‰€è¿°ï¼Œæè¿°åœ¨ä¸€ä¸ªå°æ—¶é—´çª—å£å†…æ‰€æœ‰ç¥ç»å…ƒ $i = 1, 2,\cdots,N$ çš„æ´»åŠ¨æ¨¡å¼ï¼ˆ$\sigma_{i} = 1$ï¼‰å’Œé™é»˜ï¼ˆ$\sigma_{i} = 0$ï¼‰ã€‚ä¸ºäº†å¼ºè°ƒè¿™æ˜¯æœ€å¾®è§‚çš„æè¿°ï¼Œæˆ‘ä»¬å°†å…¶å†™ä¸º $\sigma_{i} = \sigma_{i}^{(1)}$ã€‚ç„¶ååƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡å€¼ã€åæ–¹å·®å’Œ<strong>ç›¸å…³çŸ©é˜µ</strong>ï¼š</p>
<p>$$
\begin{aligned}
m_{i}^{(1)} &amp;= \langle \sigma_{i}^{(1)}\rangle \\
C_{ij}^{(1)} &amp;= \left\langle \left[\sigma_{i}^{(1)}-m_{i}^{(1)}\right] \left[\sigma_{j}^{(1)}-m_{j}^{(1)}\right] \right\rangle\\
c_{ij}^{(1)} &amp;= \frac{C_{ij}^{(1)}}{\sqrt{C_{ii}^{(1)}C_{jj}^{(1)}}}
\end{aligned}
$$</p>
<p>ç°åœ¨æˆ‘ä»¬åœ¨ç›¸å…³ç³»æ•°çŸ©é˜µä¸­æœç´¢éå¯¹è§’å…ƒç´ çš„æœ€å¤§å€¼ï¼Œç„¶åå°†ä¸è¯¥ç»†èƒå¯¹ $i, j_{*}(i)$ ç›¸å…³çš„è¡Œå’Œåˆ—å½’é›¶ï¼Œå¹¶é‡å¤ã€‚ç»“æœæ˜¯ä¸€ç»„æœ€å¤§ç›¸å…³å¯¹ $\{i, j_{*}(i)\}$ï¼Œç„¶åæˆ‘ä»¬å®šä¹‰ç²—ç²’åŒ–å˜é‡</p>
<p>$$
\sigma_{i}^{(2)} = \sigma_{i}^{(1)} + \sigma_{j_{*}(i)}^{(1)}
$$</p>
<p>ç°åœ¨ $i = 1, 2, \cdots , N/2$ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥è·¨å°ºåº¦è¿­ä»£è¿™ä¸ªè¿‡ç¨‹ï¼šæˆ‘ä»¬è®¡ç®—å˜é‡ $\{\sigma_{i}^{(2)}\}$ çš„ç›¸å…³çŸ©é˜µå¹¶å†æ¬¡æœç´¢æœ€å¤§ç›¸å…³å¯¹ $\{i, j_{*}(i)\}$ï¼Œç„¶åå®šä¹‰</p>
<p>$$
\sigma_{i}^{(3)} = \sigma_{i}^{(2)} + \sigma_{j_{*}(i)}^{(2)}
$$</p>
<p>ä¾æ­¤ç±»æ¨ï¼›åœ¨æ¯ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å‰©ä¸‹ $N_{k} = \lfloor N/2^{kâˆ’1}\rfloor$ ä¸ªå˜é‡ã€‚è¿™ä¸ªç²—ç²’åŒ–äº§ç”Ÿäº† $K = 2, 4,\cdots , 2^{kâˆ’1}$ ä¸ªç¥ç»å…ƒç°‡ï¼Œå˜é‡ $\sigma_{i}^{(k)}$ æ˜¯ç°‡ $i$ çš„æ€»æ´»åŠ¨é‡ã€‚</p>
<blockquote>
<p>We emphasize that one could have different criteria for coarseâ€“graining, and different ways of combing the variables. We return to some of these points below (Â§VII.D), but for now we explore what happens when we apply this simplest scheme to a network of real neurons. The first such example used the experiments on the activity of 1000+ neurons described in Â§V (Meshulam et al., 2018, 2019).</p>
</blockquote>
<p>æˆ‘ä»¬å¼ºè°ƒï¼Œå¯ä»¥æœ‰ä¸åŒçš„ç²—ç²’åŒ–æ ‡å‡†å’Œä¸åŒçš„å˜é‡ç»„åˆæ–¹å¼ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„æŸäº›ç‚¹ä¸Šå›åˆ°è¿™äº›é—®é¢˜ï¼ˆÂ§VII.Dï¼‰ï¼Œä½†ç°åœ¨æˆ‘ä»¬æ¢ç´¢å½“æˆ‘ä»¬å°†è¿™ä¸ªæœ€ç®€å•çš„æ–¹æ¡ˆåº”ç”¨äºçœŸå®ç¥ç»å…ƒç½‘ç»œæ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ç¬¬ä¸€ä¸ªè¿™æ ·çš„ä¾‹å­ä½¿ç”¨äº†Â§Vä¸­æè¿°çš„ 1000 å¤šä¸ªç¥ç»å…ƒæ´»åŠ¨çš„å®éªŒï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼Œ2019ï¼‰ã€‚</p>
<blockquote>
<p>We are interested in how the probability distributions transform and flow as we pass through successive scales of coarseâ€“graining. Of course looking at the joint distribution $P(\{\sigma_{i}^{(k)}\})$ is essentially impossible. But  much can be learned by looking at slices through this distribution, even the distribution of individual coarsegrained variables, as with the magnetization in the Ising model (Binder, 1981).</p>
</blockquote>
<p>æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯éšç€æˆ‘ä»¬é€šè¿‡è¿ç»­çš„ç²—ç²’åŒ–å°ºåº¦ï¼Œæ¦‚ç‡åˆ†å¸ƒå¦‚ä½•å˜æ¢å’ŒæµåŠ¨ã€‚å½“ç„¶ï¼ŒæŸ¥çœ‹è”åˆåˆ†å¸ƒ $P(\{\sigma_{i}^{(k)}\})$ æœ¬è´¨ä¸Šæ˜¯ä¸å¯èƒ½çš„ã€‚ä½†æ˜¯ï¼Œé€šè¿‡æŸ¥çœ‹è¯¥åˆ†å¸ƒçš„åˆ‡ç‰‡ï¼Œç”šè‡³æ˜¯å•ä¸ªç²—ç²’åŒ–å˜é‡çš„åˆ†å¸ƒï¼Œå°±åƒ Ising æ¨¡å‹ä¸­çš„ç£åŒ–ä¸€æ ·ï¼Œå¯ä»¥å­¦åˆ°å¾ˆå¤šä¸œè¥¿ï¼ˆBinderï¼Œ1981ï¼‰ã€‚</p>
<blockquote>
<p>Since this coarseâ€“graining is based simply on adding the â€œneighboringâ€ variables, the first moment of the distribution of the individual variables must scale linearly,</p>
<p>$$
M_{1}(k) \equiv \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\langle\sigma_{i}^{(k)}\rangle = \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}m_{i}^{(k)} = KM_{1}(1)
$$</p>
<p>where after $k$ steps we have $N_{k}$ clusters each involving $K = 2^{kâˆ’1}$ of the original variables. The first nonâ€“trivial question is about the second moment, or the variance in activity,</p>
<p>$$
\begin{aligned}
M_{2}(K) \equiv \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\left\langle \left(
\sigma_{i}^{(k)} - m_{i}^{(k)}
\right)^{2}\right\rangle
\end{aligned}
$$</p>
<p>Note that if neurons are independent we expect $M_{2}(K)\propto K$, and many weakly correlated populations should approach this behavior at large $K$. If neurons are perfectly correlated, on the other hand, we expect $M_{2}(K)\propto K^{2}$. Looking at the data, in Fig 34A, we see  that for neurons in the hippocampus $M_{2}\propto K^{\widetilde{\alpha}}$, with $\widetilde{\alpha}= 1.4 \pm 0.06$. This nonâ€“trivial scaling is visible over more than two decades.</p>
</blockquote>
<p>ç”±äºè¿™ç§ç²—ç²’åŒ–ä»…åŸºäºæ·»åŠ â€œé‚»è¿‘â€å˜é‡ï¼Œå•ä¸ªå˜é‡åˆ†å¸ƒçš„ç¬¬ä¸€çŸ©å¿…é¡»çº¿æ€§ç¼©æ”¾ï¼Œ</p>
<p>$$
M_{1}(k) \equiv \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\langle\sigma_{i}^{(k)}\rangle = \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}m_{i}^{(k)} = KM_{1}(1)
$$</p>
<p>å…¶ä¸­ç»è¿‡ $k$ æ­¥åï¼Œæˆ‘ä»¬æœ‰ $N_{k}$ ä¸ªç°‡ï¼Œæ¯ä¸ªç°‡æ¶‰åŠåŸå§‹å˜é‡çš„ $K = 2^{kâˆ’1}$ã€‚ç¬¬ä¸€ä¸ªéå¹³å‡¡çš„é—®é¢˜æ˜¯å…³äºç¬¬äºŒçŸ©ï¼Œæˆ–æ´»åŠ¨çš„æ–¹å·®ï¼Œ</p>
<p>$$
\begin{aligned}
M_{2}(K) \equiv \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\left\langle \left(
\sigma_{i}^{(k)} - m_{i}^{(k)}
\right)^{2}\right\rangle
\end{aligned}
$$</p>
<p>è¯·æ³¨æ„ï¼Œå¦‚æœç¥ç»å…ƒæ˜¯ç‹¬ç«‹çš„ï¼Œæˆ‘ä»¬æœŸæœ› $M_{2}(K)\propto K$ï¼Œå¹¶ä¸”è®¸å¤šå¼±ç›¸å…³çš„ç¾¤ä½“åº”è¯¥åœ¨å¤§ $K$ æ—¶æ¥è¿‘è¿™ç§è¡Œä¸ºã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœç¥ç»å…ƒæ˜¯å®Œå…¨ç›¸å…³çš„ï¼Œæˆ‘ä»¬æœŸæœ› $M_{2}(K)\propto K^{2}$ã€‚æŸ¥çœ‹æ•°æ®ï¼Œåœ¨å›¾ 34A ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¯¹äºæµ·é©¬ä½“ä¸­çš„ç¥ç»å…ƒï¼Œ$M_{2}\propto K^{\widetilde{\alpha}}$ï¼Œå…¶ä¸­ $\widetilde{\alpha}= 1.4 \pm 0.06$ã€‚è¿™ç§éå¹³å‡¡çš„ç¼©æ”¾åœ¨ä¸¤ä¸ªæ•°é‡çº§ä»¥ä¸Šæ˜¯å¯è§çš„ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/LHkXUa8D9GTPcmJ.png" alt=""  /></p>
<p>Three slices through the distribution of coarsegrained variables (Meshulam et al., 2018, 2019). (A) Variance of the activity vs. the (real space) coarse graining scale, from  Eq (151). Solid line is $M_{2}\propto K^{\widetilde{\alpha}}$, $\widetilde{\alpha}= 1.4 \pm 0.06$; dashed lines are predictions for independent ($\widetilde{\alpha}= 1$) or perfectly correlated ($\widetilde{\alpha}= 2$) neurons. (B) Probability of silence vs. the coarsegraining scale. Solid line is Eq (152) with $\widetilde{\beta}= 0.88 \pm 0.01$; dashed line is the expectation for independent neurons, $\widetilde{\beta}= 1$. (C) Distribution of the normalized nonâ€“zero activity, as defined in Eq (153).</p>
</blockquote>
<p>ä¸‰ä¸ªé€šè¿‡ç²—ç²’åŒ–å˜é‡åˆ†å¸ƒçš„åˆ‡ç‰‡ï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼Œ2019ï¼‰ã€‚(A) æ´»åŠ¨æ–¹å·®ä¸ï¼ˆå®ç©ºé—´ï¼‰ç²—ç²’åŒ–å°ºåº¦çš„å…³ç³»ï¼Œæ¥è‡ªæ–¹ç¨‹ï¼ˆ151ï¼‰ã€‚å®çº¿ä¸º $M_{2}\propto K^{\widetilde{\alpha}}$ï¼Œ$\widetilde{\alpha}= 1.4 \pm 0.06$ï¼›è™šçº¿ä¸ºç‹¬ç«‹ç¥ç»å…ƒï¼ˆ$\widetilde{\alpha}= 1$ï¼‰æˆ–å®Œå…¨ç›¸å…³ç¥ç»å…ƒï¼ˆ$\widetilde{\alpha}= 2$ï¼‰çš„é¢„æµ‹ã€‚(B) é™é»˜æ¦‚ç‡ä¸ç²—ç²’åŒ–å°ºåº¦çš„å…³ç³»ã€‚å®çº¿ä¸ºæ–¹ç¨‹ï¼ˆ152ï¼‰ï¼Œ$\widetilde{\beta}= 0.88 \pm 0.01$ï¼›è™šçº¿ä¸ºç‹¬ç«‹ç¥ç»å…ƒçš„æœŸæœ›ï¼Œ$\widetilde{\beta}= 1$ã€‚(C) å½’ä¸€åŒ–éé›¶æ´»åŠ¨çš„åˆ†å¸ƒï¼Œå¦‚æ–¹ç¨‹ï¼ˆ153ï¼‰ä¸­å®šä¹‰ã€‚</p>
</blockquote>
<blockquote>
<p>We can take another slice through the distribution by asking for the probability $P_{k}(0)$ that the coarseâ€“grained variable $\sigma_{i}^{(k)} = 0$. Since we started with variables $\sigma_{i} =  \{0, 1\}$, this is the same as asking for the probability that all of the neurons inside the cluster of size $K = 2^{kâˆ’1}$ are silent. If the neurons are independent we expect a simple scaling $P_{k}(0)\propto \exp{(âˆ’aK)}$, and once more expect to see this at large $K$ even if the cells are weakly correlated. Experimentally we see in Fig 34B that</p>
<p>$$
P_{k}(0) = \exp{(-a K^{\widetilde{\beta}})}
$$</p>
<p>with the exponent $\widetilde{\beta}= 0.88 \pm 0.01$. Again scaling is precise over more than two decades.</p>
</blockquote>
<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡è¯¢é—®ç²—ç²’åŒ–å˜é‡ $\sigma_{i}^{(k)} = 0$ çš„æ¦‚ç‡ $P_{k}(0)$ æ¥è·å–åˆ†å¸ƒçš„å¦ä¸€ä¸ªåˆ‡ç‰‡ã€‚ç”±äºæˆ‘ä»¬ä»å˜é‡ $\sigma_{i} =  \{0, 1\}$ å¼€å§‹ï¼Œè¿™ä¸è¯¢é—®ç°‡å¤§å°ä¸º $K = 2^{kâˆ’1}$ çš„æ‰€æœ‰ç¥ç»å…ƒéƒ½å¤„äºé™é»˜çŠ¶æ€çš„æ¦‚ç‡ç›¸åŒã€‚å¦‚æœç¥ç»å…ƒæ˜¯ç‹¬ç«‹çš„ï¼Œæˆ‘ä»¬æœŸæœ›ä¸€ä¸ªç®€å•çš„ç¼©æ”¾ $P_{k}(0)\propto \exp{(âˆ’aK)}$ï¼Œå³ä½¿ç»†èƒæ˜¯å¼±ç›¸å…³çš„ï¼Œæˆ‘ä»¬ä¹ŸæœŸæœ›åœ¨å¤§ $K$ æ—¶çœ‹åˆ°è¿™ç§æƒ…å†µã€‚å®éªŒä¸Šæˆ‘ä»¬åœ¨å›¾ 34B ä¸­çœ‹åˆ°</p>
<p>$$
P_{k}(0) = \exp{(-a K^{\widetilde{\beta}})}
$$</p>
<p>å…¶ä¸­æŒ‡æ•° $\widetilde{\beta}= 0.88 \pm 0.01$ã€‚åŒæ ·ï¼Œç¼©æ”¾åœ¨ä¸¤ä¸ªæ•°é‡çº§ä»¥ä¸Šæ˜¯ç²¾ç¡®çš„ã€‚</p>
<blockquote>
<p>If we imagine making an explicit model for the joint activity of all the neurons inside one of the clusters, perhaps in the form of the pairwise models above [Eq (83)], then the probability of complete silence is dependent only on the partition function, $P_{k}(0) = 1/Z$. This generalizes if we include higherâ€“order terms, so that Fig 34B probes the effective free energy, which apparently  behaves as $F(K) = -aK^{\widetilde{\beta}}$. Since $\widetilde{\beta}&lt; 1$, the free energy is subâ€“extensive, and hence the free energy per neuron will vanish in the thermodynamic limit. This is consistent with the equality of entropy and energy that we saw for retinal neurons in Â§VI.B (Fig 29).</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬æƒ³è±¡ä¸ºç°‡å†…æ‰€æœ‰ç¥ç»å…ƒçš„è”åˆæ´»åŠ¨å»ºç«‹ä¸€ä¸ªæ˜¾å¼æ¨¡å‹ï¼Œå¯èƒ½é‡‡ç”¨ä¸Šè¿°æˆå¯¹æ¨¡å‹çš„å½¢å¼[æ–¹ç¨‹ï¼ˆ83ï¼‰]ï¼Œé‚£ä¹ˆå®Œå…¨é™é»˜çš„æ¦‚ç‡ä»…ä¾èµ–äºé…åˆ†å‡½æ•°ï¼Œ$P_{k}(0) = 1/Z$ã€‚å¦‚æœæˆ‘ä»¬åŒ…æ‹¬æ›´é«˜é˜¶é¡¹ï¼Œè¿™ç§æƒ…å†µä¼šè¢«æ¨å¹¿ï¼Œå› æ­¤å›¾ 34B æ¢æµ‹äº†æœ‰æ•ˆè‡ªç”±èƒ½ï¼Œæ˜¾ç„¶è¡¨ç°ä¸º $F(K) = -aK^{\widetilde{\beta}}$ã€‚ç”±äº $\widetilde{\beta}&lt; 1$ï¼Œè‡ªç”±èƒ½æ˜¯äºšå¹¿å»¶çš„ï¼Œå› æ­¤åœ¨çƒ­åŠ›å­¦æé™ä¸‹æ¯ä¸ªç¥ç»å…ƒçš„è‡ªç”±èƒ½å°†æ¶ˆå¤±ã€‚è¿™ä¸æˆ‘ä»¬åœ¨ Â§VI.Bï¼ˆå›¾29ï¼‰ä¸­çœ‹åˆ°çš„è§†ç½‘è†œç¥ç»å…ƒçš„ç†µå’Œèƒ½é‡ç›¸ç­‰æ˜¯ä¸€è‡´çš„ã€‚</p>
<blockquote>
<p>More generally if we define the normalized variable $x = \sigma^{(k)}/K$, then</p>
<p>$$
P_{k}(x) = P_{k}(0)\delta_{x,0} + [1-P_{k}(0)]Q_{k}(x)
$$</p>
<p>Figure 34C shows the evolution of $Q_{k}(x)$ as $K$ increases. We see that the tail of the distribution is gradually absorbed into the bulk, which seems to approach a fixed form $Q(x)\sim e^{âˆ’x/x_{0}}$ . If the neurons were independent the central limit theorem would drive this distribution toward a Gaussian, but instead we see the emergence of a fixed nonâ€“Gaussian form.</p>
</blockquote>
<p>æ›´ä¸€èˆ¬åœ°ï¼Œå¦‚æœæˆ‘ä»¬å®šä¹‰å½’ä¸€åŒ–å˜é‡ $x = \sigma^{(k)}/K$ï¼Œé‚£ä¹ˆ</p>
<p>$$
P_{k}(x) = P_{k}(0)\delta_{x,0} + [1-P_{k}(0)]Q_{k}(x)
$$</p>
<p>å›¾ 34C æ˜¾ç¤ºäº†éšç€ $K$ å¢åŠ ï¼Œ$Q_{k}(x)$ çš„æ¼”å˜ã€‚æˆ‘ä»¬çœ‹åˆ°åˆ†å¸ƒçš„å°¾éƒ¨é€æ¸è¢«å¸æ”¶åˆ°ä¸»ä½“ä¸­ï¼Œä¼¼ä¹æ¥è¿‘ä¸€ä¸ªå›ºå®šå½¢å¼ $Q(x)\sim e^{âˆ’x/x_{0}}$ã€‚å¦‚æœç¥ç»å…ƒæ˜¯ç‹¬ç«‹çš„ï¼Œä¸­å¿ƒæé™å®šç†ä¼šå°†è¯¥åˆ†å¸ƒé©±åŠ¨å‘é«˜æ–¯åˆ†å¸ƒï¼Œä½†ç›¸åï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€ä¸ªå›ºå®šçš„éé«˜æ–¯å½¢å¼çš„å‡ºç°ã€‚</p>
<blockquote>
<p>In addition to looking at the distribution of single coarseâ€“grained variables we can look at the covariance matrix of the microscopic variables within each cluster of size $K$. The eigenvalue spectrum of this covariance matrix depends on the rank scaled by $K$, and there is a substantial region over which the spectrum is a power $\lambda\sim (K/\text{rank})^{\mu}$, with $Î¼ = 0.71 \pm 0.06$, although this is less crisp than the other examples of scaling.</p>
</blockquote>
<p>é™¤äº†æŸ¥çœ‹å•ä¸ªç²—ç²’åŒ–å˜é‡çš„åˆ†å¸ƒå¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹æ¯ä¸ªå¤§å°ä¸º $K$ çš„ç°‡å†…å¾®è§‚å˜é‡çš„åæ–¹å·®çŸ©é˜µã€‚è¯¥åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼è°±å–å†³äºæŒ‰ $K$ ç¼©æ”¾çš„ç§©ï¼Œå¹¶ä¸”åœ¨å¾ˆå¤§ä¸€éƒ¨åˆ†åŒºåŸŸå†…ï¼Œè°±æ˜¯ä¸€ä¸ªå¹‚ $\lambda\sim (K/\text{rank})^{\mu}$ï¼Œå…¶ä¸­ $Î¼ = 0.71 \pm 0.06$ï¼Œå°½ç®¡è¿™ä¸å¦‚å…¶ä»–ç¼©æ”¾ç¤ºä¾‹é‚£ä¹ˆæ¸…æ™°ã€‚</p>
<blockquote>
<p>Our discussion of thus far has focused on the distribution of variables at a single moment in time. In the applications of the RG that we understand, however, we can often observe dynamic scaling (Hohenberg and Halperin, 1977). Intuitively, fluctuations on longer length scales take longer to relax because the underlying interactions are local. What is nonâ€“trivial is that correlation functions for variables coarseâ€“grained to different length scales collapse to a universal form if we measure time in units of the correlation time, and this correlation time itself varies as a power of the length scale. An elegant example of these ideas in a fully biological context is provided by dynamic scaling of the velocity fluctuations in natural swarms of insects (Cavagna et al., 2017).</p>
</blockquote>
<p>åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„è®¨è®ºé›†ä¸­åœ¨å•ä¸€æ—¶é—´ç‚¹ä¸Šå˜é‡çš„åˆ†å¸ƒã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬ç†è§£çš„ RG åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥è§‚å¯Ÿåˆ°åŠ¨æ€ç¼©æ”¾ï¼ˆHohenberg å’Œ Halperinï¼Œ1977ï¼‰ã€‚ç›´è§‚åœ°è¯´ï¼Œæ›´é•¿é•¿åº¦å°ºåº¦ä¸Šçš„æ³¢åŠ¨éœ€è¦æ›´é•¿æ—¶é—´æ‰èƒ½å¼›è±«ï¼Œå› ä¸ºåŸºç¡€ç›¸äº’ä½œç”¨æ˜¯å±€éƒ¨çš„ã€‚éå¹³å‡¡çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ä»¥ç›¸å…³æ—¶é—´ä¸ºå•ä½æµ‹é‡æ—¶é—´ï¼Œé‚£ä¹ˆå¯¹ä¸åŒé•¿åº¦å°ºåº¦ç²—ç²’åŒ–å˜é‡çš„ç›¸å…³å‡½æ•°ä¼šåç¼©ä¸ºä¸€ä¸ªé€šç”¨å½¢å¼ï¼Œå¹¶ä¸”è¯¥ç›¸å…³æ—¶é—´æœ¬èº«éšç€é•¿åº¦å°ºåº¦çš„å¹‚å˜åŒ–ã€‚åœ¨å®Œå…¨ç”Ÿç‰©å­¦èƒŒæ™¯ä¸‹ï¼Œè¿™äº›æ€æƒ³çš„ä¸€ä¸ªä¼˜é›…ä¾‹å­æ˜¯æ˜†è™«è‡ªç„¶ç¾¤ä½“ä¸­é€Ÿåº¦æ³¢åŠ¨çš„åŠ¨æ€ç¼©æ”¾ï¼ˆCavagna ç­‰äººï¼Œ2017ï¼‰ã€‚</p>
<blockquote>
<p>With networks of neurons we donâ€™t expect locality to be a good guide, but it still is plausible that more strongly coarseâ€“grained variables will have slower dynamics, and we can search for dynamic scaling. Concretely we define the correlation function for individual variables at coarseâ€“graining scale $k$,</p>
<p>$$
\widetilde{C}_{i}^{(k)}(t) = \left\langle\left[
\sigma_{i}^{k}(t_{0})-m_{i}^{(k)}
\right]\left[
\sigma_{i}^{k}(t_{0}+t)-m_{i}^{(k)}
\right]\right\rangle
$$</p>
<p>and then we can normalize and average over the clusters to give</p>
<p>$$
C^{(k)}(t) = \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\frac{\widetilde{C}_{i}^{(k)}(t)}{\widetilde{C}_{i}^{(k)}(0)}
$$</p>
<p>Dynamic scaling is the hypothesis that the dependence on scale is captured by a single correlation time,</p>
<p>$$
C^{(k)}(t) = C[t/\tau_{c}(k)]
$$</p>
<p>with $\tau_{c}(k)\propto K^{\widetilde{z}}$. In Figure 35 we see that all of this works for the population of hippocampal neurons. We note that dynamic range of correlation times accessed in this experiment is limited, at short times by the dynamics of the indicator molecules and at long times by the small value of the exponent $\widetilde{z}= 0.16 \pm 0.02$.</p>
</blockquote>
<p>å¯¹äºç¥ç»å…ƒç½‘ç»œï¼Œæˆ‘ä»¬ä¸æœŸæœ›å±€éƒ¨æ€§æ˜¯ä¸€ä¸ªå¥½çš„æŒ‡å¯¼ï¼Œä½†æ›´å¼ºç²—ç²’åŒ–çš„å˜é‡ä»ç„¶å¯èƒ½å…·æœ‰è¾ƒæ…¢çš„åŠ¨æ€ï¼Œæˆ‘ä»¬å¯ä»¥æœç´¢åŠ¨æ€ç¼©æ”¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å®šä¹‰ç²—ç²’åŒ–å°ºåº¦ $k$ ä¸‹å•ä¸ªå˜é‡çš„ç›¸å…³å‡½æ•°ï¼Œ</p>
<p>$$
\widetilde{C}_{i}^{(k)}(t) = \left\langle\left[
\sigma_{i}^{k}(t_{0})-m_{i}^{(k)}
\right]\left[
\sigma_{i}^{k}(t_{0}+t)-m_{i}^{(k)}
\right]\right\rangle
$$</p>
<p>ç„¶åæˆ‘ä»¬å¯ä»¥å½’ä¸€åŒ–å¹¶å¯¹ç°‡è¿›è¡Œå¹³å‡ï¼Œå¾—åˆ°</p>
<p>$$
C^{(k)}(t) = \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\frac{\widetilde{C}_{i}^{(k)}(t)}{\widetilde{C}_{i}^{(k)}(0)}
$$</p>
<p>åŠ¨æ€ç¼©æ”¾æ˜¯å‡è®¾å°ºåº¦çš„ä¾èµ–æ€§ç”±å•ä¸€ç›¸å…³æ—¶é—´æ•è·ï¼Œ</p>
<p>$$
C^{(k)}(t) = C[t/\tau_{c}(k)]
$$</p>
<p>å…¶ä¸­ $\tau_{c}(k)\propto K^{\widetilde{z}}$ã€‚åœ¨å›¾ 35 ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¯¹äºæµ·é©¬ä½“ç¥ç»å…ƒç¾¤ä½“ï¼Œæ‰€æœ‰è¿™äº›éƒ½æœ‰æ•ˆã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨è¿™ä¸ªå®éªŒä¸­è®¿é—®çš„ç›¸å…³æ—¶é—´çš„åŠ¨æ€èŒƒå›´æ˜¯æœ‰é™çš„ï¼Œåœ¨çŸ­æ—¶é—´å†…å—åˆ°æŒ‡ç¤ºå‰‚åˆ†å­åŠ¨åŠ›å­¦çš„é™åˆ¶ï¼Œåœ¨é•¿æ—¶é—´å†…å—åˆ°æŒ‡æ•° $\widetilde{z}= 0.16 \pm 0.02$ çš„å°å€¼çš„é™åˆ¶ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/vwUmpPsOQC8V4r2.png" alt=""  /></p>
<p>Dynamic scaling across 1000+ neurons in the hippocampus (Meshulam et al., 2018). (A) Mean correlation functions for coarseâ€“grained variables, Eq (155), in clusters of $K = 2,4,\cdots, 256$ neurons (lightest orange corresponds to the largest cluster), with larger clusters exhibiting slower dynamics. In dashed gray, $\pm$ one standard deviation across the K = 256 neuron clusters. (B) Collapse under scaling of the time axis, Eq (156). (C) Correlation time vs cluster size, fit to $\tau_{c}\propto K^{\widetilde{z}}$, with $\widetilde{z}= 0.16 \pm 0.02$.</p>
</blockquote>
<p>æµ·é©¬ä½“ä¸­ 1000 å¤šä¸ªç¥ç»å…ƒçš„åŠ¨æ€ç¼©æ”¾ï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼‰ã€‚(A) ç²—ç²’åŒ–å˜é‡çš„å¹³å‡ç›¸å…³å‡½æ•°ï¼Œæ–¹ç¨‹ï¼ˆ155ï¼‰ï¼Œåœ¨ $K = 2,4,\cdots, 256$ ä¸ªç¥ç»å…ƒçš„ç°‡ä¸­ï¼ˆæœ€æµ…çš„æ©™è‰²å¯¹åº”äºæœ€å¤§çš„ç°‡ï¼‰ï¼Œè¾ƒå¤§çš„ç°‡è¡¨ç°å‡ºè¾ƒæ…¢çš„åŠ¨æ€ã€‚è™šçº¿ç°è‰²è¡¨ç¤º K = 256 ç¥ç»å…ƒç°‡çš„æ­£è´Ÿä¸€ä¸ªæ ‡å‡†å·®ã€‚(B) æ—¶é—´è½´çš„ç¼©æ”¾åç¼©ï¼Œæ–¹ç¨‹ï¼ˆ156ï¼‰ã€‚(C) ç°‡å¤§å°ä¸ç›¸å…³æ—¶é—´çš„å…³ç³»ï¼Œæ‹Ÿåˆä¸º $\tau_{c}\propto K^{\widetilde{z}}$ï¼Œå…¶ä¸­ $\widetilde{z}= 0.16 \pm 0.02$ã€‚</p>
</blockquote>
<blockquote>
<p>It is important that these scaling behavior are not somehow driven by our choice to describe neural activity with binary variables. In these experiments, neural activity was recorded by imaging of fluorescence from indicator molecules that provide a continuous signal as in Figs 6 and 16. We can follow the same steps of coarsegraining for these continuous signals, and the results are the same (Meshulam et al., 2019).</p>
</blockquote>
<p>é‡è¦çš„æ˜¯ï¼Œè¿™äº›ç¼©æ”¾è¡Œä¸ºå¹¶ä¸æ˜¯ç”±æˆ‘ä»¬é€‰æ‹©ç”¨äºŒè¿›åˆ¶å˜é‡æ¥æè¿°ç¥ç»æ´»åŠ¨æ‰€é©±åŠ¨çš„ã€‚åœ¨è¿™äº›å®éªŒä¸­ï¼Œç¥ç»æ´»åŠ¨æ˜¯é€šè¿‡æˆåƒæŒ‡ç¤ºå‰‚åˆ†å­çš„è§å…‰è®°å½•çš„ï¼Œè¿™äº›åˆ†å­æä¾›äº†ä¸€ä¸ªè¿ç»­çš„ä¿¡å·ï¼Œå¦‚å›¾ 6 å’Œ 16 æ‰€ç¤ºã€‚æˆ‘ä»¬å¯ä»¥å¯¹è¿™äº›è¿ç»­ä¿¡å·éµå¾ªç›¸åŒçš„ç²—ç²’åŒ–æ­¥éª¤ï¼Œç»“æœæ˜¯ç›¸åŒçš„ï¼ˆMeshulam ç­‰äººï¼Œ2019ï¼‰ã€‚</p>
<blockquote>
<p>In the full theoretical structure of the RG, scaling exponents are signatures of <strong>universality classes</strong>. Before we can ask about universality we have to ask about reproducibility, especially in such complex systems. As a first step, the same analyses have been done with data from experiments on multiple mice. Because scaling is precise across more than two decades, the error bars in determining the exponents in individual mice are small, which sets a high standard for reproducibility. For example, the exponent describing the scaling of the free  energy (Fig 34B) is $\widetilde{\beta}= 0.87Â±0.014Â±0.015$ for the mean, the rms error in single experiments, and the standard deviation across experiments in three mice. This holds out the hope that we have uncovered features of the emergent behavior that are reproducible in the second decimal place.</p>
</blockquote>
<p>åœ¨ RG çš„å®Œæ•´ç†è®ºç»“æ„ä¸­ï¼Œç¼©æ”¾æŒ‡æ•°æ˜¯<strong>æ™®é€‚ç±»</strong>çš„æ ‡å¿—ã€‚åœ¨æˆ‘ä»¬è¯¢é—®æ™®é€‚æ€§ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»è¯¢é—®å¯é‡å¤æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¦‚æ­¤å¤æ‚çš„ç³»ç»Ÿä¸­ã€‚ä½œä¸ºç¬¬ä¸€æ­¥ï¼Œå·²ç»ä½¿ç”¨æ¥è‡ªå¤šåªå°é¼ å®éªŒçš„æ•°æ®è¿›è¡Œäº†ç›¸åŒçš„åˆ†æã€‚ç”±äºç¼©æ”¾åœ¨ä¸¤ä¸ªæ•°é‡çº§ä»¥ä¸Šæ˜¯ç²¾ç¡®çš„ï¼Œå› æ­¤åœ¨å•åªå°é¼ ä¸­ç¡®å®šæŒ‡æ•°çš„è¯¯å·®æ¡å¾ˆå°ï¼Œè¿™ä¸ºå¯é‡å¤æ€§è®¾å®šäº†é«˜æ ‡å‡†ã€‚ä¾‹å¦‚ï¼Œæè¿°è‡ªç”±èƒ½ç¼©æ”¾çš„æŒ‡æ•°ï¼ˆå›¾ 34Bï¼‰ä¸º $\widetilde{\beta}= 0.87Â±0.014Â±0.015$ï¼Œåˆ†åˆ«è¡¨ç¤ºå•æ¬¡å®éªŒä¸­çš„å‡å€¼ã€å‡æ–¹æ ¹è¯¯å·®å’Œä¸‰åªå°é¼ å®éªŒä¸­çš„æ ‡å‡†åå·®ã€‚è¿™è®©äººå¸Œæœ›æˆ‘ä»¬å·²ç»å‘ç°äº†åœ¨ç¬¬äºŒä¸ªå°æ•°ä½ä¸Šå¯é‡å¤çš„æ¶Œç°è¡Œä¸ºç‰¹å¾ã€‚</p>
<blockquote>
<p>A more ambitious search for universality was undertaken by Morales et al. (2023). They analyzed experiments that are part of a large effort at the Allen Institute for Brain Science, in this case using multiple neuropixels probes (Fig 5) to record 100+ neurons from each of many different areas of the mouse brain, simultaneously. Note that in addition to exploring many different brain regions, the technique for recording activity is completely different than in the hippocampal imaging data analyzed in Figs 34 and 35. Nonetheless, all aspects of scaling are reproduced across all these brain areas; examples include the scaling of the variance in coarseâ€“grained activity (Fig 36A) and dynamic scaling (Fig 36B).</p>
</blockquote>
<p>Morales ç­‰äººï¼ˆ2023ï¼‰è¿›è¡Œäº†æ›´é›„å¿ƒå‹ƒå‹ƒçš„æ™®é€‚æ€§æœç´¢ã€‚ä»–ä»¬åˆ†æäº†å±äº Allen Institute for Brain Science å¤§å‹é¡¹ç›®çš„ä¸€äº›å®éªŒï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤šä¸ªç¥ç»åƒç´ æ¢é’ˆï¼ˆå›¾ 5ï¼‰åŒæ—¶è®°å½•æ¥è‡ªå°é¼ å¤§è„‘è®¸å¤šä¸åŒåŒºåŸŸçš„ 100 å¤šä¸ªç¥ç»å…ƒã€‚è¯·æ³¨æ„ï¼Œé™¤äº†æ¢ç´¢è®¸å¤šä¸åŒçš„å¤§è„‘åŒºåŸŸå¤–ï¼Œè®°å½•æ´»åŠ¨çš„æŠ€æœ¯ä¸å›¾ 34 å’Œ 35 ä¸­åˆ†æçš„æµ·é©¬ä½“æˆåƒæ•°æ®å®Œå…¨ä¸åŒã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ‰€æœ‰è¿™äº›å¤§è„‘åŒºåŸŸéƒ½é‡ç°äº†ç¼©æ”¾çš„å„ä¸ªæ–¹é¢ï¼›ç¤ºä¾‹åŒ…æ‹¬ç²—ç²’åŒ–æ´»åŠ¨ä¸­æ–¹å·®çš„ç¼©æ”¾ï¼ˆå›¾ 36Aï¼‰å’ŒåŠ¨æ€ç¼©æ”¾ï¼ˆå›¾ 36Bï¼‰ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/6L7pMntTX2ZiWGN.png" alt=""  /></p>
<p>FIG. 36 Scaling in mutliple distinct areas of the mouse brain (MoralÃ©s et al., 2023). neuronal data after the â€œreal spaceâ€ (direct correlations) coarse-graining procedure. (A) Variance of the coarse-grained activity vs cluster size for neurons in sixteen different brain regions (depicted as different markers), comparable to Fig 34A. (B) Dynamic scaling for the same brain areas. Correlation time vs cluster size, comparable to Fig 35. Inset: Decay of the autocorrelation function for the neurons in one brain region (primary motor cortex) showing the collapse once time is rescaled.</p>
</blockquote>
<p>å›¾ 36 å°é¼ å¤§è„‘å¤šä¸ªä¸åŒåŒºåŸŸçš„ç¼©æ”¾ï¼ˆMoralÃ©s ç­‰äººï¼Œ2023ï¼‰ã€‚ç¥ç»å…ƒæ•°æ®ç»è¿‡â€œå®ç©ºé—´â€ï¼ˆç›´æ¥ç›¸å…³ï¼‰ç²—ç²’åŒ–ç¨‹åºã€‚(A) åå…­ä¸ªä¸åŒå¤§è„‘åŒºåŸŸä¸­ç²—ç²’åŒ–æ´»åŠ¨çš„æ–¹å·®ä¸ç°‡å¤§å°çš„å…³ç³»ï¼ˆä»¥ä¸åŒæ ‡è®°è¡¨ç¤ºï¼‰ï¼Œå¯ä¸å›¾ 34A ç›¸å¯¹æ¯”ã€‚(B) ç›¸åŒå¤§è„‘åŒºåŸŸçš„åŠ¨æ€ç¼©æ”¾ã€‚ç›¸å…³æ—¶é—´ä¸ç°‡å¤§å°çš„å…³ç³»ï¼Œå¯ä¸å›¾ 35 ç›¸å¯¹æ¯”ã€‚æ’å›¾ï¼šä¸€ä¸ªå¤§è„‘åŒºåŸŸï¼ˆåˆçº§è¿åŠ¨çš®å±‚ï¼‰ä¸­ç¥ç»å…ƒè‡ªç›¸å…³å‡½æ•°çš„è¡°å‡ï¼Œæ˜¾ç¤ºå‡ºä¸€æ—¦æ—¶é—´é‡æ–°ç¼©æ”¾åçš„åç¼©ã€‚</p>
</blockquote>
<blockquote>
<p>As we were completing this review a striking result was reported by Munn et al. (2024). Rather than looking at experiments across multiple brain areas in a single organism, they looked at experiments on many different organisms, from the tiny worm C. elegans to primates much like us. There are significant technical differences among these experiments, including differences in the calcium indicator proteins (Â§III.C) and differences in the sampling rate; complete resolution of individual neurons vs â€œregions of interest;â€ and recording from the entire brain is smaller model organisms vs. a single sensory or motor area in larger organisms. Many microscopic features of these networks also are very different, with the extreme being that C. elegans neurons generate slow, graded potentials instead of discrete action potentials or spikes. Despite these caveats, we can ask how the patterns of neural activity in these systems transform under coarseâ€“graining across a range from two to five decades. Results for the variance of the coarseâ€“grained activity, $M_{2}(k)$ from Eq (151), are shown in Fig 37. The apparent universality of these results is tantalizing.</p>
</blockquote>
<p>åœ¨æˆ‘ä»¬å®Œæˆè¿™ç¯‡ç»¼è¿°æ—¶ï¼ŒMunn ç­‰äººï¼ˆ2024ï¼‰æŠ¥é“äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„ç»“æœã€‚ä»–ä»¬æ²¡æœ‰æŸ¥çœ‹å•ä¸ªæœ‰æœºä½“ä¸­å¤šä¸ªå¤§è„‘åŒºåŸŸçš„å®éªŒï¼Œè€Œæ˜¯æŸ¥çœ‹äº†è®¸å¤šä¸åŒæœ‰æœºä½“çš„å®éªŒï¼Œä»å¾®å°çš„çº¿è™« C. elegans åˆ°ä¸æˆ‘ä»¬éå¸¸ç›¸ä¼¼çš„çµé•¿ç±»åŠ¨ç‰©ã€‚è¿™äº›å®éªŒä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æŠ€æœ¯å·®å¼‚ï¼ŒåŒ…æ‹¬é’™æŒ‡ç¤ºè›‹ç™½ï¼ˆÂ§III.Cï¼‰çš„å·®å¼‚å’Œé‡‡æ ·ç‡çš„å·®å¼‚ï¼›å®Œå…¨åˆ†è¾¨å•ä¸ªç¥ç»å…ƒä¸â€œæ„Ÿå…´è¶£åŒºåŸŸâ€ï¼›ä»¥åŠåœ¨è¾ƒå°æ¨¡å‹æœ‰æœºä½“ä¸­è®°å½•æ•´ä¸ªå¤§è„‘ä¸åœ¨è¾ƒå¤§æœ‰æœºä½“ä¸­è®°å½•å•ä¸ªæ„Ÿè§‰æˆ–è¿åŠ¨åŒºåŸŸã€‚ è¿™äº›ç½‘ç»œçš„è®¸å¤šå¾®è§‚ç‰¹å¾ä¹Ÿéå¸¸ä¸åŒï¼Œæç«¯æƒ…å†µæ˜¯ C. elegans ç¥ç»å…ƒäº§ç”Ÿç¼“æ…¢çš„æ¸å˜ç”µä½ï¼Œè€Œä¸æ˜¯ç¦»æ•£çš„åŠ¨ä½œç”µä½æˆ–å°–å³°ã€‚å°½ç®¡å­˜åœ¨è¿™äº›è­¦å‘Šï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥è¯¢é—®è¿™äº›ç³»ç»Ÿä¸­çš„ç¥ç»æ´»åŠ¨æ¨¡å¼å¦‚ä½•åœ¨ä»ä¸¤ä¸ªåˆ°äº”ä¸ªæ•°é‡çº§çš„èŒƒå›´å†…é€šè¿‡ç²—ç²’åŒ–è¿›è¡Œè½¬æ¢ã€‚å›¾ 37 æ˜¾ç¤ºäº†ç²—ç²’åŒ–æ´»åŠ¨æ–¹å·® $M_{2}(k)$ï¼ˆæ¥è‡ªæ–¹ç¨‹ï¼ˆ151ï¼‰ï¼‰çš„ç»“æœã€‚è¿™äº›ç»“æœçš„æ˜æ˜¾æ™®é€‚æ€§ä»¤äººç€è¿·ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/DRIsqQXhzF7KAg5.png" alt=""  /></p>
<p>Scaling in the variance of neural activity, Eq (151), as a function of scale across multiple species (Munn et al., 2024). (A) Zebrafish. (B) The worm C. elegans. (C) The fruit fly Drosophila melanogaster. (D) Mouse primary visual cortex. (E) Macaque primary visual and motor cortices. Grey lines are results from individual animals, red points with errors are means  within species, and red lines are fits to $M_{2}\propto K^{\widetilde{\alpha}}$, with exponents as shown. Expectations for independent (blue) and completely correlated (green) populations corresponding to the dashed lines in Fig 34A.</p>
</blockquote>
<p>ç¥ç»æ´»åŠ¨æ–¹å·®çš„ç¼©æ”¾ï¼Œæ–¹ç¨‹ï¼ˆ151ï¼‰ï¼Œä½œä¸ºè·¨å¤šä¸ªç‰©ç§çš„å°ºåº¦å‡½æ•°ï¼ˆMunn ç­‰äººï¼Œ2024ï¼‰ã€‚(A) æ–‘é©¬é±¼ã€‚(B) çº¿è™« C. elegansã€‚(C) æœè‡ Drosophila melanogasterã€‚(D) å°é¼ åˆçº§è§†è§‰çš®å±‚ã€‚(E) çŒ•çŒ´åˆçº§è§†è§‰å’Œè¿åŠ¨çš®å±‚ã€‚ç°çº¿æ˜¯å•ä¸ªåŠ¨ç‰©çš„ç»“æœï¼Œå¸¦è¯¯å·®çš„çº¢ç‚¹æ˜¯ç‰©ç§å†…çš„å‡å€¼ï¼Œçº¢çº¿æ˜¯å¯¹ $M_{2}\propto K^{\widetilde{\alpha}}$ çš„æ‹Ÿåˆï¼ŒæŒ‡æ•°å¦‚å›¾æ‰€ç¤ºã€‚ç‹¬ç«‹ï¼ˆè“è‰²ï¼‰å’Œå®Œå…¨ç›¸å…³ï¼ˆç»¿è‰²ï¼‰ç¾¤ä½“çš„æœŸæœ›å¯¹åº”äºå›¾ 34A ä¸­çš„è™šçº¿ã€‚</p>
</blockquote>
<h1 id="by-analogy-with-momentum-shell-methods">By analogy with momentum shell methods<a hidden class="anchor" aria-hidden="true" href="#by-analogy-with-momentum-shell-methods">#</a></h1>
<blockquote>
<p>In problems where â€œscaleâ€ really is a length scale, coarseâ€“graining is a gradual blurring out of spatial detail much as what happens when we look through a microscope and defocus. In that analogy, the spatial pattern is Fourier transformed and then reconstructed using only a limited range of wavelengths. Concretely, if we start with variables $\phi(\vec{x})$ in a $d$â€“dimensional space with coordinates $\vec{x}$, the coarseâ€“graining operation becomes</p>
<p>$$
\begin{aligned}
\phi(\vec{x}) &amp;\rightarrow \phi_{\Lambda}(\vec{x}) = z_{\Lambda}\int_{|\vec{k}|&lt;\Lambda}\frac{\mathrm{d}^{d}k}{(2\pi)^{d}}e^{i\vec{k}\cdot\vec{x}}\widetilde{\phi}(\vec{k})\\
\widetilde{\phi}(\vec{k}) &amp;= \int \mathrm{d}^{d}x e^{-i\vec{k}\cdot\vec{x}}\phi(\vec{x})
\end{aligned}
$$</p>
<p>where $\Lambda = \pi/l$ cuts off contributions below a length scale $l$ and $z_{\Lambda}$ serves to (re)normalize the variables; in the microscopic analogy this compensates for the loss of contrast as we defocus. As in real space we are interested in how the probability distribution $P_{\lambda}[\phi_{\Lambda}]$ evolves as a function of the cutoff $\Lambda$. Since the Fourier variables are continuous (in the limit of a large system) we can make infinitesimal changes $\Lambda\to\Lambda-\mathrm{d}\Lambda$. In quantum mechanics wave with wavevector $\vec{k}$ describe particles with momentum $\vec{p}=\hbar\vec{k}$, so that average over the details in a range $\Lambda âˆ’ \mathrm{d}\Lambda &lt; |\vec{k}| &lt; \Lambda$ is equivalent to integrating out a â€œmomentum shellâ€ (Wilson and Kogut, 1974).</p>
</blockquote>
<p>åœ¨â€œå°ºåº¦â€çœŸæ­£æ˜¯é•¿åº¦å°ºåº¦çš„é—®é¢˜ä¸­ï¼Œç²—ç²’åŒ–æ˜¯ç©ºé—´ç»†èŠ‚çš„é€æ¸æ¨¡ç³Šï¼Œå°±åƒæˆ‘ä»¬é€šè¿‡æ˜¾å¾®é•œè§‚å¯Ÿå¹¶å¤±ç„¦æ—¶å‘ç”Ÿçš„æƒ…å†µä¸€æ ·ã€‚åœ¨è¿™ä¸ªç±»æ¯”ä¸­ï¼Œç©ºé—´æ¨¡å¼è¢«å‚…é‡Œå¶å˜æ¢ï¼Œç„¶åä»…ä½¿ç”¨æœ‰é™èŒƒå›´çš„æ³¢é•¿è¿›è¡Œé‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬ä» $d$ ç»´ç©ºé—´ä¸­åæ ‡ä¸º $\vec{x}$ çš„å˜é‡ $\phi(\vec{x})$ å¼€å§‹ï¼Œç²—ç²’åŒ–æ“ä½œå˜ä¸º</p>
<p>$$
\begin{aligned}
\phi(\vec{x}) &amp;\rightarrow \phi_{\Lambda}(\vec{x}) = z_{\Lambda}\int_{|\vec{k}|&lt;\Lambda}\frac{\mathrm{d}^{d}k}{(2\pi)^{d}}e^{i\vec{k}\cdot\vec{x}}\widetilde{\phi}(\vec{k})\\
\widetilde{\phi}(\vec{k}) &amp;= \int \mathrm{d}^{d}x e^{-i\vec{k}\cdot\vec{x}}\phi(\vec{x})
\end{aligned}
$$</p>
<p>å…¶ä¸­ $\Lambda = \pi/l$ æˆªæ–­äº†ä½äºé•¿åº¦å°ºåº¦ $l$ çš„è´¡çŒ®ï¼Œ$z_{\Lambda}$ ç”¨äºï¼ˆé‡æ–°ï¼‰å½’ä¸€åŒ–å˜é‡ï¼›åœ¨å¾®è§‚ç±»æ¯”ä¸­ï¼Œè¿™è¡¥å¿äº†æˆ‘ä»¬å¤±ç„¦æ—¶å¯¹æ¯”åº¦çš„æŸå¤±ã€‚ä¸å®ç©ºé—´ä¸€æ ·ï¼Œæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯æ¦‚ç‡åˆ†å¸ƒ $P_{\lambda}[\phi_{\Lambda}]$ å¦‚ä½•éšç€æˆªæ­¢å€¼ $\Lambda$ çš„å˜åŒ–è€Œæ¼”å˜ã€‚ç”±äºå‚…é‡Œå¶å˜é‡æ˜¯è¿ç»­çš„ï¼ˆåœ¨å¤§ç³»ç»Ÿçš„æé™ä¸‹ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œæ— ç©·å°å˜åŒ– $\Lambda\to\Lambda-\mathrm{d}\Lambda$ã€‚åœ¨é‡å­åŠ›å­¦ä¸­ï¼Œæ³¢çŸ¢ä¸º $\vec{k}$ çš„æ³¢æè¿°åŠ¨é‡ä¸º $\vec{p}=\hbar\vec{k}$ çš„ç²’å­ï¼Œå› æ­¤åœ¨èŒƒå›´ $\Lambda âˆ’ \mathrm{d}\Lambda &lt; |\vec{k}| &lt; \Lambda$ å†…å¯¹ç»†èŠ‚è¿›è¡Œå¹³å‡ç›¸å½“äºç§¯åˆ†å‡ºä¸€ä¸ªâ€œåŠ¨é‡å£³â€ï¼ˆWilson å’Œ Kogutï¼Œ1974ï¼‰ã€‚</p>
<blockquote>
<p>Momentum is conserved in systems with translation invariance. Independent of these physical principles, spatial translation invariance privileges the Fourier transform. As an example, if variables $z_{i}$ live on a lattice of points $\vec{x}_{i}$, translation invariance means that the covariance matrix elements $C_{ij}$ can depend only on the difference in positions,</p>
<p>$$
C_{ij} = C(\vec{x}_{i} - \vec{x}_{j})
$$</p>
<p>this matrix is diagonalized in a Fourier basis,</p>
<p>$$
\begin{aligned}
\sum_{j=1}^{N}C_{ij}u_{jr} &amp;= \lambda_{r}u_{ir}\\
u_{jr} &amp;\propto \exp{(i\vec{k}_{r}\cdot\vec{x}_{j})}
\end{aligned}
$$</p>
<p>where we can put the modes in order by the rank of the eigenvalue $r$.</p>
</blockquote>
<p>åœ¨å…·æœ‰å¹³ç§»ä¸å˜æ€§çš„ç³»ç»Ÿä¸­ï¼ŒåŠ¨é‡æ˜¯å®ˆæ’çš„ã€‚ç‹¬ç«‹äºè¿™äº›ç‰©ç†åŸç†ï¼Œç©ºé—´å¹³ç§»ä¸å˜æ€§ä½¿å‚…é‡Œå¶å˜æ¢å…·æœ‰ç‰¹æƒã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œå¦‚æœå˜é‡ $z_{i}$ ä½äºç‚¹ $\vec{x}_{i}$ çš„æ™¶æ ¼ä¸Šï¼Œå¹³ç§»ä¸å˜æ€§æ„å‘³ç€åæ–¹å·®çŸ©é˜µå…ƒç´  $C_{ij}$ åªèƒ½ä¾èµ–äºä½ç½®çš„å·®å¼‚ï¼Œ</p>
<p>$$
C_{ij} = C(\vec{x}_{i} - \vec{x}_{j})
$$</p>
<p>è¯¥çŸ©é˜µåœ¨å‚…é‡Œå¶åŸºä¸­å¯¹è§’åŒ–ï¼Œ</p>
<p>$$
\begin{aligned}
\sum_{j=1}^{N}C_{ij}u_{jr} &amp;= \lambda_{r}u_{ir}\\
u_{jr} &amp;\propto \exp{(i\vec{k}_{r}\cdot\vec{x}_{j})}
\end{aligned}
$$</p>
<p>æˆ‘ä»¬å¯ä»¥æ ¹æ®ç‰¹å¾å€¼çš„ç§© $r$ å¯¹æ¨¡å¼è¿›è¡Œæ’åºã€‚</p>
<blockquote>
<p>In the usual applications of the RG, large momenta correspond to small eigenvalues of the covariance matrix. Thus suggests that we can construct coarseâ€“grained variables by filtering out the â€œmodesâ€ that correspond to small eigenvalues, without reference to space or momenta (Bradde and Bialek, 2017). This connects coarsegraining to a more familiar data analysis technique, principal components analysis (Shlens, 2014).</p>
</blockquote>
<p>åœ¨ RG çš„å¸¸è§„åº”ç”¨ä¸­ï¼Œå¤§åŠ¨é‡å¯¹åº”äºåæ–¹å·®çŸ©é˜µçš„å°ç‰¹å¾å€¼ã€‚è¿™è¡¨æ˜æˆ‘ä»¬å¯ä»¥é€šè¿‡æ»¤é™¤å¯¹åº”äºå°ç‰¹å¾å€¼çš„â€œæ¨¡å¼â€æ¥æ„å»ºç²—ç²’åŒ–å˜é‡ï¼Œè€Œä¸å‚è€ƒç©ºé—´æˆ–åŠ¨é‡ï¼ˆBradde å’Œ Bialekï¼Œ2017ï¼‰ã€‚è¿™å°†ç²—ç²’åŒ–ä¸æ›´ç†Ÿæ‚‰çš„æ•°æ®åˆ†ææŠ€æœ¯ä¸»æˆåˆ†åˆ†æï¼ˆShlensï¼Œ2014ï¼‰è”ç³»èµ·æ¥ã€‚</p>
<blockquote>
<p>Concretely, if we start with microscopic variables $\{\sigma_{i}\}$, we can compute the covariance matrix as usual</p>
<p>$$
C_{ij} = \langle(\sigma_{i}-\langle\sigma_{i}\rangle)(\sigma_{j}-\langle\sigma_{j}\rangle)\rangle
$$</p>
<p>and then we have eigenvalues and eigenvectors as in Eq (160). Letâ€™s choose the rank $r$ so that $\lambda_{1}\geq \lambda_{2}\cdots\lambda_{N}$.  We can define a projection onto the $\hat{K}$ modes that make the largest contribution to the variance,</p>
<p>$$
\begin{aligned}
\hat{P}(\hat{K}) &amp;= \sum_{r=1}^{\hat{K}} u_{ir}u_{jr}\\
\phi_{\hat{K}}(i) &amp;= z_{i}(\hat{K})\sum_{j}\hat{P}_{ij}(\hat{K})[\sigma_{i}-\langle\sigma_{i}\rangle]
\end{aligned}
$$</p>
<p>with the normalization $z_{i}(\hat{K})$ such that $\langle[\phi_{\hat{K}}(i)]^{2}\rangle = 1$.</p>
</blockquote>
<p>å…·ä½“æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬ä»å¾®è§‚å˜é‡ $\{\sigma_{i}\}$ å¼€å§‹ï¼Œæˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·è®¡ç®—åæ–¹å·®çŸ©é˜µ</p>
<p>$$
C_{ij} = \langle(\sigma_{i}-\langle\sigma_{i}\rangle)(\sigma_{j}-\langle\sigma_{j}\rangle)\rangle
$$</p>
<p>ç„¶åæˆ‘ä»¬æœ‰å¦‚æ–¹ç¨‹ï¼ˆ160ï¼‰æ‰€ç¤ºçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚è®©æˆ‘ä»¬é€‰æ‹©ç§© $r$ï¼Œä½¿å¾— $\lambda_{1}\geq \lambda_{2}\cdots\lambda_{N}$ã€‚æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªæŠ•å½±åˆ°å¯¹æ–¹å·®è´¡çŒ®æœ€å¤§çš„ $\hat{K}$ ä¸ªæ¨¡å¼ä¸Šï¼Œ</p>
<p>$$
\begin{aligned}
\hat{P}(\hat{K}) &amp;= \sum_{r=1}^{\hat{K}} u_{ir}u_{jr}\\
\phi_{\hat{K}}(i) &amp;= z_{i}(\hat{K})\sum_{j}\hat{P}_{ij}(\hat{K})[\sigma_{i}-\langle\sigma_{i}\rangle]
\end{aligned}
$$</p>
<p>å…¶ä¸­å½’ä¸€åŒ– $z_{i}(\hat{K})$ ä½¿å¾— $\langle[\phi_{\hat{K}}(i)]^{2}\rangle = 1$ã€‚</p>
<blockquote>
<p>As before, we want to follow the distribution of the individual coarseâ€“grained variables, $P_{\hat{K}}(\phi_{\hat{K}})$; results are shown in Fig 38A. To be sure that we have control over the full matrix $C_{ij}$ we look at clusters of $N = 128$ neurons identified through the real space coarseâ€“graining above. We can then filter out half of the modes, so that $\hat{K} = 32$, resulting in a distribution $P_{\hat{K}}(\phi_{\hat{K}})$ that still has some fine structure. If we reduce to $\hat{K} = 32$ these wiggles disappear but the distribution remains asymmetric with  long tails. This pattern continues as we reduce to $\hat{K} = 16$ and then $\hat{K} = 8$, and in these last steps the distribution hardly changes. This suggests that as we coarseâ€“grain, the distribution flows toward a fixed form. Importantly this form is very different from the Gaussian that would be guaranteed by the central limit theorem if correlations were weak.</p>
</blockquote>
<p>å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬æƒ³è¦è·Ÿè¸ªå•ä¸ªç²—ç²’åŒ–å˜é‡çš„åˆ†å¸ƒï¼Œ$P_{\hat{K}}(\phi_{\hat{K}})$ï¼›ç»“æœæ˜¾ç¤ºåœ¨å›¾ 38A ä¸­ã€‚ä¸ºäº†ç¡®ä¿æˆ‘ä»¬å¯¹æ•´ä¸ªçŸ©é˜µ $C_{ij}$ æœ‰æ§åˆ¶ï¼Œæˆ‘ä»¬æŸ¥çœ‹äº†é€šè¿‡ä¸Šè¿°å®ç©ºé—´ç²—ç²’åŒ–è¯†åˆ«çš„ $N = 128$ ä¸ªç¥ç»å…ƒçš„ç°‡ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥æ»¤é™¤ä¸€åŠçš„æ¨¡å¼ï¼Œä½¿å¾— $\hat{K} = 32$ï¼Œå¾—åˆ°çš„åˆ†å¸ƒ $P_{\hat{K}}(\phi_{\hat{K}})$ ä»ç„¶å…·æœ‰ä¸€äº›ç»†å¾®ç»“æ„ã€‚å¦‚æœæˆ‘ä»¬å‡å°‘åˆ° $\hat{K} = 32$ï¼Œè¿™äº›æ³¢åŠ¨æ¶ˆå¤±äº†ï¼Œä½†åˆ†å¸ƒä»ç„¶æ˜¯ä¸å¯¹ç§°çš„ï¼Œå¹¶ä¸”å…·æœ‰é•¿å°¾ã€‚è¿™ç§æ¨¡å¼åœ¨æˆ‘ä»¬å‡å°‘åˆ° $\hat{K} = 16$ ç„¶å $\hat{K} = 8$ æ—¶ç»§ç»­å­˜åœ¨ï¼Œåœ¨è¿™æœ€åå‡ ä¸ªæ­¥éª¤ä¸­ï¼Œåˆ†å¸ƒå‡ ä¹æ²¡æœ‰å˜åŒ–ã€‚è¿™è¡¨æ˜éšç€æˆ‘ä»¬è¿›è¡Œç²—ç²’åŒ–ï¼Œåˆ†å¸ƒè¶‹å‘äºä¸€ä¸ªå›ºå®šå½¢å¼ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ç§å½¢å¼ä¸å¦‚æœç›¸å…³æ€§è¾ƒå¼±ï¼Œä¸­å¿ƒæé™å®šç†æ‰€ä¿è¯çš„é«˜æ–¯å½¢å¼éå¸¸ä¸åŒã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/5dgHeQK8aONPkqz.png" alt=""  /></p>
<p>Coarseâ€“graining in groups of $N = 128$ neurons via â€œmomentum shellsâ€ (Meshulam et al., 2018). (A) Following the distribution of individual coarseâ€“grained variables from Eq (164). Different colors correspond to keeping different numbers of modes $\hat{K}$, as in inset; dashed line is a Gaussian for comparison. (B) Dynamic scaling of the correlation time for fluctuations in mode r, Eq (166), vs the associated eigenvalue of the covariance matrix, $\tau_{c}(r)\propto \lambda_{r}^{\widetilde{z}^{\prime}}$ , $\widetilde{z}^{\prime} = 0.37 \pm 0.04$.</p>
</blockquote>
<p>é€šè¿‡â€œåŠ¨é‡å£³â€å¯¹ $N = 128$ ä¸ªç¥ç»å…ƒè¿›è¡Œç²—ç²’åŒ–ï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼‰ã€‚(A) è·Ÿè¸ªæ¥è‡ªæ–¹ç¨‹ï¼ˆ164ï¼‰çš„å•ä¸ªç²—ç²’åŒ–å˜é‡çš„åˆ†å¸ƒã€‚ä¸åŒé¢œè‰²å¯¹åº”äºä¿ç•™ä¸åŒæ•°é‡çš„æ¨¡å¼ $\hat{K}$ï¼Œå¦‚æ’å›¾æ‰€ç¤ºï¼›è™šçº¿ä¸ºé«˜æ–¯åˆ†å¸ƒä»¥ä¾›æ¯”è¾ƒã€‚(B) æ¨¡å¼ r ä¸­æ³¢åŠ¨çš„ç›¸å…³æ—¶é—´çš„åŠ¨æ€ç¼©æ”¾ï¼Œæ–¹ç¨‹ï¼ˆ166ï¼‰ï¼Œä¸åæ–¹å·®çŸ©é˜µçš„ç›¸å…³ç‰¹å¾å€¼ $\tau_{c}(r)\propto \lambda_{r}^{\widetilde{z}^{\prime}}$ï¼Œå…¶ä¸­ $\widetilde{z}^{\prime} = 0.37 \pm 0.04$ã€‚</p>
</blockquote>
<blockquote>
<p>The intuition behind dynamic scaling is that fluctuations on larger length scales relax more slowly, and we have seen that this generalizes to a network of neurons even though the meaning of â€œscaleâ€ now if more abstract (Fig 35). By transforming to basis that diagonalizes the covariance matrix we have isolated the modes of fluctuation that are independent at second order, and it is natural to ask how these fluctuations along these modes relax. Variations along mode $r$ are define by</p>
<p>$$
\widetilde{\phi}_{r} = \sum_{i=1}^{N}[\sigma_{i}-\langle\sigma_{i}\rangle]u_{ir}
$$</p>
<p>and the correlation function is</p>
<p>$$
C_{r}(t) = \langle\widetilde{\phi}_{r}(t_{0})\widetilde{\phi}_{r}(t_{0}+t)\rangle
$$</p>
<p>Dynamic scaling is the statement that all these correlations collapse when time is scaled by a single correlation time, and that this correlation time itself has a powerâ€“law dependence of scale. In the usual examples this means $\tau_{c}\propto |\vec{k}|^{z}$ (Hohenberg and Halperin, 1977), but near a critical point the eigenvalues of the covariance matrix also have a powerâ€“law dependence on $|\vec{k}|$, so we can test directly for $\tau_{c}\propto \lambda^{\widetilde{z}^{\prime}}$ as shown in Fig 38B. As before, the shortest correlation times are limited by the response time of the fluorescent proteins that report on electrical activity, and the longest times are limited by the magnitude of the dynamic scaling exponent; nonetheless we can observe reasonably precise scaling across two decades in $\lambda$.</p>
</blockquote>
<p>åŠ¨æ€ç¼©æ”¾çš„ç›´è§‰æ˜¯ï¼Œæ›´å¤§é•¿åº¦å°ºåº¦ä¸Šçš„æ³¢åŠ¨å¼›è±«å¾—æ›´æ…¢ï¼Œå°½ç®¡â€œå°ºåº¦â€çš„å«ä¹‰ç°åœ¨æ›´æŠ½è±¡ï¼ˆå›¾ 35ï¼‰ï¼Œä½†æˆ‘ä»¬å·²ç»çœ‹åˆ°è¿™å¯ä»¥æ¨å¹¿åˆ°ç¥ç»å…ƒç½‘ç»œã€‚é€šè¿‡è½¬æ¢åˆ°å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œå¯¹è§’åŒ–çš„åŸºï¼Œæˆ‘ä»¬å·²ç»éš”ç¦»äº†åœ¨äºŒé˜¶ä¸Šç‹¬ç«‹çš„æ³¢åŠ¨æ¨¡å¼ï¼Œè‡ªç„¶ä¼šé—®è¿™äº›æ¨¡å¼ä¸Šçš„æ³¢åŠ¨æ˜¯å¦‚ä½•å¼›è±«çš„ã€‚æ²¿æ¨¡å¼ $r$ çš„å˜åŒ–å®šä¹‰ä¸º</p>
<p>$$
\widetilde{\phi}_{r} = \sum_{i=1}^{N}[\sigma_{i}-\langle\sigma_{i}\rangle]u_{ir}
$$</p>
<p>ç›¸å…³å‡½æ•°ä¸º</p>
<p>$$
C_{r}(t) = \langle\widetilde{\phi}_{r}(t_{0})\widetilde{\phi}_{r}(t_{0}+t)\rangle
$$</p>
<p>åŠ¨æ€ç¼©æ”¾æ˜¯è¿™æ ·ä¸€ç§è¯´æ³•ï¼šå½“æ—¶é—´æŒ‰å•ä¸€ç›¸å…³æ—¶é—´è¿›è¡Œç¼©æ”¾æ—¶ï¼Œæ‰€æœ‰è¿™äº›ç›¸å…³æ€§éƒ½ä¼šåç¼©ï¼Œå¹¶ä¸”è¯¥ç›¸å…³æ—¶é—´æœ¬èº«å…·æœ‰å°ºåº¦çš„å¹‚å¾‹ä¾èµ–ã€‚åœ¨é€šå¸¸çš„ä¾‹å­ä¸­ï¼Œè¿™æ„å‘³ç€ $\tau_{c}\propto |\vec{k}|^{z}$ï¼ˆHohenberg å’Œ Halperinï¼Œ1977ï¼‰ï¼Œä½†åœ¨ä¸´ç•Œç‚¹é™„è¿‘ï¼Œåæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼ä¹Ÿå¯¹ $|\vec{k}|$ å…·æœ‰å¹‚å¾‹ä¾èµ–ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›´æ¥æµ‹è¯• $\tau_{c}\propto \lambda^{\widetilde{z}^{\prime}}$ï¼Œå¦‚å›¾ 38B æ‰€ç¤ºã€‚å¦‚å‰æ‰€è¿°ï¼Œæœ€çŸ­çš„ç›¸å…³æ—¶é—´å—åˆ°æŠ¥å‘Šç”µæ´»åŠ¨çš„è§å…‰è›‹ç™½å“åº”æ—¶é—´çš„é™åˆ¶ï¼Œæœ€é•¿çš„æ—¶é—´å—åˆ°åŠ¨æ€ç¼©æ”¾æŒ‡æ•°å¤§å°çš„é™åˆ¶ï¼›å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥è§‚å¯Ÿåˆ° $\lambda$ ä¸Šä¸¤ä¸ªæ•°é‡çº§çš„ç›¸å½“ç²¾ç¡®çš„ç¼©æ”¾ã€‚</p>
<blockquote>
<p>The dynamic exponent $\widetilde{z}^{\prime}$ that one finds by looking at the correlation times of the modes should be related to the one we see via coarseâ€“graining in real space, $\widetilde{z}$(Fig 35C), through the exponent Î¼ that describes the decay of  the eigenvalues of the covariance matrix, $\widetilde{z} = \mu\widetilde{z}^{\prime}$. This works, although error bars are large (Meshulam et al., 2018). More importantly, these results indicate that the network has no single characteristic time scale, but rather a continuum of time scales that can be accessed by probing on different scales.</p>
</blockquote>
<p>é€šè¿‡æŸ¥çœ‹æ¨¡å¼çš„ç›¸å…³æ—¶é—´æ‰¾åˆ°çš„åŠ¨æ€æŒ‡æ•° $\widetilde{z}^{\prime}$ åº”è¯¥ä¸æˆ‘ä»¬é€šè¿‡å®ç©ºé—´ç²—ç²’åŒ–çœ‹åˆ°çš„æŒ‡æ•° $\widetilde{z}$(å›¾ 35C) ç›¸å…³ï¼Œé€šè¿‡æè¿°åæ–¹å·®çŸ©é˜µç‰¹å¾å€¼è¡°å‡çš„æŒ‡æ•° Î¼ï¼Œ$\widetilde{z} = \mu\widetilde{z}^{\prime}$ã€‚è¿™ç¡®å®æœ‰æ•ˆï¼Œå°½ç®¡è¯¯å·®æ¡å¾ˆå¤§ï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™äº›ç»“æœè¡¨æ˜ç½‘ç»œæ²¡æœ‰å•ä¸€çš„ç‰¹å¾æ—¶é—´å°ºåº¦ï¼Œè€Œæ˜¯ä¸€ä¸ªè¿ç»­çš„æ—¶é—´å°ºåº¦ï¼Œå¯ä»¥é€šè¿‡åœ¨ä¸åŒå°ºåº¦ä¸Šæ¢æµ‹æ¥è®¿é—®ã€‚</p>
<h1 id="rg-as-a-path-to-understanding">RG as a path to understanding<a hidden class="anchor" aria-hidden="true" href="#rg-as-a-path-to-understanding">#</a></h1>
<blockquote>
<p>If we believe there is an underlying simplicity to be found amidst the complexity of neural network function and activity, we might want to pause for a moment to convince ourselves that following the RG simplification can actually lead us there. This quest now feels attainable, given the explosive experimental progress in obtaining datasets with increasing number of neurons, as in the examples above. While we may not know how to manipulate â€œtemperatureâ€ or â€œmagnetizationâ€ in the brain, we are gaining decades in the sheer number of monitored neurons.</p>
</blockquote>
<p>å¦‚æœæˆ‘ä»¬ç›¸ä¿¡åœ¨ç¥ç»ç½‘ç»œåŠŸèƒ½å’Œæ´»åŠ¨çš„å¤æ‚æ€§ä¸­å¯ä»¥æ‰¾åˆ°æ½œåœ¨çš„ç®€å•æ€§ï¼Œæˆ‘ä»¬å¯èƒ½æƒ³æš‚åœç‰‡åˆ»æ¥è¯´æœè‡ªå·±ï¼Œéµå¾ª RG ç®€åŒ–å®é™…ä¸Šå¯ä»¥å¼•å¯¼æˆ‘ä»¬åˆ°è¾¾é‚£é‡Œã€‚é‰´äºåœ¨è·å¾—è¶Šæ¥è¶Šå¤šç¥ç»å…ƒæ•°æ®é›†æ–¹é¢çš„çˆ†ç‚¸æ€§å®éªŒè¿›å±•ï¼Œå¦‚ä¸Šé¢çš„ä¾‹å­æ‰€ç¤ºï¼Œè¿™ä¸€è¿½æ±‚ç°åœ¨æ„Ÿè§‰æ˜¯å¯ä»¥å®ç°çš„ã€‚è™½ç„¶æˆ‘ä»¬å¯èƒ½ä¸çŸ¥é“å¦‚ä½•æ“çºµå¤§è„‘ä¸­çš„â€œæ¸©åº¦â€æˆ–â€œç£åŒ–å¼ºåº¦â€ï¼Œä½†æˆ‘ä»¬æ­£åœ¨é€šè¿‡ç›‘æµ‹çš„ç¥ç»å…ƒæ•°é‡è·å¾—æ•°åå¹´çš„è¿›æ­¥ã€‚</p>
<blockquote>
<p>The renormalization group is a powerful theoretical structure. Because we do not have a microscopic model for neural dynamics, we are not yet able to exploit this structure. What we have done instead is to adopt an RGâ€“inspired approach to data analysis, which has been described as a â€œphenomenological renormalization groupâ€ (Nicoletti et al., 2020) or â€œiterative coarsegrainingâ€ (Munn et al., 2024). If we apply these approaches to well understood equilibrium statistical mechanics problems, the most interesting outcome would be the flow of probability distributions toward some fixed, nonâ€“Gaussian form, and the appearance of powerâ€“law scaling along this trajectory, as would happen at a critical point. Remarkably, this is what has been found, both in the initial application to the hippocampus and now in many other systems; scaling exponents are reproducible and perhaps even universal. It is tempting to conclude that the underlying network dynamics must be described by a theory which is at a nonâ€“trivial fixed point of the renormalization group.</p>
</blockquote>
<p>é‡æ•´åŒ–ç¾¤æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç†è®ºç»“æ„ã€‚ç”±äºæˆ‘ä»¬æ²¡æœ‰ç¥ç»åŠ¨åŠ›å­¦çš„å¾®è§‚æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜æ— æ³•åˆ©ç”¨è¿™ä¸ªç»“æ„ã€‚ç›¸åï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å— RG å¯å‘çš„æ•°æ®åˆ†ææ–¹æ³•ï¼Œè¿™è¢«æè¿°ä¸ºâ€œç°è±¡å­¦é‡æ•´åŒ–ç¾¤â€ï¼ˆNicoletti ç­‰äººï¼Œ2020ï¼‰æˆ–â€œè¿­ä»£ç²—ç²’åŒ–â€ï¼ˆMunn ç­‰äººï¼Œ2024ï¼‰ã€‚å¦‚æœæˆ‘ä»¬å°†è¿™äº›æ–¹æ³•åº”ç”¨äºç†è§£è‰¯å¥½çš„å¹³è¡¡ç»Ÿè®¡åŠ›å­¦é—®é¢˜ï¼Œæœ€æœ‰è¶£çš„ç»“æœå°†æ˜¯æ¦‚ç‡åˆ†å¸ƒæœç€æŸç§å›ºå®šçš„éé«˜æ–¯å½¢å¼æµåŠ¨ï¼Œä»¥åŠæ²¿ç€è¿™æ¡è½¨è¿¹å‡ºç°å¹‚å¾‹ç¼©æ”¾ï¼Œå°±åƒåœ¨ä¸´ç•Œç‚¹å‘ç”Ÿçš„é‚£æ ·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ­£æ˜¯æ‰€å‘ç°çš„ï¼Œæ— è®ºæ˜¯åœ¨å¯¹æµ·é©¬ä½“çš„åˆå§‹åº”ç”¨ä¸­ï¼Œè¿˜æ˜¯ç°åœ¨åœ¨è®¸å¤šå…¶ä»–ç³»ç»Ÿä¸­ï¼›ç¼©æ”¾æŒ‡æ•°æ˜¯å¯é‡å¤çš„ï¼Œç”šè‡³å¯èƒ½æ˜¯æ™®é€‚çš„ã€‚å¾ˆå®¹æ˜“å¾—å‡ºç»“è®ºï¼ŒåŸºç¡€ç½‘ç»œåŠ¨åŠ›å­¦å¿…é¡»ç”±é‡æ•´åŒ–ç¾¤çš„éå¹³å‡¡ä¸åŠ¨ç‚¹æè¿°çš„ç†è®ºæ¥æè¿°ã€‚</p>
<blockquote>
<p>We should be cautious. Is it possible that some of the behaviors under coarseâ€“graining that we associate with RG fixed points could emerge, more generically, in nonâ€“equilibrium systems? Nicoletti et al. (2020) addressed this by analyzing simulations of the contact process, in which binary variables are turned on with a probability per unit time proportional to the density of active variables at neighboring sites, and then deactivate with a fixed probability per unit time. This model has one parameter, the proportionality constant in the activation rate, and there is a critical value that depends on the geometry of the network (Marro and Dickman, 1999). Below the critical point the fully inactive state is absorbing, so the question is whether the phenomenological RG can distinguish the critical point from superâ€“critical behaviors.</p>
</blockquote>
<p>æˆ‘ä»¬åº”è¯¥ä¿æŒè°¨æ…ã€‚æ˜¯å¦æœ‰å¯èƒ½æˆ‘ä»¬ä¸ RG ä¸åŠ¨ç‚¹ç›¸å…³è”çš„ä¸€äº›ç²—ç²’åŒ–è¡Œä¸ºå¯ä»¥æ›´æ™®éåœ°å‡ºç°åœ¨éå¹³è¡¡ç³»ç»Ÿä¸­ï¼ŸNicoletti ç­‰äººï¼ˆ2020ï¼‰é€šè¿‡åˆ†ææ¥è§¦è¿‡ç¨‹çš„æ¨¡æ‹Ÿæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨è¯¥è¿‡ç¨‹ä¸­ï¼ŒäºŒè¿›åˆ¶å˜é‡ä»¥ä¸é‚»è¿‘ä½ç½®æ´»è·ƒå˜é‡å¯†åº¦æˆæ­£æ¯”çš„å•ä½æ—¶é—´æ¦‚ç‡è¢«æ¿€æ´»ï¼Œç„¶åä»¥å›ºå®šçš„å•ä½æ—¶é—´æ¦‚ç‡è¢«åœç”¨ã€‚è¯¥æ¨¡å‹æœ‰ä¸€ä¸ªå‚æ•°ï¼Œå³æ¿€æ´»ç‡ä¸­çš„æ¯”ä¾‹å¸¸æ•°ï¼Œå¹¶ä¸”å­˜åœ¨ä¸€ä¸ªå–å†³äºç½‘ç»œå‡ ä½•å½¢çŠ¶çš„ä¸´ç•Œå€¼ï¼ˆMarro å’Œ Dickmanï¼Œ1999ï¼‰ã€‚åœ¨ä¸´ç•Œç‚¹ä»¥ä¸‹ï¼Œå®Œå…¨ä¸æ´»è·ƒçŠ¶æ€æ˜¯å¸æ”¶æ€ï¼Œå› æ­¤é—®é¢˜æ˜¯ç°è±¡å­¦ RG æ˜¯å¦å¯ä»¥åŒºåˆ†ä¸´ç•Œç‚¹ä¸è¶…ä¸´ç•Œè¡Œä¸ºã€‚</p>
<blockquote>
<p>Perhaps surprisingly, one can see (weakly) nonâ€“trivial scaling behavior in some quantities even away from the critical point, as with the variance in activity shown in Fig 39A. But other quantities show clear deviations from scaling, even very close to criticality, as with the correlation times in Fig 39B. What is unambiguous is that the probability distributions of coarseâ€“grained variables flow toward a nonâ€“trivial fixed form at the critical point, and toward a Gaussian otherwise. We can see this by coarseâ€“graining in real space (Fig 39C) or via momentum shells (Fig 39D). Nicoletti et al. (2020) emphasize that the phenomenological RG can identify critical points unambiguously, but only if we check the full range of behaviors.</p>
</blockquote>
<p>ä¹Ÿè®¸ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿è¿œç¦»ä¸´ç•Œç‚¹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨æŸäº›é‡ä¸­çœ‹åˆ°ï¼ˆå¼±ï¼‰éå¹³å‡¡çš„ç¼©æ”¾è¡Œä¸ºï¼Œå¦‚å›¾ 39A æ‰€ç¤ºçš„æ´»åŠ¨æ–¹å·®ã€‚ä½†å…¶ä»–é‡å³ä½¿éå¸¸æ¥è¿‘ä¸´ç•Œæ€§ä¹Ÿæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„åç¦»ç¼©æ”¾è¡Œä¸ºï¼Œå¦‚å›¾ 39B æ‰€ç¤ºçš„ç›¸å…³æ—¶é—´ã€‚æ˜ç¡®æ— è¯¯çš„æ˜¯ï¼Œåœ¨ä¸´ç•Œç‚¹å¤„ï¼Œç²—ç²’åŒ–å˜é‡çš„æ¦‚ç‡åˆ†å¸ƒæœç€éå¹³å‡¡çš„å›ºå®šå½¢å¼æµåŠ¨ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹åˆ™æœç€é«˜æ–¯å½¢å¼æµåŠ¨ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å®ç©ºé—´ç²—ç²’åŒ–ï¼ˆå›¾ 39Cï¼‰æˆ–é€šè¿‡åŠ¨é‡å£³ï¼ˆå›¾ 39Dï¼‰æ¥çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚Nicoletti ç­‰äººï¼ˆ2020ï¼‰å¼ºè°ƒï¼Œç°è±¡å­¦ RG å¯ä»¥æ˜ç¡®åœ°è¯†åˆ«ä¸´ç•Œç‚¹ï¼Œä½†å‰ææ˜¯æˆ‘ä»¬æ£€æŸ¥äº†å…¨èŒƒå›´çš„è¡Œä¸ºã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/CHzbQuy9jkpa7IX.png" alt=""  /></p>
<p>FIG. 39 Coarse-graining of the contact process (Nicoletti et al., 2020). (A) Variance of activity vs. the scale of coarseâ€” graining in real space, as in Figs 34A and 37. Behavior at criticality (blue) is clearly different from the super-â€”critical case (red), which departs systematically but weakly from the expectations for independent variables (dashed lines). (B) Correlation time vs. the scale of coarse-graining in real space, as in Fig 35. The control parameter is set close to its critical value, and we see hints of scaling at small $K$ but clear departures at large $K$. (C) Distribution of individual coarseâ€” grained variables for $K = 32,64, 128, 256$ at criticality (blue) and away from criticality (red). In both cases we see flow toward a fixed distribution, but away from criticality this is Gaussian as expected from the central limit theorem. (D) As in (C), but with coarse-graning via momentum shells, keeping $N/8, N/16, N/32, N/64, N/128$ of the modes.</p>
</blockquote>
<p>å›¾ 39 æ¥è§¦è¿‡ç¨‹çš„ç²—ç²’åŒ–ï¼ˆNicoletti ç­‰äººï¼Œ2020ï¼‰ã€‚(A) å®ç©ºé—´ä¸­ç²—ç²’åŒ–å°ºåº¦ä¸æ´»åŠ¨æ–¹å·®çš„å…³ç³»ï¼Œå¦‚å›¾ 34A å’Œ 37 æ‰€ç¤ºã€‚ä¸´ç•Œæ€§ä¸‹çš„è¡Œä¸ºï¼ˆè“è‰²ï¼‰æ˜æ˜¾ä¸åŒäºè¶…ä¸´ç•Œæƒ…å†µï¼ˆçº¢è‰²ï¼‰ï¼Œåè€…ç³»ç»Ÿåœ°ä½†å¼±åœ°åç¦»äº†ç‹¬ç«‹å˜é‡çš„é¢„æœŸï¼ˆè™šçº¿ï¼‰ã€‚(B) å®ç©ºé—´ä¸­ç²—ç²’åŒ–å°ºåº¦ä¸ç›¸å…³æ—¶é—´çš„å…³ç³»ï¼Œå¦‚å›¾ 35 æ‰€ç¤ºã€‚æ§åˆ¶å‚æ•°è®¾ç½®æ¥è¿‘å…¶ä¸´ç•Œå€¼ï¼Œæˆ‘ä»¬åœ¨å° $K$ å¤„çœ‹åˆ°ç¼©æ”¾çš„è¿¹è±¡ï¼Œä½†åœ¨å¤§ $K$ å¤„æ˜æ˜¾åç¦»ã€‚(C) ä¸´ç•Œæ€§ä¸‹ï¼ˆè“è‰²ï¼‰å’Œè¿œç¦»ä¸´ç•Œæ€§ï¼ˆçº¢è‰²ï¼‰æ—¶ï¼Œ$K = 32,64, 128, 256$ çš„å•ä¸ªç²—ç²’åŒ–å˜é‡çš„åˆ†å¸ƒã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½çœ‹åˆ°äº†æœç€å›ºå®šåˆ†å¸ƒçš„æµåŠ¨ï¼Œä½†è¿œç¦»ä¸´ç•Œæ€§æ—¶ï¼Œè¿™ç¬¦åˆä¸­å¿ƒæé™å®šç†æ‰€é¢„æœŸçš„é«˜æ–¯åˆ†å¸ƒã€‚(D) ä¸ (C) ç±»ä¼¼ï¼Œä½†é€šè¿‡åŠ¨é‡å£³è¿›è¡Œç²—ç²’åŒ–ï¼Œä¿ç•™ $N/8, N/16, N/32, N/64, N/128$ ä¸ªæ¨¡å¼ã€‚</p>
</blockquote>
<blockquote>
<p>As with the (related) discussion of criticality in $VI.D, it has been suggested that some of the phenomena uncovered by iterative coarse-graining can be reproduced in a model where neurons respond independently to latent fields (Morrell et al., 2021). In this view, scaling and the flow toward fixed distributions are approximate, and it is not clear why scaling exponents should be reproducible across animals; a broader notion of universality, as in Fig 37, would be even more difhicult to understand.</p>
</blockquote>
<p>æ­£å¦‚åœ¨ VI.D ä¸­ï¼ˆç›¸å…³ï¼‰å…³äºä¸´ç•Œæ€§çš„è®¨è®ºä¸­æ‰€æåˆ°çš„é‚£æ ·ï¼Œæœ‰äººå»ºè®®é€šè¿‡è¿­ä»£ç²—ç²’åŒ–å‘ç°çš„ä¸€äº›ç°è±¡å¯ä»¥åœ¨ç¥ç»å…ƒç‹¬ç«‹å“åº”æ½œåœ¨åœºçš„æ¨¡å‹ä¸­å†ç°ï¼ˆMorrell ç­‰äººï¼Œ2021ï¼‰ã€‚åœ¨è¿™ç§è§‚ç‚¹ä¸­ï¼Œç¼©æ”¾å’Œæœå‘å›ºå®šåˆ†å¸ƒçš„æµåŠ¨æ˜¯è¿‘ä¼¼çš„ï¼Œå¹¶ä¸”ä¸æ¸…æ¥šä¸ºä»€ä¹ˆç¼©æ”¾æŒ‡æ•°åº”è¯¥åœ¨åŠ¨ç‰©ä¹‹é—´æ˜¯å¯é‡å¤çš„ï¼›å¦‚å›¾ 37 æ‰€ç¤ºï¼Œæ›´å¹¿æ³›çš„æ™®é€‚æ€§æ¦‚å¿µå°†æ›´éš¾ç†è§£ã€‚</p>
<blockquote>
<p>Certainly the suggestion that scaling behaviors emerge generically from latent variable models is incorrect. Consider models in which the effective field acting on each neuron $i$ is a linear combination of $K$ latent variables drawn from a Gaussian distribution. If the fields are weak then the covariance matrix of neural activity has the same rank as the covariance matrix of the fields. This simple result breaks down at stronger fields, but even in the limit of infinitely strong fields there remains a gap in the eigenvalue spectrum of the covariance matrix, at least for typical choices of parameters, so that it is impossible to recover precise scaling behaviors.</p>
</blockquote>
<p>æ¯«æ— ç–‘é—®ï¼Œç¼©æ”¾è¡Œä¸ºä»æ½œåœ¨å˜é‡æ¨¡å‹ä¸­æ™®éå‡ºç°çš„å»ºè®®æ˜¯é”™è¯¯çš„ã€‚è€ƒè™‘è¿™æ ·ä¸€ç§æ¨¡å‹ï¼Œå…¶ä¸­ä½œç”¨åœ¨æ¯ä¸ªç¥ç»å…ƒ $i$ ä¸Šçš„æœ‰æ•ˆåœºæ˜¯ä»é«˜æ–¯åˆ†å¸ƒä¸­æŠ½å–çš„ $K$ ä¸ªæ½œåœ¨å˜é‡çš„çº¿æ€§ç»„åˆã€‚å¦‚æœåœºæ˜¯å¼±çš„ï¼Œé‚£ä¹ˆç¥ç»æ´»åŠ¨çš„åæ–¹å·®çŸ©é˜µä¸åœºçš„åæ–¹å·®çŸ©é˜µå…·æœ‰ç›¸åŒçš„ç§©ã€‚è¿™ä¸ªç®€å•çš„ç»“æœåœ¨æ›´å¼ºçš„åœºä¸‹å¤±æ•ˆï¼Œä½†å³ä½¿åœ¨æ— é™å¼ºåœºçš„æé™ä¸‹ï¼Œåæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼è°±ä¸­ä»ç„¶å­˜åœ¨ä¸€ä¸ªé—´éš™ï¼Œè‡³å°‘å¯¹äºå…¸å‹å‚æ•°é€‰æ‹©æ¥è¯´æ˜¯è¿™æ ·ï¼Œå› æ­¤ä¸å¯èƒ½æ¢å¤ç²¾ç¡®çš„ç¼©æ”¾è¡Œä¸ºã€‚</p>
<blockquote>
<p>We note that a concrete, biologically motivated model of latent fieldsâ€”the independent place cell model discussed in Â§VI.Dâ€”fails to exhibit scaling (Meshulam et al., 2018). This result perhaps should not be surprising. In a population of place cells, there are two length scales, the approximate width of the place fields and the mean distance between place field centers. In the oneâ€“dimensional (virtual) environment that provides the background for the hippocampal experiments analyzed here, the ratio of these lengths gives us a characteristic number of neurons, $K_{c}\sim 18$. Indeed, analyses of the independent place cell model corresponding to Figs 34A, B show â€œbreaksâ€ at $K \sim K_{c}$. While these are approximate statements, they highlight the fact that, in the presence of such obvious scales, the observation of rather precise powerâ€“law scaling in both static and dynamic quantities really is surprising.</p>
</blockquote>
<p>æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå…·ä½“çš„ã€ç”Ÿç‰©å­¦åŠ¨æœºçš„æ½œåœ¨åœºæ¨¡å‹â€”â€”Â§VI.D ä¸­è®¨è®ºçš„ç‹¬ç«‹ä½ç½®ç»†èƒæ¨¡å‹â€”â€”æœªèƒ½è¡¨ç°å‡ºç¼©æ”¾è¡Œä¸ºï¼ˆMeshulam ç­‰äººï¼Œ2018ï¼‰ã€‚è¿™ä¸ªç»“æœæˆ–è®¸å¹¶ä¸ä»¤äººæƒŠè®¶ã€‚åœ¨ä½ç½®ç»†èƒç¾¤ä½“ä¸­ï¼Œæœ‰ä¸¤ä¸ªé•¿åº¦å°ºåº¦ï¼Œå³ä½ç½®åœºçš„è¿‘ä¼¼å®½åº¦å’Œä½ç½®åœºä¸­å¿ƒä¹‹é—´çš„å¹³å‡è·ç¦»ã€‚åœ¨ä¸ºæ­¤å¤„åˆ†æçš„æµ·é©¬ä½“å®éªŒæä¾›èƒŒæ™¯çš„ä¸€ç»´ï¼ˆè™šæ‹Ÿï¼‰ç¯å¢ƒä¸­ï¼Œè¿™äº›é•¿åº¦çš„æ¯”ç‡ç»™äº†æˆ‘ä»¬ä¸€ä¸ªç‰¹å¾ç¥ç»å…ƒæ•°ï¼Œ$K_{c}\sim 18$ã€‚å®é™…ä¸Šï¼Œå¯¹åº”äºå›¾ 34Aã€B çš„ç‹¬ç«‹ä½ç½®ç»†èƒæ¨¡å‹çš„åˆ†ææ˜¾ç¤ºåœ¨ $K \sim K_{c}$ å¤„æœ‰â€œæ–­ç‚¹â€ã€‚è™½ç„¶è¿™äº›éƒ½æ˜¯è¿‘ä¼¼é™ˆè¿°ï¼Œä½†å®ƒä»¬çªæ˜¾äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼šåœ¨å­˜åœ¨å¦‚æ­¤æ˜æ˜¾å°ºåº¦çš„æƒ…å†µä¸‹ï¼Œåœ¨é™æ€å’ŒåŠ¨æ€é‡ä¸­è§‚å¯Ÿåˆ°ç›¸å½“ç²¾ç¡®çš„å¹‚å¾‹ç¼©æ”¾ç¡®å®ä»¤äººæƒŠè®¶ã€‚</p>
<blockquote>
<p>Faced with highâ€“dimensional observations, a natural reaction is to search for a lower dimensional description. In some sense the renormalization group is the opposite approach (Bradde and Bialek, 2017). Rather than looking for the correct number of dimensions onto which to project the data, the RG invites us to examine how our description changes as we move the boundary between details that we ignore and features that we keep. Things simplify not because we have fewer degrees of freedom but because the model describing these degrees of freedom flows toward something simpler and more universal. The evidence thus far points toward the existence of such a simplified description. From the theoretical side, initial efforts at an RG analysis of models for networks of more realistic neurons suggest that these are described by new universality classes (Brinkman, 2023).</p>
</blockquote>
<p>é¢å¯¹é«˜ç»´è§‚å¯Ÿï¼Œä¸€ä¸ªè‡ªç„¶çš„ååº”æ˜¯å¯»æ‰¾ä¸€ä¸ªä½ç»´æè¿°ã€‚åœ¨æŸç§æ„ä¹‰ä¸Šï¼Œé‡æ•´åŒ–ç¾¤æ˜¯ç›¸åçš„æ–¹æ³•ï¼ˆBradde å’Œ Bialekï¼Œ2017ï¼‰ã€‚ä¸å¯»æ‰¾æ­£ç¡®çš„ç»´æ•°ä»¥æŠ•å½±æ•°æ®ä¸åŒï¼ŒRG é‚€è¯·æˆ‘ä»¬æ£€æŸ¥å½“æˆ‘ä»¬ç§»åŠ¨å¿½ç•¥çš„ç»†èŠ‚å’Œä¿ç•™çš„ç‰¹å¾ä¹‹é—´çš„è¾¹ç•Œæ—¶ï¼Œæˆ‘ä»¬çš„æè¿°å¦‚ä½•å˜åŒ–ã€‚äº‹æƒ…ç®€åŒ–äº†ï¼Œä¸æ˜¯å› ä¸ºæˆ‘ä»¬æœ‰æ›´å°‘çš„è‡ªç”±åº¦ï¼Œè€Œæ˜¯å› ä¸ºæè¿°è¿™äº›è‡ªç”±åº¦çš„æ¨¡å‹æœç€æ›´ç®€å•ã€æ›´æ™®éçš„ä¸œè¥¿æµåŠ¨ã€‚åˆ°ç›®å‰ä¸ºæ­¢çš„è¯æ®æŒ‡å‘è¿™æ ·ä¸€ç§ç®€åŒ–æè¿°çš„å­˜åœ¨ã€‚ä»ç†è®ºæ–¹é¢æ¥çœ‹ï¼Œå¯¹æ›´ç°å®ç¥ç»å…ƒç½‘ç»œæ¨¡å‹è¿›è¡Œ RG åˆ†æçš„åˆæ­¥åŠªåŠ›è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹ç”±æ–°çš„æ™®é€‚ç±»æè¿°ï¼ˆBrinkmanï¼Œ2023ï¼‰ã€‚</p>
<blockquote>
<p>What we have not emphasized here is the connection of coarseâ€“graining to more functional behaviors. In the hippocampus, how is position represented in the coarsegrained variables? More generally, do fineâ€“grained and coarseâ€“grained variables implement different principles for the encoding of the sensory world (Munn et al., 2024)? Can local networks of neurons access different scaling trajectories as the brain switches among different global states (Castro et al., 2024)? As coarseâ€“graining becomes a more commonly used tool for the analysis of large scale neural recordings, we expect progress on these issues over the next years.</p>
</blockquote>
<p>æˆ‘ä»¬åœ¨è¿™é‡Œæ²¡æœ‰å¼ºè°ƒçš„æ˜¯ç²—ç²’åŒ–ä¸æ›´åŠŸèƒ½æ€§è¡Œä¸ºçš„è”ç³»ã€‚åœ¨æµ·é©¬ä½“ä¸­ï¼Œä½ç½®å¦‚ä½•åœ¨ç²—ç²’åŒ–å˜é‡ä¸­è¡¨ç¤ºï¼Ÿæ›´ä¸€èˆ¬åœ°è¯´ï¼Œç»†ç²’åŒ–å’Œç²—ç²’åŒ–å˜é‡æ˜¯å¦å®ç°äº†ç¼–ç æ„Ÿå®˜ä¸–ç•Œçš„ä¸åŒåŸåˆ™ï¼ˆMunn ç­‰äººï¼Œ2024ï¼‰ï¼Ÿå½“å¤§è„‘åœ¨ä¸åŒçš„å…¨å±€çŠ¶æ€ä¹‹é—´åˆ‡æ¢æ—¶ï¼Œç¥ç»å…ƒçš„å±€éƒ¨ç½‘ç»œèƒ½å¦è®¿é—®ä¸åŒçš„ç¼©æ”¾è½¨è¿¹ï¼ˆCastro ç­‰äººï¼Œ2024ï¼‰ï¼Ÿéšç€ç²—ç²’åŒ–æˆä¸ºåˆ†æå¤§è§„æ¨¡ç¥ç»è®°å½•ä¸­æ›´å¸¸ç”¨çš„å·¥å…·ï¼Œæˆ‘ä»¬é¢„è®¡åœ¨æœªæ¥å‡ å¹´å†…åœ¨è¿™äº›é—®é¢˜ä¸Šä¼šå–å¾—è¿›å±•ã€‚</p>
<blockquote>
<p>The most detailed tests of scaling in equilibrium critical phenomena span six decades with better than one percent precision (Lipa et al., 1996). As described in Â§Â§III.B and III.C, the experimental frontier is moving toward recording from $\sim 10^{6}$ neurons simultaneously. This opens the possibility of following coarseâ€“graining trajectories across five decades with single cell resolution, and of driving error bars down to the one percent level across more limited ranges. The extension of existing tools to organisms with larger brains also means that we will see simultaneous recordings from more neurons in single brain areas, within which scaling seems more likely. We already see signs that quantities which emerge from these analyses can be reproducible in the second decimal place. One possibility is that new, larger experiments will reveal crossovers between different regimes on different scales. Alternatively, the scaling behaviors seen thus far might prove to be essentially exact. Whatever the outcome, it is extraordinary to think that experiments on real, functioning brains could soon reach a precision comparable to those on equilibrium critical phenomena. The corresponding challenge to theory should be clear.</p>
</blockquote>
<p>åœ¨å¹³è¡¡ä¸´ç•Œç°è±¡ä¸­ï¼Œå¯¹ç¼©æ”¾çš„æœ€è¯¦ç»†æµ‹è¯•è·¨è¶Šäº†å…­ä¸ªæ•°é‡çº§ï¼Œç²¾åº¦ä¼˜äºç™¾åˆ†ä¹‹ä¸€ï¼ˆLipa ç­‰äººï¼Œ1996ï¼‰ã€‚å¦‚ Â§Â§III.B å’Œ III.C æ‰€è¿°ï¼Œå®éªŒå‰æ²¿æ­£æœç€åŒæ—¶è®°å½• $\sim 10^{6}$ ä¸ªç¥ç»å…ƒçš„æ–¹å‘å‘å±•ã€‚è¿™ä¸ºæˆ‘ä»¬æ‰“å¼€äº†ä¸€ä¸ªå¯èƒ½æ€§ï¼Œå¯ä»¥åœ¨å•ç»†èƒåˆ†è¾¨ç‡ä¸‹è·¨è¶Šäº”ä¸ªæ•°é‡çº§è·Ÿè¸ªç²—ç²’åŒ–è½¨è¿¹ï¼Œå¹¶å°†è¯¯å·®æ¡é™ä½åˆ°æ›´æœ‰é™èŒƒå›´å†…çš„ç™¾åˆ†ä¹‹ä¸€æ°´å¹³ã€‚å°†ç°æœ‰å·¥å…·æ‰©å±•åˆ°å…·æœ‰æ›´å¤§è„‘å®¹é‡çš„ç”Ÿç‰©ä½“ä¹Ÿæ„å‘³ç€æˆ‘ä»¬å°†åœ¨å•ä¸ªå¤§è„‘åŒºåŸŸä¸­çœ‹åˆ°æ›´å¤šç¥ç»å…ƒçš„åŒæ—¶è®°å½•ï¼Œåœ¨è¿™äº›åŒºåŸŸä¸­ç¼©æ”¾ä¼¼ä¹æ›´æœ‰å¯èƒ½ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°è¿™äº›åˆ†æä¸­å‡ºç°çš„é‡å¯ä»¥åœ¨å°æ•°ç‚¹åç¬¬äºŒä½ä¸Šé‡å¤çš„è¿¹è±¡ã€‚ä¸€ç§å¯èƒ½æ€§æ˜¯ï¼Œæ–°çš„ã€æ›´å¤§çš„å®éªŒå°†æ­ç¤ºä¸åŒå°ºåº¦ä¸Šä¸åŒæœºåˆ¶ä¹‹é—´çš„äº¤å‰ã€‚æˆ–è€…ï¼Œåˆ°ç›®å‰ä¸ºæ­¢çœ‹åˆ°çš„ç¼©æ”¾è¡Œä¸ºå¯èƒ½è¢«è¯æ˜æ˜¯åŸºæœ¬ä¸Šæ˜¯ç²¾ç¡®çš„ã€‚æ— è®ºç»“æœå¦‚ä½•ï¼Œæƒ³åˆ°å¯¹çœŸå®ã€åŠŸèƒ½æ€§å¤§è„‘çš„å®éªŒå¾ˆå¿«å°±èƒ½è¾¾åˆ°ä¸å¹³è¡¡ä¸´ç•Œç°è±¡ç›¸å½“çš„ç²¾åº¦ï¼Œè¿™éƒ½æ˜¯éå‡¡çš„ã€‚å¯¹ç†è®ºçš„ç›¸åº”æŒ‘æˆ˜åº”è¯¥æ˜¯æ˜ç¡®çš„ã€‚</p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/read/theory-of-neural-computation/theory-of-neural-computation-7/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>Recurrent Networks</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/cs/mannual/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>æ™®æƒ ç®—åŠ›å¹³å°åŸ¹è®­</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  
</body>







<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
