<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Renormalization group for neurons | 无处惹尘埃</title>
<meta name="keywords" content="">
<meta name="description" content="真实神经元网络的统计力学">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>







<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="Renormalization group for neurons" />
<meta property="og:description" content="真实神经元网络的统计力学" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/" />
<meta property="og:image" content="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-12T00:18:23+08:00" />
<meta property="article:modified_time" content="2025-11-12T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" />
<meta name="twitter:title" content="Renormalization group for neurons"/>
<meta name="twitter:description" content="真实神经元网络的统计力学"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "📚文章",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "📕 阅读",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "📕 文献",
          "item": "https://Muatyz.github.io/posts/read/reference/"
        },

        {
          "@type": "ListItem",
          "position":  4 ,
          "name": "📕 Statistical mechanics for networks of real neurons",
          "item": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/"
        }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "Renormalization group for neurons",
      "item": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Renormalization group for neurons",
  "name": "Renormalization group for neurons",
  "description": "真实神经元网络的统计力学",
  "keywords": [
    ""
  ],
  "articleBody": " 物理学家以欣赏简化模型而闻名，甚至可能过于简化（Devine 和 Cohen，1992）。生命系统的复杂性与这种简化的驱动力明显存在矛盾；我们或许可以同情那些担心我们的理论冲动可能与生命丰富的分子细节不匹配的生物学家。一种有用的回应是，生物学并没有什么特别之处：在凝聚态物理学和统计力学中，我们经常使用比底层微观机制简单得多的模型来描述材料的宏观行为。这些简化模型之所以成功，不是因为我们幸运，而是因为重整化群（Wilson，1979，1983）。\n重整化群（RG）的核心思想是，当我们改变观察系统的尺度时，我们对系统的描述如何系统地变化。一个关键的定性结果是，随着我们 “放大” 以观察更长的长度尺度，许多不同的微观机制会趋向于相同的宏观行为。这意味着，如果我们能够将大规模现象归类到正确的普适类中，即使我们无法正确处理所有小规模细节，我们也可以定量地理解大规模现象，这赋予了我们编写复杂系统相对简单模型的许可（Anderson，1984）。我们希望在大脑的背景下行使这种许可。为此，我们需要了解如何在缺少许多通常指导（局部性、对称性等）的情况下实现 RG。然后我们可以问，当我们从单个神经元放大到更粗粒化的变量时，数据中是否有任何迹象表明简化出现了。\nTaking inspiration from the RG 重整化群的发展是二十世纪下半叶理论物理学的伟大篇章之一，其起源在于理解物质在短距离和长距离上的行为（Gell-Mann 和 Low，1954；Kadanoff，1966）。这些思想在1970年代初期得以结晶，并在彻底改变我们对基本粒子之间强相互作用、二阶相变的临界现象、向混沌的过渡等方面的理解中发挥了核心作用（Wilson，1983）。这些思想如何帮助我们思考神经元网络？\n在统计物理学的 RG 的标准表述中，我们从一组定义在某个微观长度尺度 $l_{0}$ 上的变量 $z_{l_{0}}\\equiv \\{z_{i}(l_{0})\\}$ 开始。这些变量的描述由一个哈密顿量给出，该哈密顿量反过来指定了玻尔兹曼分布 $P_{l_{0}}(z)$，或者我们可能会对该哈密顿量生成的动力学感兴趣。然后我们想象 “粗粒化” 这些变量，以平均掉某个 $l \u003e l_{0}$ 以下长度尺度的细节。结果是一组新的变量 $z_{l}$，我们可以询问支配这些变量的等效哈密顿量。如果我们认为哈密顿量是由不同类型的相互作用构建的，那么当我们将尺度从 $l_{0}$ 改变到 $l$ 时，这些相互作用的有效强度发生了变化，这就很自然地说，RG 引导我们在改变 $l$ 时跟踪这种流动。尽管这种相互作用强度的流动或耦合常数的运行通常是 RG 分析的目标，但 Jona-Lasinio（1975）早期强调，我们可以更一般地考虑概率分布 $P_{l}(z)$ 空间中的流动，而不参考任何哈密顿量。\n重整化群的一个基本结果是，随着 $l$ 变大，许多不同的起始分布 $P_{l_{0}}(z)$ 会收敛到相同的 $P_{l}(z)$。在这条轨迹上，分布的参数表现出作为 $l$ 函数的简单缩放行为。一个熟悉的例子是中心极限定理，如果 $P_{l_{0}}(z)$ 中的变量相关性足够弱，那么当 $l$ 变大时，$P_{l}(z)$ 会趋近于高斯分布，在此过程中，单个变量的方差按 $1/l$ 缩放。RG 预测，更有趣的起点可以流向稳定的非高斯分布，其矩按 $l$ 的非平凡幂缩放。\n重整化群方法提供了一个框架，帮助我们理解如何从晶格上的离散 Ising 自旋转变为平滑变化的局部磁化描述，或者从单个分子的位移和动量转变为流体的密度及其流动速度。在这些例子中，粗粒化操作是由对称性和局部性指导的。在生物学背景下，RG 思想最成功的发展可能是对于鸟群和昆虫群体，在那里对称性和局部性的思想仍然有用（§A.2）。对于神经元网络，连接可以跨越包含数千个细胞的距离，局部性原则不再是指导，并且没有明显的对称性。那么，我们如何选择粗粒化策略呢？\n也许从重整化群中获得灵感的一个更严重的问题是，RG 被表述为一种理解理论或模型的方法，驯服多尺度自由度之间相互作用的复杂性。这些理论当然对实验做出了定量预测，但在没有明确定义模型的情况下，不清楚如何继续。最近开始对适度真实的尖峰神经元网络模型进行重整化群分析（Brinkman，2023），我们希望会有更多这样的工作。但保持到目前为止讨论的精神，我们想问：我们如何使用 RG 来指导对大量真实神经元群体的新兴数据的分析？\n为了解决这些挑战，我们依赖两个关键思想。首先，如上所述，关于神经元网络电活动的现代实验使我们能够访问类似于统计物理模型上 Monte Carlo 模拟轨迹的东西，尽管这是一个我们不知道如何写下的模型。因此，我们可以遵循现在经典的此类模拟分析方法，例如 Binder（1981）：我们从最微观尺度的原始数据开始，构建粗粒化变量，并随着粗粒化尺度的变化，跟踪这些变量分布的各种特征。\n其次，在缺乏局部性的情况下，我们将使用 测量的成对相关性 作为指导，确定哪些神经元是 “邻居” （Bradde 和 Bialek，2017）。在一种版本中（§VII.B），这涉及将最相关的细胞的活动平均在一起，构建类似于块自旋的神经元簇（Kadanoff，1966）。在另一种版本中（§VII.C），我们连续滤除对整体方差贡献较小的人口活动的线性组合，这类似于动量壳构造（Wilson，1983）。我们将看到，这两种方法都揭示了简单、精确且可重复的缩放行为，这些行为现在已在多个有机体的多个大脑区域得到确认。然后我们讨论这些结果的意义和一些未来的方向 §VII.D。\nBy analogy with real–space methods 重整化群方法在统计物理学中基于粗粒化的概念，即平均微观细节。如果我们从存在于常规晶格上的变量 $\\{z_{i}\\}$ 开始，那么通过将变量与其近邻结合起来进行粗粒化是很自然的，如图 32 所示。形式上我们可以写成\n$$ z_{i}\\rightarrow \\widetilde{z}_{i} = f\\left(\\sum_{j\\in\\mathcal{N}_{i}}z_{j}\\right) $$\n其中 $\\mathcal{N}_{i}$ 是围绕结点 $i$ 的邻域。如果函数 $f(\\cdot)$ 是线性的，那么我们只是对一个邻域进行平均，例如，如果我们迭代，这将导致从离散的类 Ising 变量到更连续的局部磁化。如果 $f(\\cdot)$ 是一个阈值函数，那么我们可以实现\"大多数规则\"，这样类 Ising 变量集群就被映射到更稀疏晶格上的类似 Ising 的变量，就像原始的块自旋构造（Kadanoff，1966）一样。\n在常规晶格上的粗粒化。我们从二值（黑/白）变量 $\\{z_{i}\\}$ 开始，并用这些变量的平均值 $\\{\\widetilde{z}_{i}\\}$ 替换 $2 \\times 2$ 块，如灰度所示。有趣的问题是，当我们进行粗粒化(不仅仅是一次，而是迭代)时，联合分布会发生什么变化。\n在具有局部相互作用的系统中，邻域内的变量通常彼此之间相关性最强。这表明，即使我们没有邻域的概念，我们也可以通过搜索最相关的变量并使用它们来构建我们在粗粒化中使用的集群来取得进展。图 33 显示了这如何适用于神经活动的示意图。\n神经活动的粗粒化。 (A) 一小组神经元，链接表示最强关联的对及其关联强度。 (B) 这些细胞的动作电位示意序列。 (C) 通过对高度关联的对的活动求和进行粗粒化。 (D) 在 (C) 中找到最强相关的粗粒化变量对，并通过求和再次进行粗粒化。相关强度按 (A) 中的颜色编码。 (E) 这种 “实空间” 粗粒化的又一次迭代。\n我们从变量 $\\{\\sigma_{i}\\}$ 开始，如前所述，描述在一个小段时间窗口内所有神经元 $i = 1, 2,\\cdots,N$ 的活动模式（$\\sigma_{i} = 1$）和静默（$\\sigma_{i} = 0$）。为了强调这是最微观的描述，我们将其写为 $\\sigma_{i} = \\sigma_{i}^{(1)}$。然后像以前一样，我们可以计算均值、协方差和关联矩阵:\n$$ \\begin{aligned} m_{i}^{(1)} \u0026= \\langle \\sigma_{i}^{(1)}\\rangle \\\\ C_{ij}^{(1)} \u0026= \\left\\langle \\left[\\sigma_{i}^{(1)}-m_{i}^{(1)}\\right] \\left[\\sigma_{j}^{(1)}-m_{j}^{(1)}\\right] \\right\\rangle\\\\ c_{ij}^{(1)} \u0026= \\frac{C_{ij}^{(1)}}{\\sqrt{C_{ii}^{(1)}C_{jj}^{(1)}}} \\end{aligned} $$\n现在我们搜索关联系数矩阵中非对角元素的最大值，然后将与该细胞对 $i, j_{*}(i)$ 相关的行和列归零，并重复。结果是一组最大相关对 $\\{i, j_{*}(i)\\}$，然后我们定义粗粒化变量\n$$ \\sigma_{i}^{(2)} = \\sigma_{i}^{(1)} + \\sigma_{j_{*}(i)}^{(1)} $$\n现在 $i = 1, 2, \\cdots , N/2$。重要的是，我们可以跨尺度迭代这个过程：我们计算变量 $\\{\\sigma_{i}^{(2)}\\}$ 的相关矩阵并再次搜索最大相关对 $\\{i, j_{*}(i)\\}$，然后定义\n$$ \\sigma_{i}^{(3)} = \\sigma_{i}^{(2)} + \\sigma_{j_{*}(i)}^{(2)} $$\n依此类推；在每个阶段，我们剩下 $N_{k} = \\lfloor N/2^{k−1}\\rfloor$ 个变量。这个粗粒化产生了 $K = 2, 4,\\cdots , 2^{k−1}$ 个神经元簇，变量 $\\sigma_{i}^{(k)}$ 是簇 $i$ 的总活动量。\n我们强调，可以有不同的粗粒化标准和不同的变量组合方式。我们将在下面的某些点上回到这些问题（§VII.D），但现在我们探索当我们将这个最简单的方案应用于真实神经元网络时会发生什么。第一个这样的例子使用了§V中描述的 1000 多个神经元活动的实验（Meshulam 等人，2018，2019）。\n我们感兴趣的是随着我们通过连续的粗粒化尺度，概率分布如何变换和流动。当然，查看联合分布 $P(\\{\\sigma_{i}^{(k)}\\})$ 本质上是不可能的。但是，通过查看该分布的切片，甚至是单个粗粒化变量的分布，就像 Ising 模型中的磁化一样，可以学到很多东西（Binder，1981）。\n由于这种粗粒化仅基于添加 “近邻” 变量，单个变量分布的一阶矩缩放必须线性:\n$$ M_{1}(k) \\equiv \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}\\langle\\sigma_{i}^{(k)}\\rangle = \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}m_{i}^{(k)} = KM_{1}(1) $$\n其中经过 $k$ 步后，我们有 $N_{k}$ 个集群，每个集群内含 $K = 2^{k−1}$ 原始变量。第一个非平凡的问题是关于二阶矩，即(结点)活动的方差，\n$$ \\begin{aligned} M_{2}(K) \\equiv \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}\\left\\langle \\left( \\sigma_{i}^{(k)} - m_{i}^{(k)} \\right)^{2}\\right\\rangle \\end{aligned} $$\n请注意， 如果神经元是独立的，我们期望 $M_{2}(K)\\propto K$，并且许多弱相关的群体应该在 $K$ 很大时接近这种行为。 另一方面，如果神经元是完全关联的，我们期望 $M_{2}(K)\\propto K^{2}$。查看数据，在图 34A 中，我们看到对于海马体中的神经元，$M_{2}\\propto K^{\\widetilde{\\alpha}}$，其中 $\\widetilde{\\alpha}= 1.4 \\pm 0.06$。这种非平凡的缩放在两个数量级以上是可见的。\n图 34 三个通过粗粒化变量分布的切片（Meshulam 等人，2018，2019）。(A) 活动方差与（实空间）粗粒化尺度的关系，来自方程（151）。实线为 $M_{2}\\propto K^{\\widetilde{\\alpha}}$，$\\widetilde{\\alpha}= 1.4 \\pm 0.06$；虚线为独立神经元（$\\widetilde{\\alpha}= 1$）或完全关联神经元（$\\widetilde{\\alpha}= 2$）的预测。(B) 静默概率与粗粒化尺度的关系。实线为方程（152），$\\widetilde{\\beta}= 0.88 \\pm 0.01$；虚线为独立神经元的期望，$\\widetilde{\\beta}= 1$。(C) 归一化非零活动的分布，如方程（153）中定义。\n我们可以通过询问粗粒化变量 $\\sigma_{i}^{(k)} = 0$ 的概率 $P_{k}(0)$ 来获取分布的另一个切片。由于我们从变量 $\\sigma_{i} = \\{0, 1\\}$ 开始，这与询问簇大小为 $K = 2^{k−1}$ 的所有神经元都处于静默状态的概率相同。如果神经元是独立的，我们期望一个简单的缩放 $P_{k}(0)\\propto \\exp{(−aK)}$，即使细胞是弱相关的，我们也期望在大 $K$ 时看到这种情况。实验上我们在图 34B 中看到\n$$ P_{k}(0) = \\exp{(-a K^{\\widetilde{\\beta}})} $$\n其中指数 $\\widetilde{\\beta}= 0.88 \\pm 0.01$。同样，缩放在两个数量级以上是精确的。\n指数形式是保证能量可加($E = \\sum E_{i}$), 独立分布($P = \\prod P_{i}$) 的概率分布, 即 Boltzmann 概率形式 $\\begin{aligned}P(\\text{微观态}) = \\frac{1}{Z}e^{-\\beta E}\\end{aligned}$. 那么群体静默是罕见事件, 概率为 $P(\\text{all silent}) = q^{k} = e^{-K|\\ln q|}$\n自由能 $F = -k_{B}T\\ln{Z}$.\n如果我们想象为集群内所有神经元的联合活动建立一个显式模型，可能采用上述成对模型的形式[方程（83）]，那么完全静默的概率仅依赖于配分函数，$P_{k}(0) = 1/Z$。如果我们包括更高阶项，这种情况会被推广，因此图 34B 探测了等效自由能，显然表现为 $F(K) = -aK^{\\widetilde{\\beta}}$。由于 $\\widetilde{\\beta}\u003c 1$，自由能是亚广延的，因此在热力学极限下每个神经元的自由能将消失。这与我们在 §VI.B（图29）中看到的视网膜神经元的熵和能量相等是一致的。\n更一般地，如果我们定义归一化变量 $x = \\sigma^{(k)}/K$，那么\n$$ P_{k}(x) = P_{k}(0)\\delta_{x,0} + [1-P_{k}(0)]Q_{k}(x) $$\n图 34C 显示了随着 $K$ 增加，$Q_{k}(x)$ 的演变。我们看到分布的尾部逐渐被吸收到主体中，似乎接近一个固定形式 $Q(x)\\sim e^{−x/x_{0}}$。如果神经元是独立的，中心极限定理会将该分布驱动向高斯分布，但相反，我们看到了一个固定的非高斯形式的出现。\n除了查看单个粗粒化变量的分布外，我们还可以查看每个大小为 $K$ 的簇内微观变量的协方差矩阵。该协方差矩阵的特征值谱取决于按 $K$ 缩放的秩，并且在很大一部分区域内，谱是一个幂 $\\lambda\\sim (K/\\text{rank})^{\\mu}$，其中 $μ = 0.71 \\pm 0.06$，尽管这不如其他缩放示例那么清晰。\n到目前为止，我们的讨论集中在单一时间点上变量的分布。然而，在我们理解的 RG 应用中，我们通常可以观察到动态缩放（Hohenberg 和 Halperin，1977）。直观地说，更长长度尺度上的波动需要更长时间才能弛豫，因为基础相互作用是局部的。非平凡的是，如果我们以相关时间为单位测量时间，那么对不同长度尺度粗粒化变量的相关函数会坍缩为一个通用形式，并且该相关时间本身随着长度尺度的幂变化。在完全生物学背景下，这些思想的一个优雅例子是昆虫自然群体中速度波动的动态缩放（Cavagna 等人，2017）。\n对于神经元网络，我们不期望局部性是一个好的指导，但更强粗粒化的变量仍然可能具有较慢的动态，我们可以搜索动态缩放。具体来说，我们定义粗粒化尺度 $k$ 下单个变量的相关函数，\n$$ \\widetilde{C}_{i}^{(k)}(t) = \\left\\langle\\left[ \\sigma_{i}^{k}(t_{0})-m_{i}^{(k)} \\right]\\left[ \\sigma_{i}^{k}(t_{0}+t)-m_{i}^{(k)} \\right]\\right\\rangle $$\n然后我们可以归一化并对簇进行平均，得到\n$$ C^{(k)}(t) = \\frac{1}{N_{k}}\\sum_{i=1}^{N_{k}}\\frac{\\widetilde{C}_{i}^{(k)}(t)}{\\widetilde{C}_{i}^{(k)}(0)} $$\n动态缩放是假设尺度的依赖性由单一相关时间捕获，\n$$ C^{(k)}(t) = C[t/\\tau_{c}(k)] $$\n其中 $\\tau_{c}(k)\\propto K^{\\widetilde{z}}$。在图 35 中，我们看到对于海马体神经元群体，所有这些都有效。我们注意到，在这个实验中访问的相关时间的动态范围是有限的，在短时间内受到指示剂分子动力学的限制，在长时间内受到指数 $\\widetilde{z}= 0.16 \\pm 0.02$ 的小值的限制。\n海马体中 1000 多个神经元的动态缩放（Meshulam 等人，2018）。(A) 粗粒化变量的平均相关函数，方程（155），在 $K = 2,4,\\cdots, 256$ 个神经元的簇中（最浅的橙色对应于最大的簇），较大的簇表现出较慢的动态。虚线灰色表示 K = 256 神经元簇的正负一个标准差。(B) 时间轴的缩放坍缩，方程（156）。(C) 簇大小与相关时间的关系，拟合为 $\\tau_{c}\\propto K^{\\widetilde{z}}$，其中 $\\widetilde{z}= 0.16 \\pm 0.02$。\n重要的是，这些缩放行为并不是由我们选择用二进制变量来描述神经活动所驱动的。在这些实验中，神经活动是通过成像指示剂分子的荧光记录的，这些分子提供了一个连续的信号，如图 6 和 16 所示。我们可以对这些连续信号遵循相同的粗粒化步骤，结果是相同的（Meshulam 等人，2019）。\n在 RG 的完整理论结构中，缩放指数是普适类的标志。在我们询问普适性之前，我们必须询问可重复性，尤其是在如此复杂的系统中。作为第一步，已经使用来自多只小鼠实验的数据进行了相同的分析。由于缩放在两个数量级以上是精确的，因此在单只小鼠中确定指数的误差条很小，这为可重复性设定了高标准。例如，描述自由能缩放的指数（图 34B）为 $\\widetilde{\\beta}= 0.87±0.014±0.015$，分别表示单次实验中的均值、均方根误差和三只小鼠实验中的标准偏差。这让人希望我们已经发现了在第二个小数位上可重复的涌现行为特征。\nMorales 等人（2023）进行了更雄心勃勃的普适性搜索。他们分析了属于 Allen Institute for Brain Science 大型项目的一些实验，在这种情况下，使用多个神经像素探针（图 5）同时记录来自小鼠大脑许多不同区域的 100 多个神经元。请注意，除了探索许多不同的大脑区域外，记录活动的技术与图 34 和 35 中分析的海马体成像数据完全不同。尽管如此，所有这些大脑区域都重现了缩放的各个方面；示例包括粗粒化活动中方差的缩放（图 36A）和动态缩放（图 36B）。\n图 36 小鼠大脑多个不同区域的缩放（Moralés 等人，2023）。神经元数据经过 “实空间” （直接相关）粗粒化程序。(A) 十六个不同大脑区域中粗粒化活动的方差与簇大小的关系（以不同标记表示），可与图 34A 相对比。(B) 相同大脑区域的动态缩放。相关时间与簇大小的关系，可与图 35 相对比。插图：一个大脑区域（初级运动皮层）中神经元自相关函数的衰减，显示出一旦时间重新缩放后的坍缩。\n在我们完成这篇综述时，Munn 等人（2024）报道了一个引人注目的结果。他们没有查看单个有机体中多个大脑区域的实验，而是查看了许多不同有机体的实验，从微小的线虫 C. elegans 到与我们非常相似的灵长类动物。这些实验之间存在显著的技术差异，包括钙指示蛋白（§III.C）的差异和采样率的差异；完全分辨单个神经元与 “感兴趣区域” ；以及在较小模型有机体中记录整个大脑与在较大有机体中记录单个感觉或运动区域。 这些网络的许多微观特征也非常不同，极端情况是 C. elegans 神经元产生缓慢的渐变电位，而不是离散的动作电位或尖峰。尽管存在这些警告，我们仍然可以询问这些系统中的神经活动模式如何在从两个到五个数量级的范围内通过粗粒化进行转换。图 37 显示了粗粒化活动方差 $M_{2}(k)$（来自方程（151））的结果。这些结果的明显普适性令人着迷。\n图 37 神经活动方差的缩放，方程（151），作为跨多个物种的尺度函数（Munn 等人，2024）。(A) 斑马鱼。(B) 线虫 C. elegans。(C) 果蝇 Drosophila melanogaster。(D) 小鼠初级视觉皮层。(E) 猕猴初级视觉和运动皮层。灰线是单个动物的结果，带误差的红点是物种内的均值，红线是对 $M_{2}\\propto K^{\\widetilde{\\alpha}}$ 的拟合，指数如图所示。独立（蓝色）和完全相关（绿色）群体的期望对应于图 34A 中的虚线。\nBy analogy with momentum shell methods 在 “尺度” 真正是长度尺度的问题中，粗粒化是空间细节的逐渐模糊，就像我们通过显微镜观察并失焦时发生的情况一样。在这个类比中，空间模式被傅里叶变换，然后仅使用有限范围的波长进行重建。具体来说，如果我们从 $d$ 维空间中坐标为 $\\vec{x}$ 的变量 $\\phi(\\vec{x})$ 开始，粗粒化操作变为\n$$ \\begin{aligned} \\phi(\\vec{x}) \u0026\\rightarrow \\phi_{\\Lambda}(\\vec{x}) = z_{\\Lambda}\\int_{|\\vec{k}|\u003c\\Lambda}\\frac{\\mathrm{d}^{d}k}{(2\\pi)^{d}}e^{i\\vec{k}\\cdot\\vec{x}}\\widetilde{\\phi}(\\vec{k})\\\\ \\widetilde{\\phi}(\\vec{k}) \u0026= \\int \\mathrm{d}^{d}x e^{-i\\vec{k}\\cdot\\vec{x}}\\phi(\\vec{x}) \\end{aligned} $$\n其中 $\\Lambda = \\pi/l$ 截断了低于长度尺度 $l$ 的贡献，$z_{\\Lambda}$ 用于（重新）归一化变量；在微观类比中，这补偿了我们失焦时对比度的损失。与实空间一样，我们感兴趣的是概率分布 $P_{\\lambda}[\\phi_{\\Lambda}]$ 如何随着截止值 $\\Lambda$ 的变化而演变。由于傅里叶变量是连续的（在大系统的极限下），我们可以进行无穷小变化 $\\Lambda\\to\\Lambda-\\mathrm{d}\\Lambda$。在量子力学中，波矢为 $\\vec{k}$ 的波描述动量为 $\\vec{p}=\\hbar\\vec{k}$ 的粒子，因此在范围 $\\Lambda − \\mathrm{d}\\Lambda \u003c |\\vec{k}| \u003c \\Lambda$ 内对细节进行平均相当于在一个 “动量壳” 上积分（Wilson 和 Kogut，1974）。\n在具有平移不变性的系统中，动量是守恒的。独立于这些物理原理，空间平移不变性使傅里叶变换具有特权。举例来说，如果变量 $z_{i}$ 位于点 $\\vec{x}_{i}$ 的晶格上，平移不变性意味着协方差矩阵元素 $C_{ij}$ 只能依赖于位置的差异，\n$$ C_{ij} = C(\\vec{x}_{i} - \\vec{x}_{j}) $$\n该矩阵在傅里叶基中对角化，\n$$ \\begin{aligned} \\sum_{j=1}^{N}C_{ij}u_{jr} \u0026= \\lambda_{r}u_{ir}\\\\ u_{jr} \u0026\\propto \\exp{(i\\vec{k}_{r}\\cdot\\vec{x}_{j})} \\end{aligned} $$\n我们可以根据特征值 $r$ 对模式进行排序。\n在 RG 的常规应用中，大动量对应于协方差矩阵的小特征值。这表明我们可以通过滤除对应于小特征值的 “模式” 来构建粗粒化变量，而不参考空间或动量（Bradde 和 Bialek，2017）。这将粗粒化与更熟悉的数据分析技术, 主成分分析（Shlens，2014）联系起来。\n具体来说，如果我们从微观变量 $\\{\\sigma_{i}\\}$ 开始，我们可以像往常一样计算协方差矩阵\n$$ C_{ij} = \\langle(\\sigma_{i}-\\langle\\sigma_{i}\\rangle)(\\sigma_{j}-\\langle\\sigma_{j}\\rangle)\\rangle $$\n然后我们有如方程（160）所示的特征值和特征向量。让我们选择秩 $r$，使得 $\\lambda_{1}\\geq \\lambda_{2}\\cdots\\lambda_{N}$。我们可以定义一个投影到对方差贡献最大的 $\\hat{K}$ 个模式上，\n$$ \\begin{aligned} \\hat{P}_{ij}(\\hat{K}) \u0026= \\sum_{r=1}^{\\hat{K}} u_{ir}u_{jr}\\\\ \\phi_{\\hat{K}}(i) \u0026= z_{i}(\\hat{K})\\sum_{j}\\hat{P}_{ij}(\\hat{K})[\\sigma_{i}-\\langle\\sigma_{i}\\rangle] \\end{aligned} $$\n其中 $z_{i}(\\hat{K})$ 为归一化(系数), 使得 $\\langle[\\phi_{\\hat{K}}(i)]^{2}\\rangle = 1$。\n如前所述，我们想要跟踪单个粗粒化变量的分布，$P_{\\hat{K}}(\\phi_{\\hat{K}})$；结果显示在图 38A 中。为了确保我们对整个矩阵 $C_{ij}$ 有控制，我们查看了通过上述实空间粗粒化识别的 $N = 128$ 个神经元的集群。然后我们可以滤除一半的模式，使得 $\\hat{K} = 32$，得到的分布 $P_{\\hat{K}}(\\phi_{\\hat{K}})$ 仍然具有一些细微结构。如果我们减少到 $\\hat{K} = 32$，这些波动消失了，但分布仍然是不对称的，并且具有长尾。这种模式在我们减少到 $\\hat{K} = 16$ 然后 $\\hat{K} = 8$ 时继续存在，在这最后几个步骤中，分布几乎没有变化。这表明随着我们进行粗粒化，分布趋向于一个固定形式。重要的是，这种形式与如果相关性较弱，中心极限定理所保证的高斯形式非常不同。\n图 38 通过 “动量壳” 对 $N = 128$ 个神经元进行粗粒化（Meshulam 等人，2018）。(A) 跟踪来自方程（164）的单个粗粒化变量的分布。不同颜色对应于保留不同数量的模式 $\\hat{K}$，如插图所示；虚线为高斯分布以供比较。(B) 模式 $r$ 中波动的相关时间的动态缩放，方程（166），与协方差矩阵的相关特征值 $\\tau_{c}(r)\\propto \\lambda_{r}^{\\widetilde{z}^{\\prime}}$，其中 $\\widetilde{z}^{\\prime} = 0.37 \\pm 0.04$。\n动态缩放的直觉是，更大长度尺度上的波动弛豫得更慢，尽管 “尺度” 的含义现在更抽象（图 35），但我们已经看到这可以推广到神经元网络。通过转换到对协方差矩阵进行对角化的基，我们已经隔离了在二阶上独立的波动模式，自然会问这些模式上的波动是如何弛豫的。沿模式 $r$ 的变化定义为\n$$ \\widetilde{\\phi}_{r} = \\sum_{i=1}^{N}[\\sigma_{i}-\\langle\\sigma_{i}\\rangle]u_{ir} $$\n相关函数为\n$$ C_{r}(t) = \\langle\\widetilde{\\phi}_{r}(t_{0})\\widetilde{\\phi}_{r}(t_{0}+t)\\rangle $$\n动态缩放是这样一种说法：当时间按单一相关时间进行缩放时，所有这些相关性都会坍缩，并且该相关时间本身具有尺度的幂律依赖。在通常的例子中，这意味着 $\\tau_{c}\\propto |\\vec{k}|^{z}$（Hohenberg 和 Halperin，1977），但在临界点附近，协方差矩阵的特征值也对 $|\\vec{k}|$ 具有幂律依赖，因此我们可以直接测试 $\\tau_{c}\\propto \\lambda^{\\widetilde{z}^{\\prime}}$，如图 38B 所示。如前所述，最短的相关时间受到报告电活动的荧光蛋白响应时间的限制，最长的时间受到动态缩放指数大小的限制；尽管如此，我们仍然可以观察到 $\\lambda$ 上两个数量级的相当精确的缩放。\n通过查看模式的相关时间找到的动态指数 $\\widetilde{z}^{\\prime}$ 应该与我们通过实空间粗粒化看到的指数 $\\widetilde{z}$(图 35C) 相关，通过描述协方差矩阵特征值衰减的指数 $\\mu$，$\\widetilde{z} = \\mu\\widetilde{z}^{\\prime}$。这确实有效，尽管误差条很大（Meshulam 等人，2018）。更重要的是，这些结果表明网络没有单一的特征时间尺度，而是一个连续的时间尺度，可以通过在不同尺度上探测来访问。\nRG as a path to understanding 如果我们相信在神经网络功能和活动的复杂性中可以找到潜在的简单性，我们可能想暂停片刻来说服自己，遵循 RG 简化实际上可以引导我们到达那里。鉴于在获得越来越多神经元数据集方面的爆炸性实验进展，如上面的例子所示，这一追求现在感觉是可以实现的。虽然我们可能不知道如何操纵大脑中的 “温度” 或 “磁化强度” ，但我们正在通过监测的神经元数量获得数十年的进步。\n重整化群是一个强大的理论结构。由于我们没有神经动力学的微观模型，我们还无法利用这个结构。相反，我们采用了一种受 RG 启发的数据分析方法，这被描述为 “现象学重整化群” （Nicoletti 等人，2020）或 “迭代粗粒化” （Munn 等人，2024）。如果我们将这些方法应用于理解良好的平衡统计力学问题，最有趣的结果将是概率分布朝着某种固定的非高斯形式流动，以及沿着这条轨迹出现幂律缩放，就像在临界点发生的那样。值得注意的是，这正是所发现的，无论是在对海马体的初始应用中，还是现在在许多其他系统中；缩放指数是可重复的，甚至可能是普适的。很容易得出结论，基础网络动力学必须由重整化群的非平凡不动点描述的理论来描述。\n我们应该保持谨慎。是否有可能我们与 RG 不动点相关联的一些粗粒化行为可以更普遍地出现在非平衡系统中？Nicoletti 等人（2020）通过分析接触过程的模拟来解决这个问题，在该过程中，二进制变量以与邻近位置活跃变量密度成正比的单位时间概率被激活，然后以固定的单位时间概率被停用。该模型有一个参数，即激活率中的比例常数，并且存在一个取决于网络几何形状的临界值（Marro 和 Dickman，1999）。在临界点以下，完全不活跃状态是吸收态，因此问题是现象学 RG 是否可以区分临界点与超临界行为。\n也许令人惊讶的是，即使远离临界点，我们也可以在某些量中看到（弱）非平凡的缩放行为，如图 39A 所示的活动方差。但其他量即使非常接近临界性也显示出明显的偏离缩放行为，如图 39B 所示的相关时间。明确无误的是，在临界点处，粗粒化变量的概率分布朝着非平凡的固定形式流动，而在其他情况下则朝着高斯形式流动。我们可以通过实空间粗粒化（图 39C）或通过动量壳（图 39D）来看到这一点。Nicoletti 等人（2020）强调，现象学 RG 可以明确地识别临界点，但前提是我们检查了全范围的行为。\n图 39 接触过程的粗粒化（Nicoletti 等人，2020）。(A) 实空间中粗粒化尺度与活动方差的关系，如图 34A 和 37 所示。临界性下的行为（蓝色）明显不同于超临界情况（红色），后者系统地但弱地偏离了独立变量的预期（虚线）。(B) 实空间中粗粒化尺度与相关时间的关系，如图 35 所示。控制参数设置接近其临界值，我们在小 $K$ 处看到缩放的迹象，但在大 $K$ 处明显偏离。(C) 临界性下（蓝色）和远离临界性（红色）时，$K = 32,64, 128, 256$ 的单个粗粒化变量的分布。在这两种情况下，我们都看到了朝着固定分布的流动，但远离临界性时，这符合中心极限定理所预期的高斯分布。(D) 与 (C) 类似，但通过动量壳进行粗粒化，保留 $N/8, N/16, N/32, N/64, N/128$ 个模式。\n正如在 VI.D 中（相关）关于临界性的讨论中所提到的那样，有人建议通过迭代粗粒化发现的一些现象可以在神经元独立响应潜在场的模型中再现（Morrell 等人，2021）。在这种观点中，缩放和朝向固定分布的流动是近似的，并且不清楚为什么缩放指数应该在动物之间是可重复的；如图 37 所示，更广泛的普适性概念将更难理解。\n毫无疑问，缩放行为从潜在变量模型中普遍出现的建议是错误的。考虑这样一种模型，其中作用在每个神经元 $i$ 上的有效场是从高斯分布中抽取的 $K$ 个潜在变量的线性组合。如果场是弱的，那么神经活动的协方差矩阵与场的协方差矩阵具有相同的秩。这个简单的结果在更强的场下失效，但即使在无限强场的极限下，协方差矩阵的特征值谱中仍然存在一个间隙，至少对于典型参数选择来说是这样，因此不可能恢复精确的缩放行为。\n我们注意到，具体的、生物学动机的潜在场模型——§VI.D 中讨论的独立位置细胞模型——未能表现出缩放行为（Meshulam 等人，2018）。这个结果或许并不令人惊讶。在位置细胞群体中，有两个长度尺度，即位置场的近似宽度和位置场中心之间的平均距离。在为此处分析的海马体实验提供背景的一维（虚拟）环境中，这些长度的比率给了我们一个特征神经元数，$K_{c}\\sim 18$。实际上，对应于图 34A、B 的独立位置细胞模型的分析显示在 $K \\sim K_{c}$ 处有 “断点” 。虽然这些都是近似陈述，但它们突显了这样一个事实：在存在如此明显尺度的情况下，在静态和动态量中观察到相当精确的幂律缩放确实令人惊讶。\n面对高维观察，一个自然的反应是寻找一个低维描述。在某种意义上，重整化群是相反的方法（Bradde 和 Bialek，2017）。与寻找正确的维数以投影数据不同，RG 邀请我们检查当我们移动忽略的细节和保留的特征之间的边界时，我们的描述如何变化。事情简化了，不是因为我们有更少的自由度，而是因为描述这些自由度的模型朝着更简单、更普遍的东西流动。到目前为止的证据指向这样一种简化描述的存在。从理论方面来看，对更现实神经元网络模型进行 RG 分析的初步努力表明，这些模型由新的普适类描述（Brinkman，2023）。\n我们在这里没有强调的是粗粒化与更功能性行为的联系。在海马体中，位置如何在粗粒化变量中表示？更一般地说，细粒化和粗粒化变量是否实现了编码感官世界的不同原则（Munn 等人，2024）？当大脑在不同的全局状态之间切换时，神经元的局部网络能否访问不同的缩放轨迹（Castro 等人，2024）？随着粗粒化成为分析大规模神经记录中更常用的工具，我们预计在未来几年内在这些问题上会取得进展。\n在平衡临界现象中，对缩放的最详细测试跨越了六个数量级，精度优于百分之一（Lipa 等人，1996）。如 §§III.B 和 III.C 所述，实验前沿正朝着同时记录 $\\sim 10^{6}$ 个神经元的方向发展。这为我们打开了一个可能性，可以在单细胞分辨率下跨越五个数量级跟踪粗粒化轨迹，并将误差条降低到更有限范围内的百分之一水平。将现有工具扩展到具有更大脑容量的生物体也意味着我们将在单个大脑区域中看到更多神经元的同时记录，在这些区域中缩放似乎更有可能。我们已经看到这些分析中出现的量可以在小数点后第二位上重复的迹象。一种可能性是，新的、更大的实验将揭示不同尺度上不同机制之间的交叉。或者，到目前为止看到的缩放行为可能被证明是基本上是精确的。无论结果如何，想到对真实、功能性大脑的实验很快就能达到与平衡临界现象相当的精度，这都是非凡的。对理论的相应挑战应该是明确的。\n",
  "wordCount" : "11599",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png","datePublished": "2025-11-12T00:18:23+08:00",
  "dateModified": "2025-11-12T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "无处惹尘埃",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="计算物理学习日志 (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">计算物理学习日志</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="🔍 搜索 (Alt &#43; /)" accesskey=/>
                <span>🔍 搜索</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="🏠 主页">
                <span>🏠 主页</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="📚 文章">
                <span>📚 文章</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="🧩 标签">
                <span>🧩 标签</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="⏱️ 时间轴">
                <span>⏱️ 时间轴</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="🙋🏻‍♂️ 关于">
                <span>🙋🏻‍♂️ 关于</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="🤝 友链">
                <span>🤝 友链</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">🏠 主页</a>&nbsp;»&nbsp;<a href="https://Muatyz.github.io/posts/">📚文章</a>&nbsp;»&nbsp;<a href="https://Muatyz.github.io/posts/read/">📕 阅读</a>&nbsp;»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/">📕 文献</a>&nbsp;»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/">📕 Statistical mechanics for networks of real neurons</a></div>
            <h1 class="post-title">
                Renormalization group for neurons
            </h1>
            <div class="post-description">
                真实神经元网络的统计力学
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-11-12
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>11599字
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>24分钟
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/11/12/Z4gTrHP3WlFIA7v.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#taking-inspiration-from-the-rg" aria-label="Taking inspiration from the RG">Taking inspiration from the RG</a></li>
                <li>
                    <a href="#by-analogy-with-realspace-methods" aria-label="By analogy with real–space methods">By analogy with real–space methods</a></li>
                <li>
                    <a href="#by-analogy-with-momentum-shell-methods" aria-label="By analogy with momentum shell methods">By analogy with momentum shell methods</a></li>
                <li>
                    <a href="#rg-as-a-path-to-understanding" aria-label="RG as a path to understanding">RG as a path to understanding</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><!-- > Physicists are known for our appreciation of simplified models, perhaps even to the point of over–simplification (Devine and Cohen, 1992). The complexity of living systems is in obvious tension with this drive for simplification; we can perhaps sympathize with biologists who worry that our theoretical impulses may be mismatched to the richness of life’s molecular details. A useful response is that there is nothing special about biology: in condensed matter physics and statistical mechanics we routinely describe the macroscopic behavior of materials using models that are much simpler than the underlying microscopic mechanisms. These simplified models succeed, not because we are lucky but because of the **renormalization group** (Wilson, 1979, 1983). -->
<p>物理学家以欣赏简化模型而闻名，甚至可能过于简化（Devine 和 Cohen，1992）。生命系统的复杂性与这种简化的驱动力明显存在矛盾；我们或许可以同情那些担心我们的理论冲动可能与生命丰富的分子细节不匹配的生物学家。一种有用的回应是，生物学并没有什么特别之处：在凝聚态物理学和统计力学中，我们经常使用比底层微观机制简单得多的模型来描述材料的宏观行为。这些简化模型之所以成功，不是因为我们幸运，而是因为<strong>重整化群</strong>（Wilson，1979，1983）。</p>
<!-- > The central idea of the renormalization group (RG) is to ask how our description of a system changes, systematically, as we change the scale on which we look. The crucial qualitative result is that many different microscopic mechanisms flow toward the same macroscopic behavior as we  "zoom out"  to look at longer length scales. This means that we can understand large scale phenomena quantitatively if we can assign them to the correct **universality class**, even if we can’t get all the small scale details right, and this gives us license to write relatively simple models of complex systems (Anderson, 1984). We would like to exercise this license in the context of the brain. To do this we need to understand how to implement the RG when many of our usual guides (locality, symmetry, $\cdots$) are absent. We then can ask whether there is any sign that simplification emerges from the data as we zoom out from individual neurons to more coarse–grained variables. -->
<p>重整化群（RG）的核心思想是，当我们改变观察系统的尺度时，我们对系统的描述如何系统地变化。一个关键的定性结果是，随着我们 &ldquo;放大&rdquo; 以观察更长的长度尺度，许多不同的微观机制会趋向于相同的宏观行为。这意味着，如果我们能够将大规模现象归类到正确的<strong>普适类</strong>中，即使我们无法正确处理所有小规模细节，我们也可以定量地理解大规模现象，这赋予了我们编写复杂系统相对简单模型的许可（Anderson，1984）。我们希望在大脑的背景下行使这种许可。为此，我们需要了解如何在缺少许多通常指导（局部性、对称性等）的情况下实现 RG。然后我们可以问，当我们从单个神经元放大到更粗粒化的变量时，数据中是否有任何迹象表明简化出现了。</p>
<h1 id="taking-inspiration-from-the-rg">Taking inspiration from the RG<a hidden class="anchor" aria-hidden="true" href="#taking-inspiration-from-the-rg">#</a></h1>
<!-- > The development of the renormalization group is one the great chapters of theoretical physics from the second half of the twentieth century, with origins in efforts to understand matter at both short and long distances (Gell-Mann and Low, 1954; Kadanoff, 1966). These ideas crystallized in the early 1970s and played a central role in revolutionizing our understanding of the strong interaction among elementary particles, critical phenomena at second order phase transitions, the transition to chaos, and more (Wilson, 1983). How can these ideas help us to think about networks of neurons? -->
<p>重整化群的发展是二十世纪下半叶理论物理学的伟大篇章之一，其起源在于理解物质在短距离和长距离上的行为（Gell-Mann 和 Low，1954；Kadanoff，1966）。这些思想在1970年代初期得以结晶，并在彻底改变我们对基本粒子之间强相互作用、二阶相变的临界现象、向混沌的过渡等方面的理解中发挥了核心作用（Wilson，1983）。这些思想如何帮助我们思考神经元网络？</p>
<!-- > In the standard formulation of the RG for statistical physics we start with a set of variables $z_{l_{0}}\equiv \\{z_{i}(l_{0})\\}$ defined on some microscopic length scale $l_{0}$. Our description of these variables is given by a Hamiltonian that in turns specifies the Boltzmann distribution $P_{l_{0}}(z)$, or perhaps we will be interested in the dynamics generated by this Hamiltonian. We then imagine  "coarse–graining"  the variables to average out the details on length scales below some $l > l_{0}$. The result is a new set of variables $z_{l}$, and we can ask for the effective Hamiltonian that governs these variables. If we think of the Hamiltonian as being built from different kinds of interactions, it becomes natural to say that the effective strengths of these interactions has changed as change scale from $l_{0}$ to $l$, and the RG invites us to follow this flow as we change $l$. Although this flow of interaction strengths or running of coupling constants often is the goal an RG analysis, it was emphasized early on by Jona-Lasinio (1975) that we can think more generally about flow in the space of probability distributions $P_{l}(z)$, leaving aside any reference to Hamiltonians. -->
<p>在统计物理学的 RG 的标准表述中，我们从一组定义在某个微观长度尺度 $l_{0}$ 上的变量 $z_{l_{0}}\equiv \{z_{i}(l_{0})\}$ 开始。这些变量的描述由一个哈密顿量给出，该哈密顿量反过来指定了玻尔兹曼分布 $P_{l_{0}}(z)$，或者我们可能会对该哈密顿量生成的动力学感兴趣。然后我们想象 &ldquo;粗粒化&rdquo; 这些变量，以平均掉某个 $l &gt; l_{0}$ 以下长度尺度的细节。结果是一组新的变量 $z_{l}$，我们可以询问支配这些变量的等效哈密顿量。如果我们认为哈密顿量是由不同类型的相互作用构建的，那么当我们将尺度从 $l_{0}$ 改变到 $l$ 时，这些相互作用的有效强度发生了变化，这就很自然地说，RG 引导我们在改变 $l$ 时跟踪这种流动。尽管这种相互作用强度的流动或耦合常数的运行通常是 RG 分析的目标，但 Jona-Lasinio（1975）早期强调，我们可以更一般地考虑概率分布 $P_{l}(z)$ 空间中的流动，而不参考任何哈密顿量。</p>
<!-- > An essential result of the renormalization group is that many different starting distributions $P_{l_{0}}(z)$ converge to the same $P_{l}(z)$ as $l$ becomes large. Along this trajectory parameters of the distribution exhibit simple scaling behaviors as a function of $l$. A familiar example is the central limit theorem, where if variables in $P_{l_{0}}(z)$ are sufficiently weakly correlated then $P_{l}(z)$ approaches a Gaussian as $l$ becomes large, and along the way the variances of the individual variables scale as $1/l$. The RG predicts that more interesting starting points can flow toward stable non–Gaussian distributions, with moments scaling as non–trivial powers of $l$. -->
<p>重整化群的一个基本结果是，随着 $l$ 变大，许多不同的起始分布 $P_{l_{0}}(z)$ 会收敛到相同的 $P_{l}(z)$。在这条轨迹上，分布的参数表现出作为 $l$ 函数的简单缩放行为。一个熟悉的例子是中心极限定理，如果 $P_{l_{0}}(z)$ 中的变量相关性足够弱，那么当 $l$ 变大时，$P_{l}(z)$ 会趋近于高斯分布，在此过程中，单个变量的方差按 $1/l$ 缩放。RG 预测，更有趣的起点可以流向稳定的非高斯分布，其矩按 $l$ 的非平凡幂缩放。</p>
<!-- > The renormalization group approach provides a framework to understand how we can go from discrete Ising spins on a lattice to a description of smoothly varying local magnetization, or from the positions and momenta of individual molecules to the density of a fluid and the velocity of its flow. In these examples, the coarse–graining operation is guided by symmetry and locality. Perhaps the most successful development of RG ideas in a biological context has been for flocks of birds and swarms of insects, where the ideas of symmetry and locality continue to be useful (§A.2). For networks of neurons, where connections can span distances encompassing thousands of cells, the principle of locality is less of a guide, and there are no obvious symmetries. How then do we choose a coarse–graining strategy? -->
<p>重整化群方法提供了一个框架，帮助我们理解如何从晶格上的离散 Ising 自旋转变为平滑变化的局部磁化描述，或者从单个分子的位移和动量转变为流体的密度及其流动速度。在这些例子中，粗粒化操作是由对称性和局部性指导的。在生物学背景下，RG 思想最成功的发展可能是对于鸟群和昆虫群体，在那里对称性和局部性的思想仍然有用（§A.2）。对于神经元网络，连接可以跨越包含数千个细胞的距离，局部性原则不再是指导，并且没有明显的对称性。那么，我们如何选择粗粒化策略呢？</p>
<!-- > Perhaps a more serious problem in taking inspiration from the renormalization group is that the RG is formulated as an approach to understanding theories or models, taming the complexities of interactions among degrees of freedom at many scales. These theories of course make quantitative predictions for experiment, but in the absence of a well defined model it is not clear how to proceed. There is a recent start on renormalization group analysis of models for a network of moderately realistic spiking neurons (Brinkman, 2023), and we hope there will be more of this. But, keeping to the spirit of the discussion thus far, we want to ask: How can we use the RG to guide the analysis of emerging data on large populations of real neurons? -->
<p>也许从重整化群中获得灵感的一个更严重的问题是，RG 被表述为一种理解理论或模型的方法，驯服多尺度自由度之间相互作用的复杂性。这些理论当然对实验做出了定量预测，但在没有明确定义模型的情况下，不清楚如何继续。最近开始对适度真实的尖峰神经元网络模型进行重整化群分析（Brinkman，2023），我们希望会有更多这样的工作。但保持到目前为止讨论的精神，我们想问：我们如何使用 RG 来指导对大量真实神经元群体的新兴数据的分析？</p>
<!-- > To address these challenges we rely on two key ideas. First, as emphasized above, modern experiments on the electrical activity in networks of neurons give us access to something analogous to the trajectory of a Monte Carlo simulation on a statistical physics model, albeit a model that we don’t know how to write down. Thus we can follow the approach used in now classical analysis of such simulations, for example by Binder (1981): We start with raw data on the most microscopic scale, construct coarse–grained variables, and follow various features of the distribution of thee variables as we change the scale of coarse–graining. -->
<p>为了解决这些挑战，我们依赖两个关键思想。首先，如上所述，关于神经元网络电活动的现代实验使我们能够访问类似于统计物理模型上 Monte Carlo 模拟轨迹的东西，尽管这是一个我们不知道如何写下的模型。因此，我们可以遵循现在经典的此类模拟分析方法，例如 Binder（1981）：我们从最微观尺度的原始数据开始，构建粗粒化变量，并随着粗粒化尺度的变化，跟踪这些变量分布的各种特征。</p>
<!-- > Second, we will use **the measured pairwise correlations** as guide to which neurons are  "neighbors,"  in the absence of locality (Bradde and Bialek, 2017). In one version (§VII.B), this involves averaging together the activities of the most correlated cells, building clusters of neurons that are analogous to block spins (Kadanoff, 1966). In another version (§VII.C), we successively filter out linear combinations of the population activity that make small contributions to the overall variance, and this is analogous to the momentum shell construction (Wilson, 1983). We will see that both these approaches uncover simple, precise, and reproducible scaling behaviors that now have been confirmed in multiple brain areas from multiple organisms. We then discuss the implications of these results and some future direction §VII.D. -->
<p>其次，在缺乏局部性的情况下，我们将使用 <strong>测量的成对相关性</strong> 作为指导，确定哪些神经元是 &ldquo;邻居&rdquo; （Bradde 和 Bialek，2017）。在一种版本中（§VII.B），这涉及将最相关的细胞的活动平均在一起，构建类似于块自旋的神经元簇（Kadanoff，1966）。在另一种版本中（§VII.C），我们连续滤除对整体方差贡献较小的人口活动的线性组合，这类似于动量壳构造（Wilson，1983）。我们将看到，这两种方法都揭示了简单、精确且可重复的缩放行为，这些行为现在已在多个有机体的多个大脑区域得到确认。然后我们讨论这些结果的意义和一些未来的方向 §VII.D。</p>
<h1 id="by-analogy-with-realspace-methods">By analogy with real–space methods<a hidden class="anchor" aria-hidden="true" href="#by-analogy-with-realspace-methods">#</a></h1>
<!-- > Renormalization group methods in statistical physics rest on a notion of coarse–graining, averaging over microscopic details. If we start with variables $\\{z_{i}\\}$ that live on a regular lattice, the it is natural to do this by combining variables with their neighbors, as in Fig 32. Formally we can write
> 
> $$
> z_{i}\rightarrow \widetilde{z}\_{i} = f\left(\sum_{j\in\mathcal{N}\_{i}}z_{j}\right)
> $$
> 
> where $\mathcal{N}\_{i}$ is a neighborhood surrounding site $i$. If the function $f(\cdot)$ is linear then we are just averaging over a neighborhood, and for example this will lead from discrete Ising–like variables to a more continuous local magnetization if we iterate. If $f(\cdot)$ is a threshold function then we can implement majority rule, so that clusters of Ising–like variables are mapped into Ising–like variables on the sparser lattice, as in the original block spin construction (Kadanoff, 1966). -->
<p>重整化群方法在统计物理学中基于粗粒化的概念，即平均微观细节。如果我们从存在于常规晶格上的变量 $\{z_{i}\}$ 开始，那么通过将变量与其近邻结合起来进行粗粒化是很自然的，如图 32 所示。形式上我们可以写成</p>
<p>$$
z_{i}\rightarrow \widetilde{z}_{i} = f\left(\sum_{j\in\mathcal{N}_{i}}z_{j}\right)
$$</p>
<p>其中 $\mathcal{N}_{i}$ 是围绕结点 $i$ 的邻域。如果函数 $f(\cdot)$ 是线性的，那么我们只是对一个邻域进行平均，例如，如果我们迭代，这将导致从离散的类 Ising 变量到更连续的局部磁化。如果 $f(\cdot)$ 是一个阈值函数，那么我们可以实现&quot;大多数规则&quot;，这样类 Ising 变量集群就被映射到更稀疏晶格上的类似 Ising 的变量，就像原始的块自旋构造（Kadanoff，1966）一样。</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/LfiEyK2De6NWbIH.png" alt=""  /></p>
</blockquote>
</blockquote>
<!-- > > Coarse–graining on a regular lattice. We start with binary (black/white) variables $\\{z_{i}\\}$, and replace $2 \times 2$ blocks with the average of these variables $\\{\widetilde{z}\_{i}\\}$, shown as grey levels. The interesting question is what happens to the joint distribution as we coarse–grain, not just once but iteratively. -->
<blockquote>
<p>在常规晶格上的粗粒化。我们从二值（黑/白）变量 $\{z_{i}\}$ 开始，并用这些变量的平均值 $\{\widetilde{z}_{i}\}$ 替换 $2 \times 2$ 块，如灰度所示。有趣的问题是，当我们进行粗粒化(不仅仅是一次，而是迭代)时，联合分布会发生什么变化。</p>
</blockquote>
<!-- > In a system with local interactions, the variables in the neighborhood typically are the most strongly correlated with one another. This suggests that even if we don’t have a notion of neighborhood, we can make progress by searching for the most correlated variables and using these to build the clusters that we use in coarse–graining. A schematic of how this can work for neural activity is shown in Fig 33. -->
<p>在具有局部相互作用的系统中，邻域内的变量通常彼此之间相关性最强。这表明，<ins>即使我们没有邻域的概念，我们也可以通过搜索最相关的变量并使用它们来构建我们在粗粒化中使用的集群来取得进展</ins>。图 33 显示了这如何适用于神经活动的示意图。</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/bZcoh18zGEvpUQT.png" alt=""  /></p>
</blockquote>
</blockquote>
<!-- > > Coarse–graining neural activity. 
(A) A small group of neurons with links indicating the most strongly correlated pairs, and the strength of these correlations. 
(B) Schematic sequence of action potentials from these cells. 
(C) Coarse–graining by summing the activity in highly correlated pairs. 
(D) Finding the most strongly correlated pairs of coarse–grained variables in (C) and coarse–graining again by summing. The strengths of the correlations are color coded as in (A). 
(E) One more iteration of this  "real space"  coarsegraining.
>  -->
<blockquote>
<p>神经活动的粗粒化。
(A) 一小组神经元，链接表示最强关联的对及其关联强度。
(B) 这些细胞的动作电位示意序列。
(C) 通过对高度关联的对的活动求和进行粗粒化。
(D) 在 (C) 中找到最强相关的粗粒化变量对，并通过求和再次进行粗粒化。相关强度按 (A) 中的颜色编码。
(E) 这种 &ldquo;实空间&rdquo; 粗粒化的又一次迭代。</p>
</blockquote>
<!-- > We start with variables $\\{\sigma_{i}\\}$, as before, describing the patterns of activity ($\sigma_{i} = 1$) and silence ($\sigma_{i} = 0$) across all the neurons $i = 1, 2,\cdots,N$ in a small window of time. To emphasize that this is the most microscopic description we will write this as $\sigma_{i} = \sigma_{i}^{(1)}$ . Then as before we can compute the means, covariance, and **correlation matrices**:
> 
> $$
> \begin{aligned}
> m_{i}^{(1)} &= \langle \sigma_{i}^{(1)}\rangle \\\\
> C_{ij}^{(1)} &= \left\langle \left[\sigma_{i}^{(1)}-m_{i}^{(1)}\right] \left[\sigma_{j}^{(1)}-m_{j}^{(1)}\right] \right\rangle\\\\
> c_{ij}^{(1)} &= \frac{C_{ij}^{(1)}}{\sqrt{C_{ii}^{(1)}C_{jj}^{(1)}}}
> \end{aligned}
> $$
> 
> Now we search for the maximal non–diagonal element in the matrix of correlation coefficients, then zero the rows and columns associated with this pair of cells $i, j_{\*}(i)$, and repeat. The result is a set of maximally correlated pairs $\\{i, j_{\*}(i)\\}$, and we then define coarse–grained variables
> 
> $$
> \sigma_{i}^{(2)} = \sigma_{i}^{(1)} + \sigma_{j_{\*}(i)}^{(1)}
> $$
> 
> where now $i = 1, 2, \cdots , N/2$. Importantly, we can iterate this process across scales: we compute the correlation matrix of the variables $\\{\sigma_{i}^{(2)}\\}$ and search  again for the maximally correlated pairs $\\{i, j_{\*}(i)\\}$, then define
> 
> $$
> \sigma_{i}^{(3)} = \sigma_{i}^{(2)} + \sigma_{j_{\*}(i)}^{(2)}
> $$
> 
> and so on; at each stage we have $N_{k} = \lfloor N/2^{k−1}\rfloor$ variables remaining. This coarse graining produces  clusters of $K = 2, 4,\cdots , 2^{k−1}$ neurons, and the variable $\sigma_{i}^{(k)}$ is the summed activity of cluster $i$. -->
<p>我们从变量 $\{\sigma_{i}\}$ 开始，如前所述，描述在一个小段时间窗口内所有神经元 $i = 1, 2,\cdots,N$ 的活动模式（$\sigma_{i} = 1$）和静默（$\sigma_{i} = 0$）。为了强调这是最微观的描述，我们将其写为 $\sigma_{i} = \sigma_{i}^{(1)}$。然后像以前一样，我们可以计算均值、协方差和<strong>关联矩阵</strong>:</p>
<p>$$
\begin{aligned}
m_{i}^{(1)} &amp;= \langle \sigma_{i}^{(1)}\rangle \\
C_{ij}^{(1)} &amp;= \left\langle \left[\sigma_{i}^{(1)}-m_{i}^{(1)}\right] \left[\sigma_{j}^{(1)}-m_{j}^{(1)}\right] \right\rangle\\
c_{ij}^{(1)} &amp;= \frac{C_{ij}^{(1)}}{\sqrt{C_{ii}^{(1)}C_{jj}^{(1)}}}
\end{aligned}
$$</p>
<p>现在我们搜索关联系数矩阵中非对角元素的最大值，然后将与该细胞对 $i, j_{*}(i)$ 相关的行和列归零，并重复。结果是一组最大相关对 $\{i, j_{*}(i)\}$，然后我们定义粗粒化变量</p>
<p>$$
\sigma_{i}^{(2)} = \sigma_{i}^{(1)} + \sigma_{j_{*}(i)}^{(1)}
$$</p>
<p>现在 $i = 1, 2, \cdots , N/2$。重要的是，我们可以跨尺度迭代这个过程：我们计算变量 $\{\sigma_{i}^{(2)}\}$ 的相关矩阵并再次搜索最大相关对 $\{i, j_{*}(i)\}$，然后定义</p>
<p>$$
\sigma_{i}^{(3)} = \sigma_{i}^{(2)} + \sigma_{j_{*}(i)}^{(2)}
$$</p>
<p>依此类推；在每个阶段，我们剩下 $N_{k} = \lfloor N/2^{k−1}\rfloor$ 个变量。这个粗粒化产生了 $K = 2, 4,\cdots , 2^{k−1}$ 个神经元簇，变量 $\sigma_{i}^{(k)}$ 是簇 $i$ 的总活动量。</p>
<!-- > We emphasize that one could have different criteria for coarse–graining, and different ways of combing the variables. We return to some of these points below (§VII.D), but for now we explore what happens when we apply this simplest scheme to a network of real neurons. The first such example used the experiments on the activity of 1000+ neurons described in §V (Meshulam et al., 2018, 2019). -->
<p>我们强调，可以有不同的粗粒化标准和不同的变量组合方式。我们将在下面的某些点上回到这些问题（§VII.D），但现在我们探索当我们将这个最简单的方案应用于真实神经元网络时会发生什么。第一个这样的例子使用了§V中描述的 1000 多个神经元活动的实验（Meshulam 等人，2018，2019）。</p>
<!-- > We are interested in how the probability distributions transform and flow as we pass through successive scales of coarse–graining. Of course looking at the joint distribution $P(\\{\sigma_{i}^{(k)}\\})$ is essentially impossible. But  much can be learned by looking at slices through this distribution, even the distribution of individual coarsegrained variables, as with the magnetization in the Ising model (Binder, 1981). -->
<p>我们感兴趣的是随着我们通过连续的粗粒化尺度，概率分布如何变换和流动。当然，查看联合分布 $P(\{\sigma_{i}^{(k)}\})$ 本质上是不可能的。但是，通过查看该分布的切片，甚至是单个粗粒化变量的分布，就像 Ising 模型中的磁化一样，可以学到很多东西（Binder，1981）。</p>
<!-- > Since this coarse–graining is based simply on adding the  "neighboring"  variables, the first moment of the distribution of the individual variables must scale linearly,
> 
> $$
> M_{1}(k) \equiv \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\langle\sigma_{i}^{(k)}\rangle = \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}m_{i}^{(k)} = KM_{1}(1)
> $$
> 
> where after $k$ steps we have $N_{k}$ clusters each involving $K = 2^{k−1}$ of the original variables. The first non–trivial question is about the second moment, or the variance in activity,
> 
> $$
> \begin{aligned}
> M_{2}(K) \equiv \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\left\langle \left(
    \sigma_{i}^{(k)} - m_{i}^{(k)}
    \right)^{2}\right\rangle
> \end{aligned}
> $$
> 
> Note that if neurons are independent we expect $M_{2}(K)\propto K$, and many weakly correlated populations should approach this behavior at large $K$. If neurons are perfectly correlated, on the other hand, we expect $M_{2}(K)\propto K^{2}$. Looking at the data, in Fig 34A, we see  that for neurons in the hippocampus $M_{2}\propto K^{\widetilde{\alpha}}$, with $\widetilde{\alpha}= 1.4 \pm 0.06$. This non–trivial scaling is visible over more than two decades. -->
<p>由于这种粗粒化仅基于添加 &ldquo;近邻&rdquo; 变量，单个变量分布的一阶矩缩放必须线性:</p>
<p>$$
M_{1}(k) \equiv \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\langle\sigma_{i}^{(k)}\rangle = \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}m_{i}^{(k)} = KM_{1}(1)
$$</p>
<p>其中经过 $k$ 步后，我们有 $N_{k}$ 个集群，每个集群内含 $K = 2^{k−1}$ 原始变量。第一个非平凡的问题是关于二阶矩，即(结点)活动的方差，</p>
<p>$$
\begin{aligned}
M_{2}(K) \equiv \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\left\langle \left(
\sigma_{i}^{(k)} - m_{i}^{(k)}
\right)^{2}\right\rangle
\end{aligned}
$$</p>
<p>请注意，
如果神经元是独立的，我们期望 $M_{2}(K)\propto K$，并且许多弱相关的群体应该在 $K$ 很大时接近这种行为。
另一方面，如果神经元是完全关联的，我们期望 $M_{2}(K)\propto K^{2}$。查看数据，在图 34A 中，我们看到对于海马体中的神经元，$M_{2}\propto K^{\widetilde{\alpha}}$，其中 $\widetilde{\alpha}= 1.4 \pm 0.06$。这种非平凡的缩放在两个数量级以上是可见的。</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/LHkXUa8D9GTPcmJ.png" alt=""  /></p>
</blockquote>
</blockquote>
<!-- > > Three slices through the distribution of coarsegrained variables (Meshulam et al., 2018, 2019). (A) Variance of the activity vs. the (real space) coarse graining scale, from  Eq (151). Solid line is $M_{2}\propto K^{\widetilde{\alpha}}$, $\widetilde{\alpha}= 1.4 \pm 0.06$; dashed lines are predictions for independent ($\widetilde{\alpha}= 1$) or perfectly correlated ($\widetilde{\alpha}= 2$) neurons. (B) Probability of silence vs. the coarsegraining scale. Solid line is Eq (152) with $\widetilde{\beta}= 0.88 \pm 0.01$; dashed line is the expectation for independent neurons, $\widetilde{\beta}= 1$. (C) Distribution of the normalized non–zero activity, as defined in Eq (153). -->
<blockquote>
<p>图 34 三个通过粗粒化变量分布的切片（Meshulam 等人，2018，2019）。(A) 活动方差与（实空间）粗粒化尺度的关系，来自方程（151）。实线为 $M_{2}\propto K^{\widetilde{\alpha}}$，$\widetilde{\alpha}= 1.4 \pm 0.06$；虚线为独立神经元（$\widetilde{\alpha}= 1$）或完全关联神经元（$\widetilde{\alpha}= 2$）的预测。(B) 静默概率与粗粒化尺度的关系。实线为方程（152），$\widetilde{\beta}= 0.88 \pm 0.01$；虚线为独立神经元的期望，$\widetilde{\beta}= 1$。(C) 归一化非零活动的分布，如方程（153）中定义。</p>
</blockquote>
<!-- > We can take another slice through the distribution by asking for the probability $P_{k}(0)$ that the coarse–grained variable $\sigma_{i}^{(k)} = 0$. Since we started with variables $\sigma_{i} =  \\{0, 1\\}$, this is the same as asking for the probability that all of the neurons inside the cluster of size $K = 2^{k−1}$ are silent. If the neurons are independent we expect a simple scaling $P_{k}(0)\propto \exp{(−aK)}$, and once more expect to see this at large $K$ even if the cells are weakly correlated. Experimentally we see in Fig 34B that
> 
> $$
> P_{k}(0) = \exp{(-a K^{\widetilde{\beta}})}
> $$
> 
> with the exponent $\widetilde{\beta}= 0.88 \pm 0.01$. Again scaling is precise over more than two decades. -->
<p>我们可以通过询问粗粒化变量 $\sigma_{i}^{(k)} = 0$ 的概率 $P_{k}(0)$ 来获取分布的另一个切片。由于我们从变量 $\sigma_{i} =  \{0, 1\}$ 开始，这与询问簇大小为 $K = 2^{k−1}$ 的所有神经元都处于静默状态的概率相同。如果神经元是独立的，我们期望一个简单的缩放 $P_{k}(0)\propto \exp{(−aK)}$，即使细胞是弱相关的，我们也期望在大 $K$ 时看到这种情况。实验上我们在图 34B 中看到</p>
<p>$$
P_{k}(0) = \exp{(-a K^{\widetilde{\beta}})}
$$</p>
<p>其中指数 $\widetilde{\beta}= 0.88 \pm 0.01$。同样，缩放在两个数量级以上是精确的。</p>
<blockquote>
<p>指数形式是保证能量可加($E = \sum E_{i}$), 独立分布($P = \prod P_{i}$) 的概率分布, 即 Boltzmann 概率形式 $\begin{aligned}P(\text{微观态}) = \frac{1}{Z}e^{-\beta E}\end{aligned}$. 那么群体静默是罕见事件, 概率为 $P(\text{all silent}) = q^{k} = e^{-K|\ln q|}$</p>
<p>自由能 $F = -k_{B}T\ln{Z}$.</p>
</blockquote>
<!-- > If we imagine making an explicit model for the joint activity of all the neurons inside one of the clusters, perhaps in the form of the pairwise models above [Eq (83)], then the probability of complete silence is dependent only on the partition function, $P_{k}(0) = 1/Z$. This generalizes if we include higher–order terms, so that Fig 34B probes the effective free energy, which apparently  behaves as $F(K) = -aK^{\widetilde{\beta}}$. Since $\widetilde{\beta}< 1$, the free energy is sub–extensive, and hence the free energy per neuron will vanish in the thermodynamic limit. This is consistent with the equality of entropy and energy that we saw for retinal neurons in §VI.B (Fig 29). -->
<p>如果我们想象为集群内所有神经元的联合活动建立一个显式模型，可能采用上述成对模型的形式[方程（83）]，那么完全静默的概率仅依赖于配分函数，$P_{k}(0) = 1/Z$。如果我们包括更高阶项，这种情况会被推广，因此图 34B 探测了等效自由能，显然表现为 $F(K) = -aK^{\widetilde{\beta}}$。由于 $\widetilde{\beta}&lt; 1$，自由能是亚广延的，因此在热力学极限下每个神经元的自由能将消失。这与我们在 §VI.B（图29）中看到的视网膜神经元的熵和能量相等是一致的。</p>
<!-- > More generally if we define the normalized variable $x = \sigma^{(k)}/K$, then
> 
> $$
> P_{k}(x) = P_{k}(0)\delta_{x,0} + [1-P_{k}(0)]Q_{k}(x)
> $$
> 
> Figure 34C shows the evolution of $Q_{k}(x)$ as $K$ increases. We see that the tail of the distribution is gradually absorbed into the bulk, which seems to approach a fixed form $Q(x)\sim e^{−x/x_{0}}$ . If the neurons were independent the central limit theorem would drive this distribution toward a Gaussian, but instead we see the emergence of a fixed non–Gaussian form. -->
<p>更一般地，如果我们定义归一化变量 $x = \sigma^{(k)}/K$，那么</p>
<p>$$
P_{k}(x) = P_{k}(0)\delta_{x,0} + [1-P_{k}(0)]Q_{k}(x)
$$</p>
<p>图 34C 显示了随着 $K$ 增加，$Q_{k}(x)$ 的演变。我们看到分布的尾部逐渐被吸收到主体中，似乎接近一个固定形式 $Q(x)\sim e^{−x/x_{0}}$。如果神经元是独立的，中心极限定理会将该分布驱动向高斯分布，但相反，我们看到了一个固定的非高斯形式的出现。</p>
<!-- > In addition to looking at the distribution of single coarse–grained variables we can look at the covariance matrix of the microscopic variables within each cluster of size $K$. The eigenvalue spectrum of this covariance matrix depends on the rank scaled by $K$, and there is a substantial region over which the spectrum is a power $\lambda\sim (K/\text{rank})^{\mu}$, with $μ = 0.71 \pm 0.06$, although this is less crisp than the other examples of scaling. -->
<p>除了查看单个粗粒化变量的分布外，我们还可以查看每个大小为 $K$ 的簇内微观变量的协方差矩阵。该协方差矩阵的特征值谱取决于按 $K$ 缩放的秩，并且在很大一部分区域内，谱是一个幂 $\lambda\sim (K/\text{rank})^{\mu}$，其中 $μ = 0.71 \pm 0.06$，尽管这不如其他缩放示例那么清晰。</p>
<!-- > Our discussion of thus far has focused on the distribution of variables at a single moment in time. In the applications of the RG that we understand, however, we can often observe dynamic scaling (Hohenberg and Halperin, 1977). Intuitively, fluctuations on longer length scales take longer to relax because the underlying interactions are local. What is non–trivial is that correlation functions for variables coarse–grained to different length scales collapse to a universal form if we measure time in units of the correlation time, and this correlation time itself varies as a power of the length scale. An elegant example of these ideas in a fully biological context is provided by dynamic scaling of the velocity fluctuations in natural swarms of insects (Cavagna et al., 2017). -->
<p>到目前为止，我们的讨论集中在单一时间点上变量的分布。然而，在我们理解的 RG 应用中，我们通常可以观察到动态缩放（Hohenberg 和 Halperin，1977）。直观地说，更长长度尺度上的波动需要更长时间才能弛豫，因为基础相互作用是局部的。非平凡的是，如果我们以相关时间为单位测量时间，那么对不同长度尺度粗粒化变量的相关函数会坍缩为一个通用形式，并且该相关时间本身随着长度尺度的幂变化。在完全生物学背景下，这些思想的一个优雅例子是昆虫自然群体中速度波动的动态缩放（Cavagna 等人，2017）。</p>
<!-- > With networks of neurons we don’t expect locality to be a good guide, but it still is plausible that more strongly coarse–grained variables will have slower dynamics, and we can search for dynamic scaling. Concretely we define the correlation function for individual variables at coarse–graining scale $k$,
> 
> $$
> \widetilde{C}\_{i}^{(k)}(t) = \left\langle\left[
    \sigma_{i}^{k}(t_{0})-m_{i}^{(k)}
    \right]\left[
    \sigma_{i}^{k}(t_{0}+t)-m_{i}^{(k)}
    \right]\right\rangle
> $$
> 
> and then we can normalize and average over the clusters to give
> 
> $$
> C^{(k)}(t) = \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\frac{\widetilde{C}\_{i}^{(k)}(t)}{\widetilde{C}\_{i}^{(k)}(0)}
> $$
> 
> Dynamic scaling is the hypothesis that the dependence on scale is captured by a single correlation time,
> 
> $$
> C^{(k)}(t) = C[t/\tau_{c}(k)]
> $$
> 
> with $\tau_{c}(k)\propto K^{\widetilde{z}}$. In Figure 35 we see that all of this works for the population of hippocampal neurons. We note that dynamic range of correlation times accessed in this experiment is limited, at short times by the dynamics of the indicator molecules and at long times by the small value of the exponent $\widetilde{z}= 0.16 \pm 0.02$. -->
<p>对于神经元网络，我们不期望局部性是一个好的指导，但更强粗粒化的变量仍然可能具有较慢的动态，我们可以搜索动态缩放。具体来说，我们定义粗粒化尺度 $k$ 下单个变量的相关函数，</p>
<p>$$
\widetilde{C}_{i}^{(k)}(t) = \left\langle\left[
\sigma_{i}^{k}(t_{0})-m_{i}^{(k)}
\right]\left[
\sigma_{i}^{k}(t_{0}+t)-m_{i}^{(k)}
\right]\right\rangle
$$</p>
<p>然后我们可以归一化并对簇进行平均，得到</p>
<p>$$
C^{(k)}(t) = \frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\frac{\widetilde{C}_{i}^{(k)}(t)}{\widetilde{C}_{i}^{(k)}(0)}
$$</p>
<p>动态缩放是假设尺度的依赖性由单一相关时间捕获，</p>
<p>$$
C^{(k)}(t) = C[t/\tau_{c}(k)]
$$</p>
<p>其中 $\tau_{c}(k)\propto K^{\widetilde{z}}$。在图 35 中，我们看到对于海马体神经元群体，所有这些都有效。我们注意到，在这个实验中访问的相关时间的动态范围是有限的，在短时间内受到指示剂分子动力学的限制，在长时间内受到指数 $\widetilde{z}= 0.16 \pm 0.02$ 的小值的限制。</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/vwUmpPsOQC8V4r2.png" alt=""  /></p>
</blockquote>
</blockquote>
<!-- > > Dynamic scaling across 1000+ neurons in the hippocampus (Meshulam et al., 2018). (A) Mean correlation functions for coarse–grained variables, Eq (155), in clusters of $K = 2,4,\cdots, 256$ neurons (lightest orange corresponds to the largest cluster), with larger clusters exhibiting slower dynamics. In dashed gray, $\pm$ one standard deviation across the K = 256 neuron clusters. (B) Collapse under scaling of the time axis, Eq (156). (C) Correlation time vs cluster size, fit to $\tau_{c}\propto K^{\widetilde{z}}$, with $\widetilde{z}= 0.16 \pm 0.02$. -->
<blockquote>
<p>海马体中 1000 多个神经元的动态缩放（Meshulam 等人，2018）。(A) 粗粒化变量的平均相关函数，方程（155），在 $K = 2,4,\cdots, 256$ 个神经元的簇中（最浅的橙色对应于最大的簇），较大的簇表现出较慢的动态。虚线灰色表示 K = 256 神经元簇的正负一个标准差。(B) 时间轴的缩放坍缩，方程（156）。(C) 簇大小与相关时间的关系，拟合为 $\tau_{c}\propto K^{\widetilde{z}}$，其中 $\widetilde{z}= 0.16 \pm 0.02$。</p>
</blockquote>
<!-- > It is important that these scaling behavior are not somehow driven by our choice to describe neural activity with binary variables. In these experiments, neural activity was recorded by imaging of fluorescence from indicator molecules that provide a continuous signal as in Figs 6 and 16. We can follow the same steps of coarsegraining for these continuous signals, and the results are the same (Meshulam et al., 2019). -->
<p>重要的是，这些缩放行为并不是由我们选择用二进制变量来描述神经活动所驱动的。在这些实验中，神经活动是通过成像指示剂分子的荧光记录的，这些分子提供了一个连续的信号，如图 6 和 16 所示。我们可以对这些连续信号遵循相同的粗粒化步骤，结果是相同的（Meshulam 等人，2019）。</p>
<!-- > In the full theoretical structure of the RG, scaling exponents are signatures of **universality classes**. Before we can ask about universality we have to ask about reproducibility, especially in such complex systems. As a first step, the same analyses have been done with data from experiments on multiple mice. Because scaling is precise across more than two decades, the error bars in determining the exponents in individual mice are small, which sets a high standard for reproducibility. For example, the exponent describing the scaling of the free  energy (Fig 34B) is $\widetilde{\beta}= 0.87±0.014±0.015$ for the mean, the rms error in single experiments, and the standard deviation across experiments in three mice. This holds out the hope that we have uncovered features of the emergent behavior that are reproducible in the second decimal place. -->
<p>在 RG 的完整理论结构中，缩放指数是<strong>普适类</strong>的标志。在我们询问普适性之前，我们必须询问可重复性，尤其是在如此复杂的系统中。作为第一步，已经使用来自多只小鼠实验的数据进行了相同的分析。由于缩放在两个数量级以上是精确的，因此在单只小鼠中确定指数的误差条很小，这为可重复性设定了高标准。例如，描述自由能缩放的指数（图 34B）为 $\widetilde{\beta}= 0.87±0.014±0.015$，分别表示单次实验中的均值、均方根误差和三只小鼠实验中的标准偏差。这让人希望我们已经发现了在第二个小数位上可重复的涌现行为特征。</p>
<!-- > A more ambitious search for universality was undertaken by Morales et al. (2023). They analyzed experiments that are part of a large effort at the Allen Institute for Brain Science, in this case using multiple neuropixels probes (Fig 5) to record 100+ neurons from each of many different areas of the mouse brain, simultaneously. Note that in addition to exploring many different brain regions, the technique for recording activity is completely different than in the hippocampal imaging data analyzed in Figs 34 and 35. Nonetheless, all aspects of scaling are reproduced across all these brain areas; examples include the scaling of the variance in coarse–grained activity (Fig 36A) and dynamic scaling (Fig 36B).  -->
<p>Morales 等人（2023）进行了更雄心勃勃的普适性搜索。他们分析了属于 Allen Institute for Brain Science 大型项目的一些实验，在这种情况下，使用多个神经像素探针（图 5）同时记录来自小鼠大脑许多不同区域的 100 多个神经元。请注意，除了探索许多不同的大脑区域外，记录活动的技术与图 34 和 35 中分析的海马体成像数据完全不同。尽管如此，所有这些大脑区域都重现了缩放的各个方面；示例包括粗粒化活动中方差的缩放（图 36A）和动态缩放（图 36B）。</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/6L7pMntTX2ZiWGN.png" alt=""  /></p>
</blockquote>
</blockquote>
<!-- > > FIG. 36 Scaling in mutliple distinct areas of the mouse brain (Moralés et al., 2023). neuronal data after the  "real space"  (direct correlations) coarse-graining procedure. (A) Variance of the coarse-grained activity vs cluster size for neurons in sixteen different brain regions (depicted as different markers), comparable to Fig 34A. (B) Dynamic scaling for the same brain areas. Correlation time vs cluster size, comparable to Fig 35. Inset: Decay of the autocorrelation function for the neurons in one brain region (primary motor cortex) showing the collapse once time is rescaled. -->
<blockquote>
<p>图 36 小鼠大脑多个不同区域的缩放（Moralés 等人，2023）。神经元数据经过 &ldquo;实空间&rdquo; （直接相关）粗粒化程序。(A) 十六个不同大脑区域中粗粒化活动的方差与簇大小的关系（以不同标记表示），可与图 34A 相对比。(B) 相同大脑区域的动态缩放。相关时间与簇大小的关系，可与图 35 相对比。插图：一个大脑区域（初级运动皮层）中神经元自相关函数的衰减，显示出一旦时间重新缩放后的坍缩。</p>
</blockquote>
<!-- > As we were completing this review a striking result was reported by Munn et al. (2024). Rather than looking at experiments across multiple brain areas in a single organism, they looked at experiments on many different organisms, from the tiny worm C. elegans to primates much like us. There are significant technical differences among these experiments, including differences in the calcium indicator proteins (§III.C) and differences in the sampling rate; complete resolution of individual neurons vs  "regions of interest;"  and recording from the entire brain is smaller model organisms vs. a single sensory or motor area in larger organisms. Many microscopic features of these networks also are very different, with the extreme being that C. elegans neurons generate slow, graded potentials instead of discrete action potentials or spikes. Despite these caveats, we can ask how the patterns of neural activity in these systems transform under coarse–graining across a range from two to five decades. Results for the variance of the coarse–grained activity, $M_{2}(k)$ from Eq (151), are shown in Fig 37. The apparent universality of these results is tantalizing. -->
<p>在我们完成这篇综述时，Munn 等人（2024）报道了一个引人注目的结果。他们没有查看单个有机体中多个大脑区域的实验，而是查看了许多不同有机体的实验，从微小的线虫 C. elegans 到与我们非常相似的灵长类动物。这些实验之间存在显著的技术差异，包括钙指示蛋白（§III.C）的差异和采样率的差异；完全分辨单个神经元与 &ldquo;感兴趣区域&rdquo; ；以及在较小模型有机体中记录整个大脑与在较大有机体中记录单个感觉或运动区域。 这些网络的许多微观特征也非常不同，极端情况是 C. elegans 神经元产生缓慢的渐变电位，而不是离散的动作电位或尖峰。尽管存在这些警告，我们仍然可以询问这些系统中的神经活动模式如何在从两个到五个数量级的范围内通过粗粒化进行转换。图 37 显示了粗粒化活动方差 $M_{2}(k)$（来自方程（151））的结果。这些结果的明显普适性令人着迷。</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/DRIsqQXhzF7KAg5.png" alt=""  /></p>
</blockquote>
</blockquote>
<!-- > > Scaling in the variance of neural activity, Eq (151), as a function of scale across multiple species (Munn et al., 2024). (A) Zebrafish. (B) The worm C. elegans. (C) The fruit fly Drosophila melanogaster. (D) Mouse primary visual cortex. (E) Macaque primary visual and motor cortices. Grey lines are results from individual animals, red points with errors are means  within species, and red lines are fits to $M_{2}\propto K^{\widetilde{\alpha}}$, with exponents as shown. Expectations for independent (blue) and completely correlated (green) populations corresponding to the dashed lines in Fig 34A. -->
<blockquote>
<p>图 37 神经活动方差的缩放，方程（151），作为跨多个物种的尺度函数（Munn 等人，2024）。(A) 斑马鱼。(B) 线虫 C. elegans。(C) 果蝇 Drosophila melanogaster。(D) 小鼠初级视觉皮层。(E) 猕猴初级视觉和运动皮层。灰线是单个动物的结果，带误差的红点是物种内的均值，红线是对 $M_{2}\propto K^{\widetilde{\alpha}}$ 的拟合，指数如图所示。独立（蓝色）和完全相关（绿色）群体的期望对应于图 34A 中的虚线。</p>
</blockquote>
<h1 id="by-analogy-with-momentum-shell-methods">By analogy with momentum shell methods<a hidden class="anchor" aria-hidden="true" href="#by-analogy-with-momentum-shell-methods">#</a></h1>
<!-- > In problems where  "scale"  really is a length scale, coarse–graining is a gradual blurring out of spatial detail much as what happens when we look through a microscope and defocus. In that analogy, the spatial pattern is Fourier transformed and then reconstructed using only a limited range of wavelengths. Concretely, if we start with variables $\phi(\vec{x})$ in a $d$–dimensional space with coordinates $\vec{x}$, the coarse–graining operation becomes
> 
> $$
> \begin{aligned}
> \phi(\vec{x}) &\rightarrow \phi_{\Lambda}(\vec{x}) = z_{\Lambda}\int_{|\vec{k}|<\Lambda}\frac{\mathrm{d}^{d}k}{(2\pi)^{d}}e^{i\vec{k}\cdot\vec{x}}\widetilde{\phi}(\vec{k})\\\\
> \widetilde{\phi}(\vec{k}) &= \int \mathrm{d}^{d}x e^{-i\vec{k}\cdot\vec{x}}\phi(\vec{x})
> \end{aligned}
> $$
> 
> where $\Lambda = \pi/l$ cuts off contributions below a length scale $l$ and $z_{\Lambda}$ serves to (re)normalize the variables; in the microscopic analogy this compensates for the loss of contrast as we defocus. As in real space we are interested in how the probability distribution $P_{\lambda}[\phi_{\Lambda}]$ evolves as a function of the cutoff $\Lambda$. Since the Fourier variables are continuous (in the limit of a large system) we can make infinitesimal changes $\Lambda\to\Lambda-\mathrm{d}\Lambda$. In quantum mechanics wave with wavevector $\vec{k}$ describe particles with momentum $\vec{p}=\hbar\vec{k}$, so that average over the details in a range $\Lambda − \mathrm{d}\Lambda < |\vec{k}| < \Lambda$ is equivalent to integrating out a  "momentum shell"  (Wilson and Kogut, 1974). -->
<p>在 &ldquo;尺度&rdquo; 真正是长度尺度的问题中，粗粒化是空间细节的逐渐模糊，就像我们通过显微镜观察并失焦时发生的情况一样。在这个类比中，空间模式被傅里叶变换，然后仅使用有限范围的波长进行重建。具体来说，如果我们从 $d$ 维空间中坐标为 $\vec{x}$ 的变量 $\phi(\vec{x})$ 开始，粗粒化操作变为</p>
<p>$$
\begin{aligned}
\phi(\vec{x}) &amp;\rightarrow \phi_{\Lambda}(\vec{x}) = z_{\Lambda}\int_{|\vec{k}|&lt;\Lambda}\frac{\mathrm{d}^{d}k}{(2\pi)^{d}}e^{i\vec{k}\cdot\vec{x}}\widetilde{\phi}(\vec{k})\\
\widetilde{\phi}(\vec{k}) &amp;= \int \mathrm{d}^{d}x e^{-i\vec{k}\cdot\vec{x}}\phi(\vec{x})
\end{aligned}
$$</p>
<p>其中 $\Lambda = \pi/l$ 截断了低于长度尺度 $l$ 的贡献，$z_{\Lambda}$ 用于（重新）归一化变量；在微观类比中，这补偿了我们失焦时对比度的损失。与实空间一样，我们感兴趣的是概率分布 $P_{\lambda}[\phi_{\Lambda}]$ 如何随着截止值 $\Lambda$ 的变化而演变。由于傅里叶变量是连续的（在大系统的极限下），我们可以进行无穷小变化 $\Lambda\to\Lambda-\mathrm{d}\Lambda$。在量子力学中，波矢为 $\vec{k}$ 的波描述动量为 $\vec{p}=\hbar\vec{k}$ 的粒子，因此在范围 $\Lambda − \mathrm{d}\Lambda &lt; |\vec{k}| &lt; \Lambda$ 内对细节进行平均相当于在一个 &ldquo;动量壳&rdquo; 上积分（Wilson 和 Kogut，1974）。</p>
<!-- > Momentum is conserved in systems with translation invariance. Independent of these physical principles, spatial translation invariance privileges the Fourier transform. As an example, if variables $z_{i}$ live on a lattice of points $\vec{x}\_{i}$, translation invariance means that the covariance matrix elements $C_{ij}$ can depend only on the difference in positions,
> 
> $$
> C_{ij} = C(\vec{x}\_{i} - \vec{x}\_{j})
> $$
> 
> this matrix is diagonalized in a Fourier basis,
> 
> $$
> \begin{aligned}
> \sum_{j=1}^{N}C_{ij}u_{jr} &= \lambda_{r}u_{ir}\\\\
> u_{jr} &\propto \exp{(i\vec{k}\_{r}\cdot\vec{x}\_{j})}
> \end{aligned}
> $$
> 
> where we can put the modes in order by the rank of the eigenvalue $r$. -->
<p>在具有平移不变性的系统中，动量是守恒的。独立于这些物理原理，空间平移不变性使傅里叶变换具有特权。举例来说，如果变量 $z_{i}$ 位于点 $\vec{x}_{i}$ 的晶格上，平移不变性意味着协方差矩阵元素 $C_{ij}$ 只能依赖于位置的差异，</p>
<p>$$
C_{ij} = C(\vec{x}_{i} - \vec{x}_{j})
$$</p>
<p>该矩阵在傅里叶基中对角化，</p>
<p>$$
\begin{aligned}
\sum_{j=1}^{N}C_{ij}u_{jr} &amp;= \lambda_{r}u_{ir}\\
u_{jr} &amp;\propto \exp{(i\vec{k}_{r}\cdot\vec{x}_{j})}
\end{aligned}
$$</p>
<p>我们可以根据特征值 $r$ 对模式进行排序。</p>
<!-- > In the usual applications of the RG, large momenta correspond to small eigenvalues of the covariance matrix. Thus suggests that we can construct coarse–grained variables by filtering out the  "modes"  that correspond to small eigenvalues, without reference to space or momenta (Bradde and Bialek, 2017). This connects coarsegraining to a more familiar data analysis technique, principal components analysis (Shlens, 2014). -->
<p>在 RG 的常规应用中，大动量对应于协方差矩阵的小特征值。这表明我们可以通过滤除对应于小特征值的 &ldquo;模式&rdquo; 来构建粗粒化变量，而不参考空间或动量（Bradde 和 Bialek，2017）。这将粗粒化与更熟悉的数据分析技术, <strong>主成分分析</strong>（Shlens，2014）联系起来。</p>
<!-- > Concretely, if we start with microscopic variables $\\{\sigma_{i}\\}$, we can compute the covariance matrix as usual
> 
> $$
> C_{ij} = \langle(\sigma_{i}-\langle\sigma_{i}\rangle)(\sigma_{j}-\langle\sigma_{j}\rangle)\rangle
> $$
> 
> and then we have eigenvalues and eigenvectors as in Eq (160). Let’s choose the rank $r$ so that $\lambda_{1}\geq \lambda_{2}\cdots\lambda_{N}$.  We can define a projection onto the $\hat{K}$ modes that make the largest contribution to the variance,
> 
> $$
> \begin{aligned}
> \hat{P}(\hat{K}) &= \sum_{r=1}^{\hat{K}} u_{ir}u_{jr}\\\\
> \phi_{\hat{K}}(i) &= z_{i}(\hat{K})\sum_{j}\hat{P}\_{ij}(\hat{K})[\sigma_{i}-\langle\sigma_{i}\rangle]
> \end{aligned}
> $$
> 
> with the normalization $z_{i}(\hat{K})$ such that $\langle[\phi_{\hat{K}}(i)]^{2}\rangle = 1$. -->
<p>具体来说，如果我们从微观变量 $\{\sigma_{i}\}$ 开始，我们可以像往常一样计算协方差矩阵</p>
<p>$$
C_{ij} = \langle(\sigma_{i}-\langle\sigma_{i}\rangle)(\sigma_{j}-\langle\sigma_{j}\rangle)\rangle
$$</p>
<p>然后我们有如方程（160）所示的特征值和特征向量。让我们选择秩 $r$，使得 $\lambda_{1}\geq \lambda_{2}\cdots\lambda_{N}$。我们可以定义一个投影到对方差贡献最大的 $\hat{K}$ 个模式上，</p>
<p>$$
\begin{aligned}
\hat{P}_{ij}(\hat{K}) &amp;= \sum_{r=1}^{\hat{K}} u_{ir}u_{jr}\\
\phi_{\hat{K}}(i) &amp;= z_{i}(\hat{K})\sum_{j}\hat{P}_{ij}(\hat{K})[\sigma_{i}-\langle\sigma_{i}\rangle]
\end{aligned}
$$</p>
<p>其中 $z_{i}(\hat{K})$ 为归一化(系数), 使得 $\langle[\phi_{\hat{K}}(i)]^{2}\rangle = 1$。</p>
<!-- > As before, we want to follow the distribution of the individual coarse–grained variables, $P_{\hat{K}}(\phi_{\hat{K}})$; results are shown in Fig 38A. To be sure that we have control over the full matrix $C_{ij}$ we look at clusters of $N = 128$ neurons identified through the real space coarse–graining above. We can then filter out half of the modes, so that $\hat{K} = 32$, resulting in a distribution $P_{\hat{K}}(\phi_{\hat{K}})$ that still has some fine structure. If we reduce to $\hat{K} = 32$ these wiggles disappear but the distribution remains asymmetric with  long tails. This pattern continues as we reduce to $\hat{K} = 16$ and then $\hat{K} = 8$, and in these last steps the distribution hardly changes. This suggests that as we coarse–grain, the distribution flows toward a fixed form. Importantly this form is very different from the Gaussian that would be guaranteed by the central limit theorem if correlations were weak. -->
<p>如前所述，我们想要跟踪单个粗粒化变量的分布，$P_{\hat{K}}(\phi_{\hat{K}})$；结果显示在图 38A 中。为了确保我们对整个矩阵 $C_{ij}$ 有控制，我们查看了通过上述实空间粗粒化识别的 $N = 128$ 个神经元的集群。然后我们可以滤除一半的模式，使得 $\hat{K} = 32$，得到的分布 $P_{\hat{K}}(\phi_{\hat{K}})$ 仍然具有一些细微结构。如果我们减少到 $\hat{K} = 32$，这些波动消失了，但分布仍然是不对称的，并且具有长尾。这种模式在我们减少到 $\hat{K} = 16$ 然后 $\hat{K} = 8$ 时继续存在，在这最后几个步骤中，分布几乎没有变化。这表明随着我们进行粗粒化，分布趋向于一个固定形式。重要的是，这种形式与如果相关性较弱，中心极限定理所保证的高斯形式非常不同。</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/5dgHeQK8aONPkqz.png" alt=""  /></p>
</blockquote>
</blockquote>
<!-- > > Coarse–graining in groups of $N = 128$ neurons via  "momentum shells"  (Meshulam et al., 2018). (A) Following the distribution of individual coarse–grained variables from Eq (164). Different colors correspond to keeping different numbers of modes $\hat{K}$, as in inset; dashed line is a Gaussian for comparison. (B) Dynamic scaling of the correlation time for fluctuations in mode r, Eq (166), vs the associated eigenvalue of the covariance matrix, $\tau_{c}(r)\propto \lambda_{r}^{\widetilde{z}^{\prime}}$ , $\widetilde{z}^{\prime} = 0.37 \pm 0.04$. -->
<blockquote>
<p>图 38 通过 &ldquo;动量壳&rdquo; 对 $N = 128$ 个神经元进行粗粒化（Meshulam 等人，2018）。(A) 跟踪来自方程（164）的单个粗粒化变量的分布。不同颜色对应于保留不同数量的模式 $\hat{K}$，如插图所示；虚线为高斯分布以供比较。(B) 模式 $r$ 中波动的相关时间的动态缩放，方程（166），与协方差矩阵的相关特征值 $\tau_{c}(r)\propto \lambda_{r}^{\widetilde{z}^{\prime}}$，其中 $\widetilde{z}^{\prime} = 0.37 \pm 0.04$。</p>
</blockquote>
<!-- > The intuition behind dynamic scaling is that fluctuations on larger length scales relax more slowly, and we have seen that this generalizes to a network of neurons even though the meaning of  "scale"  now if more abstract (Fig 35). By transforming to basis that diagonalizes the covariance matrix we have isolated the modes of fluctuation that are independent at second order, and it is natural to ask how these fluctuations along these modes relax. Variations along mode $r$ are define by
> 
> $$
> \widetilde{\phi}\_{r} = \sum_{i=1}^{N}[\sigma_{i}-\langle\sigma_{i}\rangle]u_{ir}
> $$
> 
> and the correlation function is
> 
> $$
> C_{r}(t) = \langle\widetilde{\phi}\_{r}(t_{0})\widetilde{\phi}\_{r}(t_{0}+t)\rangle
> $$
> 
> Dynamic scaling is the statement that all these correlations collapse when time is scaled by a single correlation time, and that this correlation time itself has a power–law dependence of scale. In the usual examples this means $\tau_{c}\propto |\vec{k}|^{z}$ (Hohenberg and Halperin, 1977), but near a critical point the eigenvalues of the covariance matrix also have a power–law dependence on $|\vec{k}|$, so we can test directly for $\tau_{c}\propto \lambda^{\widetilde{z}^{\prime}}$ as shown in Fig 38B. As before, the shortest correlation times are limited by the response time of the fluorescent proteins that report on electrical activity, and the longest times are limited by the magnitude of the dynamic scaling exponent; nonetheless we can observe reasonably precise scaling across two decades in $\lambda$. -->
<p>动态缩放的直觉是，更大长度尺度上的波动弛豫得更慢，尽管 &ldquo;尺度&rdquo; 的含义现在更抽象（图 35），但我们已经看到这可以推广到神经元网络。通过转换到对协方差矩阵进行对角化的基，我们已经隔离了在二阶上独立的波动模式，自然会问这些模式上的波动是如何弛豫的。沿模式 $r$ 的变化定义为</p>
<p>$$
\widetilde{\phi}_{r} = \sum_{i=1}^{N}[\sigma_{i}-\langle\sigma_{i}\rangle]u_{ir}
$$</p>
<p>相关函数为</p>
<p>$$
C_{r}(t) = \langle\widetilde{\phi}_{r}(t_{0})\widetilde{\phi}_{r}(t_{0}+t)\rangle
$$</p>
<p>动态缩放是这样一种说法：当时间按单一相关时间进行缩放时，所有这些相关性都会坍缩，并且该相关时间本身具有尺度的幂律依赖。在通常的例子中，这意味着 $\tau_{c}\propto |\vec{k}|^{z}$（Hohenberg 和 Halperin，1977），但在临界点附近，协方差矩阵的特征值也对 $|\vec{k}|$ 具有幂律依赖，因此我们可以直接测试 $\tau_{c}\propto \lambda^{\widetilde{z}^{\prime}}$，如图 38B 所示。如前所述，最短的相关时间受到报告电活动的荧光蛋白响应时间的限制，最长的时间受到动态缩放指数大小的限制；尽管如此，我们仍然可以观察到 $\lambda$ 上两个数量级的相当精确的缩放。</p>
<!-- > The dynamic exponent $\widetilde{z}^{\prime}$ that one finds by looking at the correlation times of the modes should be related to the one we see via coarse–graining in real space, $\widetilde{z}$(Fig 35C), through the exponent $\mu$ that describes the decay of  the eigenvalues of the covariance matrix, $\widetilde{z} = \mu\widetilde{z}^{\prime}$. This works, although error bars are large (Meshulam et al., 2018). More importantly, these results indicate that the network has no single characteristic time scale, but rather a continuum of time scales that can be accessed by probing on different scales. -->
<p>通过查看模式的相关时间找到的动态指数 $\widetilde{z}^{\prime}$ 应该与我们通过实空间粗粒化看到的指数 $\widetilde{z}$(图 35C) 相关，通过描述协方差矩阵特征值衰减的指数 $\mu$，$\widetilde{z} = \mu\widetilde{z}^{\prime}$。这确实有效，尽管误差条很大（Meshulam 等人，2018）。更重要的是，这些结果表明网络没有单一的特征时间尺度，而是一个连续的时间尺度，可以通过在不同尺度上探测来访问。</p>
<h1 id="rg-as-a-path-to-understanding">RG as a path to understanding<a hidden class="anchor" aria-hidden="true" href="#rg-as-a-path-to-understanding">#</a></h1>
<!-- > If we believe there is an underlying simplicity to be found amidst the complexity of neural network function and activity, we might want to pause for a moment to convince ourselves that following the RG simplification can actually lead us there. This quest now feels attainable, given the explosive experimental progress in obtaining datasets with increasing number of neurons, as in the examples above. While we may not know how to manipulate  "temperature"  or  "magnetization"  in the brain, we are gaining decades in the sheer number of monitored neurons. -->
<p>如果我们相信在神经网络功能和活动的复杂性中可以找到潜在的简单性，我们可能想暂停片刻来说服自己，遵循 RG 简化实际上可以引导我们到达那里。鉴于在获得越来越多神经元数据集方面的爆炸性实验进展，如上面的例子所示，这一追求现在感觉是可以实现的。虽然我们可能不知道如何操纵大脑中的 &ldquo;温度&rdquo; 或 &ldquo;磁化强度&rdquo; ，但我们正在通过监测的神经元数量获得数十年的进步。</p>
<!-- > The renormalization group is a powerful theoretical structure. Because we do not have a microscopic model for neural dynamics, we are not yet able to exploit this structure. What we have done instead is to adopt an RG–inspired approach to data analysis, which has been described as a  "phenomenological renormalization group"  (Nicoletti et al., 2020) or  "iterative coarsegraining"  (Munn et al., 2024). If we apply these approaches to well understood equilibrium statistical mechanics problems, the most interesting outcome would be the flow of probability distributions toward some fixed, non–Gaussian form, and the appearance of power–law scaling along this trajectory, as would happen at a critical point. Remarkably, this is what has been found, both in the initial application to the hippocampus and now in many other systems; scaling exponents are reproducible and perhaps even universal. It is tempting to conclude that the underlying network dynamics must be described by a theory which is at a non–trivial fixed point of the renormalization group. -->
<p>重整化群是一个强大的理论结构。由于我们没有神经动力学的微观模型，我们还无法利用这个结构。相反，我们采用了一种受 RG 启发的数据分析方法，这被描述为 &ldquo;现象学重整化群&rdquo; （Nicoletti 等人，2020）或 &ldquo;迭代粗粒化&rdquo; （Munn 等人，2024）。如果我们将这些方法应用于理解良好的平衡统计力学问题，最有趣的结果将是概率分布朝着某种固定的非高斯形式流动，以及沿着这条轨迹出现幂律缩放，就像在临界点发生的那样。值得注意的是，这正是所发现的，无论是在对海马体的初始应用中，还是现在在许多其他系统中；缩放指数是可重复的，甚至可能是普适的。很容易得出结论，基础网络动力学必须由重整化群的非平凡不动点描述的理论来描述。</p>
<!-- > We should be cautious. Is it possible that some of the behaviors under coarse–graining that we associate with RG fixed points could emerge, more generically, in non–equilibrium systems? Nicoletti et al. (2020) addressed this by analyzing simulations of the contact process, in which binary variables are turned on with a probability per unit time proportional to the density of active variables at neighboring sites, and then deactivate with a fixed probability per unit time. This model has one parameter, the proportionality constant in the activation rate, and there is a critical value that depends on the geometry of the network (Marro and Dickman, 1999). Below the critical point the fully inactive state is absorbing, so the question is whether the phenomenological RG can distinguish the critical point from super–critical behaviors. -->
<p>我们应该保持谨慎。是否有可能我们与 RG 不动点相关联的一些粗粒化行为可以更普遍地出现在非平衡系统中？Nicoletti 等人（2020）通过分析接触过程的模拟来解决这个问题，在该过程中，二进制变量以与邻近位置活跃变量密度成正比的单位时间概率被激活，然后以固定的单位时间概率被停用。该模型有一个参数，即激活率中的比例常数，并且存在一个取决于网络几何形状的临界值（Marro 和 Dickman，1999）。在临界点以下，完全不活跃状态是吸收态，因此问题是现象学 RG 是否可以区分临界点与超临界行为。</p>
<!-- > Perhaps surprisingly, one can see (weakly) non–trivial scaling behavior in some quantities even away from the critical point, as with the variance in activity shown in Fig 39A. But other quantities show clear deviations from scaling, even very close to criticality, as with the correlation times in Fig 39B. What is unambiguous is that the probability distributions of coarse–grained variables flow toward a non–trivial fixed form at the critical point, and toward a Gaussian otherwise. We can see this by coarse–graining in real space (Fig 39C) or via momentum shells (Fig 39D). Nicoletti et al. (2020) emphasize that the phenomenological RG can identify critical points unambiguously, but only if we check the full range of behaviors. -->
<p>也许令人惊讶的是，即使远离临界点，我们也可以在某些量中看到（弱）非平凡的缩放行为，如图 39A 所示的活动方差。但其他量即使非常接近临界性也显示出明显的偏离缩放行为，如图 39B 所示的相关时间。明确无误的是，在临界点处，粗粒化变量的概率分布朝着非平凡的固定形式流动，而在其他情况下则朝着高斯形式流动。我们可以通过实空间粗粒化（图 39C）或通过动量壳（图 39D）来看到这一点。Nicoletti 等人（2020）强调，现象学 RG 可以明确地识别临界点，但前提是我们检查了全范围的行为。</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/CHzbQuy9jkpa7IX.png" alt=""  /></p>
</blockquote>
</blockquote>
<!-- > > FIG. 39 Coarse-graining of the contact process (Nicoletti et al., 2020). (A) Variance of activity vs. the scale of coarse— graining in real space, as in Figs 34A and 37. Behavior at criticality (blue) is clearly different from the super-—critical case (red), which departs systematically but weakly from the expectations for independent variables (dashed lines). (B) Correlation time vs. the scale of coarse-graining in real space, as in Fig 35. The control parameter is set close to its critical value, and we see hints of scaling at small $K$ but clear departures at large $K$. (C) Distribution of individual coarse— grained variables for $K = 32,64, 128, 256$ at criticality (blue) and away from criticality (red). In both cases we see flow toward a fixed distribution, but away from criticality this is Gaussian as expected from the central limit theorem. (D) As in (C), but with coarse-graning via momentum shells, keeping $N/8, N/16, N/32, N/64, N/128$ of the modes. -->
<blockquote>
<p>图 39 接触过程的粗粒化（Nicoletti 等人，2020）。(A) 实空间中粗粒化尺度与活动方差的关系，如图 34A 和 37 所示。临界性下的行为（蓝色）明显不同于超临界情况（红色），后者系统地但弱地偏离了独立变量的预期（虚线）。(B) 实空间中粗粒化尺度与相关时间的关系，如图 35 所示。控制参数设置接近其临界值，我们在小 $K$ 处看到缩放的迹象，但在大 $K$ 处明显偏离。(C) 临界性下（蓝色）和远离临界性（红色）时，$K = 32,64, 128, 256$ 的单个粗粒化变量的分布。在这两种情况下，我们都看到了朝着固定分布的流动，但远离临界性时，这符合中心极限定理所预期的高斯分布。(D) 与 (C) 类似，但通过动量壳进行粗粒化，保留 $N/8, N/16, N/32, N/64, N/128$ 个模式。</p>
</blockquote>
<!-- > As with the (related) discussion of criticality in $VI.D, it has been suggested that some of the phenomena uncovered by iterative coarse-graining can be reproduced in a model where neurons respond independently to latent fields (Morrell et al., 2021). In this view, scaling and the flow toward fixed distributions are approximate, and it is not clear why scaling exponents should be reproducible across animals; a broader notion of universality, as in Fig 37, would be even more difhicult to understand. -->
<p>正如在 VI.D 中（相关）关于临界性的讨论中所提到的那样，有人建议通过迭代粗粒化发现的一些现象可以在神经元独立响应潜在场的模型中再现（Morrell 等人，2021）。在这种观点中，缩放和朝向固定分布的流动是近似的，并且不清楚为什么缩放指数应该在动物之间是可重复的；如图 37 所示，更广泛的普适性概念将更难理解。</p>
<!-- > Certainly the suggestion that scaling behaviors emerge generically from latent variable models is incorrect. Consider models in which the effective field acting on each neuron $i$ is a linear combination of $K$ latent variables drawn from a Gaussian distribution. If the fields are weak then the covariance matrix of neural activity has the same rank as the covariance matrix of the fields. This simple result breaks down at stronger fields, but even in the limit of infinitely strong fields there remains a gap in the eigenvalue spectrum of the covariance matrix, at least for typical choices of parameters, so that it is impossible to recover precise scaling behaviors. -->
<p>毫无疑问，缩放行为从潜在变量模型中普遍出现的建议是错误的。考虑这样一种模型，其中作用在每个神经元 $i$ 上的有效场是从高斯分布中抽取的 $K$ 个潜在变量的线性组合。如果场是弱的，那么神经活动的协方差矩阵与场的协方差矩阵具有相同的秩。这个简单的结果在更强的场下失效，但即使在无限强场的极限下，协方差矩阵的特征值谱中仍然存在一个间隙，至少对于典型参数选择来说是这样，因此不可能恢复精确的缩放行为。</p>
<!-- > We note that a concrete, biologically motivated model of latent fields—the independent place cell model discussed in §VI.D—fails to exhibit scaling (Meshulam et al., 2018). This result perhaps should not be surprising. In a population of place cells, there are two length scales, the approximate width of the place fields and the mean distance between place field centers. In the one–dimensional (virtual) environment that provides the background for the hippocampal experiments analyzed here, the ratio of these lengths gives us a characteristic number of neurons, $K_{c}\sim 18$. Indeed, analyses of the independent place cell model corresponding to Figs 34A, B show  "breaks"  at $K \sim K_{c}$. While these are approximate statements, they highlight the fact that, in the presence of such obvious scales, the observation of rather precise power–law scaling in both static and dynamic quantities really is surprising. -->
<p>我们注意到，具体的、生物学动机的潜在场模型——§VI.D 中讨论的独立位置细胞模型——未能表现出缩放行为（Meshulam 等人，2018）。这个结果或许并不令人惊讶。在位置细胞群体中，有两个长度尺度，即位置场的近似宽度和位置场中心之间的平均距离。在为此处分析的海马体实验提供背景的一维（虚拟）环境中，这些长度的比率给了我们一个特征神经元数，$K_{c}\sim 18$。实际上，对应于图 34A、B 的独立位置细胞模型的分析显示在 $K \sim K_{c}$ 处有 &ldquo;断点&rdquo; 。虽然这些都是近似陈述，但它们突显了这样一个事实：在存在如此明显尺度的情况下，在静态和动态量中观察到相当精确的幂律缩放确实令人惊讶。</p>
<!-- > Faced with high–dimensional observations, a natural reaction is to search for a lower dimensional description. In some sense the renormalization group is the opposite approach (Bradde and Bialek, 2017). Rather than looking for the correct number of dimensions onto which to project the data, the RG invites us to examine how our description changes as we move the boundary between details that we ignore and features that we keep. Things simplify not because we have fewer degrees of freedom but because the model describing these degrees of freedom flows toward something simpler and more universal. The evidence thus far points toward the existence of such a simplified description. From the theoretical side, initial efforts at an RG analysis of models for networks of more realistic neurons suggest that these are described by new universality classes (Brinkman, 2023). -->
<p>面对高维观察，一个自然的反应是寻找一个低维描述。在某种意义上，重整化群是相反的方法（Bradde 和 Bialek，2017）。与寻找正确的维数以投影数据不同，RG 邀请我们检查当我们移动忽略的细节和保留的特征之间的边界时，我们的描述如何变化。事情简化了，不是因为我们有更少的自由度，而是因为描述这些自由度的模型朝着更简单、更普遍的东西流动。到目前为止的证据指向这样一种简化描述的存在。从理论方面来看，对更现实神经元网络模型进行 RG 分析的初步努力表明，这些模型由新的普适类描述（Brinkman，2023）。</p>
<!-- > What we have not emphasized here is the connection of coarse–graining to more functional behaviors. In the hippocampus, how is position represented in the coarsegrained variables? More generally, do fine–grained and coarse–grained variables implement different principles for the encoding of the sensory world (Munn et al., 2024)? Can local networks of neurons access different scaling trajectories as the brain switches among different global states (Castro et al., 2024)? As coarse–graining becomes a more commonly used tool for the analysis of large scale neural recordings, we expect progress on these issues over the next years. -->
<p>我们在这里没有强调的是粗粒化与更功能性行为的联系。在海马体中，位置如何在粗粒化变量中表示？更一般地说，细粒化和粗粒化变量是否实现了编码感官世界的不同原则（Munn 等人，2024）？当大脑在不同的全局状态之间切换时，神经元的局部网络能否访问不同的缩放轨迹（Castro 等人，2024）？随着粗粒化成为分析大规模神经记录中更常用的工具，我们预计在未来几年内在这些问题上会取得进展。</p>
<!-- > The most detailed tests of scaling in equilibrium critical phenomena span six decades with better than one percent precision (Lipa et al., 1996). As described in §§III.B and III.C, the experimental frontier is moving toward recording from $\sim 10^{6}$ neurons simultaneously. This opens the possibility of following coarse–graining trajectories across five decades with single cell resolution, and of driving error bars down to the one percent level across more limited ranges. The extension of existing tools to organisms with larger brains also means that we will see simultaneous recordings from more neurons in single brain areas, within which scaling seems more likely. We already see signs that quantities which emerge from these analyses can be reproducible in the second decimal place. One possibility is that new, larger experiments will reveal crossovers between different regimes on different scales. Alternatively, the scaling behaviors seen thus far might prove to be essentially exact. Whatever the outcome, it is extraordinary to think that experiments on real, functioning brains could soon reach a precision comparable to those on equilibrium critical phenomena. The corresponding challenge to theory should be clear. -->
<p>在平衡临界现象中，对缩放的最详细测试跨越了六个数量级，精度优于百分之一（Lipa 等人，1996）。如 §§III.B 和 III.C 所述，实验前沿正朝着同时记录 $\sim 10^{6}$ 个神经元的方向发展。这为我们打开了一个可能性，可以在单细胞分辨率下跨越五个数量级跟踪粗粒化轨迹，并将误差条降低到更有限范围内的百分之一水平。将现有工具扩展到具有更大脑容量的生物体也意味着我们将在单个大脑区域中看到更多神经元的同时记录，在这些区域中缩放似乎更有可能。我们已经看到这些分析中出现的量可以在小数点后第二位上重复的迹象。一种可能性是，新的、更大的实验将揭示不同尺度上不同机制之间的交叉。或者，到目前为止看到的缩放行为可能被证明是基本上是精确的。无论结果如何，想到对真实、功能性大脑的实验很快就能达到与平衡临界现象相当的精度，这都是非凡的。对理论的相应挑战应该是明确的。</p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/read/theory-of-neural-computation/theory-of-neural-computation-7/">
    <span class="title">« 上一页</span>
    <br>
    <span>Recurrent Networks</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/read/reference/how-people-learn/">
    <span class="title">下一页 »</span>
    <br>
    <span>人是如何学习的 I&amp;II 节选</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: '👉展开评论';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: '👇关闭评论';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">💬评论</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>


<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = '复制';

        function copyingDone() {
            copybutton.innerText = '已复制！';
            setTimeout(() => {
                copybutton.innerText = '复制';
            }, 2000);
        }

        
        
        
        
        
        
        
        
        
        

        
        
        
        
        
        
        
        
        
        
        

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  
</body>







<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
