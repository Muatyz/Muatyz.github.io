<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Generalizable, real-time neural decoding with hybrid state-space models | æ— å¤„æƒ¹å°˜åŸƒ</title>
<meta name="keywords" content="">
<meta name="description" content="Computation through dynamics">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/reference/generalizable-real-time-neural-decoding-with-hybrid-state-space-models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/reference/generalizable-real-time-neural-decoding-with-hybrid-state-space-models/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>







<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="Generalizable, real-time neural decoding with hybrid state-space models" />
<meta property="og:description" content="Computation through dynamics" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/reference/generalizable-real-time-neural-decoding-with-hybrid-state-space-models/" />
<meta property="og:image" content="https://s2.loli.net/2025/12/11/v2WCjygtn9MrIOp.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-12-11T00:18:23+08:00" />
<meta property="article:modified_time" content="2025-12-11T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/12/11/v2WCjygtn9MrIOp.png" />
<meta name="twitter:title" content="Generalizable, real-time neural decoding with hybrid state-space models"/>
<meta name="twitter:description" content="Computation through dynamics"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ“• é˜…è¯»",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "ğŸ“• æ–‡çŒ®",
          "item": "https://Muatyz.github.io/posts/read/reference/"
        }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Generalizable, real-time neural decoding with hybrid state-space models",
      "item": "https://Muatyz.github.io/posts/read/reference/generalizable-real-time-neural-decoding-with-hybrid-state-space-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Generalizable, real-time neural decoding with hybrid state-space models",
  "name": "Generalizable, real-time neural decoding with hybrid state-space models",
  "description": "Computation through dynamics",
  "keywords": [
    ""
  ],
  "articleBody": "Abstract Real-time decoding of neural activity is central to neuroscience and neurotechnology applications, from closed-loop experiments to brain-computer interfaces, where models are subject to strict latency constraints.\nTraditional methods, including simple recurrent neural networks, are fast and lightweight but often struggle to generalize to unseen data. In contrast, recent Transformer-based approaches leverage large-scale pretraining for strong generalization performance, but typically have much larger computational requirements and are not always suitable for lowresource or real-time settings.\nå®æ—¶è§£ç ç¥ç»æ´»åŠ¨ æ˜¯ç¥ç»ç§‘å­¦å’Œç¥ç»æŠ€æœ¯åº”ç”¨çš„æ ¸å¿ƒï¼Œä»é—­ç¯å®éªŒåˆ°è„‘æœºæ¥å£ï¼Œæ¨¡å‹éƒ½å—åˆ°ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶ã€‚\nä¼ ç»Ÿæ–¹æ³•ï¼ŒåŒ…æ‹¬ç®€å•çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼Œé€Ÿåº¦å¿«ä¸”è½»é‡ï¼Œä½†é€šå¸¸éš¾ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„æ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘åŸºäº Transformer çš„æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒå®ç°äº†å¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ï¼Œä½†é€šå¸¸å…·æœ‰æ›´å¤§çš„è®¡ç®—éœ€æ±‚ï¼Œå¹¶ä¸æ€»æ˜¯é€‚åˆä½èµ„æºæˆ–å®æ—¶è®¾ç½®ã€‚\nTo address these shortcomings, we present POSSM, a novel hybrid architecture that combines individual spike tokenization via a cross-attention module with a recurrent state-space model (SSM) backbone to enable\nfast and causal online prediction on neural activity and efficient generalization to new sessions, individuals, and tasks through multi-dataset pretraining. ä¸ºäº†è§£å†³è¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº† POSSMï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ··åˆæ¶æ„ï¼Œç»“åˆäº†é€šè¿‡ äº¤å‰æ³¨æ„åŠ›æ¨¡å— è¿›è¡Œçš„å•ä¸ªè„‰å†²æ ‡è®°åŒ–ä¸å¾ªç¯çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰éª¨å¹²ï¼Œä»¥å®ç°\nå¯¹ç¥ç»æ´»åŠ¨çš„å¿«é€Ÿå’Œå› æœåœ¨çº¿é¢„æµ‹ï¼Œä»¥åŠ é€šè¿‡å¤šæ•°æ®é›†é¢„è®­ç»ƒå®ç°å¯¹æ–°ä¼šè¯ã€ä¸ªä½“å’Œä»»åŠ¡çš„é«˜æ•ˆæ³›åŒ–ã€‚ We evaluate POSSMâ€™s decoding performance and inference speed on intracortical decoding of monkey motor tasks, and show that it extends to clinical applications, namely handwriting and speech decoding in human subjects.\nNotably, we demonstrate that pretraining on monkey motor-cortical recordings improves decoding performance on the human handwriting task, highlighting the exciting potential for cross-species transfer. In all of these tasks, we find that POSSM achieves decoding accuracy comparable to state-of-the-art Transformers, at a fraction of the inference cost (up to 9Ã— faster on GPU). These results suggest that hybrid SSMs are a promising approach to bridging the gap between accuracy, inference speed, and generalization when training neural decoders for real-time, closed-loop applications.\næˆ‘ä»¬è¯„ä¼°äº† POSSM åœ¨çŒ´å­è¿åŠ¨ä»»åŠ¡çš„çš®å±‚å†…è§£ç ä¸­çš„è§£ç æ€§èƒ½å’Œæ¨ç†é€Ÿåº¦ï¼Œå¹¶å±•ç¤ºäº†å®ƒåœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æ‰©å±•ï¼Œå³äººç±»å—è¯•è€…çš„æ‰‹å†™å’Œè¯­éŸ³è§£ç ã€‚\nå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨çŒ´å­è¿åŠ¨çš®å±‚è®°å½•ä¸Šçš„é¢„è®­ç»ƒæé«˜äº†äººç±»æ‰‹å†™ä»»åŠ¡çš„è§£ç æ€§èƒ½ï¼Œçªæ˜¾äº†è·¨ç‰©ç§è½¬ç§»çš„ä»¤äººå…´å¥‹çš„æ½œåŠ›ã€‚åœ¨æ‰€æœ‰è¿™äº›ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å‘ç° POSSM ä»¥ GPU ä¸Šé«˜è¾¾ 9 å€çš„é€Ÿåº¦å®ç°äº†ä¸æœ€å…ˆè¿› Transformer ç›¸å½“çš„è§£ç ç²¾åº¦ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ··åˆ SSM æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¯ä»¥å¼¥åˆå®æ—¶é—­ç¯åº”ç”¨ä¸­è®­ç»ƒç¥ç»è§£ç å™¨æ—¶å‡†ç¡®æ€§ã€æ¨ç†é€Ÿåº¦å’Œæ³›åŒ–ä¹‹é—´çš„å·®è·ã€‚\nIntroduction Neural decoding â€“ the process of mapping neural activity to behavioural or cognitive variables â€“ is a core component of modern neuroscience and neurotechnology.\nAs neural recording techniques evolve and datasets grow in size, there is increasing interest in building generalist decoders that scale and flexibly adapt across subjects and experiments.\nSeveral important downstream applications including closed-loop neuroscience experiments and brain-computer interfaces (BCIs) â€“ require fine-grained, low-latency decoding for real-time control. Advances in these technologies would enable next-generation clinical interventions in motor decoding, speech prostheses, and closed-loop neuromodulation.\nç¥ç»è§£ç â€”â€”å°†ç¥ç»æ´»åŠ¨æ˜ å°„åˆ°è¡Œä¸ºæˆ–è®¤çŸ¥å˜é‡çš„è¿‡ç¨‹â€”â€”æ˜¯ç°ä»£ç¥ç»ç§‘å­¦å’Œç¥ç»æŠ€æœ¯çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚\néšç€ç¥ç»è®°å½•æŠ€æœ¯çš„å‘å±•å’Œæ•°æ®é›†è§„æ¨¡çš„å¢é•¿ï¼Œè¶Šæ¥è¶Šå¤šçš„äººå¯¹æ„å»ºé€šç”¨è§£ç å™¨æ„Ÿå…´è¶£ï¼Œè¿™äº›è§£ç å™¨å¯ä»¥è·¨å—è¯•è€…å’Œå®éªŒè¿›è¡Œæ‰©å±•å’Œçµæ´»é€‚åº”ã€‚\nä¸€äº›é‡è¦çš„ä¸‹æ¸¸åº”ç”¨ï¼ŒåŒ…æ‹¬é—­ç¯ç¥ç»ç§‘å­¦å®éªŒå’Œ è„‘æœºæ¥å£ï¼ˆBCIï¼‰â€”â€”éœ€è¦é«˜ç²¾ç»†åº¦ã€ä½å»¶è¿Ÿçš„è§£ç ä»¥å®æ—¶æ§åˆ¶ã€‚è¿™äº›æŠ€æœ¯çš„è¿›æ­¥, å°†ä½¿ä¸‹ä¸€ä»£ä¸´åºŠå¹²é¢„æªæ–½(è¿åŠ¨è§£ç ã€è¯­éŸ³å‡ä½“å’Œé—­ç¯ç¥ç»è°ƒèŠ‚)æˆä¸ºå¯èƒ½ã€‚\nBuilding towards these applications will require neural decoders that meet three requirements:\nrobust and accurate predictions, causal, low-latency inference that is viable in an online setting, and flexible generalization to new subjects, tasks, and experimental settings. Although recent developments in machine learning (ML) have enabled significant strides in each of these axes, building a neural decoder that achieves all three remains an open challenge.\nå®ç°è¿™äº›åº”ç”¨éœ€è¦æ»¡è¶³ä¸‰ä¸ªè¦æ±‚çš„ç¥ç»è§£ç å™¨ï¼š\nç¨³å¥ä¸”å‡†ç¡®çš„é¢„æµ‹ï¼Œ å› æœã€ä½å»¶è¿Ÿçš„æ¨ç†ï¼Œåœ¨åœ¨çº¿ç¯å¢ƒä¸­å¯è¡Œï¼Œä»¥åŠ å¯¹æ–°å—è¯•è€…ã€ä»»åŠ¡å’Œå®éªŒè®¾ç½®çš„çµæ´»æ³›åŒ–ã€‚ å°½ç®¡ æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ çš„æœ€æ–°å‘å±•åœ¨è¿™äº›æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†æ„å»ºä¸€ä¸ªåŒæ—¶å®ç°è¿™ä¸‰ç‚¹çš„ç¥ç»è§£ç å™¨ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„æŒ‘æˆ˜ã€‚\nRecurrent neural networks (RNNs) and attention-based models such as Transformers have shown significant promise for neural decoding tasks.\nRNNs (Figure 1a) offer fast, low-latency inference on sequential data and strong performance when trained on specific tasks.\nHowever, their ability to generalize to new subjects is limited due to their rigid input format. Specifically, their reliance on fixed-size, time-binned inputs means that they typically cannot learn from new sessions with different neuron identities or sampling rates without full re-training and/or modifying the architecture.\nå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ å’Œ åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ï¼ˆå¦‚ Transformerï¼‰åœ¨ç¥ç»è§£ç ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‰æ™¯ã€‚\nRNNï¼ˆå›¾ 1aï¼‰åœ¨åºåˆ—æ•°æ®ä¸Šæä¾›å¿«é€Ÿã€ä½å»¶è¿Ÿçš„æ¨ç†ï¼Œå¹¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè®­ç»ƒæ—¶è¡¨ç°å‡ºè‰²ã€‚\nç„¶è€Œï¼Œç”±äºå…¶ç¡¬æ€§çš„è¾“å…¥æ ¼å¼ï¼Œå®ƒä»¬å¯¹æ–°å—è¯•è€…çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä»¬ä¾èµ–äºå›ºå®šå¤§å°çš„æ—¶é—´åˆ†ç®±è¾“å…¥ï¼Œè¿™æ„å‘³ç€å¦‚æœä¸å®Œå…¨é‡æ–°è®­ç»ƒå’Œ/æˆ–ä¿®æ”¹æ¶æ„, å®ƒä»¬é€šå¸¸æ— æ³•ä»å…·æœ‰ä¸åŒç¥ç»å…ƒèº«ä»½æˆ–é‡‡æ ·ç‡çš„æ–°æƒ…æ™¯ä¸­å­¦ä¹ ã€‚\nIn contrast, Transformerbased architectures (Figure 1b) offer greater flexibility thanks to more adaptable neural tokenization approaches.\nNonetheless, they struggle with applications involving real-time processing due to their quadratic computational complexity, in addition to the overall computational load of the attention mechanism.\nRecent efforts in sequence modelling with large language models have explored hybrid architectures that combine recurrent layers, such as gated recurrent units (GRUs) or state-space models (SSMs), with attention layers.\nThese models show encouraging gains in long-context understanding and computational efficiency. While hybrid attention-SSM approaches offer a promising solution for real-time neural decoding, to the best of our knowledge, they remain unexplored in this area.\nç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäº Transformer çš„æ¶æ„ï¼ˆå›¾ 1bï¼‰ç”±äºæ›´çµæ´»çš„ç¥ç»æ ‡è®°åŒ–æ–¹æ³•è€Œæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚\nå°½ç®¡å¦‚æ­¤ï¼Œç”±äºå…¶äºŒæ¬¡è®¡ç®—å¤æ‚æ€§ä»¥åŠæ³¨æ„åŠ›æœºåˆ¶çš„æ•´ä½“è®¡ç®—è´Ÿè½½ï¼Œå®ƒä»¬åœ¨æ¶‰åŠå®æ—¶å¤„ç†çš„åº”ç”¨ä¸­ä»ç„¶å­˜åœ¨å›°éš¾ã€‚\næœ€è¿‘åœ¨ å¤§è¯­è¨€æ¨¡å‹ çš„åºåˆ—å»ºæ¨¡æ–¹é¢çš„åŠªåŠ›æ¢ç´¢äº†å°† é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ æˆ– çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ ç­‰å¾ªç¯å±‚ä¸æ³¨æ„åŠ›å±‚ç›¸ç»“åˆçš„æ··åˆæ¶æ„ã€‚\nè¿™äº›æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œè®¡ç®—æ•ˆç‡æ–¹é¢æ˜¾ç¤ºå‡ºä»¤äººé¼“èˆçš„æå‡ã€‚è™½ç„¶æ··åˆæ³¨æ„åŠ›-SSM æ–¹æ³•ä¸ºå®æ—¶ç¥ç»è§£ç æä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒä»¬åœ¨è¯¥é¢†åŸŸä»æœªè¢«æ¢ç´¢ã€‚\nWe address this gap with POSSM, a hybrid model that combines the flexible input-processing of Transformers with the efficient, online inference capabilities of a recurrent SSM backbone.\nUnlike traditional methods that rely on rigid time-binning, POSSM operates at a millisecond-level resolution by tokenizing individual spikes. In essence, POSSM builds on a POYO-style cross-attention encoder that projects a variable number of spike tokens to a fixed-size latent space. The resulting output is then fed to an SSM that updates its hidden state across consecutive chunks in time.\næˆ‘ä»¬é€šè¿‡ POSSM å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¨¡å‹ï¼Œç»“åˆäº† Transformer çš„çµæ´»è¾“å…¥å¤„ç†å’Œé€’å½’ SSM éª¨å¹²çš„é«˜æ•ˆåœ¨çº¿æ¨ç†èƒ½åŠ›ã€‚\nä¸ä¾èµ– åˆšæ€§æ—¶é—´åˆ†ç®± çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒPOSSM é€šè¿‡å¯¹å•ä¸ªè„‰å†²è¿›è¡Œæ ‡è®°åŒ–ï¼Œåœ¨æ¯«ç§’çº§åˆ†è¾¨ç‡ä¸‹è¿è¡Œã€‚æœ¬è´¨ä¸Šï¼ŒPOSSM å»ºç«‹åœ¨ POYO é£æ ¼çš„äº¤å‰æ³¨æ„åŠ›ç¼–ç å™¨ä¹‹ä¸Šï¼Œè¯¥ç¼–ç å™¨å°†å¯å˜æ•°é‡çš„è„‰å†²æ ‡è®°æŠ•å½±åˆ°å›ºå®šå¤§å°çš„æ½œåœ¨ç©ºé—´ã€‚ç„¶åå°†ç”Ÿæˆçš„è¾“å‡ºé¦ˆé€åˆ° SSMï¼Œè¯¥ SSM åœ¨è¿ç»­æ—¶é—´å—ä¸­æ›´æ–°å…¶éšè—çŠ¶æ€ã€‚\nThis architecture, as illustrated in Figure 2, offers two key benefits:\nthe recurrent backbone allows for lightweight, constant-time predictions over consecutive chunks of time and by adopting POYOâ€™s spike tokenization, encoding, and decoding schemes, POSSM can effectively generalize to different sessions, tasks, and subjects. å¦‚å›¾ 2 æ‰€ç¤ºï¼Œè¿™ç§æ¶æ„æä¾›äº†ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼š\nå¾ªç¯éª¨å¹²å…è®¸å¯¹è¿ç»­æ—¶é—´å—è¿›è¡Œè½»é‡çº§ã€æ’å®šæ—¶é—´çš„é¢„æµ‹ï¼› é€šè¿‡é‡‡ç”¨ POYO çš„è„‰å†²æ ‡è®°åŒ–ã€ç¼–ç å’Œè§£ç æ–¹æ¡ˆï¼ŒPOSSM å¯ä»¥æœ‰æ•ˆåœ°æ³›åŒ–åˆ°ä¸åŒçš„ä¼šè¯ã€ä»»åŠ¡å’Œå—è¯•è€…ã€‚ Figure 2: An architecture for generalizable, real-time neural decoding. POSSM combines individual spike tokenization and input-output cross-attention with a recurrent SSM backbone. In this paper, we typically consider $k = 3$ and $T_{c} = 50\\text{ ms}$.\nå›¾ 2ï¼šä¸€ç§é€šç”¨çš„å®æ—¶ç¥ç»è§£ç æ¶æ„ã€‚POSSM ç»“åˆäº†å•ä¸ªè„‰å†²æ ‡è®°åŒ–å’Œè¾“å…¥è¾“å‡ºäº¤å‰æ³¨æ„åŠ›ä¸å¾ªç¯ SSM éª¨å¹²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸è€ƒè™‘ $k = 3$ å’Œ $T_{c} = 50\\text{ ms}$ã€‚\nIn this paper, we introduce the POSSM architecture and evaluate its performance on intracortical recordings of spiking activity from experiments in both non-human primates (NHPs) and humans.\nAlthough our current evaluations are conducted offline, POSSM is designed for real-time inference and can be readily implemented for online experiments.\nFinally, while this paper is concerned with invasive electrophysiology recordings, this method could be extended further to other modalities using POYO-style tokenization (see Section 5 for a discussion). Our contributions are summarized as follows:\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† POSSM æ¶æ„ï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨ éäººç±»çµé•¿ç±»åŠ¨ç‰©ï¼ˆNHPï¼‰ å’Œäººç±»å®éªŒä¸­è„‰å†²æ´»åŠ¨çš®å±‚å†…è®°å½•çš„æ€§èƒ½ã€‚\nå°½ç®¡æˆ‘ä»¬å½“å‰çš„è¯„ä¼°æ˜¯åœ¨ç¦»çº¿è¿›è¡Œçš„ï¼Œä½† POSSM æ—¨åœ¨å®ç° å®æ—¶æ¨ç†ï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå®¹æ˜“åœ°å®ç°åœ¨çº¿å®éªŒã€‚\næœ€åï¼Œè™½ç„¶æœ¬æ–‡å…³æ³¨çš„æ˜¯ä¾µå…¥æ€§ç”µç”Ÿç†è®°å½•ï¼Œä½†è¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°ä½¿ç”¨ POYO é£æ ¼æ ‡è®°åŒ–çš„å…¶ä»–æ¨¡æ€ï¼ˆæœ‰å…³è®¨è®ºï¼Œè¯·å‚è§ç¬¬ 5 èŠ‚ï¼‰ã€‚æˆ‘ä»¬çš„è´¡çŒ®æ€»ç»“å¦‚ä¸‹ï¼š\nPerformance and efficiency: We evaluate POSSM against other popular models using NHP datasets that contain multiple sessions, subjects, and different reaching tasks (centre-out, random target, maze, etc.). We find that POSSM matches or outperforms other models on all these tests, doing so with greater speed and significantly reduced computational cost. è¡¨å¾å’Œæ•ˆç‡ï¼šæˆ‘ä»¬ä½¿ç”¨åŒ…å«å¤šä¸ªä¼šè¯ã€å—è¯•è€…å’Œä¸åŒåˆ°è¾¾ä»»åŠ¡ï¼ˆä¸­å¿ƒå¤–ã€éšæœºç›®æ ‡ã€è¿·å®«ç­‰ï¼‰çš„ NHP æ•°æ®é›†è¯„ä¼° POSSM ä¸å…¶ä»–æµè¡Œæ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç° POSSM åœ¨æ‰€æœ‰è¿™äº›æµ‹è¯•ä¸­éƒ½èƒ½åŒ¹é…æˆ–ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå¹¶ä¸”é€Ÿåº¦æ›´å¿«ï¼Œè®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½ã€‚ Multi-dataset pretraining improves performance: Through large-scale pretraining, we find that POSSM delivers improved performance on NHP datasets across sessions, subjects, and even tasks. å¤šæ•°æ®é›†é¢„è®­ç»ƒæé«˜äº†æ€§èƒ½ï¼šé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œæˆ‘ä»¬å‘ç° POSSM åœ¨ NHP æ•°æ®é›†çš„å„ä¸ªä¼šè¯ã€å—è¯•è€…ç”šè‡³ä»»åŠ¡ä¸Šéƒ½æä¾›äº†æ”¹è¿›çš„æ€§èƒ½ã€‚ Cross-species transfer learning: Pretraining on diverse NHP datasets and then finetuning POSSM leads to state-of-the-art performance when decoding imagined handwritten letters from human cortical activity. This cross-species transfer not only outlines the remarkable transferability of neural dynamics across different primates, but also shows the potential for leveraging abundant non-human data to augment limited human datasets for improved decoding performance. è·¨ç‰©ç§è¿ç§»å­¦ä¹ ï¼šåœ¨å¤šæ ·åŒ–çš„ NHP æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå¾®è°ƒ POSSMï¼Œåœ¨è§£ç äººç±»çš®å±‚æ´»åŠ¨ä¸­æƒ³è±¡çš„æ‰‹å†™å­—æ¯æ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™ç§è·¨ç‰©ç§è½¬ç§»ä¸ä»…æ¦‚è¿°äº†ä¸åŒçµé•¿ç±»åŠ¨ç‰©ä¹‹é—´ç¥ç»åŠ¨åŠ›å­¦çš„æ˜¾è‘—å¯è½¬ç§»æ€§ï¼Œè¿˜å±•ç¤ºäº†åˆ©ç”¨ä¸°å¯Œçš„éäººç±»æ•°æ®æ¥å¢å¼ºæœ‰é™çš„äººç±»æ•°æ®é›†ä»¥æé«˜è§£ç æ€§èƒ½çš„æ½œåŠ›ã€‚ Long sequence complex decoding: When trained on human motor-cortical data during attempted speech, POSSM achieves strong decoding performance. In contrast, attentionbased models struggle with the taskâ€™s long-context demands, making them computationally prohibitive. é•¿åºåˆ—å¤æ‚è§£ç ï¼šåœ¨å°è¯•è¯­éŸ³æ—¶å¯¹äººç±»è¿åŠ¨çš®å±‚æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ï¼ŒPOSSM å®ç°äº†å¼ºå¤§çš„è§£ç æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹åœ¨ä»»åŠ¡çš„é•¿ä¸Šä¸‹æ–‡éœ€æ±‚æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½¿å¾—å®ƒä»¬åœ¨è®¡ç®—ä¸Šå˜å¾—ä¸å¯è¡Œã€‚ Methods We focus on decoding neuronal spiking activity, which consists of discrete events triggered when excitatory input to a neuron exceeds a certain threshold.\nThe timing and frequency of spikes encode information conveyed to other neurons throughout the brain, underlying a communication system that is central to all neural function.\næˆ‘ä»¬ä¸“æ³¨äºè§£ç ç¥ç»å…ƒè„‰å†²æ´»åŠ¨ï¼Œè¿™äº›æ´»åŠ¨ç”±å½“ç¥ç»å…ƒçš„å…´å¥‹æ€§è¾“å…¥è¶…è¿‡æŸä¸ªé˜ˆå€¼æ—¶è§¦å‘çš„ç¦»æ•£äº‹ä»¶ç»„æˆã€‚\nè„‰å†²çš„æ—¶æœºå’Œé¢‘ç‡ç¼–ç ä¼ é€’ç»™å¤§è„‘å…¶ä»–ç¥ç»å…ƒçš„ä¿¡æ¯ï¼Œæ„æˆäº†ç¥ç»åŠŸèƒ½çš„æ ¸å¿ƒé€šä¿¡ç³»ç»Ÿã€‚\nHowever, their sparse nature requires an effective input representation that can handle their temporal irregularity.\nFurthermore, there exists no direct mapping between the neurons of one living organism to another, highlighting the non-triviality of the alignment problem when training across multiple individuals.\nIn this section, we describe how POSSM is designed to sequentially process sub-second windows of these spikes for the online prediction of behaviour, while maintaining a flexible input framework that allows it to efficiently generalize to an entirely new set of neurons during finetuning.\nç„¶è€Œï¼Œå®ƒä»¬çš„ ç¨€ç–æ€§è´¨ éœ€è¦ä¸€ç§æœ‰æ•ˆçš„è¾“å…¥è¡¨ç¤ºæ¥å¤„ç†å®ƒä»¬çš„æ—¶é—´ä¸è§„åˆ™æ€§ã€‚\næ­¤å¤–ï¼Œä¸åŒç”Ÿç‰©ä½“çš„ç¥ç»å…ƒä¹‹é—´ä¸å­˜åœ¨ç›´æ¥æ˜ å°„ï¼Œè¿™çªæ˜¾äº†åœ¨å¤šä¸ªä¸ªä½“ä¹‹é—´è®­ç»ƒæ—¶ å¯¹é½é—®é¢˜ çš„éå¹³å‡¡æ€§ã€‚\nåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æè¿°äº† POSSM å¦‚ä½•è®¾è®¡ç”¨äºåºåˆ—å¤„ç†è¿™äº›è„‰å†²çš„äºšç§’çª—å£ï¼Œä»¥å®ç°è¡Œä¸ºçš„åœ¨çº¿é¢„æµ‹ï¼ŒåŒæ—¶ä¿æŒçµæ´»çš„è¾“å…¥æ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¾®è°ƒæœŸé—´æœ‰æ•ˆåœ°æ¨å¹¿åˆ°ä¸€ç»„å…¨æ–°çš„ç¥ç»å…ƒã€‚\nInput processing Streaming neural activity as input Our focus is on real-time performance, which requires minimal latency between the moment that neural activity is observed and when a corresponding prediction is generated. This constraint significantly limits the time duration, and consequently the amount of data, that a model can use for each new prediction.\næˆ‘ä»¬ä¸“æ³¨äºå®æ—¶æ€§èƒ½ï¼Œè¿™éœ€è¦åœ¨è§‚å¯Ÿåˆ°ç¥ç»æ´»åŠ¨çš„æ—¶åˆ»å’Œç”Ÿæˆç›¸åº”é¢„æµ‹ä¹‹é—´çš„å»¶è¿Ÿæœ€å°åŒ–ã€‚è¿™ä¸ªé™åˆ¶æ˜¾è‘—é™åˆ¶äº†æ¨¡å‹å¯ä»¥ç”¨äºæ¯ä¸ªæ–°é¢„æµ‹çš„æ—¶é—´æŒç»­æ—¶é—´ï¼Œå› æ­¤ä¹Ÿé™åˆ¶äº†æ•°æ®é‡ã€‚\nTo this end, POSSM maintains a hidden state as data is streamed in, allowing it to incorporate past information without reprocessing previous inputs. In each chunk, the number of spikes varies, meaning each input is represented as a variable-length sequence of spikes.\nWhile POSSM generally uses contiguous 50 ms time chunks here, we also demonstrate strong performance with 20 ms chunks (see Section D.1). In theory, these windows could even be shorter or longer (and even overlapping) depending on the task, with the understanding that there would be some trade-off between temporal resolution and computational complexity.\nä¸ºæ­¤ï¼ŒPOSSM åœ¨æ•°æ®æµå…¥æ—¶ä¿æŒéšè—çŠ¶æ€ï¼Œä½¿å…¶èƒ½å¤Ÿç»“åˆè¿‡å»çš„ä¿¡æ¯è€Œæ— éœ€é‡æ–°å¤„ç†å…ˆå‰çš„è¾“å…¥ã€‚åœ¨æ¯ä¸ªå—ä¸­ï¼Œè„‰å†²æ•°é‡å„ä¸ç›¸åŒï¼Œè¿™æ„å‘³ç€æ¯ä¸ªè¾“å…¥è¡¨ç¤ºä¸ºå¯å˜é•¿åº¦çš„è„‰å†²åºåˆ—ã€‚\nè™½ç„¶ POSSM é€šå¸¸åœ¨è¿™é‡Œä½¿ç”¨è¿ç»­çš„ 50 æ¯«ç§’æ—¶é—´å—ï¼Œä½†æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä½¿ç”¨ 20 æ¯«ç§’å—æ—¶çš„å¼ºå¤§æ€§èƒ½ï¼ˆè§ç¬¬ D.1 èŠ‚ï¼‰ã€‚ç†è®ºä¸Šï¼Œè¿™äº›çª—å£ç”šè‡³å¯ä»¥æ›´çŸ­æˆ–æ›´é•¿ï¼ˆç”šè‡³é‡å ï¼‰ï¼Œå…·ä½“å–å†³äºä»»åŠ¡ï¼ŒåŒæ—¶ç†è§£åœ¨æ—¶é—´åˆ†è¾¨ç‡å’Œè®¡ç®—å¤æ‚æ€§ä¹‹é—´ä¼šæœ‰ä¸€äº›æƒè¡¡ã€‚\nSpike tokenization. We adopt the tokenization scheme from the original POYO model, where each neuronal spike is represented using two pieces of information:\nthe identity of the neural unit which it came from and the timestamp at which it occurred (see Figure 2). The former corresponds to a unique learnable unit embedding for each neuron, while the latter is encoded with a rotary position embedding (RoPE) that allows the tokens to be processed based on their relative timing rather than their absolute timestamps.\næˆ‘ä»¬é‡‡ç”¨äº†åŸå§‹ POYO æ¨¡å‹çš„æ ‡è®°åŒ–æ–¹æ¡ˆï¼Œå…¶ä¸­æ¯ä¸ªç¥ç»å…ƒè„‰å†²ä½¿ç”¨ä¸¤æ¡ä¿¡æ¯è¡¨ç¤ºï¼š\nå®ƒæ¥è‡ªçš„ç¥ç»å•å…ƒçš„èº«ä»½ å®ƒå‘ç”Ÿçš„æ—¶é—´æˆ³ï¼ˆè§å›¾ 2ï¼‰ã€‚ å‰è€…å¯¹åº”äºæ¯ä¸ªç¥ç»å…ƒçš„å”¯ä¸€å¯å­¦ä¹ å•å…ƒåµŒå…¥ï¼Œè€Œåè€…åˆ™ä½¿ç”¨ æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ è¿›è¡Œç¼–ç ï¼Œå…è®¸åŸºäºç›¸å¯¹æ—¶é—´è€Œéç»å¯¹æ—¶é—´æˆ³å¤„ç†æ ‡è®°ã€‚\nFor example, a spike from some neuron with integer ID $i$ that occurs at time $t_{\\text{spike}}$ would be represented as a $D$-dimensional token $x$, given by:\n$$ x = (\\text{UnitEmb}(i), t_{\\text{spike}}), $$\nwhere $\\text{UnitEmb}$ : $\\mathbb{Z}\\to \\mathbb{R}^D$ is the unit embedding map and $D$ is a hyperparameter. As tokenization is an element-wise operation, a time chunk with $N$ spikes will yield $N$ spike tokens.\nä¾‹å¦‚ï¼ŒæŸä¸ªç¥ç»å…ƒçš„è„‰å†²ï¼Œå…¶æ•´æ•° ID ä¸º $i$ï¼Œå‘ç”Ÿåœ¨æ—¶é—´ $t_{\\text{spike}}$ï¼Œå°†è¡¨ç¤ºä¸ºä¸€ä¸ª $D$ ç»´æ ‡è®° $x$ï¼Œè¡¨ç¤ºä¸ºï¼š\n$$ x = (\\text{UnitEmb}(i), t_{\\text{spike}}), $$\nå…¶ä¸­ $\\text{UnitEmb}$ : $\\mathbb{Z}\\to \\mathbb{R}^D$ æ˜¯å•å…ƒåµŒå…¥æ˜ å°„ï¼Œ$D$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚ç”±äºæ ‡è®°åŒ–æ˜¯ä¸€ä¸ªé€å…ƒç´ æ“ä½œï¼Œå…·æœ‰ $N$ ä¸ªè„‰å†²çš„æ—¶é—´å—å°†äº§ç”Ÿ $N$ ä¸ªè„‰å†²æ ‡è®°ã€‚\nWe opt to use the general term â€œneural unitâ€, as spikes could be assigned at a coarser specificity than an individual neuron (e.g., multi-unit activity on a single electrode channel) depending on the dataset and task at hand.\nThis flexibility, coupled with the modelâ€™s ability to handle a variable number of units, facilitates both training and efficient finetuning (see Section 2.3) on multiple sessions and even across datasets.\næˆ‘ä»¬é€‰æ‹©ä½¿ç”¨é€šç”¨æœ¯è¯­â€œç¥ç»å•å…ƒâ€ï¼Œå› ä¸ºæ ¹æ®æ‰‹å¤´çš„æ•°æ®é›†å’Œä»»åŠ¡ï¼Œè„‰å†²å¯ä»¥åˆ†é…å¾—æ¯”å•ä¸ªç¥ç»å…ƒæ›´ç²—ç³™ï¼ˆä¾‹å¦‚ï¼Œå•ä¸ªç”µæé€šé“ä¸Šçš„å¤šå•å…ƒæ´»åŠ¨ï¼‰ã€‚\nè¿™ç§çµæ´»æ€§ï¼ŒåŠ ä¸Šæ¨¡å‹å¤„ç†å¯å˜æ•°é‡å•å…ƒçš„èƒ½åŠ›ï¼Œæœ‰åŠ©äºåœ¨å¤šä¸ªä¼šè¯ç”šè‡³è·¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œé«˜æ•ˆå¾®è°ƒï¼ˆè§ç¬¬ 2.3 èŠ‚ï¼‰ã€‚\nArchitecture Input cross-attention We employ the original POYO encoder, where a cross-attention module inspired by the PerceiverIO architecture compresses a variable-length sequence of input spike tokens into a fixed-size latent representation. Unlike POYO, however, the encoder is applied on short 50 ms time chunks, each of which is mapped to a single latent vector.\næˆ‘ä»¬é‡‡ç”¨äº†åŸå§‹ POYO ç¼–ç å™¨ï¼Œå…¶ä¸­å— PerceiverIO æ¶æ„ å¯å‘çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—å°†å¯å˜é•¿åº¦çš„è¾“å…¥è„‰å†²æ ‡è®°åºåˆ—å‹ç¼©ä¸ºå›ºå®šå¤§å°çš„æ½œåœ¨è¡¨ç¤ºã€‚ç„¶è€Œï¼Œä¸ POYO ä¸åŒï¼Œç¼–ç å™¨åº”ç”¨äºçŸ­çš„ 50 æ¯«ç§’æ—¶é—´å—ï¼Œæ¯ä¸ªæ—¶é—´å—æ˜ å°„åˆ°å•ä¸ª æ½œåœ¨å‘é‡ã€‚\nThis is achieved by setting the spike tokens as the attention keys and values, and using a learnable query vector $q\\in \\mathbb{R}^{1\\times M}$ , where $M$ is a hyperparameter. Given a sequence $X_{t} = [x_{0}, x_{1}, \\dots, x_{N}]^{\\top}\\in \\mathbb{R}^{N\\times D}$ of $N$ spike tokens from some chunk of time indexed by $t$, the latent output of the cross-attention module is computed as such:\n$$ z^{(t)} = \\text{softmax}\\left(\\frac{qK_{t}^{\\top}}{\\sqrt{D}}\\right)V_{t} $$\nwhere $K_{t} = X_{t}W_{k}$ and $V_{t} = X_{t}W_{v}$, with $W_{k}, W_{v}\\in \\mathbb{R}^{D\\times M}$ , are the projections of the input token sequence, following standard Transformer notation. Following POYO, our implementation also uses the standard Transformer block with pre-normalization layers and feed-forward networks.\né€šè¿‡å°†è„‰å†²æ ‡è®°è®¾ç½®ä¸ºæ³¨æ„åŠ›é”®å’Œå€¼ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡ $q\\in \\mathbb{R}^{1\\times M}$ å®ç°è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­ $M$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚ç»™å®šæŸä¸ªæ—¶é—´å—ç´¢å¼•ä¸º $t$ çš„ $N$ ä¸ªè„‰å†²æ ‡è®°åºåˆ— $X_{t} = [x_{0}, x_{1}, \\dots, x_{N}]^{\\top}\\in \\mathbb{R}^{N\\times D}$ï¼Œäº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„æ½œåœ¨è¾“å‡ºè®¡ç®—å¦‚ä¸‹ï¼š\n$$ z^{(t)} = \\text{softmax}\\left(\\frac{qK_{t}^{\\top}}{\\sqrt{D}}\\right)V_{t} $$\nå…¶ä¸­ $K_{t} = X_{t}W_{k}$ å’Œ $V_{t} = X_{t}W_{v}$ï¼Œ$W_{k}, W_{v}\\in \\mathbb{R}^{D\\times M}$ æ˜¯è¾“å…¥æ ‡è®°åºåˆ—çš„æŠ•å½±ï¼Œéµå¾ªæ ‡å‡† Transformer ç¬¦å·ã€‚æŒ‰ç…§ POYO çš„åšæ³•ï¼Œæˆ‘ä»¬çš„å®ç°è¿˜ä½¿ç”¨äº†å¸¦æœ‰é¢„å½’ä¸€åŒ–å±‚å’Œå‰é¦ˆç½‘ç»œçš„æ ‡å‡† Transformer å—ã€‚\nRecurrent backbone The output of the cross-attention $z^{(t)}$ is then fed to an SSM (or another variety of RNN), which we refer to as the recurrent backbone. The hidden state of the recurrent backbone is updated as follows:\n$$ h^{(t)} = f_{\\text{SSM}}(z^{(t)}, h^{(tâˆ’1)}). $$\nWhile the input cross-attention captures local temporal structure (i.e., within the 50 ms chunk), the SSM integrates this information with historical context through its hidden state, allowing POSSM to process information at both local and global timescales. We run experiments with three different backbone architectures: diagonal structured state-space models (S4D), GRU, and Mamba. However, we wish to note that this method is compatible with any other type of recurrent model. Specifics regarding each of these backbone architectures can be found in Section B.2.\näº¤å‰æ³¨æ„åŠ›çš„è¾“å‡º $z^{(t)}$ ç„¶åè¢«é¦ˆé€åˆ° SSMï¼ˆæˆ–å…¶ä»–ç±»å‹çš„ RNNï¼‰ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºå¾ªç¯éª¨å¹²ã€‚å¾ªç¯éª¨å¹²çš„éšè—çŠ¶æ€æ›´æ–°å¦‚ä¸‹ï¼š\n$$ h^{(t)} = f_{\\text{SSM}}(z^{(t)}, h^{(tâˆ’1)}). $$\nè™½ç„¶è¾“å…¥äº¤å‰æ³¨æ„åŠ›æ•è·äº†å±€éƒ¨æ—¶é—´ç»“æ„ï¼ˆå³åœ¨ 50 æ¯«ç§’å—å†…ï¼‰ï¼Œä½† SSM é€šè¿‡å…¶éšè—çŠ¶æ€å°†è¿™äº›ä¿¡æ¯ä¸å†å²ä¸Šä¸‹æ–‡é›†æˆï¼Œä½¿ POSSM èƒ½å¤Ÿåœ¨å±€éƒ¨å’Œå…¨å±€æ—¶é—´å°ºåº¦ä¸Šå¤„ç†ä¿¡æ¯ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç§ä¸åŒçš„éª¨å¹²æ¶æ„è¿›è¡Œå®éªŒï¼šå¯¹è§’ç»“æ„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆS4Dï¼‰ã€GRU å’Œ Mambaã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¸Œæœ›æŒ‡å‡ºï¼Œè¿™ç§æ–¹æ³•ä¸ä»»ä½•å…¶ä»–ç±»å‹çš„å¾ªç¯æ¨¡å‹å…¼å®¹ã€‚æœ‰å…³æ¯ç§éª¨å¹²æ¶æ„çš„å…·ä½“ä¿¡æ¯ï¼Œè¯·å‚è§ç¬¬ B.2 èŠ‚ã€‚\nOutput cross-attention and readout To decode behaviour, we select the $k$ most recent hidden states $\\{h^{(tâˆ’k+1):(t)}\\}$ ($k = 3$ in our experiments), and use a cross-attention module to query them for behavioural predictions. For a given time chunk $t$, we generate $P$ queries, one for each timestamp at which we wish to predict behaviour. Each query encodes the associated timestamp (using RoPE), along with a learnable session embedding that captures latent factors of the recording session. The design of our output module enables flexibility in several ways:\nwe can predict multiple outputs per time chunk, enabling a denser and richer supervision signal, we are not required to precisely align behaviour to chunk boundaries, and we can predict behaviour beyond the current chunk, allowing us to account for lags between neural activity and behavioural action (see Section D.5). ä¸ºäº†å¯¹è¡Œä¸ºè¿›è¡Œè§£ç ï¼Œæˆ‘ä»¬é€‰æ‹©æœ€è¿‘çš„ $k$ ä¸ªéšè—çŠ¶æ€ $\\{h^{(tâˆ’k+1):(t)}\\}$ï¼ˆåœ¨æˆ‘ä»¬çš„å®éªŒä¸­ä¸º $k = 3$ï¼‰ï¼Œå¹¶ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æ¨¡å—å¯¹å®ƒä»¬è¿›è¡ŒæŸ¥è¯¢ä»¥è¿›è¡Œè¡Œä¸ºé¢„æµ‹ã€‚å¯¹äºç»™å®šçš„æ—¶é—´å— $t$ï¼Œæˆ‘ä»¬ç”Ÿæˆ $P$ ä¸ªæŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢å¯¹åº”æˆ‘ä»¬å¸Œæœ›é¢„æµ‹è¡Œä¸ºçš„æ—¶é—´æˆ³ã€‚æ¯ä¸ªæŸ¥è¯¢ä½¿ç”¨ RoPE ç¼–ç ç›¸å…³çš„æ—¶é—´æˆ³ï¼Œä»¥åŠä¸€ä¸ªå¯å­¦ä¹ çš„ä¼šè¯åµŒå…¥ï¼Œç”¨äºæ•è·è®°å½•ä¼šè¯çš„æ½œåœ¨å› ç´ ã€‚æˆ‘ä»¬è¾“å‡ºæ¨¡å—çš„è®¾è®¡åœ¨å‡ ä¸ªæ–¹é¢æä¾›äº†çµæ´»æ€§ï¼š\næˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªæ—¶é—´å—é¢„æµ‹å¤šä¸ªè¾“å‡ºï¼Œä»è€Œå®ç°æ›´å¯†é›†å’Œæ›´ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œ æˆ‘ä»¬ä¸éœ€è¦å°†è¡Œä¸ºç²¾ç¡®å¯¹é½åˆ°å—è¾¹ç•Œï¼Œä»¥åŠ æˆ‘ä»¬å¯ä»¥é¢„æµ‹å½“å‰å—ä¹‹å¤–çš„è¡Œä¸ºï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿè€ƒè™‘ç¥ç»æ´»åŠ¨ä¸è¡Œä¸ºåŠ¨ä½œä¹‹é—´çš„æ»åï¼ˆè§ç¬¬ D.5 èŠ‚ï¼‰ã€‚ Generalizing to unseen data with pretrained models Given a pretrained model, we outline two strategies for adapting it to a new set of neural units, with a trade-off between efficiency and decoding accuracy.\nç»™å®šä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†ä¸¤ç§å°†å…¶é€‚åº”äºä¸€ç»„æ–°çš„ç¥ç»å•å…ƒçš„ç­–ç•¥ï¼Œåœ¨æ•ˆç‡å’Œè§£ç ç²¾åº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚\nUnit identification To enable efficient generalization to previously unseen neural units, we adopt unit identification (UI), a finetuning strategy enabled by the spike tokenization scheme adopted from POYO. In UI, new neural units can be processed by a model simply by learning new embeddings. Using this approach, we freeze the model weights, initialize new unit and session embeddings, and then train them on the data from the new session while the rest of the model is kept unchanged. This allows us to preserve the neural dynamics learned during pretraining, resulting in an efficient generalization strategy that typically updates less than $1\\%$ of the modelâ€™s total parameters.\nä¸ºäº†å®ç°å¯¹å…ˆå‰æœªè§è¿‡çš„ç¥ç»å•å…ƒçš„é«˜æ•ˆæ³›åŒ–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº† å•å…ƒè¯†åˆ«ï¼ˆUIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”± POYO é‡‡ç”¨çš„è„‰å†²æ ‡è®°åŒ–æ–¹æ¡ˆå¯ç”¨çš„å¾®è°ƒç­–ç•¥ã€‚åœ¨ UI ä¸­ï¼Œé€šè¿‡å­¦ä¹ æ–°çš„åµŒå…¥ï¼Œæ¨¡å‹å¯ä»¥ç®€å•åœ°å¤„ç†æ–°çš„ç¥ç»å•å…ƒã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å†»ç»“æ¨¡å‹æƒé‡ï¼Œåˆå§‹åŒ–æ–°çš„å•å…ƒå’Œä¼šè¯åµŒå…¥ï¼Œç„¶ååœ¨æ–°ä¼šè¯çš„æ•°æ®ä¸Šè®­ç»ƒå®ƒä»¬ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†ä¸å˜ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¿ç•™é¢„è®­ç»ƒæœŸé—´å­¦åˆ°çš„ç¥ç»åŠ¨åŠ›å­¦ï¼Œäº§ç”Ÿä¸€ç§é«˜æ•ˆçš„æ³›åŒ–ç­–ç•¥ï¼Œé€šå¸¸åªæ›´æ–°æ¨¡å‹æ€»å‚æ•°çš„ä¸åˆ° $1\\%$ã€‚\nFull finetuning While unit identification is an efficient finetuning method, it does not reliably match the performance of models trained end-to-end on individual sessions.\nTo address this, we also explored full finetuning (FT), a complementary approach which uses a gradual unfreezing strategy.\nWe begin by only doing UI for some number of epochs before unfreezing the entire model and training end-to-end. This allows us to retain the benefits of pretraining while gradually adapting to the new session.\nAs shown below, full finetuning consistently outperforms both single-session training and unit identification across all tasks explored, demonstrating effective transfer to new animals, tasks, and remarkably, across species.\nè™½ç„¶å•å…ƒè¯†åˆ«æ˜¯ä¸€ç§é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œä½†å®ƒå¹¶ä¸å¯é åœ°åŒ¹é…åœ¨å•ä¸ªä¼šè¯ä¸Šç«¯åˆ°ç«¯è®­ç»ƒçš„æ¨¡å‹çš„æ€§èƒ½ã€‚\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº† å®Œå…¨å¾®è°ƒï¼ˆFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§äº’è¡¥çš„æ–¹æ³•ï¼Œä½¿ç”¨æ¸è¿›è§£å†»ç­–ç•¥ã€‚\næˆ‘ä»¬é¦–å…ˆåªè¿›è¡Œ UI ä¸€æ®µæ—¶é—´ï¼Œç„¶åè§£å†»æ•´ä¸ªæ¨¡å‹å¹¶è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¿ç•™é¢„è®­ç»ƒçš„å¥½å¤„ï¼ŒåŒæ—¶é€æ­¥é€‚åº”æ–°ä¼šè¯ã€‚\nå¦‚ä¸‹é¢æ‰€ç¤ºï¼Œå®Œå…¨å¾®è°ƒåœ¨æ‰€æœ‰æ¢ç´¢çš„ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºå•ä¼šè¯è®­ç»ƒå’Œå•å…ƒè¯†åˆ«ï¼Œå±•ç¤ºäº†å¯¹æ–°åŠ¨ç‰©ã€ä»»åŠ¡ï¼Œç”šè‡³è·¨ç‰©ç§çš„æœ‰æ•ˆè½¬ç§»ã€‚\nExperiments We evaluate POSSM across three categories of cortical activity datasets: NHP reaching tasks, human imagined handwriting, and human attempted speech.\næˆ‘ä»¬åœ¨ä¸‰ç±»çš®å±‚æ´»åŠ¨æ•°æ®é›†ä¸Šè¯„ä¼° POSSMï¼šNHP åˆ°è¾¾ä»»åŠ¡ã€äººç±»æƒ³è±¡æ‰‹å†™å’Œäººç±»å°è¯•è¯­éŸ³ã€‚\nFor the NHP reaching tasks, we highlight the benefits of scale by introducing o-POSSM, a POSSM variant pretrained on multiple datasets.\nWe compare it to single-session models on held-out sessions with varying similarity with the training data, demonstrating improved decoding accuracy with pretraining.\nå¯¹äº NHP åˆ°è¾¾ä»»åŠ¡ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ o-POSSMï¼ˆä¸€ç§åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ POSSM å˜ä½“ï¼‰æ¥çªå‡ºè§„æ¨¡çš„å¥½å¤„ã€‚\næˆ‘ä»¬å°†å…¶ä¸åœ¨å…·æœ‰ä¸åŒç›¸ä¼¼æ€§çš„è®­ç»ƒæ•°æ®çš„ä¿ç•™ä¼šè¯ä¸Šçš„å•ä¼šè¯æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œè¡¨æ˜ é¢„è®­ç»ƒ æé«˜äº†è§£ç ç²¾åº¦ã€‚\nNext, we show that o-POSSM achieves powerful downstream decoding performance on a human handwriting task, illustrating the POSSM frameworkâ€™s capacity for cross-species transfer.\nFinally, we demonstrate that POSSM effectively leverages its recurrent architecture to efficiently decode human speech - a long-context task that can become computationally expensive for standard Transformer-based models with quadratic complexity.\nIn each of these tasks, we see that POSSM consistently matches or outperforms other architectures in a causal evaluation setting, with performance improving as model size and pretraining scale increase.\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å±•ç¤ºäº† o-POSSM åœ¨äººç±»æ‰‹å†™ä»»åŠ¡ä¸Šå®ç°äº†å¼ºå¤§çš„ä¸‹æ¸¸è§£ç æ€§èƒ½ï¼Œè¯´æ˜äº† POSSM æ¡†æ¶è·¨ç‰©ç§è½¬ç§»çš„èƒ½åŠ›ã€‚\næœ€åï¼Œæˆ‘ä»¬è¯æ˜äº† POSSM æœ‰æ•ˆåœ°åˆ©ç”¨å…¶é€’å½’æ¶æ„æ¥é«˜æ•ˆè§£ç äººç±»è¯­éŸ³â€”â€”è¿™æ˜¯ä¸€é¡¹é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼Œå¯¹äºå…·æœ‰äºŒæ¬¡å¤æ‚åº¦çš„æ ‡å‡†åŸºäº Transformer çš„æ¨¡å‹æ¥è¯´å¯èƒ½å˜å¾—è®¡ç®—æ˜‚è´µã€‚\nåœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ¯ä¸€ä¸ªä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ° POSSM åœ¨å› æœè¯„ä¼°è®¾ç½®ä¸­å§‹ç»ˆåŒ¹é…æˆ–ä¼˜äºå…¶ä»–æ¶æ„ï¼Œéšç€æ¨¡å‹è§„æ¨¡å’Œé¢„è®­ç»ƒè§„æ¨¡çš„å¢åŠ ï¼Œæ€§èƒ½ä¹Ÿåœ¨æé«˜ã€‚\nTask schematics and outputs. (a) Centre-out (CO) task with a manipulandum. (b) Random target (RT) task with a manipulandum. (c) RT task with a touchscreen. (d) Maze task with a touchscreen. (e) Ground truth vs. predicted behaviour outputs from a held-out CO session. (f) Same as (e) but for an RT session. (g) Human handwriting decoding task. (h) Human speech decoding task.\nä»»åŠ¡ç¤ºæ„å›¾å’Œè¾“å‡ºã€‚(a) å¸¦æœ‰æ“çºµæ†çš„ä¸­å¿ƒå¤–ï¼ˆCOï¼‰ä»»åŠ¡ã€‚(b) å¸¦æœ‰æ“çºµæ†çš„éšæœºç›®æ ‡ï¼ˆRTï¼‰ä»»åŠ¡ã€‚(c) å¸¦æœ‰è§¦æ‘¸å±çš„ RT ä»»åŠ¡ã€‚(d) å¸¦æœ‰è§¦æ‘¸å±çš„è¿·å®«ä»»åŠ¡ã€‚(e) æ¥è‡ªä¿ç•™ CO ä¼šè¯çš„çœŸå®å€¼ä¸é¢„æµ‹è¡Œä¸ºè¾“å‡ºã€‚(f) ä¸ (e) ç›¸åŒï¼Œä½†ç”¨äº RT ä¼šè¯ã€‚(g) äººç±»æ‰‹å†™è§£ç ä»»åŠ¡ã€‚(h) äººç±»è¯­éŸ³è§£ç ä»»åŠ¡ã€‚\nNon-human primate reaching We first evaluate POSSM on the task of decoding two-dimensional hand velocities in monkeys performing various reaching tasks (shown in Figure 3a-f). Each training sample consists of 1 s of spiking activity paired with the corresponding 2D hand velocity time series over that same interval. This 1 s window is split up into 20 non-overlapping chunks of 50 ms, which are fed sequentially to POSSM (see Section D.1 for results on 20 ms chunks). This streaming setup enables efficient real-time decoding, where only the most recent 50 ms chunk is processed at each step. In sharp contrast, the original POYO model reprocesses an entire 1 s window of activity with each new input, resulting in significantly higher computational cost.\næˆ‘ä»¬é¦–å…ˆåœ¨çŒ´å­æ‰§è¡Œå„ç§åˆ°è¾¾ä»»åŠ¡æ—¶è§£ç äºŒç»´æ‰‹é€Ÿåº¦çš„ä»»åŠ¡ä¸Šè¯„ä¼° POSSMï¼ˆå¦‚å›¾ 3a-f æ‰€ç¤ºï¼‰ã€‚æ¯ä¸ªè®­ç»ƒæ ·æœ¬ç”± 1 ç§’çš„è„‰å†²æ´»åŠ¨ä¸è¯¥æ—¶é—´é—´éš”å†…ç›¸åº”çš„ 2D æ‰‹é€Ÿåº¦æ—¶é—´åºåˆ—é…å¯¹ç»„æˆã€‚è¿™ä¸ª 1 ç§’çª—å£è¢«åˆ†æˆ 20 ä¸ªä¸é‡å çš„ 50 æ¯«ç§’å—ï¼Œä¾æ¬¡é¦ˆé€ç»™ POSSMï¼ˆæœ‰å…³ 20 æ¯«ç§’å—çš„ç»“æœï¼Œè¯·å‚è§ç¬¬ D.1 èŠ‚ï¼‰ã€‚è¿™ç§æµå¼è®¾ç½®å®ç°äº†é«˜æ•ˆçš„å®æ—¶è§£ç ï¼Œæ¯ä¸ªæ­¥éª¤åªå¤„ç†æœ€è¿‘çš„ 50 æ¯«ç§’å—ã€‚ä¸æ­¤å½¢æˆé²œæ˜å¯¹æ¯”çš„æ˜¯ï¼ŒåŸå§‹ POYO æ¨¡å‹åœ¨æ¯ä¸ªæ–°è¾“å…¥æ—¶é‡æ–°å¤„ç†æ•´ä¸ª 1 ç§’çš„æ´»åŠ¨çª—å£ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬æ˜¾è‘—æ›´é«˜ã€‚\nExperimental Setup We use a similar experimental setup to POYO. The pretraining dataset includes four NHP reaching datasets collected by different laboratories, covering three types of reaching tasks: centre-out (CO), random target (RT), and Maze.\nCO is a highly-structured task involving movements from the centre of the screen to one of eight targets (Figure 3a). Conversely, the RT (Figure 3b-c) and Maze (Figure 3d) tasks are behaviourally more complex, requiring movement to randomly placed targets and navigating through a maze, respectively.\næˆ‘ä»¬ä½¿ç”¨ä¸ POYO ç±»ä¼¼çš„å®éªŒè®¾ç½®ã€‚é¢„è®­ç»ƒæ•°æ®é›†åŒ…æ‹¬ç”±ä¸åŒå®éªŒå®¤æ”¶é›†çš„å››ä¸ª NHP åˆ°è¾¾æ•°æ®é›†ï¼Œæ¶µç›–ä¸‰ç§ç±»å‹çš„åˆ°è¾¾ä»»åŠ¡ï¼šä¸­å¿ƒå¤–ï¼ˆCOï¼‰ã€éšæœºç›®æ ‡ï¼ˆRTï¼‰ å’Œè¿·å®«ã€‚\nCO æ˜¯ä¸€ç§é«˜åº¦ç»“æ„åŒ–çš„ä»»åŠ¡ï¼Œæ¶‰åŠä»å±å¹•ä¸­å¿ƒç§»åŠ¨åˆ°å…«ä¸ªç›®æ ‡ä¹‹ä¸€ï¼ˆå›¾ 3aï¼‰ã€‚ç›¸åï¼ŒRTï¼ˆå›¾ 3b-cï¼‰å’Œè¿·å®«ï¼ˆå›¾ 3dï¼‰ä»»åŠ¡åœ¨è¡Œä¸ºä¸Šæ›´å¤æ‚ï¼Œåˆ†åˆ«éœ€è¦ç§»åŠ¨åˆ°éšæœºæ”¾ç½®çš„ç›®æ ‡å’Œç©¿è¶Šè¿·å®«ã€‚\nThe testing sessions include:\nnew sessions from Monkey C which was seen during pretraining, new sessions from Monkey T, not seen during pretraining but collected in the same lab as Monkey C, and a new session from a dataset unseen during pretraining. æµ‹è¯•ä¼šè¯åŒ…æ‹¬ï¼š\næ¥è‡ªé¢„è®­ç»ƒæœŸé—´çœ‹åˆ°çš„çŒ´å­ C çš„æ–°ä¼šè¯ï¼Œ æ¥è‡ªçŒ´å­ T çš„æ–°ä¼šè¯ï¼Œåœ¨é¢„è®­ç»ƒæœŸé—´æœªè§è¿‡ï¼Œä½†ä¸çŒ´å­ C åœ¨åŒä¸€å®éªŒå®¤æ”¶é›†ï¼Œä»¥åŠ æ¥è‡ªé¢„è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ•°æ®é›†çš„æ–°ä¼šè¯ã€‚ We call our model pretrained on this dataset o-POSSM (see Section B.3 for details).\nIn total, o-POSSM was trained on 148 sessions comprising more than 670 million spikes from 26,032 neural units recorded across the primary motor (M1), dorsal premotor (PMd), and primary somatosensory (S1) cortices (see Section A for details). We also pretrain two baseline models, NDT-2 and POYO-1. Additionally, we report the results of single-session models trained from scratch on individual sessions. This includes single-session variants of POSSM (across all backbone architectures) and POYO, as well as other baselines such as a multi-layer perceptron (MLP), S4D, GRU, and Mamba.\næˆ‘ä»¬ç§°åœ¨æ­¤æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ä¸º o-POSSMï¼ˆè¯¦æƒ…è§ç¬¬ B.3 èŠ‚ï¼‰ã€‚\næ€»çš„æ¥è¯´ï¼Œo-POSSM åœ¨ 148 ä¸ªä¼šè¯ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œè¿™äº›ä¼šè¯åŒ…å«æ¥è‡ªåˆçº§è¿åŠ¨çš®å±‚ï¼ˆM1ï¼‰ã€èƒŒä¾§å‰è¿åŠ¨çš®å±‚ï¼ˆPMdï¼‰å’Œåˆçº§èº¯ä½“æ„Ÿè§‰çš®å±‚ï¼ˆS1ï¼‰çš„ 26,032 ä¸ªç¥ç»å•å…ƒè®°å½•çš„è¶…è¿‡ 6.7 äº¿ä¸ªè„‰å†²ï¼ˆè¯¦æƒ…è§ç¬¬ A èŠ‚ï¼‰ã€‚æˆ‘ä»¬è¿˜é¢„è®­ç»ƒäº†ä¸¤ä¸ªåŸºçº¿æ¨¡å‹ï¼ŒNDT-2 å’Œ POYO-1ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä»å¤´å¼€å§‹åœ¨å•ä¸ªä¼šè¯ä¸Šè®­ç»ƒçš„å•ä¼šè¯æ¨¡å‹çš„ç»“æœã€‚è¿™åŒ…æ‹¬ POSSMï¼ˆè·¨æ‰€æœ‰éª¨å¹²æ¶æ„ï¼‰å’Œ POYO çš„å•ä¼šè¯å˜ä½“ï¼Œä»¥åŠå…¶ä»–åŸºçº¿ï¼Œå¦‚å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€S4Dã€GRU å’Œ Mambaã€‚\nCausal evaluation To simulate real-world decoding scenarios, we adopt a causal evaluation strategy for all models. This is straightforward for POSSM and the recurrent baselines we consider â€“ sequences are split into 50 ms time chunks and fed sequentially to the model.\nFor the MLP and POYO, we provide a fixed 1 s history of neural activity at each inference timestep, sliding forward in small increments of 50 ms to collect predictions for all behavioural timestamps. For POYO, we only recorded predictions for timestamps in the final 50 ms of each 1 s context window.\nä¸ºäº†æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„è§£ç åœºæ™¯ï¼Œæˆ‘ä»¬ä¸ºæ‰€æœ‰æ¨¡å‹é‡‡ç”¨äº† å› æœè¯„ä¼°ç­–ç•¥ã€‚è¿™å¯¹äºæˆ‘ä»¬è€ƒè™‘çš„ POSSM å’Œå¾ªç¯åŸºçº¿æ¥è¯´æ˜¯ç›´æ¥çš„â€”â€”åºåˆ—è¢«åˆ†æˆ 50 æ¯«ç§’çš„æ—¶é—´å—ï¼Œå¹¶ä¾æ¬¡é¦ˆé€ç»™æ¨¡å‹ã€‚\nå¯¹äº MLP å’Œ POYOï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªæ¨ç†æ—¶é—´æ­¥æä¾›å›ºå®šçš„ 1 ç§’ç¥ç»æ´»åŠ¨å†å²ï¼Œä»¥ 50 æ¯«ç§’çš„å°å¢é‡å‘å‰æ»‘åŠ¨ï¼Œä»¥æ”¶é›†æ‰€æœ‰è¡Œä¸ºæ—¶é—´æˆ³çš„é¢„æµ‹ã€‚å¯¹äº POYOï¼Œæˆ‘ä»¬ä»…è®°å½•æ¯ä¸ª 1 ç§’ä¸Šä¸‹æ–‡çª—å£ä¸­æœ€å 50 æ¯«ç§’çš„æ—¶é—´æˆ³çš„é¢„æµ‹ã€‚\nDuring training, the models are presented with contiguous and non-overlapping 1 s sequences, which are not trial-aligned. However, we evaluate our models on entire trials, which are typically much longer than a second.\nFor example, in sessions from the Perich et al. dataset, trials in the validation and testing sets are at least $3\\times$ and up to $5\\times$ longer than the training sequences. This means that a recurrent model must generalize to sequences longer than the ones seen during training.\nåœ¨è®­ç»ƒæœŸé—´ï¼Œæ¨¡å‹å‘ˆç°è¿ç»­ä¸”ä¸é‡å çš„ 1 ç§’åºåˆ—ï¼Œè¿™äº›åºåˆ—æœªå¯¹é½è¯•éªŒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨æ•´ä¸ªè¯•éªŒä¸Šè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¿™äº›è¯•éªŒé€šå¸¸æ¯”ä¸€ç§’é’Ÿé•¿å¾—å¤šã€‚\nä¾‹å¦‚ï¼Œåœ¨ Perich ç­‰äººçš„æ•°æ®é›†ä¸­ï¼ŒéªŒè¯å’Œæµ‹è¯•é›†ä¸­çš„è¯•éªŒè‡³å°‘æ¯”è®­ç»ƒåºåˆ—é•¿ $3\\times$ï¼Œæœ€å¤šé•¿ $5\\times$ã€‚è¿™æ„å‘³ç€å¾ªç¯æ¨¡å‹å¿…é¡»æ¨å¹¿åˆ°æ¯”è®­ç»ƒæœŸé—´çœ‹åˆ°çš„åºåˆ—æ›´é•¿çš„åºåˆ—ã€‚\nTransfer to new sessions In Table 1, we evaluate the transferability of our pretrained models to new sessions, animals, and datasets, and compare them to single-session models. When trained on a single session, POSSM is on par with or outperforms POYO on most sessions. When using pretrained models, o-POSSM-S4D shows the best overall performance. Finally, we observe that FT is noticeably better than UI when transferring to new animals or datasets. However, UI performs on par with or better than several single-session models, and requires far fewer parameters to be trained.\nåœ¨è¡¨ 1 ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†é¢„è®­ç»ƒæ¨¡å‹å‘æ–°ä¼šè¯ã€åŠ¨ç‰©å’Œæ•°æ®é›†çš„å¯è½¬ç§»æ€§ï¼Œå¹¶å°†å…¶ä¸å•ä¼šè¯æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚å½“åœ¨å•ä¸ªä¼šè¯ä¸Šè®­ç»ƒæ—¶ï¼ŒPOSSM åœ¨å¤§å¤šæ•°ä¼šè¯ä¸­ä¸ POYO ç›¸å½“æˆ–ä¼˜äº POYOã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œo-POSSM-S4D æ˜¾ç¤ºå‡ºæœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨è½¬ç§»åˆ°æ–°åŠ¨ç‰©æˆ–æ•°æ®é›†æ—¶ï¼ŒFT æ˜æ˜¾ä¼˜äº UIã€‚ç„¶è€Œï¼ŒUI çš„è¡¨ç°ä¸å‡ ä¸ªå•ä¼šè¯æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ï¼Œå¹¶ä¸”éœ€è¦è®­ç»ƒçš„å‚æ•°è¿œå°‘äº FTã€‚\nSample and training compute efficiency A key motivation behind training neural decoders on large, heterogeneous datasets is to enable efficient transfer to new individuals, tasks, or species with minimal finetuning. To this end, we evaluated the effectiveness of our finetuning strategies through few-shot experiments. Our results, shown in Figure 4a-b, demonstrate that o-POSSM outperforms single-session models trained from scratch in low-data regimes. Notably, we observe that pretraining results in a considerable initial boost in performance when adapting to a new session, even when only unit and session embeddings are updated. In some cases, single-session models fail to match the performance of a finetuned model, even after extensive training. Overall, our results are in line with observations from Azabou et al., supporting the idea that with the right tokenization and data aggregation schemes, pretrained models can amortize both data and training costs, leading to efficient adaptation to downstream applications.\nè®­ç»ƒç¥ç»è§£ç å™¨çš„ä¸€ä¸ªå…³é”®åŠ¨æœºæ˜¯èƒ½å¤Ÿé€šè¿‡æœ€å°çš„å¾®è°ƒå®ç°å¯¹æ–°ä¸ªä½“ã€ä»»åŠ¡æˆ–ç‰©ç§çš„é«˜æ•ˆè½¬ç§»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é€šè¿‡å°‘é‡å®éªŒè¯„ä¼°äº†å¾®è°ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœå¦‚å›¾ 4a-b æ‰€ç¤ºï¼Œè¡¨æ˜ o-POSSM åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒçš„å•ä¼šè¯æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨é€‚åº”æ–°ä¼šè¯æ—¶ï¼Œé¢„è®­ç»ƒå³ä½¿ä»…æ›´æ–°å•å…ƒå’Œä¼šè¯åµŒå…¥ï¼Œä¹Ÿä¼šå¸¦æ¥æ˜¾è‘—çš„åˆå§‹æ€§èƒ½æå‡ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå•ä¼šè¯æ¨¡å‹å³ä½¿ç»è¿‡å¤§é‡è®­ç»ƒä¹Ÿæ— æ³•åŒ¹é…å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç»“æœä¸ Azabou ç­‰äººçš„è§‚å¯Ÿç»“æœä¸€è‡´ï¼Œæ”¯æŒè¿™æ ·ä¸€ç§è§‚ç‚¹ï¼šé€šè¿‡æ­£ç¡®çš„æ ‡è®°åŒ–å’Œæ•°æ®èšåˆæ–¹æ¡ˆï¼Œé¢„è®­ç»ƒæ¨¡å‹å¯ä»¥æ‘Šé”€æ•°æ®å’Œè®­ç»ƒæˆæœ¬ï¼Œä»è€Œå®ç°å¯¹ä¸‹æ¸¸åº”ç”¨çš„é«˜æ•ˆé€‚åº”ã€‚\nSample and compute efficiency benchmarking. (a) Results on a held-out CO session from Monkey C. On the left, we show the sample efficiency of adapting a pretrained model versus training from scratch. On the right, we compare training compute efficiency between these two approaches. (b) Same as (a) but for a held-out RT session from Monkey T â€“ a new subject not seen during training. (c) Comparing model performance and compute efficiency to baseline models. Inference times are computed on a workstation-class GPU (NVIDIA RTX8000). For all these results, we used a GRU backbone for POSSM.\næ ·æœ¬å’Œè®¡ç®—æ•ˆç‡åŸºå‡†æµ‹è¯•ã€‚(a) æ¥è‡ªçŒ´å­ C çš„ä¿ç•™ CO ä¼šè¯çš„ç»“æœã€‚åœ¨å·¦ä¾§ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹ä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„æ ·æœ¬æ•ˆç‡ã€‚åœ¨å³ä¾§ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„è®­ç»ƒè®¡ç®—æ•ˆç‡ã€‚(b) ä¸ (a) ç›¸åŒï¼Œä½†ç”¨äºæ¥è‡ªçŒ´å­ T çš„ä¿ç•™ RT ä¼šè¯â€”â€”ä¸€ä¸ªåœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ–°å—è¯•è€…ã€‚(c) å°†æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¸åŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚æ¨ç†æ—¶é—´æ˜¯åœ¨å·¥ä½œç«™çº§ GPUï¼ˆNVIDIA RTX8000ï¼‰ä¸Šè®¡ç®—çš„ã€‚å¯¹äºæ‰€æœ‰è¿™äº›ç»“æœï¼Œæˆ‘ä»¬ä¸º POSSM ä½¿ç”¨äº† GRU éª¨å¹²ã€‚\nInference efficiency In addition to being efficient to train and finetune, we also evaluated whether POSSM is efficient at inference time. In Figure 4c, we report parameter counts and inference time per time chunk on a workstation-class GPU (NVIDIA RTX8000) for POSSM and several state-of-the-art neural decoders.\né™¤äº†è®­ç»ƒå’Œå¾®è°ƒæ•ˆç‡é«˜ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº† POSSM åœ¨æ¨ç†æ—¶æ˜¯å¦é«˜æ•ˆã€‚åœ¨å›¾ 4c ä¸­ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†å·¥ä½œç«™çº§ GPUï¼ˆNVIDIA RTX8000ï¼‰ä¸Š POSSM å’Œå‡ ç§æœ€å…ˆè¿›ç¥ç»è§£ç å™¨çš„å‚æ•°è®¡æ•°å’Œæ¯ä¸ªæ—¶é—´å—çš„æ¨ç†æ—¶é—´ã€‚\nWe find that our single and multi-session models achieve inference speeds that are comparable to lightweight models like GRUs and MLPs and significantly lower than complex models like NDT-2 and POYO, while retaining competitive performance.\nSingle-session POSSM models (POSSMSS) contained the fewest parameters, and even o-POSSM, with about 8M parameters, maintained low latency. These results held in a CPU environment (AMD EPYC 7502 32-Core) as well, with POSSMSS and o-POSSM achieving inference speeds of $\\sim 2.44\\text{ ms/chunk}$ and $\\sim 5.65\\text{ ms/chunk}$, respectively.\nOverall, our results show that POSSMâ€™s inference time is well within the optimal real-time BCI decoding latency of $\\leq 10\\text{ ms}$, making it a viable option for real-time BCI decoding applications.\næˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„å•ä¼šè¯å’Œå¤šä¼šè¯æ¨¡å‹å®ç°çš„æ¨ç†é€Ÿåº¦ä¸è½»é‡çº§æ¨¡å‹ï¼ˆå¦‚ GRU å’Œ MLPï¼‰ç›¸å½“ï¼Œå¹¶ä¸”æ˜¾è‘—ä½äºå¤æ‚æ¨¡å‹ï¼ˆå¦‚ NDT-2 å’Œ POYOï¼‰ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§èƒ½ã€‚\nå•ä¼šè¯ POSSM æ¨¡å‹ï¼ˆPOSSMSSï¼‰åŒ…å«æœ€å°‘çš„å‚æ•°ï¼Œå³ä½¿æ˜¯å…·æœ‰çº¦ 800 ä¸‡å‚æ•°çš„ o-POSSM ä¹Ÿä¿æŒäº†ä½å»¶è¿Ÿã€‚è¿™äº›ç»“æœåœ¨ CPU ç¯å¢ƒï¼ˆAMD EPYC 7502 32-Coreï¼‰ä¸­ä¹Ÿæˆç«‹ï¼ŒPOSSMSS å’Œ o-POSSM åˆ†åˆ«å®ç°äº† $\\sim 2.44\\text{ ms/chunk}$ å’Œ $\\sim 5.65\\text{ ms/chunk}$ çš„æ¨ç†é€Ÿåº¦ã€‚\næ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ POSSM çš„æ¨ç†æ—¶é—´è¿œä½äºæœ€ä½³å®æ—¶ BCI è§£ç å»¶è¿Ÿ $\\leq 10\\text{ ms}$ï¼Œä½¿å…¶æˆä¸ºå®æ—¶ BCI è§£ç åº”ç”¨çš„å¯è¡Œé€‰æ‹©ã€‚\nHuman handwriting Next, we evaluated POSSM on a human handwriting dataset. This dataset contains 11 sessions recorded from a single individual, where they imagined writing individual characters and drawing straight lines (Figure 3g). Spike counts from multi-unit threshold crossings were recorded from two 96-channel microelectrode arrays implanted in the participantâ€™s motor cortex. These were then binned at 10 ms intervals. The models were trained to classify the intended characters or lines based on this neural activity. For evaluation, we used 9 sessions, each containing 10 trials per character class. Each individual trial consisted of a 1.6 s time-window centred around the â€œgoâ€ cue.\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªäººç±»æ‰‹å†™æ•°æ®é›†ä¸Šè¯„ä¼°äº† POSSMã€‚è¯¥æ•°æ®é›†åŒ…å«ä»å•ä¸ªä¸ªä½“è®°å½•çš„ 11 ä¸ªä¼šè¯ï¼Œå…¶ä¸­ä»–ä»¬æƒ³è±¡å†™å•ä¸ªå­—ç¬¦å’Œç»˜åˆ¶ç›´çº¿ï¼ˆå›¾ 3gï¼‰ã€‚æ¥è‡ªå‚ä¸è€…è¿åŠ¨çš®å±‚ä¸­æ¤å…¥çš„ä¸¤ä¸ª 96 é€šé“å¾®ç”µæé˜µåˆ—çš„å¤šå•å…ƒé˜ˆå€¼äº¤å‰çš„è„‰å†²è®¡æ•°è¢«è®°å½•ä¸‹æ¥ã€‚ç„¶åï¼Œè¿™äº›æ•°æ®ä»¥ 10 æ¯«ç§’é—´éš”è¿›è¡Œåˆ†ç®±ã€‚æ¨¡å‹è¢«è®­ç»ƒç”¨æ¥æ ¹æ®è¿™äº›ç¥ç»æ´»åŠ¨å¯¹é¢„æœŸçš„å­—ç¬¦æˆ–çº¿æ¡è¿›è¡Œåˆ†ç±»ã€‚ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† 9 ä¸ªä¼šè¯ï¼Œæ¯ä¸ªä¼šè¯åŒ…å«æ¯ä¸ªå­—ç¬¦ç±»åˆ«çš„ 10 æ¬¡è¯•éªŒã€‚æ¯ä¸ªå•ç‹¬çš„è¯•éªŒç”±å›´ç»•â€œå¼€å§‹â€æç¤ºçš„ 1.6 ç§’æ—¶é—´çª—å£ç»„æˆã€‚\nResults We compared POSSM with five baselines: the previously published statistical method PCA-KNN, GRU, S4D, Mamba and POYO. For POSSM and all baselines except POYO, we adopted the causal evaluation strategy described in Section 3.1, training on 1 s intervals and evaluating on full 1.6 s trials. For POYO, we followed the evaluation scheme from the original paper, using fixed 1 s context windows for both training and testing.\næˆ‘ä»¬å°† POSSM ä¸äº”ä¸ªåŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒï¼šå…ˆå‰å‘å¸ƒçš„ç»Ÿè®¡æ–¹æ³• PCA-KNNã€GRUã€S4Dã€Mamba å’Œ POYOã€‚å¯¹äº POSSM å’Œé™¤ POYO ä¹‹å¤–çš„æ‰€æœ‰åŸºçº¿ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç¬¬ 3.1 èŠ‚ä¸­æè¿°çš„å› æœè¯„ä¼°ç­–ç•¥ï¼Œåœ¨ 1 ç§’é—´éš”ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å®Œæ•´çš„ 1.6 ç§’è¯•éªŒä¸Šè¿›è¡Œè¯„ä¼°ã€‚å¯¹äº POYOï¼Œæˆ‘ä»¬éµå¾ªäº†åŸå§‹è®ºæ–‡ä¸­çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä½¿ç”¨å›ºå®šçš„ 1 ç§’ä¸Šä¸‹æ–‡çª—å£è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚\nAs shown in Table 2, POSSM-GRU outperforms all baseline models when trained from scratch on the 9 sessions. Remarkably, finetuning o-POSSM, which was only pretrained on NHP data, led to significant performance gains: $2\\%$ for POSSM-GRU and more than $5\\%$ for both POSSM-S4D and POSSM-Mamba. All of the POSSM models achieve state-ofthe-art performance on this task, with the finetuned o-POSSM variants considerably outperforming the baseline PCA-KNN, achieving test accuracies that are about $16\\%$ greater.\nå¦‚è¡¨ 2 æ‰€ç¤ºï¼ŒPOSSM-GRU åœ¨ä»å¤´å¼€å§‹åœ¨ 9 ä¸ªä¼šè¯ä¸Šè®­ç»ƒæ—¶ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¹ä»…åœ¨ NHP æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ o-POSSM è¿›è¡Œå¾®è°ƒå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼šPOSSM-GRU æå‡äº† $2\\%$ï¼Œè€Œ POSSM-S4D å’Œ POSSM-Mamba åˆ™æå‡äº†è¶…è¿‡ $5\\%$ã€‚æ‰€æœ‰ POSSM æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šéƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç»è¿‡å¾®è°ƒçš„ o-POSSM å˜ä½“æ˜¾è‘—ä¼˜äºåŸºçº¿ PCA-KNNï¼Œå®ç°äº†çº¦ $16\\%$ æ›´é«˜çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚\nThese results establish a critical finding: neural dynamics learned from NHP datasets can generalize across different species performing distinct tasks. This is especially impactful given the challenges of collecting large-scale human electrophysiology datasets, suggesting that the abundance of NHP datasets can be used to effectively improve human BCI decoders.\nè¿™äº›ç»“æœç¡®ç«‹äº†ä¸€ä¸ªå…³é”®å‘ç°ï¼šä» NHP æ•°æ®é›†ä¸­å­¦åˆ°çš„ç¥ç»åŠ¨åŠ›å­¦å¯ä»¥è·¨ä¸åŒç‰©ç§æ‰§è¡Œä¸åŒä»»åŠ¡è¿›è¡Œæ³›åŒ–ã€‚é‰´äºæ”¶é›†å¤§è§„æ¨¡äººç±»ç”µç”Ÿç†æ•°æ®é›†çš„æŒ‘æˆ˜ï¼Œè¿™ä¸€ç‚¹å°¤å…¶å…·æœ‰å½±å“åŠ›ï¼Œè¡¨æ˜ä¸°å¯Œçš„ NHP æ•°æ®é›†å¯ä»¥ç”¨æ¥æœ‰æ•ˆåœ°æ”¹å–„äººç±» BCI è§£ç å™¨ã€‚\nHuman speech Finally, we evaluated POSSM on the task of human speech decoding. Unlike the reaching and handwriting tasks, which involved fixed-length context windows, speech decoding involves modelling variable-length phoneme sequences that depend on both the length of the sentence and the individualâ€™s speaking pace. We used a public dataset consisting of 24 sessions in which a human participant with speech deficits attempted to speak sentences that appeared on a screen (Figure 3h).\næœ€åï¼Œæˆ‘ä»¬åœ¨äººç±»è¯­éŸ³è§£ç ä»»åŠ¡ä¸Šè¯„ä¼°äº† POSSMã€‚ä¸æ¶‰åŠå›ºå®šé•¿åº¦ä¸Šä¸‹æ–‡çª—å£çš„åˆ°è¾¾å’Œæ‰‹å†™ä»»åŠ¡ä¸åŒï¼Œè¯­éŸ³è§£ç æ¶‰åŠå»ºæ¨¡å¯å˜é•¿åº¦çš„éŸ³ç´ åºåˆ—ï¼Œè¿™äº›åºåˆ—å–å†³äºå¥å­çš„é•¿åº¦å’Œä¸ªä½“çš„è¯´è¯é€Ÿåº¦ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå…¬å…±æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å« 24 ä¸ªä¼šè¯ï¼Œåœ¨è¿™äº›ä¼šè¯ä¸­ï¼Œä¸€ä½æœ‰è¯­è¨€éšœç¢çš„äººç±»å‚ä¸è€…è¯•å›¾è¯´å‡ºå±å¹•ä¸Šå‡ºç°çš„å¥å­ï¼ˆå›¾ 3hï¼‰ã€‚\nNeural activity was recorded from four 64-channel microelectrode arrays that covered the premotor cortex and Brocaâ€™s area. Multi-unit spiking activity was extracted and binned at a resolution of 20 ms, where the length of each trial ranged from 2 to 18 seconds. This poses a problem for Transformers like POYO that were designed for 1 s contexts, as the quadratic complexity of the attention mechanism would result in a substantial increase in computation for longer sentences.\nç¥ç»æ´»åŠ¨æ˜¯ä»è¦†ç›–å‰è¿åŠ¨çš®å±‚å’Œ Broca åŒºçš„å››ä¸ª 64 é€šé“å¾®ç”µæé˜µåˆ—ä¸­è®°å½•çš„ã€‚å¤šå•å…ƒè„‰å†²æ´»åŠ¨è¢«æå–å¹¶ä»¥ 20 æ¯«ç§’çš„åˆ†è¾¨ç‡è¿›è¡Œåˆ†ç®±ï¼Œæ¯æ¬¡è¯•éªŒçš„é•¿åº¦ä» 2 ç§’åˆ° 18 ç§’ä¸ç­‰ã€‚è¿™å¯¹åƒ POYO è¿™æ ·çš„ä¸º 1 ç§’ä¸Šä¸‹æ–‡è®¾è®¡çš„ Transformer æ¶æ„æ¥è¯´æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦ä¼šå¯¼è‡´å¯¹äºè¾ƒé•¿å¥å­è®¡ç®—é‡çš„å¤§å¹…å¢åŠ ã€‚\nAlthough both uni- and bi-directional GRUs were used in the original study, we focused primarily on the causal, uni-directional case, as it is more relevant for real-time decoding. In line with Willett et al., we z-scored the neural data and added Gaussian noise as an augmentation. We used the phoneme error rate (PER) as our primary evaluation metric. While prior work has successfully incorporated language models to leverage textual priors, we leave this as a future research direction, instead focusing here on POSSMâ€™s capabilities.\nå°½ç®¡åŸå§‹ç ”ç©¶ä¸­ä½¿ç”¨äº†å•å‘å’ŒåŒå‘ GRUï¼Œä½†æˆ‘ä»¬ä¸»è¦å…³æ³¨å› æœçš„å•å‘æƒ…å†µï¼Œå› ä¸ºå®ƒä¸å®æ—¶è§£ç æ›´ç›¸å…³ã€‚ä¸ Willett ç­‰äººä¸€è‡´ï¼Œæˆ‘ä»¬å¯¹ç¥ç»æ•°æ®è¿›è¡Œäº† z-score æ ‡å‡†åŒ–ï¼Œå¹¶æ·»åŠ äº†é«˜æ–¯å™ªå£°ä½œä¸ºå¢å¼ºã€‚æˆ‘ä»¬ä½¿ç”¨ éŸ³ç´ é”™è¯¯ç‡ï¼ˆPERï¼‰ ä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œå·²ç»æˆåŠŸåœ°ç»“åˆäº†è¯­è¨€æ¨¡å‹æ¥åˆ©ç”¨æ–‡æœ¬å…ˆéªŒçŸ¥è¯†ï¼Œä½†æˆ‘ä»¬å°†å…¶ä½œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œåœ¨è¿™é‡Œä¸“æ³¨äº POSSM çš„èƒ½åŠ›ã€‚\nPrevious work has shown that Transformer-based decoders perform poorly on this task compared to GRU baselines. Here, we demonstrate the value of instead integrating attention with a recurrent model by using POSSM, specifically with a GRU backbone.\nå…ˆå‰çš„å·¥ä½œè¡¨æ˜ï¼Œä¸ GRU åŸºçº¿ç›¸æ¯”ï¼ŒåŸºäº Transformer çš„è§£ç å™¨åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒå·®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ POSSMï¼ˆç‰¹åˆ«æ˜¯å¸¦æœ‰ GRU éª¨å¹²ï¼‰æ¥å±•ç¤ºå°†æ³¨æ„åŠ›ä¸å¾ªç¯æ¨¡å‹é›†æˆçš„ä»·å€¼ã€‚\nHowever, since only normalized spike counts (and not spike times) were available in the dataset, we were unable to use the POYO-style tokenization as-is. Instead, we treated each multi-unit channel as a neural unit and encoded the normalized spike counts with value embeddings. Furthermore, we replaced the output cross-attention module with a 1D strided convolution layer to control the length of the output sequence. This approach significantly reduced the number of model parameters compared to the GRU baseline, which used strided sliding windows of neural activity as inputs instead.\nç„¶è€Œï¼Œç”±äºæ•°æ®é›†ä¸­ä»…æä¾›äº†å½’ä¸€åŒ–çš„è„‰å†²è®¡æ•°ï¼ˆè€Œä¸æ˜¯è„‰å†²æ—¶é—´ï¼‰ï¼Œæˆ‘ä»¬æ— æ³•æŒ‰åŸæ ·ä½¿ç”¨ POYO é£æ ¼çš„æ ‡è®°åŒ–ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå¤šå•å…ƒé€šé“è§†ä¸ºä¸€ä¸ªç¥ç»å•å…ƒï¼Œå¹¶ä½¿ç”¨å€¼åµŒå…¥å¯¹å½’ä¸€åŒ–çš„è„‰å†²è®¡æ•°è¿›è¡Œç¼–ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç”¨ 1D æ­¥å¹…å·ç§¯å±‚æ›¿æ¢äº†è¾“å‡ºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥æ§åˆ¶è¾“å‡ºåºåˆ—çš„é•¿åº¦ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ä¸ GRU åŸºçº¿ç›¸æ¯”çš„æ¨¡å‹å‚æ•°æ•°é‡ï¼Œåè€…ä½¿ç”¨ç¥ç»æ´»åŠ¨çš„æ­¥å¹…æ»‘åŠ¨çª—å£ä½œä¸ºè¾“å…¥ã€‚\nWe found that a two-phase training procedure yielded the best results. In the first phase, we trained the input cross-attention module along with the latent and unit embeddings by reconstructing the spike counts at each individual time bin. In the second phase, we trained the entire POSSM model on the target phoneme sequences using Connectionist Temporal Classification (CTC) loss.\næˆ‘ä»¬å‘ç°ï¼Œä¸¤é˜¶æ®µçš„è®­ç»ƒç¨‹åºäº§ç”Ÿäº†æœ€ä½³ç»“æœã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡é‡å»ºæ¯ä¸ªå•ç‹¬æ—¶é—´ç®±çš„è„‰å†²è®¡æ•°æ¥è®­ç»ƒè¾“å…¥äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä»¥åŠæ½œåœ¨å’Œå•å…ƒåµŒå…¥ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰æŸå¤±å¯¹æ•´ä¸ª POSSM æ¨¡å‹è¿›è¡Œç›®æ ‡éŸ³ç´ åºåˆ—çš„è®­ç»ƒã€‚\nResults POSSM achieved a significant improvement over all other baselines, as shown in Table 3. Notably, POSSM maintained comparable performance even without the Gaussian noise augmentation, while the performance of the baseline GRU was greatly impaired under the same conditions. Furthermore, we show in preliminary experiments with multiple input modalities (i.e., both spike counts and spiking-band powers) that POSSM yet again outperforms the baseline. These results illustrate the robustness of the POSSM architecture to variability in data preprocessing and its flexibility with respect to input modalities, further strengthening its case as a feasible real-world decoder.\nPOSSM åœ¨æ‰€æœ‰å…¶ä»–åŸºçº¿ä¹‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¦‚è¡¨ 3 æ‰€ç¤ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æ²¡æœ‰é«˜æ–¯å™ªå£°å¢å¼ºï¼ŒPOSSM ä»ç„¶ä¿æŒäº†ç›¸å½“çš„æ€§èƒ½ï¼Œè€Œåœ¨ç›¸åŒæ¡ä»¶ä¸‹ï¼ŒåŸºçº¿ GRU çš„æ€§èƒ½å¤§å¤§å—æŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨å¤šç§è¾“å…¥æ¨¡æ€ï¼ˆå³è„‰å†²è®¡æ•°å’Œè„‰å†²å¸¦åŠŸç‡ï¼‰çš„åˆæ­¥å®éªŒä¸­è¡¨æ˜ï¼ŒPOSSM å†æ¬¡ä¼˜äºåŸºçº¿ã€‚è¿™äº›ç»“æœè¯´æ˜äº† POSSM æ¶æ„å¯¹æ•°æ®é¢„å¤„ç†å˜å¼‚æ€§çš„é²æ£’æ€§åŠå…¶å¯¹è¾“å…¥æ¨¡æ€çš„çµæ´»æ€§ï¼Œè¿›ä¸€æ­¥åŠ å¼ºäº†å…¶ä½œä¸ºå¯è¡Œçš„ç°å®ä¸–ç•Œè§£ç å™¨çš„æ¡ˆä¾‹ã€‚\nRelated Work Neural decoding Neural decoding for continuous tasks such as motor control in BCIs was traditionally accomplished using statistical models such as the Kalman filter.\nWhile these models are reliable and perform well for specific users and sessions, they require careful adaptation to generalize to new days, users, and even tasks.\nSuch adaptation is typically accomplished using model fitting approaches to estimate the Kalman filter parameters, a process that requires considerable new training data for each application.\nTraditional approaches to multi-session neural decoding often consist of learning the decoderâ€™s parameters on a specific day or session (e.g., the first day), followed by learning models to align the neural activity on subsequent days to facilitate generalization.\nç¥ç»è§£ç ç”¨äº BCI ä¸­çš„è¿ç»­ä»»åŠ¡ï¼ˆå¦‚è¿åŠ¨æ§åˆ¶ï¼‰ä¼ ç»Ÿä¸Šæ˜¯é€šè¿‡ç»Ÿè®¡æ¨¡å‹ï¼ˆå¦‚å¡å°”æ›¼æ»¤æ³¢å™¨ï¼‰æ¥å®Œæˆçš„ã€‚\nè™½ç„¶è¿™äº›æ¨¡å‹å¯é ä¸”åœ¨ç‰¹å®šç”¨æˆ·å’Œä¼šè¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬éœ€è¦ä»”ç»†è°ƒæ•´ä»¥å®ç°å¯¹æ–°æ—¥æœŸã€ç”¨æˆ·ç”šè‡³ä»»åŠ¡çš„æ³›åŒ–ã€‚\nè¿™ç§è°ƒæ•´é€šå¸¸æ˜¯é€šè¿‡æ¨¡å‹æ‹Ÿåˆæ–¹æ³•æ¥ä¼°è®¡å¡å°”æ›¼æ»¤æ³¢å™¨å‚æ•°æ¥å®Œæˆçš„ï¼Œè¿™ä¸€è¿‡ç¨‹éœ€è¦ä¸ºæ¯ä¸ªåº”ç”¨æä¾›å¤§é‡æ–°çš„è®­ç»ƒæ•°æ®ã€‚\nä¼ ç»Ÿçš„å¤šä¼šè¯ç¥ç»è§£ç æ–¹æ³•é€šå¸¸åŒ…æ‹¬åœ¨ç‰¹å®šæ—¥æœŸæˆ–ä¼šè¯ï¼ˆä¾‹å¦‚ç¬¬ä¸€å¤©ï¼‰ä¸Šå­¦ä¹ è§£ç å™¨çš„å‚æ•°ï¼Œç„¶åå­¦ä¹ æ¨¡å‹ä»¥å¯¹é½åç»­æ—¥æœŸçš„ç¥ç»æ´»åŠ¨ä»¥ä¿ƒè¿›æ³›åŒ–ã€‚\nGiven the recent availability of large, public neural recordings datasets, modern neural decoding approaches have attempted to leverage advances in large-scale deep learning to build data-driven BCI decoders.\nFor example, in the context of decoders for neuronal spiking activity, NDT jointly embeds spikes from a neural population into a single token per time bin, spatiotemporal NDT (STNDT) separately tokenizes across units and time and learns a joint representation across these two contexts, NDT-2 tokenizes spatiotemporal patches of neural data akin to a ViT, and POYO eschews bin-based tokenization, opting to tokenize individual spikes and using a PerceiverIO Transformer backbone to query behaviours from within specific context windows.\né‰´äºæœ€è¿‘å¤§å‹å…¬å…±ç¥ç»è®°å½•æ•°æ®é›†çš„å¯ç”¨æ€§ï¼Œç°ä»£ç¥ç»è§£ç æ–¹æ³•è¯•å›¾åˆ©ç”¨å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ çš„è¿›æ­¥æ¥æ„å»ºæ•°æ®é©±åŠ¨çš„ BCI è§£ç å™¨ã€‚\nä¾‹å¦‚ï¼Œåœ¨ç¥ç»å…ƒè„‰å†²æ´»åŠ¨è§£ç å™¨çš„èƒŒæ™¯ä¸‹ï¼ŒNDT å°†ç¥ç»ç¾¤ä½“çš„è„‰å†²åµŒå…¥åˆ°æ¯ä¸ªæ—¶é—´ç®±ä¸­çš„å•ä¸ªæ ‡è®°ä¸­ï¼Œæ—¶ç©º NDTï¼ˆSTNDTï¼‰åœ¨å•å…ƒå’Œæ—¶é—´ä¸Šåˆ†åˆ«è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å­¦ä¹ è¿™ä¸¤ä¸ªä¸Šä¸‹æ–‡ä¹‹é—´çš„è”åˆè¡¨ç¤ºï¼ŒNDT-2 ç±»ä¼¼äº ViT å¯¹ç¥ç»æ•°æ®çš„æ—¶ç©ºè¡¥ä¸è¿›è¡Œæ ‡è®°åŒ–ï¼Œè€Œ POYO æ”¾å¼ƒäº†åŸºäºç®±çš„æ ‡è®°åŒ–ï¼Œé€‰æ‹©å¯¹å•ä¸ªè„‰å†²è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶ä½¿ç”¨ PerceiverIO Transformer éª¨å¹²ä»ç‰¹å®šä¸Šä¸‹æ–‡çª—å£ä¸­æŸ¥è¯¢è¡Œä¸ºã€‚\nWhile several of these works excel at neural decoding, they do not focus on enabling generalizable, online decoding in spike-based BCIs and closed-loop protocols.\nThe BRAND platform enables the deployment of specialized deep learning models in real-time closed-loop brain-computer interface experiments with invasive recordings, demonstrating suitable low-latency neural decoding in other models such as LFADS.\nFinally, we note other ML methods for neural data processing other than direct behaviour decoding. Contrastive learning methods that aim to identify joint latent variables between neural activity and behaviour can be useful for decoding but work is needed for online use. Diffusion-based approaches are promising for jointly forecasting neural activity and behaviour, but again are not readily suited for online use.\nè™½ç„¶è¿™äº›å·¥ä½œä¸­çš„å‡ ä¸ªåœ¨ç¥ç»è§£ç æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¹¶ä¸ä¸“æ³¨äºå®ç°å¯æ³›åŒ–çš„åœ¨çº¿è§£ç ï¼Œä»¥ç”¨äºåŸºäºè„‰å†²çš„ BCI å’Œé—­ç¯åè®®ã€‚\nBRAND å¹³å°ä½¿å¾—åœ¨å…·æœ‰ä¾µå…¥æ€§è®°å½•çš„å®æ—¶é—­ç¯è„‘æœºæ¥å£å®éªŒä¸­éƒ¨ç½²ä¸“é—¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œåœ¨ LFADS ç­‰å…¶ä»–æ¨¡å‹ä¸­å±•ç¤ºäº†é€‚å½“çš„ä½å»¶è¿Ÿç¥ç»è§£ç ã€‚\næœ€åï¼Œæˆ‘ä»¬æ³¨æ„åˆ°é™¤äº†ç›´æ¥è¡Œä¸ºè§£ç ä¹‹å¤–ï¼Œè¿˜æœ‰å…¶ä»–ç”¨äºç¥ç»æ•°æ®å¤„ç†çš„ ML æ–¹æ³•ã€‚æ—¨åœ¨è¯†åˆ«ç¥ç»æ´»åŠ¨å’Œè¡Œä¸ºä¹‹é—´è”åˆæ½œå˜é‡çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¯¹äºè§£ç å¯èƒ½æœ‰ç”¨ï¼Œä½†éœ€è¦è¿›è¡Œåœ¨çº¿ä½¿ç”¨çš„å·¥ä½œã€‚åŸºäºæ‰©æ•£çš„æ–¹æ³•æœ‰æœ›è”åˆé¢„æµ‹ç¥ç»æ´»åŠ¨å’Œè¡Œä¸ºï¼Œä½†åŒæ ·ä¸é€‚åˆåœ¨çº¿ä½¿ç”¨ã€‚\nHybrid attention-recurrence models Several works have attempted to combine self- and cross-attention layers with recurrent architectures, usually with the goal of combining the expressivity of attention over shorter timescales with the long-term context modelling abilities of recurrent models such as structured SSMs.\nä¸€äº›å·¥ä½œè¯•å›¾å°†è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚ä¸å¾ªç¯æ¶æ„ç›¸ç»“åˆï¼Œé€šå¸¸ç›®çš„æ˜¯ç»“åˆæ³¨æ„åŠ›åœ¨è¾ƒçŸ­æ—¶é—´å°ºåº¦ä¸Šçš„è¡¨ç°åŠ›ä¸å¾ªç¯æ¨¡å‹ï¼ˆå¦‚ç»“æ„åŒ– SSMï¼‰åœ¨é•¿æœŸä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢çš„èƒ½åŠ›ã€‚\nWhile traditional SSMs have been used for several neuroscience applications, modern SSMs and hybrid models remain underexplored in the field.\nDidolkar et al. propose an architecture comprising Transformer blocks which process information at faster, shorter timescales while a recurrent backbone integrates information from these blocks over longer timescales for long-term contextual modeling.\nA similar approach is the Block-Recurrent Transformer, wherein a recurrent cell operates on a block of tokens in parallel, thus propagating a block of state vectors through timesteps. Pilault et al. propose the Block-State Transformer architecture, which introduces a layer consisting of an SSM to process long input sequences, the outputs of which are sent in blocks to several Transformers that process them in parallel to produce output tokens. Furthermore, several recent works on high-throughput language modelling have leveraged hybrid models, where self-attention layers are replaced with SSM blocks to take advantage of their subquadratic computational complexity.\nè™½ç„¶ä¼ ç»Ÿçš„ SSM å·²è¢«ç”¨äºå¤šä¸ªç¥ç»ç§‘å­¦åº”ç”¨ï¼Œä½†ç°ä»£ SSM å’Œæ··åˆæ¨¡å‹åœ¨è¯¥é¢†åŸŸä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚\nDidolkar ç­‰äººæå‡ºäº†ä¸€ç§æ¶æ„ï¼Œè¯¥æ¶æ„åŒ…æ‹¬ Transformer å—ï¼Œè¿™äº›å—åœ¨æ›´å¿«ã€æ›´çŸ­çš„æ—¶é—´å°ºåº¦ä¸Šå¤„ç†ä¿¡æ¯ï¼Œè€Œå¾ªç¯éª¨å¹²åˆ™æ•´åˆæ¥è‡ªè¿™äº›å—çš„ä¿¡æ¯ä»¥è¿›è¡Œé•¿æœŸä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚\nç±»ä¼¼çš„æ–¹æ³•æ˜¯å—å¾ªç¯ Transformerï¼Œå…¶ä¸­å¾ªç¯å•å…ƒå¹¶è¡Œåœ°åœ¨ä¸€å—æ ‡è®°ä¸Šè¿è¡Œï¼Œä»è€Œé€šè¿‡æ—¶é—´æ­¥ä¼ æ’­ä¸€å—çŠ¶æ€å‘é‡ã€‚Pilault ç­‰äººæå‡ºäº†å—çŠ¶æ€ Transformer æ¶æ„ï¼Œè¯¥æ¶æ„å¼•å…¥äº†ä¸€å±‚åŒ…å« SSM çš„å±‚æ¥å¤„ç†é•¿è¾“å…¥åºåˆ—ï¼Œå…¶è¾“å‡ºä»¥å—çš„å½¢å¼å‘é€åˆ°å‡ ä¸ª Transformerï¼Œè¿™äº› Transformer å¹¶è¡Œå¤„ç†å®ƒä»¬ä»¥ç”Ÿæˆè¾“å‡ºæ ‡è®°ã€‚æ­¤å¤–ï¼Œæœ€è¿‘å…³äºé«˜ååé‡è¯­è¨€å»ºæ¨¡çš„å‡ é¡¹å·¥ä½œåˆ©ç”¨äº†æ··åˆæ¨¡å‹ï¼Œå…¶ä¸­è‡ªæ³¨æ„åŠ›å±‚è¢« SSM å—æ›¿æ¢ï¼Œä»¥åˆ©ç”¨å…¶äºšäºŒæ¬¡è®¡ç®—å¤æ‚åº¦ã€‚\nDiscussion Summary We introduced POSSM, a scalable and generalizable hybrid architecture that pairs spike tokenization and input-output cross-attention with SSMs. This architecture enables efficient online decoding applications, achieving state-of-the-art performance for several neural decoding tasks while having fewer parameters and faster inference compared to fully Transformer-based approaches. Our model achieves high performance with millisecond-scale inference times on standard workstations, even without GPUs, making it suitable for real-time deployment in clinical settings.\næˆ‘ä»¬ä»‹ç»äº† POSSMï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•ä¸”å¯æ³›åŒ–çš„æ··åˆæ¶æ„ï¼Œå°†è„‰å†²æ ‡è®°åŒ–å’Œè¾“å…¥è¾“å‡ºäº¤å‰æ³¨æ„åŠ›ä¸ SSM é…å¯¹ã€‚è¯¥æ¶æ„å®ç°äº†é«˜æ•ˆçš„åœ¨çº¿è§£ç åº”ç”¨ï¼Œåœ¨å¤šä¸ªç¥ç»è§£ç ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸å®Œå…¨åŸºäº Transformer çš„æ–¹æ³•ç›¸æ¯”ï¼Œå‚æ•°æ›´å°‘ä¸”æ¨ç†æ›´å¿«ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ ‡å‡†å·¥ä½œç«™ä¸Šå®ç°äº†æ¯«ç§’çº§çš„é«˜æ€§èƒ½æ¨ç†æ—¶é—´ï¼Œå³ä½¿æ²¡æœ‰ GPUï¼Œä¹Ÿé€‚åˆåœ¨ä¸´åºŠç¯å¢ƒä¸­è¿›è¡Œå®æ—¶éƒ¨ç½²ã€‚\nA key contribution of this work is demonstrating, to our knowledge, the first successful cross-species transfer of learned neural dynamics for a deep learning-based decoder â€“ from NHP motor cortex to human clinical data (see [11, 62] for related efforts). This outlines a solution to a major clinical hurdle, where obtaining sufficient data for large-scale modelling is challenging or impossible in patient populations. We demonstrate that we can leverage the large corpus of existing non-human experimental data to improve personalized clinical outcomes by finetuning pretrained models.\nè¿™é¡¹å·¥ä½œçš„ä¸€ä¸ªå…³é”®è´¡çŒ®æ˜¯å±•ç¤ºäº†æˆ‘ä»¬æ‰€çŸ¥çš„ç¬¬ä¸€ä¸ªæˆåŠŸçš„è·¨ç‰©ç§è½¬ç§»å­¦ä¹ ç¥ç»åŠ¨åŠ›å­¦çš„æ·±åº¦å­¦ä¹ è§£ç å™¨â€”â€”ä» NHP è¿åŠ¨çš®å±‚åˆ°äººç±»ä¸´åºŠæ•°æ®ï¼ˆæœ‰å…³ç›¸å…³å·¥ä½œï¼Œè¯·å‚è§ [11, 62]ï¼‰ã€‚è¿™ä¸ºä¸€ä¸ªä¸»è¦çš„ä¸´åºŠéšœç¢æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œåœ¨æ‚£è€…ç¾¤ä½“ä¸­è·å¾—è¶³å¤Ÿçš„å¤§è§„æ¨¡å»ºæ¨¡æ•°æ®æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§æˆ–ä¸å¯èƒ½çš„ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç°æœ‰çš„å¤§é‡éäººç±»å®éªŒæ•°æ®ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥æ”¹å–„ä¸ªæ€§åŒ–ä¸´åºŠç»“æœã€‚\nFuture directions and applications The POSSM architecture is applied here for motor neural decoding, but it can be adapted to achieve a variety of outcomes. Our current model focuses on processing spiking data from implanted arrays in the motor cortex of monkeys and human clinical trial participants. However, our hybrid architecture is flexible and could readily accommodate data of other neural data modalities through a variety of proven tokenization schemes (e.g., Calcium imaging, EEG). While in the present work we focus on decoding of behavioural timestamps immediately following our input time chunks, our hybrid architecture is well-suited towards forecasting over longer timescales. Further, in the future we plan to explore the ability of POSSM to learn and generalize across different regions beyond the motor cortex.\nPOSSM æ¶æ„åœ¨è¿™é‡Œåº”ç”¨äºè¿åŠ¨ç¥ç»è§£ç ï¼Œä½†å®ƒå¯ä»¥é€‚åº”ä»¥å®ç°å„ç§ç»“æœã€‚æˆ‘ä»¬å½“å‰çš„æ¨¡å‹ä¸“æ³¨äºå¤„ç†æ¥è‡ªçŒ´å­å’Œäººç±»ä¸´åºŠè¯•éªŒå‚ä¸è€…è¿åŠ¨çš®å±‚ä¸­æ¤å…¥é˜µåˆ—çš„è„‰å†²æ•°æ®ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ··åˆæ¶æ„å…·æœ‰çµæ´»æ€§ï¼Œå¯ä»¥é€šè¿‡å„ç§ç»è¿‡éªŒè¯çš„æ ‡è®°åŒ–æ–¹æ¡ˆè½»æ¾é€‚åº”å…¶ä»–ç¥ç»æ•°æ®æ¨¡æ€çš„æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œé’™æˆåƒã€EEGï¼‰ã€‚è™½ç„¶åœ¨å½“å‰å·¥ä½œä¸­æˆ‘ä»¬ä¸“æ³¨äºè§£ç ç´§éšè¾“å…¥æ—¶é—´å—ä¹‹åçš„è¡Œä¸ºæ—¶é—´æˆ³ï¼Œä½†æˆ‘ä»¬çš„æ··åˆæ¶æ„éå¸¸é€‚åˆäºæ›´é•¿æ—¶é—´å°ºåº¦çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæœªæ¥æˆ‘ä»¬è®¡åˆ’æ¢ç´¢ POSSM å­¦ä¹ å’Œæ³›åŒ–åˆ°è¿åŠ¨çš®å±‚ä»¥å¤–ä¸åŒåŒºåŸŸçš„èƒ½åŠ›ã€‚\nUltimately, we envision POSSM as a first step towards a fast, generalizable neural foundation model for various neural interfacing tasks, with downstream applications such as clinical diagnostics and the development of smart, closed-loop neuromodulation techniques that link predicted states to optimized neural stimulators (e.g., [4, 5]).\nFuture steps include multimodal pretraining and decoding as well as a principled self-supervised pretraining scheme. By enabling efficient inference and flexible generalization through transfer learning, POSSM marks a new direction for general-purpose neural decoders with real-world practicality.\næœ€ç»ˆï¼Œæˆ‘ä»¬å°† POSSM è§†ä¸ºå„ç§ç¥ç»æ¥å£ä»»åŠ¡çš„å¿«é€Ÿã€å¯æ³›åŒ–ç¥ç»åŸºç¡€æ¨¡å‹çš„ç¬¬ä¸€æ­¥ï¼Œä¸‹æ¸¸åº”ç”¨åŒ…æ‹¬ä¸´åºŠè¯Šæ–­å’Œæ™ºèƒ½é—­ç¯ç¥ç»è°ƒèŠ‚æŠ€æœ¯çš„å‘å±•ï¼Œå°†é¢„æµ‹çŠ¶æ€ä¸ä¼˜åŒ–çš„ç¥ç»åˆºæ¿€å™¨ï¼ˆä¾‹å¦‚ï¼Œ[4, 5]ï¼‰è”ç³»èµ·æ¥ã€‚\næœªæ¥çš„æ­¥éª¤åŒ…æ‹¬å¤šæ¨¡æ€é¢„è®­ç»ƒå’Œè§£ç ä»¥åŠæœ‰åŸåˆ™çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ¡ˆã€‚é€šè¿‡é€šè¿‡è¿ç§»å­¦ä¹ å®ç°é«˜æ•ˆæ¨ç†å’Œçµæ´»æ³›åŒ–ï¼ŒPOSSM æ ‡å¿—ç€å…·æœ‰ç°å®ä¸–ç•Œå®ç”¨æ€§çš„é€šç”¨ç¥ç»è§£ç å™¨çš„æ–°æ–¹å‘ã€‚\nBroader Impact This research could potentially contribute to the development of neural decoders that are not only accurate but also amenable to deployment in online systems, such as those found in neuroprosthetics and other brain-computer interfaces. In addition to POSSMâ€™s general performance, it reduces the need for extensive individual calibration due to the pretraining/finetuning scheme. Additionally, the crossspecies transfer results on the handwriting task suggest that patients with limited availability of data could benefit from models pretrained with larger datasets from different species. While the potential downstream applications of POSSM are exciting, it is important to consider the ethical concerns that exist for any medical technology, including but not limited to data privacy and humane data collection from animals. Strict testing should be implemented before deployment in any human-related setting.\nè¿™é¡¹ç ”ç©¶æœ‰å¯èƒ½æœ‰åŠ©äºå¼€å‘ä¸ä»…å‡†ç¡®è€Œä¸”é€‚åˆéƒ¨ç½²åœ¨åœ¨çº¿ç³»ç»Ÿä¸­çš„ç¥ç»è§£ç å™¨ï¼Œä¾‹å¦‚ç¥ç»å‡ä½“å’Œå…¶ä»–è„‘æœºæ¥å£ä¸­ã€‚é™¤äº† POSSM çš„æ•´ä½“æ€§èƒ½å¤–ï¼Œç”±äºé¢„è®­ç»ƒ/å¾®è°ƒæ–¹æ¡ˆï¼Œå®ƒå‡å°‘äº†å¯¹å¹¿æ³›ä¸ªäººæ ¡å‡†çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæ‰‹å†™ä»»åŠ¡ä¸Šçš„è·¨ç‰©ç§è½¬ç§»ç»“æœè¡¨æ˜ï¼Œæ•°æ®å¯ç”¨æ€§æœ‰é™çš„æ‚£è€…å¯ä»¥ä»ä½¿ç”¨æ¥è‡ªä¸åŒç‰©ç§çš„å¤§å‹æ•°æ®é›†é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­å—ç›Šã€‚è™½ç„¶ POSSM çš„æ½œåœ¨ä¸‹æ¸¸åº”ç”¨ä»¤äººå…´å¥‹ï¼Œä½†é‡è¦çš„æ˜¯è¦è€ƒè™‘ä»»ä½•åŒ»ç–—æŠ€æœ¯å­˜åœ¨çš„ä¼¦ç†é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®éšç§å’Œå¯¹åŠ¨ç‰©çš„äººé“æ•°æ®æ”¶é›†ã€‚åœ¨ä»»ä½•ä¸äººç±»ç›¸å…³çš„ç¯å¢ƒä¸­éƒ¨ç½²ä¹‹å‰ï¼Œéƒ½åº”å®æ–½ä¸¥æ ¼çš„æµ‹è¯•ã€‚\n",
  "wordCount" : "15594",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/12/11/v2WCjygtn9MrIOp.png","datePublished": "2025-12-11T00:18:23+08:00",
  "dateModified": "2025-12-11T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/reference/generalizable-real-time-neural-decoding-with-hybrid-state-space-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "æ— å¤„æƒ¹å°˜åŸƒ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿— (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿—</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/">ğŸ“• é˜…è¯»</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/">ğŸ“• æ–‡çŒ®</a></div>
            <h1 class="post-title">
                Generalizable, real-time neural decoding with hybrid state-space models
            </h1>
            <div class="post-description">
                Computation through dynamics
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-12-11
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>15594å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>32åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/12/11/v2WCjygtn9MrIOp.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#methods" aria-label="Methods">Methods</a><ul>
                        
                <li>
                    <a href="#input-processing" aria-label="Input processing">Input processing</a><ul>
                        
                <li>
                    <a href="#streaming-neural-activity-as-input" aria-label="Streaming neural activity as input">Streaming neural activity as input</a></li>
                <li>
                    <a href="#spike-tokenization" aria-label="Spike tokenization.">Spike tokenization.</a></li></ul>
                </li>
                <li>
                    <a href="#architecture" aria-label="Architecture">Architecture</a><ul>
                        
                <li>
                    <a href="#input-cross-attention" aria-label="Input cross-attention">Input cross-attention</a></li>
                <li>
                    <a href="#recurrent-backbone" aria-label="Recurrent backbone">Recurrent backbone</a></li>
                <li>
                    <a href="#output-cross-attention-and-readout" aria-label="Output cross-attention and readout">Output cross-attention and readout</a></li></ul>
                </li>
                <li>
                    <a href="#generalizing-to-unseen-data-with-pretrained-models" aria-label="Generalizing to unseen data with pretrained models">Generalizing to unseen data with pretrained models</a><ul>
                        
                <li>
                    <a href="#unit-identification" aria-label="Unit identification">Unit identification</a></li>
                <li>
                    <a href="#full-finetuning" aria-label="Full finetuning">Full finetuning</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#experiments" aria-label="Experiments">Experiments</a><ul>
                        
                <li>
                    <a href="#non-human-primate-reaching" aria-label="Non-human primate reaching">Non-human primate reaching</a><ul>
                        
                <li>
                    <a href="#experimental-setup" aria-label="Experimental Setup">Experimental Setup</a></li>
                <li>
                    <a href="#causal-evaluation" aria-label="Causal evaluation">Causal evaluation</a></li>
                <li>
                    <a href="#transfer-to-new-sessions" aria-label="Transfer to new sessions">Transfer to new sessions</a></li>
                <li>
                    <a href="#sample-and-training-compute-efficiency" aria-label="Sample and training compute efficiency">Sample and training compute efficiency</a></li>
                <li>
                    <a href="#inference-efficiency" aria-label="Inference efficiency">Inference efficiency</a></li></ul>
                </li>
                <li>
                    <a href="#human-handwriting" aria-label="Human handwriting">Human handwriting</a><ul>
                        
                <li>
                    <a href="#results" aria-label="Results">Results</a></li></ul>
                </li>
                <li>
                    <a href="#human-speech" aria-label="Human speech">Human speech</a><ul>
                        
                <li>
                    <a href="#results-1" aria-label="Results">Results</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#related-work" aria-label="Related Work">Related Work</a><ul>
                        
                <li>
                    <a href="#neural-decoding" aria-label="Neural decoding">Neural decoding</a></li>
                <li>
                    <a href="#hybrid-attention-recurrence-models" aria-label="Hybrid attention-recurrence models">Hybrid attention-recurrence models</a></li></ul>
                </li>
                <li>
                    <a href="#discussion" aria-label="Discussion">Discussion</a><ul>
                        
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a></li>
                <li>
                    <a href="#future-directions-and-applications" aria-label="Future directions and applications">Future directions and applications</a></li></ul>
                </li>
                <li>
                    <a href="#broader-impact" aria-label="Broader Impact">Broader Impact</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><h1 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h1>
<blockquote>
<p><strong>Real-time decoding of neural activity</strong> is central to neuroscience and neurotechnology applications, from closed-loop experiments to brain-computer interfaces, where models are subject to strict latency constraints.</p>
<p>Traditional methods, including simple recurrent neural networks, are fast and lightweight but often struggle to generalize to unseen data. In contrast, recent Transformer-based approaches leverage large-scale pretraining for strong generalization performance, but typically have much larger computational requirements and are not always suitable for lowresource or real-time settings.</p>
</blockquote>
<p><strong>å®æ—¶è§£ç ç¥ç»æ´»åŠ¨</strong> æ˜¯ç¥ç»ç§‘å­¦å’Œç¥ç»æŠ€æœ¯åº”ç”¨çš„æ ¸å¿ƒï¼Œä»é—­ç¯å®éªŒåˆ°è„‘æœºæ¥å£ï¼Œæ¨¡å‹éƒ½å—åˆ°ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶ã€‚</p>
<p>ä¼ ç»Ÿæ–¹æ³•ï¼ŒåŒ…æ‹¬ç®€å•çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼Œé€Ÿåº¦å¿«ä¸”è½»é‡ï¼Œä½†é€šå¸¸éš¾ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„æ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘åŸºäº Transformer çš„æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒå®ç°äº†å¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ï¼Œä½†é€šå¸¸å…·æœ‰æ›´å¤§çš„è®¡ç®—éœ€æ±‚ï¼Œå¹¶ä¸æ€»æ˜¯é€‚åˆä½èµ„æºæˆ–å®æ—¶è®¾ç½®ã€‚</p>
<blockquote>
<p>To address these shortcomings, we present POSSM, a novel hybrid architecture that combines individual spike tokenization via a <strong>cross-attention module</strong> with a recurrent state-space model (SSM) backbone to enable</p>
<ol>
<li>fast and causal online prediction on neural activity and</li>
<li>efficient generalization to new sessions, individuals, and tasks through multi-dataset pretraining.</li>
</ol>
</blockquote>
<p>ä¸ºäº†è§£å†³è¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº† POSSMï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ··åˆæ¶æ„ï¼Œç»“åˆäº†é€šè¿‡ <strong>äº¤å‰æ³¨æ„åŠ›æ¨¡å—</strong> è¿›è¡Œçš„å•ä¸ªè„‰å†²æ ‡è®°åŒ–ä¸å¾ªç¯çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰éª¨å¹²ï¼Œä»¥å®ç°</p>
<ol>
<li>å¯¹ç¥ç»æ´»åŠ¨çš„å¿«é€Ÿå’Œå› æœåœ¨çº¿é¢„æµ‹ï¼Œä»¥åŠ</li>
<li>é€šè¿‡å¤šæ•°æ®é›†é¢„è®­ç»ƒå®ç°å¯¹æ–°ä¼šè¯ã€ä¸ªä½“å’Œä»»åŠ¡çš„é«˜æ•ˆæ³›åŒ–ã€‚</li>
</ol>
<blockquote>
<p>We evaluate POSSMâ€™s decoding performance and inference speed on intracortical decoding of monkey motor tasks, and show that it extends to clinical applications, namely handwriting and speech decoding in human subjects.</p>
<p>Notably, we demonstrate that pretraining on monkey motor-cortical recordings improves decoding performance on the human handwriting task, highlighting the exciting potential for cross-species transfer. In all of these tasks, we find that POSSM achieves decoding accuracy comparable to state-of-the-art Transformers, at a fraction of the inference cost (up to 9Ã— faster on GPU). These results suggest that hybrid SSMs are a promising approach to bridging the gap between accuracy, inference speed, and generalization when training neural decoders for real-time, closed-loop applications.</p>
</blockquote>
<p>æˆ‘ä»¬è¯„ä¼°äº† POSSM åœ¨çŒ´å­è¿åŠ¨ä»»åŠ¡çš„çš®å±‚å†…è§£ç ä¸­çš„è§£ç æ€§èƒ½å’Œæ¨ç†é€Ÿåº¦ï¼Œå¹¶å±•ç¤ºäº†å®ƒåœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æ‰©å±•ï¼Œå³äººç±»å—è¯•è€…çš„æ‰‹å†™å’Œè¯­éŸ³è§£ç ã€‚</p>
<p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨çŒ´å­è¿åŠ¨çš®å±‚è®°å½•ä¸Šçš„é¢„è®­ç»ƒæé«˜äº†äººç±»æ‰‹å†™ä»»åŠ¡çš„è§£ç æ€§èƒ½ï¼Œçªæ˜¾äº†è·¨ç‰©ç§è½¬ç§»çš„ä»¤äººå…´å¥‹çš„æ½œåŠ›ã€‚åœ¨æ‰€æœ‰è¿™äº›ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å‘ç° POSSM ä»¥ GPU ä¸Šé«˜è¾¾ 9 å€çš„é€Ÿåº¦å®ç°äº†ä¸æœ€å…ˆè¿› Transformer ç›¸å½“çš„è§£ç ç²¾åº¦ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ··åˆ SSM æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¯ä»¥å¼¥åˆå®æ—¶é—­ç¯åº”ç”¨ä¸­è®­ç»ƒç¥ç»è§£ç å™¨æ—¶å‡†ç¡®æ€§ã€æ¨ç†é€Ÿåº¦å’Œæ³›åŒ–ä¹‹é—´çš„å·®è·ã€‚</p>
<h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<blockquote>
<p>Neural decoding â€“ the process of mapping neural activity to behavioural or cognitive variables â€“ is a core component of modern neuroscience and neurotechnology.</p>
<p>As neural recording techniques evolve and datasets grow in size, there is increasing interest in building <strong>generalist decoders</strong> that scale and flexibly adapt across subjects and experiments.</p>
<p>Several important downstream applications including closed-loop neuroscience experiments and <strong>brain-computer interfaces (BCIs)</strong> â€“ require fine-grained, low-latency decoding for real-time control. Advances in these technologies would enable next-generation clinical interventions in motor decoding, speech prostheses, and closed-loop neuromodulation.</p>
</blockquote>
<p>ç¥ç»è§£ç â€”â€”å°†ç¥ç»æ´»åŠ¨æ˜ å°„åˆ°è¡Œä¸ºæˆ–è®¤çŸ¥å˜é‡çš„è¿‡ç¨‹â€”â€”æ˜¯ç°ä»£ç¥ç»ç§‘å­¦å’Œç¥ç»æŠ€æœ¯çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚</p>
<p>éšç€ç¥ç»è®°å½•æŠ€æœ¯çš„å‘å±•å’Œæ•°æ®é›†è§„æ¨¡çš„å¢é•¿ï¼Œè¶Šæ¥è¶Šå¤šçš„äººå¯¹æ„å»º<strong>é€šç”¨è§£ç å™¨</strong>æ„Ÿå…´è¶£ï¼Œè¿™äº›è§£ç å™¨å¯ä»¥è·¨å—è¯•è€…å’Œå®éªŒè¿›è¡Œæ‰©å±•å’Œçµæ´»é€‚åº”ã€‚</p>
<p>ä¸€äº›é‡è¦çš„ä¸‹æ¸¸åº”ç”¨ï¼ŒåŒ…æ‹¬é—­ç¯ç¥ç»ç§‘å­¦å®éªŒå’Œ <strong>è„‘æœºæ¥å£ï¼ˆBCIï¼‰</strong>â€”â€”éœ€è¦é«˜ç²¾ç»†åº¦ã€ä½å»¶è¿Ÿçš„è§£ç ä»¥å®æ—¶æ§åˆ¶ã€‚è¿™äº›æŠ€æœ¯çš„è¿›æ­¥, å°†ä½¿ä¸‹ä¸€ä»£ä¸´åºŠå¹²é¢„æªæ–½(è¿åŠ¨è§£ç ã€è¯­éŸ³å‡ä½“å’Œé—­ç¯ç¥ç»è°ƒèŠ‚)æˆä¸ºå¯èƒ½ã€‚</p>
<blockquote>
<p>Building towards these applications will require neural decoders that meet three requirements:</p>
<ol>
<li>robust and accurate predictions,</li>
<li>causal, low-latency inference that is viable in an online setting, and</li>
<li>flexible generalization to new subjects, tasks, and experimental settings.</li>
</ol>
<p>Although recent developments in <strong>machine learning (ML)</strong> have enabled significant strides in each of these axes, building a neural decoder that achieves all three remains an open challenge.</p>
</blockquote>
<p>å®ç°è¿™äº›åº”ç”¨éœ€è¦æ»¡è¶³ä¸‰ä¸ªè¦æ±‚çš„ç¥ç»è§£ç å™¨ï¼š</p>
<ol>
<li>ç¨³å¥ä¸”å‡†ç¡®çš„é¢„æµ‹ï¼Œ</li>
<li>å› æœã€ä½å»¶è¿Ÿçš„æ¨ç†ï¼Œåœ¨åœ¨çº¿ç¯å¢ƒä¸­å¯è¡Œï¼Œä»¥åŠ</li>
<li>å¯¹æ–°å—è¯•è€…ã€ä»»åŠ¡å’Œå®éªŒè®¾ç½®çš„çµæ´»æ³›åŒ–ã€‚</li>
</ol>
<p>å°½ç®¡ <strong>æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰</strong> çš„æœ€æ–°å‘å±•åœ¨è¿™äº›æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†æ„å»ºä¸€ä¸ªåŒæ—¶å®ç°è¿™ä¸‰ç‚¹çš„ç¥ç»è§£ç å™¨ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„æŒ‘æˆ˜ã€‚</p>
<blockquote>
<p><strong>Recurrent neural networks (RNNs)</strong> and <strong>attention-based models</strong> such as Transformers have shown significant promise for neural decoding tasks.</p>
<p>RNNs (Figure 1a) offer fast, low-latency inference on sequential data and strong performance when trained on specific tasks.</p>
<p>However, their ability to generalize to new subjects is limited due to their rigid input format. Specifically, their reliance on fixed-size, time-binned inputs means that they typically cannot learn from new sessions with different neuron identities or sampling rates without full re-training and/or modifying the architecture.</p>
</blockquote>
<p><strong>å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰</strong> å’Œ <strong>åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹</strong>ï¼ˆå¦‚ Transformerï¼‰åœ¨ç¥ç»è§£ç ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‰æ™¯ã€‚</p>
<p>RNNï¼ˆå›¾ 1aï¼‰åœ¨åºåˆ—æ•°æ®ä¸Šæä¾›å¿«é€Ÿã€ä½å»¶è¿Ÿçš„æ¨ç†ï¼Œå¹¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè®­ç»ƒæ—¶è¡¨ç°å‡ºè‰²ã€‚</p>
<p>ç„¶è€Œï¼Œç”±äºå…¶ç¡¬æ€§çš„è¾“å…¥æ ¼å¼ï¼Œå®ƒä»¬å¯¹æ–°å—è¯•è€…çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä»¬ä¾èµ–äºå›ºå®šå¤§å°çš„æ—¶é—´åˆ†ç®±è¾“å…¥ï¼Œè¿™æ„å‘³ç€å¦‚æœä¸å®Œå…¨é‡æ–°è®­ç»ƒå’Œ/æˆ–ä¿®æ”¹æ¶æ„, å®ƒä»¬é€šå¸¸æ— æ³•ä»å…·æœ‰ä¸åŒç¥ç»å…ƒèº«ä»½æˆ–é‡‡æ ·ç‡çš„æ–°æƒ…æ™¯ä¸­å­¦ä¹ ã€‚</p>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/11/JLHqQ5o8GPeIiUn.png" alt=""  /></p>
</blockquote>
<blockquote>
<p>In contrast, Transformerbased architectures (Figure 1b) offer greater flexibility thanks to more adaptable neural tokenization approaches.</p>
<p>Nonetheless, they struggle with applications involving real-time processing due to their quadratic computational complexity, in addition to the overall computational load of the attention mechanism.</p>
<p>Recent efforts in sequence modelling with <strong>large language models</strong> have explored hybrid architectures that combine recurrent layers, such as <strong>gated recurrent units (GRUs)</strong> or  <strong>state-space models (SSMs)</strong>, with attention layers.</p>
<p>These models show encouraging gains in long-context understanding and computational efficiency. While hybrid attention-SSM approaches offer a promising solution for real-time neural decoding, to the best of our knowledge, they remain unexplored in this area.</p>
</blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/11/haYsZ6MeWVt3BJF.png" alt=""  /></p>
</blockquote>
<p>ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäº Transformer çš„æ¶æ„ï¼ˆå›¾ 1bï¼‰ç”±äºæ›´çµæ´»çš„ç¥ç»æ ‡è®°åŒ–æ–¹æ³•è€Œæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚</p>
<p>å°½ç®¡å¦‚æ­¤ï¼Œç”±äºå…¶äºŒæ¬¡è®¡ç®—å¤æ‚æ€§ä»¥åŠæ³¨æ„åŠ›æœºåˆ¶çš„æ•´ä½“è®¡ç®—è´Ÿè½½ï¼Œå®ƒä»¬åœ¨æ¶‰åŠå®æ—¶å¤„ç†çš„åº”ç”¨ä¸­ä»ç„¶å­˜åœ¨å›°éš¾ã€‚</p>
<p>æœ€è¿‘åœ¨ <strong>å¤§è¯­è¨€æ¨¡å‹</strong> çš„åºåˆ—å»ºæ¨¡æ–¹é¢çš„åŠªåŠ›æ¢ç´¢äº†å°† <strong>é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰</strong> æˆ– <strong>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰</strong> ç­‰å¾ªç¯å±‚ä¸æ³¨æ„åŠ›å±‚ç›¸ç»“åˆçš„æ··åˆæ¶æ„ã€‚</p>
<p>è¿™äº›æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œè®¡ç®—æ•ˆç‡æ–¹é¢æ˜¾ç¤ºå‡ºä»¤äººé¼“èˆçš„æå‡ã€‚è™½ç„¶æ··åˆæ³¨æ„åŠ›-SSM æ–¹æ³•ä¸ºå®æ—¶ç¥ç»è§£ç æä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒä»¬åœ¨è¯¥é¢†åŸŸä»æœªè¢«æ¢ç´¢ã€‚</p>
<blockquote>
<p>We address this gap with POSSM, a hybrid model that combines the flexible input-processing of Transformers with the efficient, online inference capabilities of a recurrent SSM backbone.</p>
<p>Unlike traditional methods that rely on <strong>rigid time-binning</strong>, POSSM operates at a millisecond-level resolution by tokenizing individual spikes. In essence, POSSM builds on a POYO-style cross-attention encoder that projects a variable number of spike tokens to a fixed-size latent space. The resulting output is then fed to an SSM that updates its hidden state across consecutive chunks in time.</p>
</blockquote>
<p>æˆ‘ä»¬é€šè¿‡ POSSM å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¨¡å‹ï¼Œç»“åˆäº† Transformer çš„çµæ´»è¾“å…¥å¤„ç†å’Œé€’å½’ SSM éª¨å¹²çš„é«˜æ•ˆåœ¨çº¿æ¨ç†èƒ½åŠ›ã€‚</p>
<p>ä¸ä¾èµ– <strong>åˆšæ€§æ—¶é—´åˆ†ç®±</strong> çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒPOSSM é€šè¿‡å¯¹å•ä¸ªè„‰å†²è¿›è¡Œæ ‡è®°åŒ–ï¼Œåœ¨æ¯«ç§’çº§åˆ†è¾¨ç‡ä¸‹è¿è¡Œã€‚æœ¬è´¨ä¸Šï¼ŒPOSSM å»ºç«‹åœ¨ POYO é£æ ¼çš„äº¤å‰æ³¨æ„åŠ›ç¼–ç å™¨ä¹‹ä¸Šï¼Œè¯¥ç¼–ç å™¨å°†å¯å˜æ•°é‡çš„è„‰å†²æ ‡è®°æŠ•å½±åˆ°å›ºå®šå¤§å°çš„æ½œåœ¨ç©ºé—´ã€‚ç„¶åå°†ç”Ÿæˆçš„è¾“å‡ºé¦ˆé€åˆ° SSMï¼Œè¯¥ SSM åœ¨è¿ç»­æ—¶é—´å—ä¸­æ›´æ–°å…¶éšè—çŠ¶æ€ã€‚</p>
<blockquote>
<p>This architecture, as illustrated in Figure 2, offers two key benefits:</p>
<ol>
<li>the recurrent backbone allows for lightweight, constant-time predictions over consecutive chunks of time and</li>
<li>by adopting POYOâ€™s spike tokenization, encoding, and decoding schemes, POSSM can effectively generalize to different sessions, tasks, and subjects.</li>
</ol>
</blockquote>
<p>å¦‚å›¾ 2 æ‰€ç¤ºï¼Œè¿™ç§æ¶æ„æä¾›äº†ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼š</p>
<ol>
<li>å¾ªç¯éª¨å¹²å…è®¸å¯¹è¿ç»­æ—¶é—´å—è¿›è¡Œè½»é‡çº§ã€æ’å®šæ—¶é—´çš„é¢„æµ‹ï¼›</li>
<li>é€šè¿‡é‡‡ç”¨ POYO çš„è„‰å†²æ ‡è®°åŒ–ã€ç¼–ç å’Œè§£ç æ–¹æ¡ˆï¼ŒPOSSM å¯ä»¥æœ‰æ•ˆåœ°æ³›åŒ–åˆ°ä¸åŒçš„ä¼šè¯ã€ä»»åŠ¡å’Œå—è¯•è€…ã€‚</li>
</ol>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/11/LZOfFlvSi7XTn3w.png" alt=""  /></p>
<p>Figure 2: An architecture for generalizable, real-time neural decoding. POSSM combines individual spike tokenization and input-output cross-attention with a recurrent SSM backbone. In this paper, we typically consider $k = 3$ and $T_{c} = 50\text{ ms}$.</p>
</blockquote>
<p>å›¾ 2ï¼šä¸€ç§é€šç”¨çš„å®æ—¶ç¥ç»è§£ç æ¶æ„ã€‚POSSM ç»“åˆäº†å•ä¸ªè„‰å†²æ ‡è®°åŒ–å’Œè¾“å…¥è¾“å‡ºäº¤å‰æ³¨æ„åŠ›ä¸å¾ªç¯ SSM éª¨å¹²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸è€ƒè™‘ $k = 3$ å’Œ $T_{c} = 50\text{ ms}$ã€‚</p>
</blockquote>
<blockquote>
<p>In this paper, we introduce the POSSM architecture and evaluate its performance on intracortical recordings of spiking activity from experiments in both <strong>non-human primates (NHPs)</strong> and humans.</p>
<p>Although our current evaluations are conducted offline, POSSM is designed for <strong>real-time inference</strong> and can be readily implemented for online experiments.</p>
<p>Finally, while this paper is concerned with invasive electrophysiology recordings, this method could be extended further to other modalities using POYO-style tokenization (see Section 5 for a discussion). Our contributions are summarized as follows:</p>
</blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† POSSM æ¶æ„ï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨ <strong>éäººç±»çµé•¿ç±»åŠ¨ç‰©ï¼ˆNHPï¼‰</strong> å’Œäººç±»å®éªŒä¸­è„‰å†²æ´»åŠ¨çš®å±‚å†…è®°å½•çš„æ€§èƒ½ã€‚</p>
<p>å°½ç®¡æˆ‘ä»¬å½“å‰çš„è¯„ä¼°æ˜¯åœ¨ç¦»çº¿è¿›è¡Œçš„ï¼Œä½† POSSM æ—¨åœ¨å®ç° <strong>å®æ—¶æ¨ç†</strong>ï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå®¹æ˜“åœ°å®ç°åœ¨çº¿å®éªŒã€‚</p>
<p>æœ€åï¼Œè™½ç„¶æœ¬æ–‡å…³æ³¨çš„æ˜¯ä¾µå…¥æ€§ç”µç”Ÿç†è®°å½•ï¼Œä½†è¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°ä½¿ç”¨ POYO é£æ ¼æ ‡è®°åŒ–çš„å…¶ä»–æ¨¡æ€ï¼ˆæœ‰å…³è®¨è®ºï¼Œè¯·å‚è§ç¬¬ 5 èŠ‚ï¼‰ã€‚æˆ‘ä»¬çš„è´¡çŒ®æ€»ç»“å¦‚ä¸‹ï¼š</p>
<blockquote>
<ul>
<li><strong>Performance and efficiency</strong>: We evaluate POSSM against other popular models using NHP datasets that contain multiple sessions, subjects, and different reaching tasks (centre-out, random target, maze, etc.). We find that POSSM matches or outperforms other models on all these tests, doing so with greater speed and significantly reduced computational cost.</li>
</ul>
</blockquote>
<ul>
<li><strong>è¡¨å¾å’Œæ•ˆç‡</strong>ï¼šæˆ‘ä»¬ä½¿ç”¨åŒ…å«å¤šä¸ªä¼šè¯ã€å—è¯•è€…å’Œä¸åŒåˆ°è¾¾ä»»åŠ¡ï¼ˆä¸­å¿ƒå¤–ã€éšæœºç›®æ ‡ã€è¿·å®«ç­‰ï¼‰çš„ NHP æ•°æ®é›†è¯„ä¼° POSSM ä¸å…¶ä»–æµè¡Œæ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç° POSSM åœ¨æ‰€æœ‰è¿™äº›æµ‹è¯•ä¸­éƒ½èƒ½åŒ¹é…æˆ–ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå¹¶ä¸”é€Ÿåº¦æ›´å¿«ï¼Œè®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½ã€‚</li>
</ul>
<blockquote>
<ul>
<li><strong>Multi-dataset pretraining improves performance</strong>: Through large-scale pretraining, we find that POSSM delivers improved performance on NHP datasets across sessions, subjects, and even tasks.</li>
</ul>
</blockquote>
<ul>
<li><strong>å¤šæ•°æ®é›†é¢„è®­ç»ƒæé«˜äº†æ€§èƒ½</strong>ï¼šé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œæˆ‘ä»¬å‘ç° POSSM åœ¨ NHP æ•°æ®é›†çš„å„ä¸ªä¼šè¯ã€å—è¯•è€…ç”šè‡³ä»»åŠ¡ä¸Šéƒ½æä¾›äº†æ”¹è¿›çš„æ€§èƒ½ã€‚</li>
</ul>
<blockquote>
<ul>
<li><strong>Cross-species transfer learning</strong>: Pretraining on diverse NHP datasets and then finetuning POSSM leads to state-of-the-art performance when decoding imagined handwritten letters from human cortical activity. This cross-species transfer not only outlines the remarkable transferability of neural dynamics across different primates, but also shows the potential for leveraging abundant non-human data to augment limited human datasets for improved decoding performance.</li>
</ul>
</blockquote>
<ul>
<li><strong>è·¨ç‰©ç§è¿ç§»å­¦ä¹ </strong>ï¼šåœ¨å¤šæ ·åŒ–çš„ NHP æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå¾®è°ƒ POSSMï¼Œåœ¨è§£ç äººç±»çš®å±‚æ´»åŠ¨ä¸­æƒ³è±¡çš„æ‰‹å†™å­—æ¯æ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™ç§è·¨ç‰©ç§è½¬ç§»ä¸ä»…æ¦‚è¿°äº†ä¸åŒçµé•¿ç±»åŠ¨ç‰©ä¹‹é—´ç¥ç»åŠ¨åŠ›å­¦çš„æ˜¾è‘—å¯è½¬ç§»æ€§ï¼Œè¿˜å±•ç¤ºäº†åˆ©ç”¨ä¸°å¯Œçš„éäººç±»æ•°æ®æ¥å¢å¼ºæœ‰é™çš„äººç±»æ•°æ®é›†ä»¥æé«˜è§£ç æ€§èƒ½çš„æ½œåŠ›ã€‚</li>
</ul>
<blockquote>
<ul>
<li><strong>Long sequence complex decoding</strong>: When trained on human motor-cortical data during attempted speech, POSSM achieves strong decoding performance. In contrast, attentionbased models struggle with the taskâ€™s long-context demands, making them computationally prohibitive.</li>
</ul>
</blockquote>
<ul>
<li><strong>é•¿åºåˆ—å¤æ‚è§£ç </strong>ï¼šåœ¨å°è¯•è¯­éŸ³æ—¶å¯¹äººç±»è¿åŠ¨çš®å±‚æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ï¼ŒPOSSM å®ç°äº†å¼ºå¤§çš„è§£ç æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹åœ¨ä»»åŠ¡çš„é•¿ä¸Šä¸‹æ–‡éœ€æ±‚æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½¿å¾—å®ƒä»¬åœ¨è®¡ç®—ä¸Šå˜å¾—ä¸å¯è¡Œã€‚</li>
</ul>
<h1 id="methods">Methods<a hidden class="anchor" aria-hidden="true" href="#methods">#</a></h1>
<blockquote>
<p>We focus on decoding neuronal spiking activity, which consists of discrete events triggered when excitatory input to a neuron exceeds a certain threshold.</p>
<p>The timing and frequency of spikes encode information conveyed to other neurons throughout the brain, underlying a communication system that is central to all neural function.</p>
</blockquote>
<p>æˆ‘ä»¬ä¸“æ³¨äºè§£ç ç¥ç»å…ƒè„‰å†²æ´»åŠ¨ï¼Œè¿™äº›æ´»åŠ¨ç”±å½“ç¥ç»å…ƒçš„å…´å¥‹æ€§è¾“å…¥è¶…è¿‡æŸä¸ªé˜ˆå€¼æ—¶è§¦å‘çš„ç¦»æ•£äº‹ä»¶ç»„æˆã€‚</p>
<p>è„‰å†²çš„æ—¶æœºå’Œé¢‘ç‡ç¼–ç ä¼ é€’ç»™å¤§è„‘å…¶ä»–ç¥ç»å…ƒçš„ä¿¡æ¯ï¼Œæ„æˆäº†ç¥ç»åŠŸèƒ½çš„æ ¸å¿ƒé€šä¿¡ç³»ç»Ÿã€‚</p>
<blockquote>
<p>However, their <strong>sparse nature</strong> requires an effective input representation that can handle their temporal irregularity.</p>
<p>Furthermore, there exists no direct mapping between the neurons of one living organism to another, highlighting the non-triviality of the <strong>alignment problem</strong> when training across multiple individuals.</p>
<p>In this section, we describe how POSSM is designed to sequentially process sub-second windows of these spikes for the online prediction of behaviour, while maintaining a flexible input framework that allows it to efficiently generalize to an entirely new set of neurons during finetuning.</p>
</blockquote>
<p>ç„¶è€Œï¼Œå®ƒä»¬çš„ <strong>ç¨€ç–æ€§è´¨</strong> éœ€è¦ä¸€ç§æœ‰æ•ˆçš„è¾“å…¥è¡¨ç¤ºæ¥å¤„ç†å®ƒä»¬çš„æ—¶é—´ä¸è§„åˆ™æ€§ã€‚</p>
<p>æ­¤å¤–ï¼Œä¸åŒç”Ÿç‰©ä½“çš„ç¥ç»å…ƒä¹‹é—´ä¸å­˜åœ¨ç›´æ¥æ˜ å°„ï¼Œè¿™çªæ˜¾äº†åœ¨å¤šä¸ªä¸ªä½“ä¹‹é—´è®­ç»ƒæ—¶ <strong>å¯¹é½é—®é¢˜</strong> çš„éå¹³å‡¡æ€§ã€‚</p>
<p>åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æè¿°äº† POSSM å¦‚ä½•è®¾è®¡ç”¨äºåºåˆ—å¤„ç†è¿™äº›è„‰å†²çš„äºšç§’çª—å£ï¼Œä»¥å®ç°è¡Œä¸ºçš„åœ¨çº¿é¢„æµ‹ï¼ŒåŒæ—¶ä¿æŒçµæ´»çš„è¾“å…¥æ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¾®è°ƒæœŸé—´æœ‰æ•ˆåœ°æ¨å¹¿åˆ°ä¸€ç»„å…¨æ–°çš„ç¥ç»å…ƒã€‚</p>
<h2 id="input-processing">Input processing<a hidden class="anchor" aria-hidden="true" href="#input-processing">#</a></h2>
<h3 id="streaming-neural-activity-as-input">Streaming neural activity as input<a hidden class="anchor" aria-hidden="true" href="#streaming-neural-activity-as-input">#</a></h3>
<blockquote>
<p>Our focus is on real-time performance, which requires minimal latency between the moment that neural activity is observed and when a corresponding prediction is generated. This constraint significantly limits the time duration, and consequently the amount of data, that a model can use for each new prediction.</p>
</blockquote>
<p>æˆ‘ä»¬ä¸“æ³¨äºå®æ—¶æ€§èƒ½ï¼Œè¿™éœ€è¦åœ¨è§‚å¯Ÿåˆ°ç¥ç»æ´»åŠ¨çš„æ—¶åˆ»å’Œç”Ÿæˆç›¸åº”é¢„æµ‹ä¹‹é—´çš„å»¶è¿Ÿæœ€å°åŒ–ã€‚è¿™ä¸ªé™åˆ¶æ˜¾è‘—é™åˆ¶äº†æ¨¡å‹å¯ä»¥ç”¨äºæ¯ä¸ªæ–°é¢„æµ‹çš„æ—¶é—´æŒç»­æ—¶é—´ï¼Œå› æ­¤ä¹Ÿé™åˆ¶äº†æ•°æ®é‡ã€‚</p>
<blockquote>
<p>To this end, POSSM maintains a hidden state as data is streamed in, allowing it to incorporate past information without reprocessing previous inputs. In each chunk, the number of spikes varies, meaning each input is represented as a variable-length sequence of spikes.</p>
<p>While POSSM generally uses contiguous 50 ms time chunks here, we also demonstrate strong performance with 20 ms chunks (see Section D.1). In theory, these windows could even be shorter or longer (and even overlapping) depending on the task, with the understanding that there would be some trade-off between temporal resolution and computational complexity.</p>
</blockquote>
<p>ä¸ºæ­¤ï¼ŒPOSSM åœ¨æ•°æ®æµå…¥æ—¶ä¿æŒéšè—çŠ¶æ€ï¼Œä½¿å…¶èƒ½å¤Ÿç»“åˆè¿‡å»çš„ä¿¡æ¯è€Œæ— éœ€é‡æ–°å¤„ç†å…ˆå‰çš„è¾“å…¥ã€‚åœ¨æ¯ä¸ªå—ä¸­ï¼Œè„‰å†²æ•°é‡å„ä¸ç›¸åŒï¼Œè¿™æ„å‘³ç€æ¯ä¸ªè¾“å…¥è¡¨ç¤ºä¸ºå¯å˜é•¿åº¦çš„è„‰å†²åºåˆ—ã€‚</p>
<p>è™½ç„¶ POSSM é€šå¸¸åœ¨è¿™é‡Œä½¿ç”¨è¿ç»­çš„ 50 æ¯«ç§’æ—¶é—´å—ï¼Œä½†æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä½¿ç”¨ 20 æ¯«ç§’å—æ—¶çš„å¼ºå¤§æ€§èƒ½ï¼ˆè§ç¬¬ D.1 èŠ‚ï¼‰ã€‚ç†è®ºä¸Šï¼Œè¿™äº›çª—å£ç”šè‡³å¯ä»¥æ›´çŸ­æˆ–æ›´é•¿ï¼ˆç”šè‡³é‡å ï¼‰ï¼Œå…·ä½“å–å†³äºä»»åŠ¡ï¼ŒåŒæ—¶ç†è§£åœ¨æ—¶é—´åˆ†è¾¨ç‡å’Œè®¡ç®—å¤æ‚æ€§ä¹‹é—´ä¼šæœ‰ä¸€äº›æƒè¡¡ã€‚</p>
<h3 id="spike-tokenization">Spike tokenization.<a hidden class="anchor" aria-hidden="true" href="#spike-tokenization">#</a></h3>
<blockquote>
<p>We adopt the tokenization scheme from the original POYO model, where each neuronal spike is represented using two pieces of information:</p>
<ol>
<li>the identity of the neural unit which it came from and</li>
<li>the timestamp at which it occurred (see Figure 2).</li>
</ol>
<p>The former corresponds to a unique learnable unit embedding for each neuron, while the latter is encoded with a <strong>rotary position embedding (RoPE)</strong> that allows the tokens to be processed based on their relative timing rather than their absolute timestamps.</p>
</blockquote>
<p>æˆ‘ä»¬é‡‡ç”¨äº†åŸå§‹ POYO æ¨¡å‹çš„æ ‡è®°åŒ–æ–¹æ¡ˆï¼Œå…¶ä¸­æ¯ä¸ªç¥ç»å…ƒè„‰å†²ä½¿ç”¨ä¸¤æ¡ä¿¡æ¯è¡¨ç¤ºï¼š</p>
<ol>
<li>å®ƒæ¥è‡ªçš„ç¥ç»å•å…ƒçš„èº«ä»½</li>
<li>å®ƒå‘ç”Ÿçš„æ—¶é—´æˆ³ï¼ˆè§å›¾ 2ï¼‰ã€‚</li>
</ol>
<p>å‰è€…å¯¹åº”äºæ¯ä¸ªç¥ç»å…ƒçš„å”¯ä¸€å¯å­¦ä¹ å•å…ƒåµŒå…¥ï¼Œè€Œåè€…åˆ™ä½¿ç”¨ <strong>æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰</strong> è¿›è¡Œç¼–ç ï¼Œå…è®¸åŸºäºç›¸å¯¹æ—¶é—´è€Œéç»å¯¹æ—¶é—´æˆ³å¤„ç†æ ‡è®°ã€‚</p>
<blockquote>
<p>For example, a spike from some neuron with integer ID $i$ that occurs at time $t_{\text{spike}}$ would be represented as a $D$-dimensional token $x$, given by:</p>
<p>$$
x = (\text{UnitEmb}(i), t_{\text{spike}}),
$$</p>
<p>where $\text{UnitEmb}$ : $\mathbb{Z}\to \mathbb{R}^D$ is the unit embedding map and $D$ is a hyperparameter. As tokenization is an element-wise operation, a time chunk with $N$ spikes will yield $N$ spike tokens.</p>
</blockquote>
<p>ä¾‹å¦‚ï¼ŒæŸä¸ªç¥ç»å…ƒçš„è„‰å†²ï¼Œå…¶æ•´æ•° ID ä¸º $i$ï¼Œå‘ç”Ÿåœ¨æ—¶é—´ $t_{\text{spike}}$ï¼Œå°†è¡¨ç¤ºä¸ºä¸€ä¸ª $D$ ç»´æ ‡è®° $x$ï¼Œè¡¨ç¤ºä¸ºï¼š</p>
<p>$$
x = (\text{UnitEmb}(i), t_{\text{spike}}),
$$</p>
<p>å…¶ä¸­ $\text{UnitEmb}$ : $\mathbb{Z}\to \mathbb{R}^D$ æ˜¯å•å…ƒåµŒå…¥æ˜ å°„ï¼Œ$D$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚ç”±äºæ ‡è®°åŒ–æ˜¯ä¸€ä¸ªé€å…ƒç´ æ“ä½œï¼Œå…·æœ‰ $N$ ä¸ªè„‰å†²çš„æ—¶é—´å—å°†äº§ç”Ÿ $N$ ä¸ªè„‰å†²æ ‡è®°ã€‚</p>
<blockquote>
<p>We opt to use the general term â€œneural unitâ€, as spikes could be assigned at a coarser specificity than an individual neuron (e.g., multi-unit activity on a single electrode channel) depending on the dataset and task at hand.</p>
<p>This flexibility, coupled with the modelâ€™s ability to handle a variable number of units, facilitates both training and efficient finetuning (see Section 2.3) on multiple sessions and even across datasets.</p>
</blockquote>
<p>æˆ‘ä»¬é€‰æ‹©ä½¿ç”¨é€šç”¨æœ¯è¯­â€œç¥ç»å•å…ƒâ€ï¼Œå› ä¸ºæ ¹æ®æ‰‹å¤´çš„æ•°æ®é›†å’Œä»»åŠ¡ï¼Œè„‰å†²å¯ä»¥åˆ†é…å¾—æ¯”å•ä¸ªç¥ç»å…ƒæ›´ç²—ç³™ï¼ˆä¾‹å¦‚ï¼Œå•ä¸ªç”µæé€šé“ä¸Šçš„å¤šå•å…ƒæ´»åŠ¨ï¼‰ã€‚</p>
<p>è¿™ç§çµæ´»æ€§ï¼ŒåŠ ä¸Šæ¨¡å‹å¤„ç†å¯å˜æ•°é‡å•å…ƒçš„èƒ½åŠ›ï¼Œæœ‰åŠ©äºåœ¨å¤šä¸ªä¼šè¯ç”šè‡³è·¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œé«˜æ•ˆå¾®è°ƒï¼ˆè§ç¬¬ 2.3 èŠ‚ï¼‰ã€‚</p>
<h2 id="architecture">Architecture<a hidden class="anchor" aria-hidden="true" href="#architecture">#</a></h2>
<h3 id="input-cross-attention">Input cross-attention<a hidden class="anchor" aria-hidden="true" href="#input-cross-attention">#</a></h3>
<blockquote>
<p>We employ the original POYO encoder, where a cross-attention module inspired by the PerceiverIO architecture compresses a variable-length sequence of input spike tokens into a fixed-size latent representation. Unlike POYO, however, the encoder is applied on short 50 ms time chunks, each of which is mapped to a single <strong>latent vector</strong>.</p>
</blockquote>
<p>æˆ‘ä»¬é‡‡ç”¨äº†åŸå§‹ POYO ç¼–ç å™¨ï¼Œå…¶ä¸­å— PerceiverIO æ¶æ„ å¯å‘çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—å°†å¯å˜é•¿åº¦çš„è¾“å…¥è„‰å†²æ ‡è®°åºåˆ—å‹ç¼©ä¸ºå›ºå®šå¤§å°çš„æ½œåœ¨è¡¨ç¤ºã€‚ç„¶è€Œï¼Œä¸ POYO ä¸åŒï¼Œç¼–ç å™¨åº”ç”¨äºçŸ­çš„ 50 æ¯«ç§’æ—¶é—´å—ï¼Œæ¯ä¸ªæ—¶é—´å—æ˜ å°„åˆ°å•ä¸ª <strong>æ½œåœ¨å‘é‡</strong>ã€‚</p>
<blockquote>
<p>This is achieved by setting the spike tokens as the attention keys and values, and using a learnable query vector $q\in \mathbb{R}^{1\times M}$ , where $M$ is a hyperparameter. Given a sequence $X_{t} = [x_{0}, x_{1}, \dots, x_{N}]^{\top}\in \mathbb{R}^{N\times D}$ of $N$ spike tokens from some chunk of time indexed by $t$, the latent output of the cross-attention module is computed as such:</p>
<p>$$
z^{(t)} = \text{softmax}\left(\frac{qK_{t}^{\top}}{\sqrt{D}}\right)V_{t}
$$</p>
<p>where $K_{t} = X_{t}W_{k}$ and $V_{t} = X_{t}W_{v}$, with $W_{k}, W_{v}\in \mathbb{R}^{D\times M}$ , are the projections of the input token sequence, following standard Transformer notation. Following POYO, our implementation also uses the standard Transformer block with pre-normalization layers and feed-forward networks.</p>
</blockquote>
<p>é€šè¿‡å°†è„‰å†²æ ‡è®°è®¾ç½®ä¸ºæ³¨æ„åŠ›é”®å’Œå€¼ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡ $q\in \mathbb{R}^{1\times M}$ å®ç°è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­ $M$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚ç»™å®šæŸä¸ªæ—¶é—´å—ç´¢å¼•ä¸º $t$ çš„ $N$ ä¸ªè„‰å†²æ ‡è®°åºåˆ— $X_{t} = [x_{0}, x_{1}, \dots, x_{N}]^{\top}\in \mathbb{R}^{N\times D}$ï¼Œäº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„æ½œåœ¨è¾“å‡ºè®¡ç®—å¦‚ä¸‹ï¼š</p>
<p>$$
z^{(t)} = \text{softmax}\left(\frac{qK_{t}^{\top}}{\sqrt{D}}\right)V_{t}
$$</p>
<p>å…¶ä¸­ $K_{t} = X_{t}W_{k}$ å’Œ $V_{t} = X_{t}W_{v}$ï¼Œ$W_{k}, W_{v}\in \mathbb{R}^{D\times M}$ æ˜¯è¾“å…¥æ ‡è®°åºåˆ—çš„æŠ•å½±ï¼Œéµå¾ªæ ‡å‡† Transformer ç¬¦å·ã€‚æŒ‰ç…§ POYO çš„åšæ³•ï¼Œæˆ‘ä»¬çš„å®ç°è¿˜ä½¿ç”¨äº†å¸¦æœ‰é¢„å½’ä¸€åŒ–å±‚å’Œå‰é¦ˆç½‘ç»œçš„æ ‡å‡† Transformer å—ã€‚</p>
<h3 id="recurrent-backbone">Recurrent backbone<a hidden class="anchor" aria-hidden="true" href="#recurrent-backbone">#</a></h3>
<blockquote>
<p>The output of the cross-attention $z^{(t)}$ is then fed to an SSM (or another variety of RNN), which we refer to as the recurrent backbone. The hidden state of the recurrent backbone is updated as follows:</p>
<p>$$
h^{(t)} = f_{\text{SSM}}(z^{(t)}, h^{(tâˆ’1)}).
$$</p>
<p>While the input cross-attention captures local temporal structure (i.e., within the 50 ms chunk), the SSM integrates this information with historical context through its hidden state, allowing POSSM to process information at both local and global timescales. We run experiments with three different backbone architectures: diagonal structured state-space models (S4D), GRU, and Mamba. However, we wish to note that this method is compatible with any other type of recurrent model. Specifics regarding each of these backbone architectures can be found in Section B.2.</p>
</blockquote>
<p>äº¤å‰æ³¨æ„åŠ›çš„è¾“å‡º $z^{(t)}$ ç„¶åè¢«é¦ˆé€åˆ° SSMï¼ˆæˆ–å…¶ä»–ç±»å‹çš„ RNNï¼‰ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºå¾ªç¯éª¨å¹²ã€‚å¾ªç¯éª¨å¹²çš„éšè—çŠ¶æ€æ›´æ–°å¦‚ä¸‹ï¼š</p>
<p>$$
h^{(t)} = f_{\text{SSM}}(z^{(t)}, h^{(tâˆ’1)}).
$$</p>
<p>è™½ç„¶è¾“å…¥äº¤å‰æ³¨æ„åŠ›æ•è·äº†å±€éƒ¨æ—¶é—´ç»“æ„ï¼ˆå³åœ¨ 50 æ¯«ç§’å—å†…ï¼‰ï¼Œä½† SSM é€šè¿‡å…¶éšè—çŠ¶æ€å°†è¿™äº›ä¿¡æ¯ä¸å†å²ä¸Šä¸‹æ–‡é›†æˆï¼Œä½¿ POSSM èƒ½å¤Ÿåœ¨å±€éƒ¨å’Œå…¨å±€æ—¶é—´å°ºåº¦ä¸Šå¤„ç†ä¿¡æ¯ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç§ä¸åŒçš„éª¨å¹²æ¶æ„è¿›è¡Œå®éªŒï¼šå¯¹è§’ç»“æ„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆS4Dï¼‰ã€GRU å’Œ Mambaã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¸Œæœ›æŒ‡å‡ºï¼Œè¿™ç§æ–¹æ³•ä¸ä»»ä½•å…¶ä»–ç±»å‹çš„å¾ªç¯æ¨¡å‹å…¼å®¹ã€‚æœ‰å…³æ¯ç§éª¨å¹²æ¶æ„çš„å…·ä½“ä¿¡æ¯ï¼Œè¯·å‚è§ç¬¬ B.2 èŠ‚ã€‚</p>
<h3 id="output-cross-attention-and-readout">Output cross-attention and readout<a hidden class="anchor" aria-hidden="true" href="#output-cross-attention-and-readout">#</a></h3>
<blockquote>
<p>To decode behaviour, we select the $k$ most recent hidden states $\{h^{(tâˆ’k+1):(t)}\}$ ($k = 3$ in our experiments), and use a cross-attention module to query them for behavioural predictions. For a given time chunk $t$, we generate $P$ queries, one for each timestamp at which we wish to predict behaviour. Each query encodes the associated timestamp (using RoPE), along with a learnable session embedding that captures latent factors of the recording session. The design of our output module enables flexibility in several ways:</p>
<ol>
<li>we can predict multiple outputs per time chunk, enabling a denser and richer supervision signal,</li>
<li>we are not required to precisely align behaviour to chunk boundaries, and</li>
<li>we can predict behaviour beyond the current chunk, allowing us to account for lags between neural activity and behavioural action (see Section D.5).</li>
</ol>
</blockquote>
<p>ä¸ºäº†å¯¹è¡Œä¸ºè¿›è¡Œè§£ç ï¼Œæˆ‘ä»¬é€‰æ‹©æœ€è¿‘çš„ $k$ ä¸ªéšè—çŠ¶æ€ $\{h^{(tâˆ’k+1):(t)}\}$ï¼ˆåœ¨æˆ‘ä»¬çš„å®éªŒä¸­ä¸º $k = 3$ï¼‰ï¼Œå¹¶ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æ¨¡å—å¯¹å®ƒä»¬è¿›è¡ŒæŸ¥è¯¢ä»¥è¿›è¡Œè¡Œä¸ºé¢„æµ‹ã€‚å¯¹äºç»™å®šçš„æ—¶é—´å— $t$ï¼Œæˆ‘ä»¬ç”Ÿæˆ $P$ ä¸ªæŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢å¯¹åº”æˆ‘ä»¬å¸Œæœ›é¢„æµ‹è¡Œä¸ºçš„æ—¶é—´æˆ³ã€‚æ¯ä¸ªæŸ¥è¯¢ä½¿ç”¨ RoPE ç¼–ç ç›¸å…³çš„æ—¶é—´æˆ³ï¼Œä»¥åŠä¸€ä¸ªå¯å­¦ä¹ çš„ä¼šè¯åµŒå…¥ï¼Œç”¨äºæ•è·è®°å½•ä¼šè¯çš„æ½œåœ¨å› ç´ ã€‚æˆ‘ä»¬è¾“å‡ºæ¨¡å—çš„è®¾è®¡åœ¨å‡ ä¸ªæ–¹é¢æä¾›äº†çµæ´»æ€§ï¼š</p>
<ol>
<li>æˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªæ—¶é—´å—é¢„æµ‹å¤šä¸ªè¾“å‡ºï¼Œä»è€Œå®ç°æ›´å¯†é›†å’Œæ›´ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œ</li>
<li>æˆ‘ä»¬ä¸éœ€è¦å°†è¡Œä¸ºç²¾ç¡®å¯¹é½åˆ°å—è¾¹ç•Œï¼Œä»¥åŠ</li>
<li>æˆ‘ä»¬å¯ä»¥é¢„æµ‹å½“å‰å—ä¹‹å¤–çš„è¡Œä¸ºï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿè€ƒè™‘ç¥ç»æ´»åŠ¨ä¸è¡Œä¸ºåŠ¨ä½œä¹‹é—´çš„æ»åï¼ˆè§ç¬¬ D.5 èŠ‚ï¼‰ã€‚</li>
</ol>
<h2 id="generalizing-to-unseen-data-with-pretrained-models">Generalizing to unseen data with pretrained models<a hidden class="anchor" aria-hidden="true" href="#generalizing-to-unseen-data-with-pretrained-models">#</a></h2>
<blockquote>
<p>Given a pretrained model, we outline two strategies for adapting it to a new set of neural units, with a trade-off between efficiency and decoding accuracy.</p>
</blockquote>
<p>ç»™å®šä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†ä¸¤ç§å°†å…¶é€‚åº”äºä¸€ç»„æ–°çš„ç¥ç»å•å…ƒçš„ç­–ç•¥ï¼Œåœ¨æ•ˆç‡å’Œè§£ç ç²¾åº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚</p>
<h3 id="unit-identification">Unit identification<a hidden class="anchor" aria-hidden="true" href="#unit-identification">#</a></h3>
<blockquote>
<p>To enable efficient generalization to previously unseen neural units, we adopt <strong>unit identification (UI)</strong>, a finetuning strategy enabled by the spike tokenization scheme adopted from POYO. In UI, new neural units can be processed by a model simply by learning new embeddings. Using this approach, we freeze the model weights, initialize new unit and session embeddings, and then train them on the data from the new session while the rest of the model is kept unchanged. This allows us to preserve the neural dynamics learned during pretraining, resulting in an efficient generalization strategy that typically updates less than $1\%$ of the modelâ€™s total parameters.</p>
</blockquote>
<p>ä¸ºäº†å®ç°å¯¹å…ˆå‰æœªè§è¿‡çš„ç¥ç»å•å…ƒçš„é«˜æ•ˆæ³›åŒ–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº† <strong>å•å…ƒè¯†åˆ«ï¼ˆUIï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§ç”± POYO é‡‡ç”¨çš„è„‰å†²æ ‡è®°åŒ–æ–¹æ¡ˆå¯ç”¨çš„å¾®è°ƒç­–ç•¥ã€‚åœ¨ UI ä¸­ï¼Œé€šè¿‡å­¦ä¹ æ–°çš„åµŒå…¥ï¼Œæ¨¡å‹å¯ä»¥ç®€å•åœ°å¤„ç†æ–°çš„ç¥ç»å•å…ƒã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å†»ç»“æ¨¡å‹æƒé‡ï¼Œåˆå§‹åŒ–æ–°çš„å•å…ƒå’Œä¼šè¯åµŒå…¥ï¼Œç„¶ååœ¨æ–°ä¼šè¯çš„æ•°æ®ä¸Šè®­ç»ƒå®ƒä»¬ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†ä¸å˜ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¿ç•™é¢„è®­ç»ƒæœŸé—´å­¦åˆ°çš„ç¥ç»åŠ¨åŠ›å­¦ï¼Œäº§ç”Ÿä¸€ç§é«˜æ•ˆçš„æ³›åŒ–ç­–ç•¥ï¼Œé€šå¸¸åªæ›´æ–°æ¨¡å‹æ€»å‚æ•°çš„ä¸åˆ° $1\%$ã€‚</p>
<h3 id="full-finetuning">Full finetuning<a hidden class="anchor" aria-hidden="true" href="#full-finetuning">#</a></h3>
<blockquote>
<p>While unit identification is an efficient finetuning method, it does not reliably match the performance of models trained end-to-end on individual sessions.</p>
<p>To address this, we also explored <strong>full finetuning (FT)</strong>, a complementary approach which uses a gradual unfreezing strategy.</p>
<p>We begin by only doing UI for some number of epochs before unfreezing the entire model and training end-to-end. This allows us to retain the benefits of pretraining while gradually adapting to the new session.</p>
<p>As shown below, full finetuning consistently outperforms both single-session training and unit identification across all tasks explored, demonstrating effective transfer to new animals, tasks, and remarkably, across species.</p>
</blockquote>
<p>è™½ç„¶å•å…ƒè¯†åˆ«æ˜¯ä¸€ç§é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œä½†å®ƒå¹¶ä¸å¯é åœ°åŒ¹é…åœ¨å•ä¸ªä¼šè¯ä¸Šç«¯åˆ°ç«¯è®­ç»ƒçš„æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº† <strong>å®Œå…¨å¾®è°ƒï¼ˆFTï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§äº’è¡¥çš„æ–¹æ³•ï¼Œä½¿ç”¨æ¸è¿›è§£å†»ç­–ç•¥ã€‚</p>
<p>æˆ‘ä»¬é¦–å…ˆåªè¿›è¡Œ UI ä¸€æ®µæ—¶é—´ï¼Œç„¶åè§£å†»æ•´ä¸ªæ¨¡å‹å¹¶è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¿ç•™é¢„è®­ç»ƒçš„å¥½å¤„ï¼ŒåŒæ—¶é€æ­¥é€‚åº”æ–°ä¼šè¯ã€‚</p>
<p>å¦‚ä¸‹é¢æ‰€ç¤ºï¼Œå®Œå…¨å¾®è°ƒåœ¨æ‰€æœ‰æ¢ç´¢çš„ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºå•ä¼šè¯è®­ç»ƒå’Œå•å…ƒè¯†åˆ«ï¼Œå±•ç¤ºäº†å¯¹æ–°åŠ¨ç‰©ã€ä»»åŠ¡ï¼Œç”šè‡³è·¨ç‰©ç§çš„æœ‰æ•ˆè½¬ç§»ã€‚</p>
<h1 id="experiments">Experiments<a hidden class="anchor" aria-hidden="true" href="#experiments">#</a></h1>
<blockquote>
<p>We evaluate POSSM across three categories of cortical activity datasets: NHP reaching tasks, human imagined handwriting, and human attempted speech.</p>
</blockquote>
<p>æˆ‘ä»¬åœ¨ä¸‰ç±»çš®å±‚æ´»åŠ¨æ•°æ®é›†ä¸Šè¯„ä¼° POSSMï¼šNHP åˆ°è¾¾ä»»åŠ¡ã€äººç±»æƒ³è±¡æ‰‹å†™å’Œäººç±»å°è¯•è¯­éŸ³ã€‚</p>
<blockquote>
<p>For the NHP reaching tasks, we highlight the benefits of scale by introducing o-POSSM, a POSSM variant pretrained on multiple datasets.</p>
<p>We compare it to single-session models on held-out sessions with varying similarity with the training data, demonstrating improved decoding accuracy with <strong>pretraining</strong>.</p>
</blockquote>
<p>å¯¹äº NHP åˆ°è¾¾ä»»åŠ¡ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ o-POSSMï¼ˆä¸€ç§åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ POSSM å˜ä½“ï¼‰æ¥çªå‡ºè§„æ¨¡çš„å¥½å¤„ã€‚</p>
<p>æˆ‘ä»¬å°†å…¶ä¸åœ¨å…·æœ‰ä¸åŒç›¸ä¼¼æ€§çš„è®­ç»ƒæ•°æ®çš„ä¿ç•™ä¼šè¯ä¸Šçš„å•ä¼šè¯æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œè¡¨æ˜ <strong>é¢„è®­ç»ƒ</strong> æé«˜äº†è§£ç ç²¾åº¦ã€‚</p>
<blockquote>
<p>Next, we show that o-POSSM achieves powerful downstream decoding performance on a human handwriting task, illustrating the POSSM frameworkâ€™s capacity for cross-species transfer.</p>
<p>Finally, we demonstrate that POSSM effectively leverages its recurrent architecture to efficiently decode human speech - a long-context task that can become computationally expensive for standard Transformer-based models with quadratic complexity.</p>
<p>In each of these tasks, we see that POSSM consistently matches or outperforms other architectures in a causal evaluation setting, with performance improving as model size and pretraining scale increase.</p>
</blockquote>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å±•ç¤ºäº† o-POSSM åœ¨äººç±»æ‰‹å†™ä»»åŠ¡ä¸Šå®ç°äº†å¼ºå¤§çš„ä¸‹æ¸¸è§£ç æ€§èƒ½ï¼Œè¯´æ˜äº† POSSM æ¡†æ¶è·¨ç‰©ç§è½¬ç§»çš„èƒ½åŠ›ã€‚</p>
<p>æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº† POSSM æœ‰æ•ˆåœ°åˆ©ç”¨å…¶é€’å½’æ¶æ„æ¥é«˜æ•ˆè§£ç äººç±»è¯­éŸ³â€”â€”è¿™æ˜¯ä¸€é¡¹é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼Œå¯¹äºå…·æœ‰äºŒæ¬¡å¤æ‚åº¦çš„æ ‡å‡†åŸºäº Transformer çš„æ¨¡å‹æ¥è¯´å¯èƒ½å˜å¾—è®¡ç®—æ˜‚è´µã€‚</p>
<p>åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ¯ä¸€ä¸ªä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ° POSSM åœ¨å› æœè¯„ä¼°è®¾ç½®ä¸­å§‹ç»ˆåŒ¹é…æˆ–ä¼˜äºå…¶ä»–æ¶æ„ï¼Œéšç€æ¨¡å‹è§„æ¨¡å’Œé¢„è®­ç»ƒè§„æ¨¡çš„å¢åŠ ï¼Œæ€§èƒ½ä¹Ÿåœ¨æé«˜ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/12/SzdGXFn6RZJ8BPa.png" alt=""  /></p>
<p><strong>Task schematics and outputs</strong>. (a) Centre-out (CO) task with a manipulandum. (b) Random target (RT) task with a manipulandum. (c) RT task with a touchscreen. (d) Maze task with a touchscreen. (e) Ground truth vs. predicted behaviour outputs from a held-out CO session. (f) Same as (e) but for an RT session. (g) Human handwriting decoding task. (h) Human speech decoding task.</p>
</blockquote>
<p>ä»»åŠ¡ç¤ºæ„å›¾å’Œè¾“å‡ºã€‚(a) å¸¦æœ‰æ“çºµæ†çš„ä¸­å¿ƒå¤–ï¼ˆCOï¼‰ä»»åŠ¡ã€‚(b) å¸¦æœ‰æ“çºµæ†çš„éšæœºç›®æ ‡ï¼ˆRTï¼‰ä»»åŠ¡ã€‚(c) å¸¦æœ‰è§¦æ‘¸å±çš„ RT ä»»åŠ¡ã€‚(d) å¸¦æœ‰è§¦æ‘¸å±çš„è¿·å®«ä»»åŠ¡ã€‚(e) æ¥è‡ªä¿ç•™ CO ä¼šè¯çš„çœŸå®å€¼ä¸é¢„æµ‹è¡Œä¸ºè¾“å‡ºã€‚(f) ä¸ (e) ç›¸åŒï¼Œä½†ç”¨äº RT ä¼šè¯ã€‚(g) äººç±»æ‰‹å†™è§£ç ä»»åŠ¡ã€‚(h) äººç±»è¯­éŸ³è§£ç ä»»åŠ¡ã€‚</p>
</blockquote>
<h2 id="non-human-primate-reaching">Non-human primate reaching<a hidden class="anchor" aria-hidden="true" href="#non-human-primate-reaching">#</a></h2>
<blockquote>
<p>We first evaluate POSSM on the task of decoding two-dimensional hand velocities in monkeys performing various reaching tasks (shown in Figure 3a-f). Each training sample consists of 1 s of spiking activity paired with the corresponding 2D hand velocity time series over that same interval. This 1 s window is split up into 20 non-overlapping chunks of 50 ms, which are fed sequentially to POSSM (see Section D.1 for results on 20 ms chunks). This streaming setup enables efficient real-time decoding, where only the most recent 50 ms chunk is processed at each step. In sharp contrast, the original POYO model reprocesses an entire 1 s window of activity with each new input, resulting in significantly higher computational cost.</p>
</blockquote>
<p>æˆ‘ä»¬é¦–å…ˆåœ¨çŒ´å­æ‰§è¡Œå„ç§åˆ°è¾¾ä»»åŠ¡æ—¶è§£ç äºŒç»´æ‰‹é€Ÿåº¦çš„ä»»åŠ¡ä¸Šè¯„ä¼° POSSMï¼ˆå¦‚å›¾ 3a-f æ‰€ç¤ºï¼‰ã€‚æ¯ä¸ªè®­ç»ƒæ ·æœ¬ç”± 1 ç§’çš„è„‰å†²æ´»åŠ¨ä¸è¯¥æ—¶é—´é—´éš”å†…ç›¸åº”çš„ 2D æ‰‹é€Ÿåº¦æ—¶é—´åºåˆ—é…å¯¹ç»„æˆã€‚è¿™ä¸ª 1 ç§’çª—å£è¢«åˆ†æˆ 20 ä¸ªä¸é‡å çš„ 50 æ¯«ç§’å—ï¼Œä¾æ¬¡é¦ˆé€ç»™ POSSMï¼ˆæœ‰å…³ 20 æ¯«ç§’å—çš„ç»“æœï¼Œè¯·å‚è§ç¬¬ D.1 èŠ‚ï¼‰ã€‚è¿™ç§æµå¼è®¾ç½®å®ç°äº†é«˜æ•ˆçš„å®æ—¶è§£ç ï¼Œæ¯ä¸ªæ­¥éª¤åªå¤„ç†æœ€è¿‘çš„ 50 æ¯«ç§’å—ã€‚ä¸æ­¤å½¢æˆé²œæ˜å¯¹æ¯”çš„æ˜¯ï¼ŒåŸå§‹ POYO æ¨¡å‹åœ¨æ¯ä¸ªæ–°è¾“å…¥æ—¶é‡æ–°å¤„ç†æ•´ä¸ª 1 ç§’çš„æ´»åŠ¨çª—å£ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬æ˜¾è‘—æ›´é«˜ã€‚</p>
<h3 id="experimental-setup">Experimental Setup<a hidden class="anchor" aria-hidden="true" href="#experimental-setup">#</a></h3>
<blockquote>
<p>We use a similar experimental setup to POYO. The pretraining dataset includes four NHP reaching datasets collected by different laboratories, covering three types of reaching tasks: <strong>centre-out (CO)</strong>, <strong>random target (RT)</strong>, and Maze.</p>
<p>CO is a highly-structured task involving movements from the centre of the screen to one of eight targets (Figure 3a). Conversely, the RT (Figure 3b-c) and Maze (Figure 3d) tasks are behaviourally more complex, requiring movement to randomly placed targets and navigating through a maze, respectively.</p>
</blockquote>
<p>æˆ‘ä»¬ä½¿ç”¨ä¸ POYO ç±»ä¼¼çš„å®éªŒè®¾ç½®ã€‚é¢„è®­ç»ƒæ•°æ®é›†åŒ…æ‹¬ç”±ä¸åŒå®éªŒå®¤æ”¶é›†çš„å››ä¸ª NHP åˆ°è¾¾æ•°æ®é›†ï¼Œæ¶µç›–ä¸‰ç§ç±»å‹çš„åˆ°è¾¾ä»»åŠ¡ï¼š<strong>ä¸­å¿ƒå¤–ï¼ˆCOï¼‰</strong>ã€<strong>éšæœºç›®æ ‡ï¼ˆRTï¼‰</strong> å’Œè¿·å®«ã€‚</p>
<p>CO æ˜¯ä¸€ç§é«˜åº¦ç»“æ„åŒ–çš„ä»»åŠ¡ï¼Œæ¶‰åŠä»å±å¹•ä¸­å¿ƒç§»åŠ¨åˆ°å…«ä¸ªç›®æ ‡ä¹‹ä¸€ï¼ˆå›¾ 3aï¼‰ã€‚ç›¸åï¼ŒRTï¼ˆå›¾ 3b-cï¼‰å’Œè¿·å®«ï¼ˆå›¾ 3dï¼‰ä»»åŠ¡åœ¨è¡Œä¸ºä¸Šæ›´å¤æ‚ï¼Œåˆ†åˆ«éœ€è¦ç§»åŠ¨åˆ°éšæœºæ”¾ç½®çš„ç›®æ ‡å’Œç©¿è¶Šè¿·å®«ã€‚</p>
<blockquote>
<p>The testing sessions include:</p>
<ol>
<li>new sessions from Monkey C which was seen during pretraining,</li>
<li>new sessions from Monkey T, not seen during pretraining but collected in the same lab as Monkey C, and</li>
<li>a new session from a dataset unseen during pretraining.</li>
</ol>
</blockquote>
<p>æµ‹è¯•ä¼šè¯åŒ…æ‹¬ï¼š</p>
<ol>
<li>æ¥è‡ªé¢„è®­ç»ƒæœŸé—´çœ‹åˆ°çš„çŒ´å­ C çš„æ–°ä¼šè¯ï¼Œ</li>
<li>æ¥è‡ªçŒ´å­ T çš„æ–°ä¼šè¯ï¼Œåœ¨é¢„è®­ç»ƒæœŸé—´æœªè§è¿‡ï¼Œä½†ä¸çŒ´å­ C åœ¨åŒä¸€å®éªŒå®¤æ”¶é›†ï¼Œä»¥åŠ</li>
<li>æ¥è‡ªé¢„è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ•°æ®é›†çš„æ–°ä¼šè¯ã€‚</li>
</ol>
<blockquote>
<p>We call our model pretrained on this dataset o-POSSM (see Section B.3 for details).</p>
<p>In total, o-POSSM was trained on 148 sessions comprising more than 670 million spikes from 26,032 neural units recorded across the primary motor (M1), dorsal premotor (PMd), and primary somatosensory (S1) cortices (see Section A for details). We also pretrain two baseline models, NDT-2 and POYO-1. Additionally, we report the results of single-session models trained from scratch on individual sessions. This includes single-session variants of POSSM (across all backbone architectures) and POYO, as well as other baselines such as a multi-layer perceptron (MLP), S4D, GRU, and Mamba.</p>
</blockquote>
<p>æˆ‘ä»¬ç§°åœ¨æ­¤æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ä¸º o-POSSMï¼ˆè¯¦æƒ…è§ç¬¬ B.3 èŠ‚ï¼‰ã€‚</p>
<p>æ€»çš„æ¥è¯´ï¼Œo-POSSM åœ¨ 148 ä¸ªä¼šè¯ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œè¿™äº›ä¼šè¯åŒ…å«æ¥è‡ªåˆçº§è¿åŠ¨çš®å±‚ï¼ˆM1ï¼‰ã€èƒŒä¾§å‰è¿åŠ¨çš®å±‚ï¼ˆPMdï¼‰å’Œåˆçº§èº¯ä½“æ„Ÿè§‰çš®å±‚ï¼ˆS1ï¼‰çš„ 26,032 ä¸ªç¥ç»å•å…ƒè®°å½•çš„è¶…è¿‡ 6.7 äº¿ä¸ªè„‰å†²ï¼ˆè¯¦æƒ…è§ç¬¬ A èŠ‚ï¼‰ã€‚æˆ‘ä»¬è¿˜é¢„è®­ç»ƒäº†ä¸¤ä¸ªåŸºçº¿æ¨¡å‹ï¼ŒNDT-2 å’Œ POYO-1ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä»å¤´å¼€å§‹åœ¨å•ä¸ªä¼šè¯ä¸Šè®­ç»ƒçš„å•ä¼šè¯æ¨¡å‹çš„ç»“æœã€‚è¿™åŒ…æ‹¬ POSSMï¼ˆè·¨æ‰€æœ‰éª¨å¹²æ¶æ„ï¼‰å’Œ POYO çš„å•ä¼šè¯å˜ä½“ï¼Œä»¥åŠå…¶ä»–åŸºçº¿ï¼Œå¦‚å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€S4Dã€GRU å’Œ Mambaã€‚</p>
<h3 id="causal-evaluation">Causal evaluation<a hidden class="anchor" aria-hidden="true" href="#causal-evaluation">#</a></h3>
<blockquote>
<p>To simulate real-world decoding scenarios, we adopt a <strong>causal evaluation strategy</strong> for all models. This is straightforward for POSSM and the recurrent baselines we consider â€“ sequences are split into 50 ms time chunks and fed sequentially to the model.</p>
<p>For the MLP and POYO, we provide a fixed 1 s history of neural activity at each inference timestep, sliding forward in small increments of 50 ms to collect predictions for all behavioural timestamps. For POYO, we only recorded predictions for timestamps in the final 50 ms of each 1 s context window.</p>
</blockquote>
<p>ä¸ºäº†æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„è§£ç åœºæ™¯ï¼Œæˆ‘ä»¬ä¸ºæ‰€æœ‰æ¨¡å‹é‡‡ç”¨äº† <strong>å› æœè¯„ä¼°ç­–ç•¥</strong>ã€‚è¿™å¯¹äºæˆ‘ä»¬è€ƒè™‘çš„ POSSM å’Œå¾ªç¯åŸºçº¿æ¥è¯´æ˜¯ç›´æ¥çš„â€”â€”åºåˆ—è¢«åˆ†æˆ 50 æ¯«ç§’çš„æ—¶é—´å—ï¼Œå¹¶ä¾æ¬¡é¦ˆé€ç»™æ¨¡å‹ã€‚</p>
<p>å¯¹äº MLP å’Œ POYOï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªæ¨ç†æ—¶é—´æ­¥æä¾›å›ºå®šçš„ 1 ç§’ç¥ç»æ´»åŠ¨å†å²ï¼Œä»¥ 50 æ¯«ç§’çš„å°å¢é‡å‘å‰æ»‘åŠ¨ï¼Œä»¥æ”¶é›†æ‰€æœ‰è¡Œä¸ºæ—¶é—´æˆ³çš„é¢„æµ‹ã€‚å¯¹äº POYOï¼Œæˆ‘ä»¬ä»…è®°å½•æ¯ä¸ª 1 ç§’ä¸Šä¸‹æ–‡çª—å£ä¸­æœ€å 50 æ¯«ç§’çš„æ—¶é—´æˆ³çš„é¢„æµ‹ã€‚</p>
<blockquote>
<p>During training, the models are presented with contiguous and non-overlapping 1 s sequences, which are not trial-aligned. However, we evaluate our models on entire trials, which are typically much longer than a second.</p>
<p>For example, in sessions from the Perich et al. dataset, trials in the validation and testing sets are at least $3\times$ and up to $5\times$ longer than the training sequences. This means that a recurrent model must generalize to sequences longer than the ones seen during training.</p>
</blockquote>
<p>åœ¨è®­ç»ƒæœŸé—´ï¼Œæ¨¡å‹å‘ˆç°è¿ç»­ä¸”ä¸é‡å çš„ 1 ç§’åºåˆ—ï¼Œè¿™äº›åºåˆ—æœªå¯¹é½è¯•éªŒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨æ•´ä¸ªè¯•éªŒä¸Šè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¿™äº›è¯•éªŒé€šå¸¸æ¯”ä¸€ç§’é’Ÿé•¿å¾—å¤šã€‚</p>
<p>ä¾‹å¦‚ï¼Œåœ¨ Perich ç­‰äººçš„æ•°æ®é›†ä¸­ï¼ŒéªŒè¯å’Œæµ‹è¯•é›†ä¸­çš„è¯•éªŒè‡³å°‘æ¯”è®­ç»ƒåºåˆ—é•¿ $3\times$ï¼Œæœ€å¤šé•¿ $5\times$ã€‚è¿™æ„å‘³ç€å¾ªç¯æ¨¡å‹å¿…é¡»æ¨å¹¿åˆ°æ¯”è®­ç»ƒæœŸé—´çœ‹åˆ°çš„åºåˆ—æ›´é•¿çš„åºåˆ—ã€‚</p>
<h3 id="transfer-to-new-sessions">Transfer to new sessions<a hidden class="anchor" aria-hidden="true" href="#transfer-to-new-sessions">#</a></h3>
<blockquote>
<p>In Table 1, we evaluate the transferability of our pretrained models to new sessions, animals, and datasets, and compare them to single-session models. When trained on a single session, POSSM is on par with or outperforms POYO on most sessions. When using pretrained models, o-POSSM-S4D shows the best overall performance. Finally, we observe that FT is noticeably better than UI when transferring to new animals or datasets. However, UI performs on par with or better than several single-session models, and requires far fewer parameters to be trained.</p>
</blockquote>
<p>åœ¨è¡¨ 1 ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†é¢„è®­ç»ƒæ¨¡å‹å‘æ–°ä¼šè¯ã€åŠ¨ç‰©å’Œæ•°æ®é›†çš„å¯è½¬ç§»æ€§ï¼Œå¹¶å°†å…¶ä¸å•ä¼šè¯æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚å½“åœ¨å•ä¸ªä¼šè¯ä¸Šè®­ç»ƒæ—¶ï¼ŒPOSSM åœ¨å¤§å¤šæ•°ä¼šè¯ä¸­ä¸ POYO ç›¸å½“æˆ–ä¼˜äº POYOã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œo-POSSM-S4D æ˜¾ç¤ºå‡ºæœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨è½¬ç§»åˆ°æ–°åŠ¨ç‰©æˆ–æ•°æ®é›†æ—¶ï¼ŒFT æ˜æ˜¾ä¼˜äº UIã€‚ç„¶è€Œï¼ŒUI çš„è¡¨ç°ä¸å‡ ä¸ªå•ä¼šè¯æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ï¼Œå¹¶ä¸”éœ€è¦è®­ç»ƒçš„å‚æ•°è¿œå°‘äº FTã€‚</p>
<h3 id="sample-and-training-compute-efficiency">Sample and training compute efficiency<a hidden class="anchor" aria-hidden="true" href="#sample-and-training-compute-efficiency">#</a></h3>
<blockquote>
<p>A key motivation behind training neural decoders on large, heterogeneous datasets is to enable efficient transfer to new individuals, tasks, or species with minimal finetuning. To this end, we evaluated the effectiveness of our finetuning strategies through few-shot experiments. Our results, shown in Figure 4a-b, demonstrate that o-POSSM outperforms single-session models trained from scratch in low-data regimes. Notably, we observe that pretraining results in a considerable initial boost in performance when adapting to a new session, even when only unit and session embeddings are updated. In some cases, single-session models fail to match the performance of a finetuned model, even after extensive training. Overall, our results are in line with observations from Azabou et al., supporting the idea that with the right tokenization and data aggregation schemes, pretrained models can amortize both data and training costs, leading to efficient adaptation to downstream applications.</p>
</blockquote>
<p>è®­ç»ƒç¥ç»è§£ç å™¨çš„ä¸€ä¸ªå…³é”®åŠ¨æœºæ˜¯èƒ½å¤Ÿé€šè¿‡æœ€å°çš„å¾®è°ƒå®ç°å¯¹æ–°ä¸ªä½“ã€ä»»åŠ¡æˆ–ç‰©ç§çš„é«˜æ•ˆè½¬ç§»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é€šè¿‡å°‘é‡å®éªŒè¯„ä¼°äº†å¾®è°ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœå¦‚å›¾ 4a-b æ‰€ç¤ºï¼Œè¡¨æ˜ o-POSSM åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒçš„å•ä¼šè¯æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨é€‚åº”æ–°ä¼šè¯æ—¶ï¼Œé¢„è®­ç»ƒå³ä½¿ä»…æ›´æ–°å•å…ƒå’Œä¼šè¯åµŒå…¥ï¼Œä¹Ÿä¼šå¸¦æ¥æ˜¾è‘—çš„åˆå§‹æ€§èƒ½æå‡ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå•ä¼šè¯æ¨¡å‹å³ä½¿ç»è¿‡å¤§é‡è®­ç»ƒä¹Ÿæ— æ³•åŒ¹é…å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç»“æœä¸ Azabou ç­‰äººçš„è§‚å¯Ÿç»“æœä¸€è‡´ï¼Œæ”¯æŒè¿™æ ·ä¸€ç§è§‚ç‚¹ï¼šé€šè¿‡æ­£ç¡®çš„æ ‡è®°åŒ–å’Œæ•°æ®èšåˆæ–¹æ¡ˆï¼Œé¢„è®­ç»ƒæ¨¡å‹å¯ä»¥æ‘Šé”€æ•°æ®å’Œè®­ç»ƒæˆæœ¬ï¼Œä»è€Œå®ç°å¯¹ä¸‹æ¸¸åº”ç”¨çš„é«˜æ•ˆé€‚åº”ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/12/koFwtBaCUhO5S4p.png" alt=""  /></p>
<p><strong>Sample and compute efficiency benchmarking</strong>. (a) Results on a held-out CO session from Monkey C. On the left, we show the sample efficiency of adapting a pretrained model versus training from scratch. On the right, we compare training compute efficiency between these two approaches. (b) Same as (a) but for a held-out RT session from Monkey T â€“ a new subject not seen during training. (c) Comparing model performance and compute efficiency to baseline models. Inference times are computed on a workstation-class GPU (NVIDIA RTX8000). For all these results, we used a GRU backbone for POSSM.</p>
</blockquote>
<p><strong>æ ·æœ¬å’Œè®¡ç®—æ•ˆç‡åŸºå‡†æµ‹è¯•</strong>ã€‚(a) æ¥è‡ªçŒ´å­ C çš„ä¿ç•™ CO ä¼šè¯çš„ç»“æœã€‚åœ¨å·¦ä¾§ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹ä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„æ ·æœ¬æ•ˆç‡ã€‚åœ¨å³ä¾§ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„è®­ç»ƒè®¡ç®—æ•ˆç‡ã€‚(b) ä¸ (a) ç›¸åŒï¼Œä½†ç”¨äºæ¥è‡ªçŒ´å­ T çš„ä¿ç•™ RT ä¼šè¯â€”â€”ä¸€ä¸ªåœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ–°å—è¯•è€…ã€‚(c) å°†æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¸åŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚æ¨ç†æ—¶é—´æ˜¯åœ¨å·¥ä½œç«™çº§ GPUï¼ˆNVIDIA RTX8000ï¼‰ä¸Šè®¡ç®—çš„ã€‚å¯¹äºæ‰€æœ‰è¿™äº›ç»“æœï¼Œæˆ‘ä»¬ä¸º POSSM ä½¿ç”¨äº† GRU éª¨å¹²ã€‚</p>
</blockquote>
<h3 id="inference-efficiency">Inference efficiency<a hidden class="anchor" aria-hidden="true" href="#inference-efficiency">#</a></h3>
<blockquote>
<p>In addition to being efficient to train and finetune, we also evaluated whether POSSM is efficient at inference time. In Figure 4c, we report parameter counts and inference time per time chunk on a workstation-class GPU (NVIDIA RTX8000) for POSSM and several state-of-the-art neural decoders.</p>
</blockquote>
<p>é™¤äº†è®­ç»ƒå’Œå¾®è°ƒæ•ˆç‡é«˜ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº† POSSM åœ¨æ¨ç†æ—¶æ˜¯å¦é«˜æ•ˆã€‚åœ¨å›¾ 4c ä¸­ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†å·¥ä½œç«™çº§ GPUï¼ˆNVIDIA RTX8000ï¼‰ä¸Š POSSM å’Œå‡ ç§æœ€å…ˆè¿›ç¥ç»è§£ç å™¨çš„å‚æ•°è®¡æ•°å’Œæ¯ä¸ªæ—¶é—´å—çš„æ¨ç†æ—¶é—´ã€‚</p>
<blockquote>
<p>We find that our single and multi-session models achieve inference speeds that are comparable to lightweight models like GRUs and MLPs and significantly lower than complex models like NDT-2 and POYO, while retaining competitive performance.</p>
<p>Single-session POSSM models (POSSMSS) contained the fewest parameters, and even o-POSSM, with about 8M parameters, maintained low latency. These results held in a CPU environment (AMD EPYC 7502 32-Core) as well, with POSSMSS and o-POSSM achieving inference speeds of $\sim 2.44\text{ ms/chunk}$ and $\sim 5.65\text{ ms/chunk}$, respectively.</p>
<p>Overall, our results show that POSSMâ€™s inference time is well within the optimal real-time BCI decoding latency of $\leq 10\text{ ms}$, making it a viable option for real-time BCI decoding applications.</p>
</blockquote>
<p>æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„å•ä¼šè¯å’Œå¤šä¼šè¯æ¨¡å‹å®ç°çš„æ¨ç†é€Ÿåº¦ä¸è½»é‡çº§æ¨¡å‹ï¼ˆå¦‚ GRU å’Œ MLPï¼‰ç›¸å½“ï¼Œå¹¶ä¸”æ˜¾è‘—ä½äºå¤æ‚æ¨¡å‹ï¼ˆå¦‚ NDT-2 å’Œ POYOï¼‰ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§èƒ½ã€‚</p>
<p>å•ä¼šè¯ POSSM æ¨¡å‹ï¼ˆPOSSMSSï¼‰åŒ…å«æœ€å°‘çš„å‚æ•°ï¼Œå³ä½¿æ˜¯å…·æœ‰çº¦ 800 ä¸‡å‚æ•°çš„ o-POSSM ä¹Ÿä¿æŒäº†ä½å»¶è¿Ÿã€‚è¿™äº›ç»“æœåœ¨ CPU ç¯å¢ƒï¼ˆAMD EPYC 7502 32-Coreï¼‰ä¸­ä¹Ÿæˆç«‹ï¼ŒPOSSMSS å’Œ o-POSSM åˆ†åˆ«å®ç°äº† $\sim 2.44\text{ ms/chunk}$ å’Œ $\sim 5.65\text{ ms/chunk}$ çš„æ¨ç†é€Ÿåº¦ã€‚</p>
<p>æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ POSSM çš„æ¨ç†æ—¶é—´è¿œä½äºæœ€ä½³å®æ—¶ BCI è§£ç å»¶è¿Ÿ $\leq 10\text{ ms}$ï¼Œä½¿å…¶æˆä¸ºå®æ—¶ BCI è§£ç åº”ç”¨çš„å¯è¡Œé€‰æ‹©ã€‚</p>
<h2 id="human-handwriting">Human handwriting<a hidden class="anchor" aria-hidden="true" href="#human-handwriting">#</a></h2>
<blockquote>
<p>Next, we evaluated POSSM on a human handwriting dataset. This dataset contains 11 sessions recorded from a single individual, where they imagined writing individual characters and drawing straight lines (Figure 3g). Spike counts from multi-unit threshold crossings were recorded from two 96-channel microelectrode arrays implanted in the participantâ€™s motor cortex. These were then binned at 10 ms intervals. The models were trained to classify the intended characters or lines based on this neural activity. For evaluation, we used 9 sessions, each containing 10 trials per character class. Each individual trial consisted of a 1.6 s time-window centred around the â€œgoâ€ cue.</p>
</blockquote>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªäººç±»æ‰‹å†™æ•°æ®é›†ä¸Šè¯„ä¼°äº† POSSMã€‚è¯¥æ•°æ®é›†åŒ…å«ä»å•ä¸ªä¸ªä½“è®°å½•çš„ 11 ä¸ªä¼šè¯ï¼Œå…¶ä¸­ä»–ä»¬æƒ³è±¡å†™å•ä¸ªå­—ç¬¦å’Œç»˜åˆ¶ç›´çº¿ï¼ˆå›¾ 3gï¼‰ã€‚æ¥è‡ªå‚ä¸è€…è¿åŠ¨çš®å±‚ä¸­æ¤å…¥çš„ä¸¤ä¸ª 96 é€šé“å¾®ç”µæé˜µåˆ—çš„å¤šå•å…ƒé˜ˆå€¼äº¤å‰çš„è„‰å†²è®¡æ•°è¢«è®°å½•ä¸‹æ¥ã€‚ç„¶åï¼Œè¿™äº›æ•°æ®ä»¥ 10 æ¯«ç§’é—´éš”è¿›è¡Œåˆ†ç®±ã€‚æ¨¡å‹è¢«è®­ç»ƒç”¨æ¥æ ¹æ®è¿™äº›ç¥ç»æ´»åŠ¨å¯¹é¢„æœŸçš„å­—ç¬¦æˆ–çº¿æ¡è¿›è¡Œåˆ†ç±»ã€‚ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† 9 ä¸ªä¼šè¯ï¼Œæ¯ä¸ªä¼šè¯åŒ…å«æ¯ä¸ªå­—ç¬¦ç±»åˆ«çš„ 10 æ¬¡è¯•éªŒã€‚æ¯ä¸ªå•ç‹¬çš„è¯•éªŒç”±å›´ç»•â€œå¼€å§‹â€æç¤ºçš„ 1.6 ç§’æ—¶é—´çª—å£ç»„æˆã€‚</p>
<h3 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h3>
<blockquote>
<p>We compared POSSM with five baselines: the previously published statistical method PCA-KNN, GRU, S4D, Mamba and POYO. For POSSM and all baselines except POYO, we adopted the causal evaluation strategy described in Section 3.1, training on 1 s intervals and evaluating on full 1.6 s trials. For POYO, we followed the evaluation scheme from the original paper, using fixed 1 s context windows for both training and testing.</p>
</blockquote>
<p>æˆ‘ä»¬å°† POSSM ä¸äº”ä¸ªåŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒï¼šå…ˆå‰å‘å¸ƒçš„ç»Ÿè®¡æ–¹æ³• PCA-KNNã€GRUã€S4Dã€Mamba å’Œ POYOã€‚å¯¹äº POSSM å’Œé™¤ POYO ä¹‹å¤–çš„æ‰€æœ‰åŸºçº¿ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç¬¬ 3.1 èŠ‚ä¸­æè¿°çš„å› æœè¯„ä¼°ç­–ç•¥ï¼Œåœ¨ 1 ç§’é—´éš”ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å®Œæ•´çš„ 1.6 ç§’è¯•éªŒä¸Šè¿›è¡Œè¯„ä¼°ã€‚å¯¹äº POYOï¼Œæˆ‘ä»¬éµå¾ªäº†åŸå§‹è®ºæ–‡ä¸­çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä½¿ç”¨å›ºå®šçš„ 1 ç§’ä¸Šä¸‹æ–‡çª—å£è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚</p>
<blockquote>
<p>As shown in Table 2, POSSM-GRU outperforms all baseline models when trained from scratch on the 9 sessions. Remarkably, finetuning o-POSSM, which was only pretrained on NHP data, led to significant performance gains: $2\%$ for POSSM-GRU and more than $5\%$ for both POSSM-S4D and POSSM-Mamba. All of the POSSM models achieve state-ofthe-art performance on this task, with the finetuned o-POSSM variants considerably outperforming the baseline PCA-KNN, achieving test accuracies that are about $16\%$ greater.</p>
</blockquote>
<p>å¦‚è¡¨ 2 æ‰€ç¤ºï¼ŒPOSSM-GRU åœ¨ä»å¤´å¼€å§‹åœ¨ 9 ä¸ªä¼šè¯ä¸Šè®­ç»ƒæ—¶ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¹ä»…åœ¨ NHP æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ o-POSSM è¿›è¡Œå¾®è°ƒå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼šPOSSM-GRU æå‡äº† $2\%$ï¼Œè€Œ POSSM-S4D å’Œ POSSM-Mamba åˆ™æå‡äº†è¶…è¿‡ $5\%$ã€‚æ‰€æœ‰ POSSM æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šéƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç»è¿‡å¾®è°ƒçš„ o-POSSM å˜ä½“æ˜¾è‘—ä¼˜äºåŸºçº¿ PCA-KNNï¼Œå®ç°äº†çº¦ $16\%$ æ›´é«˜çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚</p>
<blockquote>
<p>These results establish a critical finding: neural dynamics learned from NHP datasets can generalize across different species performing distinct tasks. This is especially impactful given the challenges of collecting large-scale human electrophysiology datasets, suggesting that the abundance of NHP datasets can be used to effectively improve human BCI decoders.</p>
</blockquote>
<p>è¿™äº›ç»“æœç¡®ç«‹äº†ä¸€ä¸ªå…³é”®å‘ç°ï¼šä» NHP æ•°æ®é›†ä¸­å­¦åˆ°çš„ç¥ç»åŠ¨åŠ›å­¦å¯ä»¥è·¨ä¸åŒç‰©ç§æ‰§è¡Œä¸åŒä»»åŠ¡è¿›è¡Œæ³›åŒ–ã€‚é‰´äºæ”¶é›†å¤§è§„æ¨¡äººç±»ç”µç”Ÿç†æ•°æ®é›†çš„æŒ‘æˆ˜ï¼Œè¿™ä¸€ç‚¹å°¤å…¶å…·æœ‰å½±å“åŠ›ï¼Œè¡¨æ˜ä¸°å¯Œçš„ NHP æ•°æ®é›†å¯ä»¥ç”¨æ¥æœ‰æ•ˆåœ°æ”¹å–„äººç±» BCI è§£ç å™¨ã€‚</p>
<h2 id="human-speech">Human speech<a hidden class="anchor" aria-hidden="true" href="#human-speech">#</a></h2>
<blockquote>
<p>Finally, we evaluated POSSM on the task of human speech decoding. Unlike the reaching and handwriting tasks, which involved fixed-length context windows, speech decoding involves modelling variable-length phoneme sequences that depend on both the length of the sentence and the individualâ€™s speaking pace. We used a public dataset consisting of 24 sessions in which a human participant with speech deficits attempted to speak sentences that appeared on a screen (Figure 3h).</p>
</blockquote>
<p>æœ€åï¼Œæˆ‘ä»¬åœ¨äººç±»è¯­éŸ³è§£ç ä»»åŠ¡ä¸Šè¯„ä¼°äº† POSSMã€‚ä¸æ¶‰åŠå›ºå®šé•¿åº¦ä¸Šä¸‹æ–‡çª—å£çš„åˆ°è¾¾å’Œæ‰‹å†™ä»»åŠ¡ä¸åŒï¼Œè¯­éŸ³è§£ç æ¶‰åŠå»ºæ¨¡å¯å˜é•¿åº¦çš„éŸ³ç´ åºåˆ—ï¼Œè¿™äº›åºåˆ—å–å†³äºå¥å­çš„é•¿åº¦å’Œä¸ªä½“çš„è¯´è¯é€Ÿåº¦ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå…¬å…±æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å« 24 ä¸ªä¼šè¯ï¼Œåœ¨è¿™äº›ä¼šè¯ä¸­ï¼Œä¸€ä½æœ‰è¯­è¨€éšœç¢çš„äººç±»å‚ä¸è€…è¯•å›¾è¯´å‡ºå±å¹•ä¸Šå‡ºç°çš„å¥å­ï¼ˆå›¾ 3hï¼‰ã€‚</p>
<blockquote>
<p>Neural activity was recorded from four 64-channel microelectrode arrays that covered the premotor cortex and Brocaâ€™s area. Multi-unit spiking activity was extracted and binned at a resolution of 20 ms, where the length of each trial ranged from 2 to 18 seconds. This poses a problem for Transformers like POYO that were designed for 1 s contexts, as the quadratic complexity of the attention mechanism would result in a substantial increase in computation for longer sentences.</p>
</blockquote>
<p>ç¥ç»æ´»åŠ¨æ˜¯ä»è¦†ç›–å‰è¿åŠ¨çš®å±‚å’Œ Broca åŒºçš„å››ä¸ª 64 é€šé“å¾®ç”µæé˜µåˆ—ä¸­è®°å½•çš„ã€‚å¤šå•å…ƒè„‰å†²æ´»åŠ¨è¢«æå–å¹¶ä»¥ 20 æ¯«ç§’çš„åˆ†è¾¨ç‡è¿›è¡Œåˆ†ç®±ï¼Œæ¯æ¬¡è¯•éªŒçš„é•¿åº¦ä» 2 ç§’åˆ° 18 ç§’ä¸ç­‰ã€‚è¿™å¯¹åƒ POYO è¿™æ ·çš„ä¸º 1 ç§’ä¸Šä¸‹æ–‡è®¾è®¡çš„ Transformer æ¶æ„æ¥è¯´æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦ä¼šå¯¼è‡´å¯¹äºè¾ƒé•¿å¥å­è®¡ç®—é‡çš„å¤§å¹…å¢åŠ ã€‚</p>
<blockquote>
<p>Although both uni- and bi-directional GRUs were used in the original study, we focused primarily on the causal, uni-directional case, as it is more relevant for real-time decoding. In line with Willett et al., we z-scored the neural data and added Gaussian noise as an augmentation. We used the <strong>phoneme error rate (PER)</strong> as our primary evaluation metric. While prior work has successfully incorporated language models to leverage textual priors, we leave this as a future research direction, instead focusing here on POSSMâ€™s capabilities.</p>
</blockquote>
<p>å°½ç®¡åŸå§‹ç ”ç©¶ä¸­ä½¿ç”¨äº†å•å‘å’ŒåŒå‘ GRUï¼Œä½†æˆ‘ä»¬ä¸»è¦å…³æ³¨å› æœçš„å•å‘æƒ…å†µï¼Œå› ä¸ºå®ƒä¸å®æ—¶è§£ç æ›´ç›¸å…³ã€‚ä¸ Willett ç­‰äººä¸€è‡´ï¼Œæˆ‘ä»¬å¯¹ç¥ç»æ•°æ®è¿›è¡Œäº† z-score æ ‡å‡†åŒ–ï¼Œå¹¶æ·»åŠ äº†é«˜æ–¯å™ªå£°ä½œä¸ºå¢å¼ºã€‚æˆ‘ä»¬ä½¿ç”¨ <strong>éŸ³ç´ é”™è¯¯ç‡ï¼ˆPERï¼‰</strong> ä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œå·²ç»æˆåŠŸåœ°ç»“åˆäº†è¯­è¨€æ¨¡å‹æ¥åˆ©ç”¨æ–‡æœ¬å…ˆéªŒçŸ¥è¯†ï¼Œä½†æˆ‘ä»¬å°†å…¶ä½œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œåœ¨è¿™é‡Œä¸“æ³¨äº POSSM çš„èƒ½åŠ›ã€‚</p>
<blockquote>
<p>Previous work has shown that Transformer-based decoders perform poorly on this task compared to GRU baselines. Here, we demonstrate the value of instead integrating attention with a recurrent model by using POSSM, specifically with a GRU backbone.</p>
</blockquote>
<p>å…ˆå‰çš„å·¥ä½œè¡¨æ˜ï¼Œä¸ GRU åŸºçº¿ç›¸æ¯”ï¼ŒåŸºäº Transformer çš„è§£ç å™¨åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒå·®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ POSSMï¼ˆç‰¹åˆ«æ˜¯å¸¦æœ‰ GRU éª¨å¹²ï¼‰æ¥å±•ç¤ºå°†æ³¨æ„åŠ›ä¸å¾ªç¯æ¨¡å‹é›†æˆçš„ä»·å€¼ã€‚</p>
<blockquote>
<p>However, since only normalized spike counts (and not spike times) were available in the dataset, we were unable to use the POYO-style tokenization as-is. Instead, we treated each multi-unit channel as a neural unit and encoded the normalized spike counts with value embeddings. Furthermore, we replaced the output cross-attention module with a 1D strided convolution layer to control the length of the output sequence. This approach significantly reduced the number of model parameters compared to the GRU baseline, which used strided sliding windows of neural activity as inputs instead.</p>
</blockquote>
<p>ç„¶è€Œï¼Œç”±äºæ•°æ®é›†ä¸­ä»…æä¾›äº†å½’ä¸€åŒ–çš„è„‰å†²è®¡æ•°ï¼ˆè€Œä¸æ˜¯è„‰å†²æ—¶é—´ï¼‰ï¼Œæˆ‘ä»¬æ— æ³•æŒ‰åŸæ ·ä½¿ç”¨ POYO é£æ ¼çš„æ ‡è®°åŒ–ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå¤šå•å…ƒé€šé“è§†ä¸ºä¸€ä¸ªç¥ç»å•å…ƒï¼Œå¹¶ä½¿ç”¨å€¼åµŒå…¥å¯¹å½’ä¸€åŒ–çš„è„‰å†²è®¡æ•°è¿›è¡Œç¼–ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç”¨ 1D æ­¥å¹…å·ç§¯å±‚æ›¿æ¢äº†è¾“å‡ºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥æ§åˆ¶è¾“å‡ºåºåˆ—çš„é•¿åº¦ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ä¸ GRU åŸºçº¿ç›¸æ¯”çš„æ¨¡å‹å‚æ•°æ•°é‡ï¼Œåè€…ä½¿ç”¨ç¥ç»æ´»åŠ¨çš„æ­¥å¹…æ»‘åŠ¨çª—å£ä½œä¸ºè¾“å…¥ã€‚</p>
<blockquote>
<p>We found that a two-phase training procedure yielded the best results. In the first phase, we trained the input cross-attention module along with the latent and unit embeddings by reconstructing the spike counts at each individual time bin. In the second phase, we trained the entire POSSM model on the target phoneme sequences using Connectionist Temporal Classification (CTC) loss.</p>
</blockquote>
<p>æˆ‘ä»¬å‘ç°ï¼Œä¸¤é˜¶æ®µçš„è®­ç»ƒç¨‹åºäº§ç”Ÿäº†æœ€ä½³ç»“æœã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡é‡å»ºæ¯ä¸ªå•ç‹¬æ—¶é—´ç®±çš„è„‰å†²è®¡æ•°æ¥è®­ç»ƒè¾“å…¥äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä»¥åŠæ½œåœ¨å’Œå•å…ƒåµŒå…¥ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰æŸå¤±å¯¹æ•´ä¸ª POSSM æ¨¡å‹è¿›è¡Œç›®æ ‡éŸ³ç´ åºåˆ—çš„è®­ç»ƒã€‚</p>
<h3 id="results-1">Results<a hidden class="anchor" aria-hidden="true" href="#results-1">#</a></h3>
<blockquote>
<p>POSSM achieved a significant improvement over all other baselines, as shown in Table 3. Notably, POSSM maintained comparable performance even without the Gaussian noise augmentation, while the performance of the baseline GRU was greatly impaired under the same conditions. Furthermore, we show in preliminary experiments with multiple input modalities (i.e., both spike counts and spiking-band powers) that POSSM yet again outperforms the baseline. These results illustrate the robustness of the POSSM architecture to variability in data preprocessing and its flexibility with respect to input modalities, further strengthening its case as a feasible real-world decoder.</p>
</blockquote>
<p>POSSM åœ¨æ‰€æœ‰å…¶ä»–åŸºçº¿ä¹‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¦‚è¡¨ 3 æ‰€ç¤ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æ²¡æœ‰é«˜æ–¯å™ªå£°å¢å¼ºï¼ŒPOSSM ä»ç„¶ä¿æŒäº†ç›¸å½“çš„æ€§èƒ½ï¼Œè€Œåœ¨ç›¸åŒæ¡ä»¶ä¸‹ï¼ŒåŸºçº¿ GRU çš„æ€§èƒ½å¤§å¤§å—æŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨å¤šç§è¾“å…¥æ¨¡æ€ï¼ˆå³è„‰å†²è®¡æ•°å’Œè„‰å†²å¸¦åŠŸç‡ï¼‰çš„åˆæ­¥å®éªŒä¸­è¡¨æ˜ï¼ŒPOSSM å†æ¬¡ä¼˜äºåŸºçº¿ã€‚è¿™äº›ç»“æœè¯´æ˜äº† POSSM æ¶æ„å¯¹æ•°æ®é¢„å¤„ç†å˜å¼‚æ€§çš„é²æ£’æ€§åŠå…¶å¯¹è¾“å…¥æ¨¡æ€çš„çµæ´»æ€§ï¼Œè¿›ä¸€æ­¥åŠ å¼ºäº†å…¶ä½œä¸ºå¯è¡Œçš„ç°å®ä¸–ç•Œè§£ç å™¨çš„æ¡ˆä¾‹ã€‚</p>
<h1 id="related-work">Related Work<a hidden class="anchor" aria-hidden="true" href="#related-work">#</a></h1>
<h2 id="neural-decoding">Neural decoding<a hidden class="anchor" aria-hidden="true" href="#neural-decoding">#</a></h2>
<blockquote>
<p>Neural decoding for continuous tasks such as motor control in BCIs was traditionally accomplished using statistical models such as the Kalman filter.</p>
<p>While these models are reliable and perform well for specific users and sessions, they require careful adaptation to generalize to new days, users, and even tasks.</p>
<p>Such adaptation is typically accomplished using model fitting approaches to estimate the Kalman filter parameters, a process that requires considerable new training data for each application.</p>
<p>Traditional approaches to multi-session neural decoding often consist of learning the decoderâ€™s parameters on a specific day or session (e.g., the first day), followed by learning models to align the neural activity on subsequent days to facilitate generalization.</p>
</blockquote>
<p>ç¥ç»è§£ç ç”¨äº BCI ä¸­çš„è¿ç»­ä»»åŠ¡ï¼ˆå¦‚è¿åŠ¨æ§åˆ¶ï¼‰ä¼ ç»Ÿä¸Šæ˜¯é€šè¿‡ç»Ÿè®¡æ¨¡å‹ï¼ˆå¦‚å¡å°”æ›¼æ»¤æ³¢å™¨ï¼‰æ¥å®Œæˆçš„ã€‚</p>
<p>è™½ç„¶è¿™äº›æ¨¡å‹å¯é ä¸”åœ¨ç‰¹å®šç”¨æˆ·å’Œä¼šè¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬éœ€è¦ä»”ç»†è°ƒæ•´ä»¥å®ç°å¯¹æ–°æ—¥æœŸã€ç”¨æˆ·ç”šè‡³ä»»åŠ¡çš„æ³›åŒ–ã€‚</p>
<p>è¿™ç§è°ƒæ•´é€šå¸¸æ˜¯é€šè¿‡æ¨¡å‹æ‹Ÿåˆæ–¹æ³•æ¥ä¼°è®¡å¡å°”æ›¼æ»¤æ³¢å™¨å‚æ•°æ¥å®Œæˆçš„ï¼Œè¿™ä¸€è¿‡ç¨‹éœ€è¦ä¸ºæ¯ä¸ªåº”ç”¨æä¾›å¤§é‡æ–°çš„è®­ç»ƒæ•°æ®ã€‚</p>
<p>ä¼ ç»Ÿçš„å¤šä¼šè¯ç¥ç»è§£ç æ–¹æ³•é€šå¸¸åŒ…æ‹¬åœ¨ç‰¹å®šæ—¥æœŸæˆ–ä¼šè¯ï¼ˆä¾‹å¦‚ç¬¬ä¸€å¤©ï¼‰ä¸Šå­¦ä¹ è§£ç å™¨çš„å‚æ•°ï¼Œç„¶åå­¦ä¹ æ¨¡å‹ä»¥å¯¹é½åç»­æ—¥æœŸçš„ç¥ç»æ´»åŠ¨ä»¥ä¿ƒè¿›æ³›åŒ–ã€‚</p>
<blockquote>
<p>Given the recent availability of large, public neural recordings datasets, modern neural decoding approaches have attempted to leverage advances in large-scale deep learning to build data-driven BCI decoders.</p>
<p>For example, in the context of decoders for neuronal spiking activity, NDT jointly embeds spikes from a neural population into a single token per time bin, spatiotemporal NDT (STNDT) separately tokenizes across units and time and learns a joint representation across these two contexts, NDT-2 tokenizes spatiotemporal patches of neural data akin to a ViT, and POYO eschews bin-based tokenization, opting to tokenize individual spikes and using a PerceiverIO Transformer backbone to query behaviours from within specific context windows.</p>
</blockquote>
<p>é‰´äºæœ€è¿‘å¤§å‹å…¬å…±ç¥ç»è®°å½•æ•°æ®é›†çš„å¯ç”¨æ€§ï¼Œç°ä»£ç¥ç»è§£ç æ–¹æ³•è¯•å›¾åˆ©ç”¨å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ çš„è¿›æ­¥æ¥æ„å»ºæ•°æ®é©±åŠ¨çš„ BCI è§£ç å™¨ã€‚</p>
<p>ä¾‹å¦‚ï¼Œåœ¨ç¥ç»å…ƒè„‰å†²æ´»åŠ¨è§£ç å™¨çš„èƒŒæ™¯ä¸‹ï¼ŒNDT å°†ç¥ç»ç¾¤ä½“çš„è„‰å†²åµŒå…¥åˆ°æ¯ä¸ªæ—¶é—´ç®±ä¸­çš„å•ä¸ªæ ‡è®°ä¸­ï¼Œæ—¶ç©º NDTï¼ˆSTNDTï¼‰åœ¨å•å…ƒå’Œæ—¶é—´ä¸Šåˆ†åˆ«è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å­¦ä¹ è¿™ä¸¤ä¸ªä¸Šä¸‹æ–‡ä¹‹é—´çš„è”åˆè¡¨ç¤ºï¼ŒNDT-2 ç±»ä¼¼äº ViT å¯¹ç¥ç»æ•°æ®çš„æ—¶ç©ºè¡¥ä¸è¿›è¡Œæ ‡è®°åŒ–ï¼Œè€Œ POYO æ”¾å¼ƒäº†åŸºäºç®±çš„æ ‡è®°åŒ–ï¼Œé€‰æ‹©å¯¹å•ä¸ªè„‰å†²è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶ä½¿ç”¨ PerceiverIO Transformer éª¨å¹²ä»ç‰¹å®šä¸Šä¸‹æ–‡çª—å£ä¸­æŸ¥è¯¢è¡Œä¸ºã€‚</p>
<blockquote>
<p>While several of these works excel at neural decoding, they do not focus on enabling generalizable, online decoding in spike-based BCIs and closed-loop protocols.</p>
<p>The BRAND platform enables the deployment of specialized deep learning models in real-time closed-loop brain-computer interface experiments with invasive recordings, demonstrating suitable low-latency neural decoding in other models such as LFADS.</p>
<p>Finally, we note other ML methods for neural data processing other than direct behaviour decoding. Contrastive learning methods that aim to identify joint latent variables between neural activity and behaviour can be useful for decoding but work is needed for online use. Diffusion-based approaches are promising for jointly forecasting neural activity and behaviour, but again are not readily suited for online use.</p>
</blockquote>
<p>è™½ç„¶è¿™äº›å·¥ä½œä¸­çš„å‡ ä¸ªåœ¨ç¥ç»è§£ç æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¹¶ä¸ä¸“æ³¨äºå®ç°å¯æ³›åŒ–çš„åœ¨çº¿è§£ç ï¼Œä»¥ç”¨äºåŸºäºè„‰å†²çš„ BCI å’Œé—­ç¯åè®®ã€‚</p>
<p>BRAND å¹³å°ä½¿å¾—åœ¨å…·æœ‰ä¾µå…¥æ€§è®°å½•çš„å®æ—¶é—­ç¯è„‘æœºæ¥å£å®éªŒä¸­éƒ¨ç½²ä¸“é—¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œåœ¨ LFADS ç­‰å…¶ä»–æ¨¡å‹ä¸­å±•ç¤ºäº†é€‚å½“çš„ä½å»¶è¿Ÿç¥ç»è§£ç ã€‚</p>
<p>æœ€åï¼Œæˆ‘ä»¬æ³¨æ„åˆ°é™¤äº†ç›´æ¥è¡Œä¸ºè§£ç ä¹‹å¤–ï¼Œè¿˜æœ‰å…¶ä»–ç”¨äºç¥ç»æ•°æ®å¤„ç†çš„ ML æ–¹æ³•ã€‚æ—¨åœ¨è¯†åˆ«ç¥ç»æ´»åŠ¨å’Œè¡Œä¸ºä¹‹é—´è”åˆæ½œå˜é‡çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¯¹äºè§£ç å¯èƒ½æœ‰ç”¨ï¼Œä½†éœ€è¦è¿›è¡Œåœ¨çº¿ä½¿ç”¨çš„å·¥ä½œã€‚åŸºäºæ‰©æ•£çš„æ–¹æ³•æœ‰æœ›è”åˆé¢„æµ‹ç¥ç»æ´»åŠ¨å’Œè¡Œä¸ºï¼Œä½†åŒæ ·ä¸é€‚åˆåœ¨çº¿ä½¿ç”¨ã€‚</p>
<h2 id="hybrid-attention-recurrence-models">Hybrid attention-recurrence models<a hidden class="anchor" aria-hidden="true" href="#hybrid-attention-recurrence-models">#</a></h2>
<blockquote>
<p>Several works have attempted to combine self- and cross-attention layers with recurrent architectures, usually with the goal of combining the expressivity of attention over shorter timescales with the long-term context modelling abilities of recurrent models such as structured SSMs.</p>
</blockquote>
<p>ä¸€äº›å·¥ä½œè¯•å›¾å°†è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚ä¸å¾ªç¯æ¶æ„ç›¸ç»“åˆï¼Œé€šå¸¸ç›®çš„æ˜¯ç»“åˆæ³¨æ„åŠ›åœ¨è¾ƒçŸ­æ—¶é—´å°ºåº¦ä¸Šçš„è¡¨ç°åŠ›ä¸å¾ªç¯æ¨¡å‹ï¼ˆå¦‚ç»“æ„åŒ– SSMï¼‰åœ¨é•¿æœŸä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢çš„èƒ½åŠ›ã€‚</p>
<blockquote>
<p>While traditional SSMs have been used for several neuroscience applications, modern SSMs and hybrid models remain underexplored in the field.</p>
<p>Didolkar et al. propose an architecture comprising Transformer blocks which process information at faster, shorter timescales while a recurrent backbone integrates information from these blocks over longer timescales for long-term contextual modeling.</p>
<p>A similar approach is the Block-Recurrent Transformer, wherein a recurrent cell operates on a block of tokens in parallel, thus propagating a block of state vectors through timesteps. Pilault et al. propose the Block-State Transformer architecture, which introduces a layer consisting of an SSM to process long input sequences, the outputs of which are sent in blocks to several Transformers that process them in parallel to produce output tokens. Furthermore, several recent works on high-throughput language modelling have leveraged hybrid models, where self-attention layers are replaced with SSM blocks to take advantage of their subquadratic computational complexity.</p>
</blockquote>
<p>è™½ç„¶ä¼ ç»Ÿçš„ SSM å·²è¢«ç”¨äºå¤šä¸ªç¥ç»ç§‘å­¦åº”ç”¨ï¼Œä½†ç°ä»£ SSM å’Œæ··åˆæ¨¡å‹åœ¨è¯¥é¢†åŸŸä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚</p>
<p>Didolkar ç­‰äººæå‡ºäº†ä¸€ç§æ¶æ„ï¼Œè¯¥æ¶æ„åŒ…æ‹¬ Transformer å—ï¼Œè¿™äº›å—åœ¨æ›´å¿«ã€æ›´çŸ­çš„æ—¶é—´å°ºåº¦ä¸Šå¤„ç†ä¿¡æ¯ï¼Œè€Œå¾ªç¯éª¨å¹²åˆ™æ•´åˆæ¥è‡ªè¿™äº›å—çš„ä¿¡æ¯ä»¥è¿›è¡Œé•¿æœŸä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚</p>
<p>ç±»ä¼¼çš„æ–¹æ³•æ˜¯å—å¾ªç¯ Transformerï¼Œå…¶ä¸­å¾ªç¯å•å…ƒå¹¶è¡Œåœ°åœ¨ä¸€å—æ ‡è®°ä¸Šè¿è¡Œï¼Œä»è€Œé€šè¿‡æ—¶é—´æ­¥ä¼ æ’­ä¸€å—çŠ¶æ€å‘é‡ã€‚Pilault ç­‰äººæå‡ºäº†å—çŠ¶æ€ Transformer æ¶æ„ï¼Œè¯¥æ¶æ„å¼•å…¥äº†ä¸€å±‚åŒ…å« SSM çš„å±‚æ¥å¤„ç†é•¿è¾“å…¥åºåˆ—ï¼Œå…¶è¾“å‡ºä»¥å—çš„å½¢å¼å‘é€åˆ°å‡ ä¸ª Transformerï¼Œè¿™äº› Transformer å¹¶è¡Œå¤„ç†å®ƒä»¬ä»¥ç”Ÿæˆè¾“å‡ºæ ‡è®°ã€‚æ­¤å¤–ï¼Œæœ€è¿‘å…³äºé«˜ååé‡è¯­è¨€å»ºæ¨¡çš„å‡ é¡¹å·¥ä½œåˆ©ç”¨äº†æ··åˆæ¨¡å‹ï¼Œå…¶ä¸­è‡ªæ³¨æ„åŠ›å±‚è¢« SSM å—æ›¿æ¢ï¼Œä»¥åˆ©ç”¨å…¶äºšäºŒæ¬¡è®¡ç®—å¤æ‚åº¦ã€‚</p>
<h1 id="discussion">Discussion<a hidden class="anchor" aria-hidden="true" href="#discussion">#</a></h1>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<blockquote>
<p>We introduced POSSM, a scalable and generalizable hybrid architecture that pairs spike tokenization and input-output cross-attention with SSMs. This architecture enables efficient online decoding applications, achieving state-of-the-art performance for several neural decoding tasks while having fewer parameters and faster inference compared to fully Transformer-based approaches. Our model achieves high performance with millisecond-scale inference times on standard workstations, even without GPUs, making it suitable for real-time deployment in clinical settings.</p>
</blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº† POSSMï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•ä¸”å¯æ³›åŒ–çš„æ··åˆæ¶æ„ï¼Œå°†è„‰å†²æ ‡è®°åŒ–å’Œè¾“å…¥è¾“å‡ºäº¤å‰æ³¨æ„åŠ›ä¸ SSM é…å¯¹ã€‚è¯¥æ¶æ„å®ç°äº†é«˜æ•ˆçš„åœ¨çº¿è§£ç åº”ç”¨ï¼Œåœ¨å¤šä¸ªç¥ç»è§£ç ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸å®Œå…¨åŸºäº Transformer çš„æ–¹æ³•ç›¸æ¯”ï¼Œå‚æ•°æ›´å°‘ä¸”æ¨ç†æ›´å¿«ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ ‡å‡†å·¥ä½œç«™ä¸Šå®ç°äº†æ¯«ç§’çº§çš„é«˜æ€§èƒ½æ¨ç†æ—¶é—´ï¼Œå³ä½¿æ²¡æœ‰ GPUï¼Œä¹Ÿé€‚åˆåœ¨ä¸´åºŠç¯å¢ƒä¸­è¿›è¡Œå®æ—¶éƒ¨ç½²ã€‚</p>
<blockquote>
<p>A key contribution of this work is demonstrating, to our knowledge, the first successful cross-species transfer of learned neural dynamics for a deep learning-based decoder â€“ from NHP motor cortex to human clinical data (see [11, 62] for related efforts). This outlines a solution to a major clinical hurdle, where obtaining sufficient data for large-scale modelling is challenging or impossible in patient populations. We demonstrate that we can leverage the large corpus of existing non-human experimental data to improve personalized clinical outcomes by finetuning pretrained models.</p>
</blockquote>
<p>è¿™é¡¹å·¥ä½œçš„ä¸€ä¸ªå…³é”®è´¡çŒ®æ˜¯å±•ç¤ºäº†æˆ‘ä»¬æ‰€çŸ¥çš„ç¬¬ä¸€ä¸ªæˆåŠŸçš„è·¨ç‰©ç§è½¬ç§»å­¦ä¹ ç¥ç»åŠ¨åŠ›å­¦çš„æ·±åº¦å­¦ä¹ è§£ç å™¨â€”â€”ä» NHP è¿åŠ¨çš®å±‚åˆ°äººç±»ä¸´åºŠæ•°æ®ï¼ˆæœ‰å…³ç›¸å…³å·¥ä½œï¼Œè¯·å‚è§ [11, 62]ï¼‰ã€‚è¿™ä¸ºä¸€ä¸ªä¸»è¦çš„ä¸´åºŠéšœç¢æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œåœ¨æ‚£è€…ç¾¤ä½“ä¸­è·å¾—è¶³å¤Ÿçš„å¤§è§„æ¨¡å»ºæ¨¡æ•°æ®æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§æˆ–ä¸å¯èƒ½çš„ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç°æœ‰çš„å¤§é‡éäººç±»å®éªŒæ•°æ®ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥æ”¹å–„ä¸ªæ€§åŒ–ä¸´åºŠç»“æœã€‚</p>
<h2 id="future-directions-and-applications">Future directions and applications<a hidden class="anchor" aria-hidden="true" href="#future-directions-and-applications">#</a></h2>
<blockquote>
<p>The POSSM architecture is applied here for motor neural decoding, but it can be adapted to achieve a variety of outcomes. Our current model focuses on processing spiking data from implanted arrays in the motor cortex of monkeys and human clinical trial participants. However, our hybrid architecture is flexible and could readily accommodate data of other neural data modalities through a variety of proven tokenization schemes (e.g., Calcium imaging, EEG). While in the present work we focus on decoding of behavioural timestamps immediately following our input time chunks, our hybrid architecture is well-suited towards forecasting over longer timescales. Further, in the future we plan to explore the ability of POSSM to learn and generalize across different regions beyond the motor cortex.</p>
</blockquote>
<p>POSSM æ¶æ„åœ¨è¿™é‡Œåº”ç”¨äºè¿åŠ¨ç¥ç»è§£ç ï¼Œä½†å®ƒå¯ä»¥é€‚åº”ä»¥å®ç°å„ç§ç»“æœã€‚æˆ‘ä»¬å½“å‰çš„æ¨¡å‹ä¸“æ³¨äºå¤„ç†æ¥è‡ªçŒ´å­å’Œäººç±»ä¸´åºŠè¯•éªŒå‚ä¸è€…è¿åŠ¨çš®å±‚ä¸­æ¤å…¥é˜µåˆ—çš„è„‰å†²æ•°æ®ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ··åˆæ¶æ„å…·æœ‰çµæ´»æ€§ï¼Œå¯ä»¥é€šè¿‡å„ç§ç»è¿‡éªŒè¯çš„æ ‡è®°åŒ–æ–¹æ¡ˆè½»æ¾é€‚åº”å…¶ä»–ç¥ç»æ•°æ®æ¨¡æ€çš„æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œé’™æˆåƒã€EEGï¼‰ã€‚è™½ç„¶åœ¨å½“å‰å·¥ä½œä¸­æˆ‘ä»¬ä¸“æ³¨äºè§£ç ç´§éšè¾“å…¥æ—¶é—´å—ä¹‹åçš„è¡Œä¸ºæ—¶é—´æˆ³ï¼Œä½†æˆ‘ä»¬çš„æ··åˆæ¶æ„éå¸¸é€‚åˆäºæ›´é•¿æ—¶é—´å°ºåº¦çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæœªæ¥æˆ‘ä»¬è®¡åˆ’æ¢ç´¢ POSSM å­¦ä¹ å’Œæ³›åŒ–åˆ°è¿åŠ¨çš®å±‚ä»¥å¤–ä¸åŒåŒºåŸŸçš„èƒ½åŠ›ã€‚</p>
<blockquote>
<p>Ultimately, we envision POSSM as a first step towards a fast, generalizable neural foundation model for various neural interfacing tasks, with downstream applications such as clinical diagnostics and the development of smart, closed-loop neuromodulation techniques that link predicted states to optimized neural stimulators (e.g., [4, 5]).</p>
<p>Future steps include multimodal pretraining and decoding as well as a principled self-supervised pretraining scheme. By enabling efficient inference and flexible generalization through transfer learning, POSSM marks a new direction for general-purpose neural decoders with real-world practicality.</p>
</blockquote>
<p>æœ€ç»ˆï¼Œæˆ‘ä»¬å°† POSSM è§†ä¸ºå„ç§ç¥ç»æ¥å£ä»»åŠ¡çš„å¿«é€Ÿã€å¯æ³›åŒ–ç¥ç»åŸºç¡€æ¨¡å‹çš„ç¬¬ä¸€æ­¥ï¼Œä¸‹æ¸¸åº”ç”¨åŒ…æ‹¬ä¸´åºŠè¯Šæ–­å’Œæ™ºèƒ½é—­ç¯ç¥ç»è°ƒèŠ‚æŠ€æœ¯çš„å‘å±•ï¼Œå°†é¢„æµ‹çŠ¶æ€ä¸ä¼˜åŒ–çš„ç¥ç»åˆºæ¿€å™¨ï¼ˆä¾‹å¦‚ï¼Œ[4, 5]ï¼‰è”ç³»èµ·æ¥ã€‚</p>
<p>æœªæ¥çš„æ­¥éª¤åŒ…æ‹¬å¤šæ¨¡æ€é¢„è®­ç»ƒå’Œè§£ç ä»¥åŠæœ‰åŸåˆ™çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ¡ˆã€‚é€šè¿‡é€šè¿‡è¿ç§»å­¦ä¹ å®ç°é«˜æ•ˆæ¨ç†å’Œçµæ´»æ³›åŒ–ï¼ŒPOSSM æ ‡å¿—ç€å…·æœ‰ç°å®ä¸–ç•Œå®ç”¨æ€§çš„é€šç”¨ç¥ç»è§£ç å™¨çš„æ–°æ–¹å‘ã€‚</p>
<h1 id="broader-impact">Broader Impact<a hidden class="anchor" aria-hidden="true" href="#broader-impact">#</a></h1>
<blockquote>
<p>This research could potentially contribute to the development of neural decoders that are not only accurate but also amenable to deployment in online systems, such as those found in neuroprosthetics and other brain-computer interfaces. In addition to POSSMâ€™s general performance, it reduces the need for extensive individual calibration due to the pretraining/finetuning scheme. Additionally, the crossspecies transfer results on the handwriting task suggest that patients with limited availability of data could benefit from models pretrained with larger datasets from different species. While the potential downstream applications of POSSM are exciting, it is important to consider the ethical concerns that exist for any medical technology, including but not limited to data privacy and humane data collection from animals. Strict testing should be implemented before deployment in any human-related setting.</p>
</blockquote>
<p>è¿™é¡¹ç ”ç©¶æœ‰å¯èƒ½æœ‰åŠ©äºå¼€å‘ä¸ä»…å‡†ç¡®è€Œä¸”é€‚åˆéƒ¨ç½²åœ¨åœ¨çº¿ç³»ç»Ÿä¸­çš„ç¥ç»è§£ç å™¨ï¼Œä¾‹å¦‚ç¥ç»å‡ä½“å’Œå…¶ä»–è„‘æœºæ¥å£ä¸­ã€‚é™¤äº† POSSM çš„æ•´ä½“æ€§èƒ½å¤–ï¼Œç”±äºé¢„è®­ç»ƒ/å¾®è°ƒæ–¹æ¡ˆï¼Œå®ƒå‡å°‘äº†å¯¹å¹¿æ³›ä¸ªäººæ ¡å‡†çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæ‰‹å†™ä»»åŠ¡ä¸Šçš„è·¨ç‰©ç§è½¬ç§»ç»“æœè¡¨æ˜ï¼Œæ•°æ®å¯ç”¨æ€§æœ‰é™çš„æ‚£è€…å¯ä»¥ä»ä½¿ç”¨æ¥è‡ªä¸åŒç‰©ç§çš„å¤§å‹æ•°æ®é›†é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­å—ç›Šã€‚è™½ç„¶ POSSM çš„æ½œåœ¨ä¸‹æ¸¸åº”ç”¨ä»¤äººå…´å¥‹ï¼Œä½†é‡è¦çš„æ˜¯è¦è€ƒè™‘ä»»ä½•åŒ»ç–—æŠ€æœ¯å­˜åœ¨çš„ä¼¦ç†é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®éšç§å’Œå¯¹åŠ¨ç‰©çš„äººé“æ•°æ®æ”¶é›†ã€‚åœ¨ä»»ä½•ä¸äººç±»ç›¸å…³çš„ç¯å¢ƒä¸­éƒ¨ç½²ä¹‹å‰ï¼Œéƒ½åº”å®æ–½ä¸¥æ ¼çš„æµ‹è¯•ã€‚</p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/read/reference/how-people-learn/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>äººæ˜¯å¦‚ä½•å­¦ä¹ çš„ I&amp;II èŠ‚é€‰</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/read/reference/integration-as-a-self-organizing-process/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>Integration as a self-organizing process</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>


<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        
        
        
        
        
        
        
        
        
        

        
        
        
        
        
        
        
        
        
        
        

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  
</body>







<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
