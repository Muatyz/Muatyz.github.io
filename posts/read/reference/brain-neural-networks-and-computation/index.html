<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Brain, neural networks, and computation | æ— å¤„æƒ¹å°˜åŸƒ</title>
<meta name="keywords" content="">
<meta name="description" content="Computation through dynamics">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/reference/brain-neural-networks-and-computation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/reference/brain-neural-networks-and-computation/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>







<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="Brain, neural networks, and computation" />
<meta property="og:description" content="Computation through dynamics" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/reference/brain-neural-networks-and-computation/" />
<meta property="og:image" content="https://s2.loli.net/2025/11/19/T2aHrkA4UjSN3Yu.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-19T00:18:23+08:00" />
<meta property="article:modified_time" content="2025-11-19T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/11/19/T2aHrkA4UjSN3Yu.png" />
<meta name="twitter:title" content="Brain, neural networks, and computation"/>
<meta name="twitter:description" content="Computation through dynamics"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ“• é˜…è¯»",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "ğŸ“• æ–‡çŒ®",
          "item": "https://Muatyz.github.io/posts/read/reference/"
        }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Brain, neural networks, and computation",
      "item": "https://Muatyz.github.io/posts/read/reference/brain-neural-networks-and-computation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Brain, neural networks, and computation",
  "name": "Brain, neural networks, and computation",
  "description": "Computation through dynamics",
  "keywords": [
    ""
  ],
  "articleBody": "Introduction The method by which brain produces mind has for centuries been discussed in terms of the most complex engineering and science metaphors of the day. Descartes described mind in terms of interacting vortices. Psychologists have metaphorized memory in terms of paths or traces worn in a landscape, a geological record of our experiences. To McCulloch and Pitts (1943) and von Neumann (1958), the appropriate metaphor was the digital computer, then in its infancy. The field of â€œneural networksâ€ is the study of the computational properties and behavior of networks of â€œneuronlikeâ€ elements. It lies somewhere between a model of neurobiology and a metaphor for how the brain computes. It is inspired by two goals: to understand how neurobiology works, and to understand how to solve problems which neurobiology solves rapidly and effortlessly and which are very hard on present digital machines.\nå‡ ä¸ªä¸–çºªä»¥æ¥ï¼Œå¤§è„‘äº§ç”Ÿå¿ƒæ™ºçš„æ–¹æ³•ä¸€ç›´ä»¥å½“æ—¶æœ€å¤æ‚çš„å·¥ç¨‹å’Œç§‘å­¦éšå–»æ¥è®¨è®ºã€‚ç¬›å¡å°”å°†å¿ƒæ™ºæè¿°ä¸ºç›¸äº’ä½œç”¨çš„æ¼©æ¶¡ã€‚å¿ƒç†å­¦å®¶å°†è®°å¿†éšå–»ä¸ºæ™¯è§‚ä¸­ç£¨æŸçš„è·¯å¾„æˆ–ç—•è¿¹ï¼Œæ˜¯æˆ‘ä»¬ç»å†çš„åœ°è´¨è®°å½•ã€‚å¯¹äº McCulloch å’Œ Pittsï¼ˆ1943ï¼‰ä»¥åŠå†¯Â·è¯ºä¾æ›¼ï¼ˆ1958ï¼‰æ¥è¯´ï¼Œé€‚å½“çš„éšå–»æ˜¯å½“æ—¶è¿˜å¤„äºèµ·æ­¥é˜¶æ®µçš„æ•°å­—è®¡ç®—æœºã€‚â€œç¥ç»ç½‘ç»œâ€ é¢†åŸŸæ˜¯å¯¹ â€œç±»ç¥ç»å…ƒâ€ å…ƒç´ ç½‘ç»œçš„è®¡ç®—å±æ€§å’Œè¡Œä¸ºçš„ç ”ç©¶ã€‚å®ƒä»‹äºç¥ç»ç”Ÿç‰©å­¦æ¨¡å‹å’Œå¤§è„‘å¦‚ä½•è®¡ç®—çš„éšå–»ä¹‹é—´ã€‚å®ƒå—ä¸¤ä¸ªç›®æ ‡çš„å¯å‘ï¼šç†è§£ç¥ç»ç”Ÿç‰©å­¦çš„å·¥ä½œåŸç†ï¼Œä»¥åŠç†è§£å¦‚ä½•è§£å†³ç¥ç»ç”Ÿç‰©å­¦èƒ½å¤Ÿå¿«é€Ÿè½»æ¾è§£å†³è€Œå½“å‰æ•°å­—æœºå™¨å´å¾ˆéš¾è§£å†³çš„é—®é¢˜ã€‚\nMost physicists will find it obvious that understanding biology might help in engineering. The obverse engineering-toward-biological link can be made by testing a circuit of â€œmodel neuronsâ€ on a difficult real-world problem such as oral word recognition. If the â€œneural circuitâ€ with some particular biological feature is capable of solving a real problem which circuits without that feature solve poorly, the plausibility that the biological feature selected is computationally useful in biology is bolstered. If not, then it is more plausible that the feature can be dispensed with in modeling biology. These are not strong arguments, but they do provide an approach to finding out what, of the myriad of details in neurobiology, is truly important and what is merely true. The study of a 1950 digital computer, in the spirit of neurobiology, would have a strong commitment to studying BaO, then the material of vacuum tube cathodes. The study of the digital computer in 1998 would have a strong commitment to SiO2, the essential insulating material below each gate. Yet the computing structure of the two machines could be identical, hidden amongst the lowest levels of detail. The study of â€œartificial neural networksâ€ in the spirit of biology will relate to aspects of how neurobiology computes in the same sense that understanding the computer of 1998 relates to understanding the computer of 1950.\nå¤§å¤šæ•°ç‰©ç†å­¦å®¶ä¼šå‘ç°ï¼Œç†è§£ç”Ÿç‰©å­¦å¯èƒ½æœ‰åŠ©äºå·¥ç¨‹å­¦æ˜¯æ˜¾è€Œæ˜“è§çš„ã€‚å¯ä»¥é€šè¿‡åœ¨å£è¯­è¯è¯†åˆ«ç­‰å›°éš¾çš„ç°å®é—®é¢˜ä¸Šæµ‹è¯• â€œæ¨¡å‹ç¥ç»å…ƒâ€ ç”µè·¯æ¥å»ºç«‹åå‘çš„å·¥ç¨‹æœå‘ç”Ÿç‰©å­¦çš„è”ç³»ã€‚å¦‚æœå…·æœ‰æŸäº›ç‰¹å®šç”Ÿç‰©ç‰¹å¾çš„ â€œç¥ç»ç”µè·¯â€ èƒ½å¤Ÿè§£å†³æ²¡æœ‰è¯¥ç‰¹å¾çš„ç”µè·¯è§£å†³å¾—å¾ˆå·®çš„å®é™…é—®é¢˜ï¼Œé‚£ä¹ˆæ‰€é€‰æ‹©çš„ç”Ÿç‰©ç‰¹å¾åœ¨ç”Ÿç‰©å­¦ä¸­å…·æœ‰è®¡ç®—ç”¨é€”çš„å¯ä¿¡åº¦å°±ä¼šå¢å¼ºã€‚åä¹‹ï¼Œåˆ™æ›´æœ‰å¯èƒ½åœ¨æ¨¡æ‹Ÿç”Ÿç‰©å­¦æ—¶å¯ä»¥çœç•¥è¯¥ç‰¹å¾ã€‚è¿™äº›ä¸æ˜¯å¼ºæœ‰åŠ›çš„è®ºæ®ï¼Œä½†å®ƒä»¬ç¡®å®æä¾›äº†ä¸€ç§æ–¹æ³•æ¥æ‰¾å‡ºç¥ç»ç”Ÿç‰©å­¦ä¸­æ— æ•°ç»†èŠ‚ä¸­å“ªäº›æ˜¯çœŸæ­£é‡è¦çš„ï¼Œå“ªäº›åªæ˜¯æ­£ç¡®çš„ã€‚ä»¥ç¥ç»ç”Ÿç‰©å­¦çš„ç²¾ç¥ç ”ç©¶ 1950 å¹´çš„æ•°å­—è®¡ç®—æœºï¼Œå°†å¼ºçƒˆè‡´åŠ›äºç ”ç©¶ BaO(å½“æ—¶æ˜¯çœŸç©ºç®¡é˜´æçš„ææ–™)ã€‚1998å¹´å¯¹æ•°å­—è®¡ç®—æœºçš„ç ”ç©¶å°†å¼ºçƒˆè‡´åŠ›äº SiO2(æ¯ä¸ªé—¨ä¸‹æ–¹çš„åŸºæœ¬ç»ç¼˜ææ–™)ã€‚ç„¶è€Œï¼Œè¿™ä¸¤å°æœºå™¨çš„è®¡ç®—ç»“æ„å¯èƒ½æ˜¯ç›¸åŒçš„ï¼Œéšè—åœ¨æœ€ä½çº§åˆ«çš„ç»†èŠ‚ä¸­ã€‚ä»¥ç”Ÿç‰©å­¦ç²¾ç¥ç ”ç©¶ â€œäººå·¥ç¥ç»ç½‘ç»œâ€ å°†ä¸ç†è§£ 1998 å¹´è®¡ç®—æœºä¸ç†è§£ 1950 å¹´è®¡ç®—æœºç›¸å…³è”ï¼Œå°±åƒç†è§£ 1998 å¹´è®¡ç®—æœºä¸ç†è§£ 1950 å¹´è®¡ç®—æœºä¸€æ ·ã€‚\nBRAIN AS A COMPUTER A digital machine can be programmed to compare a present image with a three-dimensional representation of a person, and thus the problem of recognizing a friend can be solved by a computation. Similarly, how to drive the actuators of a robot for a desired motion is a problem in classical mechanics that can be solved on a computer. While we may not know how to write efficient algorithms for these tasks, such examples do illustrate that what the nervous system does might be described as computation.\næ•°å­—æœºå™¨å¯ä»¥è¢«ç¼–ç¨‹ä¸ºå°†å½“å‰å›¾åƒä¸ä¸€ä¸ªäººçš„ä¸‰ç»´è¡¨ç¤ºè¿›è¡Œæ¯”è¾ƒï¼Œå› æ­¤è¯†åˆ«æœ‹å‹çš„é—®é¢˜å¯ä»¥é€šè¿‡è®¡ç®—æ¥è§£å†³ã€‚ç±»ä¼¼åœ°ï¼Œå¦‚ä½•é©±åŠ¨æœºå™¨äººçš„æ‰§è¡Œå™¨ä»¥å®ç°æ‰€éœ€çš„è¿åŠ¨æ˜¯ä¸€ä¸ªç»å…¸åŠ›å­¦é—®é¢˜ï¼Œå¯ä»¥åœ¨è®¡ç®—æœºä¸Šè§£å†³ã€‚è™½ç„¶æˆ‘ä»¬å¯èƒ½ä¸çŸ¥é“å¦‚ä½•ä¸ºè¿™äº›ä»»åŠ¡ç¼–å†™é«˜æ•ˆçš„ç®—æ³•ï¼Œä½†è¿™äº›ä¾‹å­ç¡®å®è¯´æ˜äº†ç¥ç»ç³»ç»Ÿæ‰€åšçš„äº‹æƒ…å¯ä»¥æè¿°ä¸ºè®¡ç®—ã€‚\nFor present purposes, a computer can be viewed as an input-output device, with input and output signals that are in the same format (Hopfield, 1994). Thus in a very simple digital computer, the input is a string of bits (in time), and the output is another string of bits. A million axons carry electrochemical pulses from the eye to the brain. Similar signaling pulses are used to drive the muscles of the vocal tract. When we look at a person and say, â€œHello, Jessica,â€ our brain is producing a complicated transformation from one (parallel) input pulse sequence coming from the eye to another (parallel) output pulse sequence which results in sound waves being generated. The idea of composition is important in this definition. The output of one computer can be used as the input for another computer of the same general type, for they are compatible signals. Within this definition, a digital chip is a computer, and large computers are built as composites of smaller ones. Each neuron is a simple computer according to this definition, and the brain is a large composite computer.\nå¯¹äºç›®å‰çš„ç›®çš„ï¼Œè®¡ç®—æœºå¯ä»¥è¢«è§†ä¸ºä¸€ç§è¾“å…¥è¾“å‡ºè®¾å¤‡ï¼Œå…¶è¾“å…¥å’Œè¾“å‡ºä¿¡å·å…·æœ‰ç›¸åŒçš„æ ¼å¼ï¼ˆHopfieldï¼Œ1994ï¼‰ã€‚å› æ­¤ï¼Œåœ¨ä¸€ä¸ªéå¸¸ç®€å•çš„æ•°å­—è®¡ç®—æœºä¸­ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªæ¯”ç‰¹ä¸²ï¼ˆéšæ—¶é—´å˜åŒ–ï¼‰ï¼Œè¾“å‡ºæ˜¯å¦ä¸€ä¸ªæ¯”ç‰¹ä¸²ã€‚ä¸€ç™¾ä¸‡ä¸ªè½´çªå°†æ¥è‡ªçœ¼ç›çš„ç”µåŒ–å­¦è„‰å†²ä¼ é€åˆ°å¤§è„‘ã€‚ç±»ä¼¼çš„ä¿¡å·è„‰å†²ç”¨äºé©±åŠ¨å‘å£°é“çš„è‚Œè‚‰ã€‚å½“æˆ‘ä»¬çœ‹ç€ä¸€ä¸ªäººå¹¶è¯´ â€œä½ å¥½ï¼Œæ°è¥¿å¡â€ æ—¶ï¼Œæˆ‘ä»¬çš„å¤§è„‘æ­£åœ¨å°†æ¥è‡ªçœ¼ç›çš„ä¸€ä¸ªï¼ˆå¹¶è¡Œï¼‰è¾“å…¥è„‰å†²åºåˆ—è½¬æ¢ä¸ºå¦ä¸€ä¸ªï¼ˆå¹¶è¡Œï¼‰è¾“å‡ºè„‰å†²åºåˆ—ï¼Œä»è€Œäº§ç”Ÿå£°æ³¢ã€‚ç»„åˆçš„æ¦‚å¿µåœ¨è¿™ä¸ªå®šä¹‰ä¸­å¾ˆé‡è¦ã€‚ä¸€ä¸ªè®¡ç®—æœºçš„è¾“å‡ºå¯ä»¥ç”¨ä½œå¦ä¸€ä¸ªç›¸åŒç±»å‹è®¡ç®—æœºçš„è¾“å…¥ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å…¼å®¹çš„ä¿¡å·ã€‚åœ¨è¿™ä¸ªå®šä¹‰ä¸­ï¼Œæ•°å­—èŠ¯ç‰‡æ˜¯ä¸€ä¸ªè®¡ç®—æœºï¼Œè€Œå¤§å‹è®¡ç®—æœºæ˜¯ç”±è¾ƒå°çš„è®¡ç®—æœºç»„æˆçš„å¤åˆä½“ã€‚æ ¹æ®è¿™ä¸ªå®šä¹‰ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½æ˜¯ä¸€ä¸ªç®€å•çš„è®¡ç®—æœºï¼Œè€Œå¤§è„‘åˆ™æ˜¯ä¸€ä¸ªå¤§å‹å¤åˆè®¡ç®—æœºã€‚\nCOMPUTERS AS DYNAMICAL SYSTEMS The operation of a digital machine is most simply illustrated for batch-mode computation. The computer has $N$ storage registers, each storing a single binary bit. The logical state of the machine at a particular time is specified by a vector $10010110000\\cdots$ of $N$ bits. The state changes each clock cycle. The transition map, describing which state follows which, is implicitly built into the machine by its design. The computer can thus be described as a dynamical system that changes its discrete state in discrete time, and the computation is carried out by following a path in state space.\næ•°å­—æœºå™¨çš„æ“ä½œæœ€ç®€å•åœ°é€šè¿‡æ‰¹å¤„ç†è®¡ç®—æ¥è¯´æ˜ã€‚è®¡ç®—æœºæœ‰ $N$ ä¸ªå­˜å‚¨å¯„å­˜å™¨ï¼Œæ¯ä¸ªå¯„å­˜å™¨å­˜å‚¨ä¸€ä¸ªäºŒè¿›åˆ¶ä½ã€‚æœºå™¨åœ¨ç‰¹å®šæ—¶é—´çš„é€»è¾‘çŠ¶æ€ç”±ä¸€ä¸ª $N$ ä½çš„å‘é‡ $10010110000\\cdots$ æŒ‡å®šã€‚çŠ¶æ€åœ¨æ¯ä¸ªæ—¶é’Ÿå‘¨æœŸéƒ½ä¼šæ”¹å˜ã€‚æè¿°å“ªä¸ªçŠ¶æ€è·Ÿéšå“ªä¸ªçŠ¶æ€çš„è½¬æ¢æ˜ å°„æ˜¯é€šè¿‡å…¶è®¾è®¡éšå¼æ„å»ºåˆ°æœºå™¨ä¸­çš„ã€‚å› æ­¤ï¼Œè®¡ç®—æœºå¯ä»¥è¢«æè¿°ä¸ºä¸€ä¸ªåœ¨ç¦»æ•£æ—¶é—´å†…æ”¹å˜å…¶ç¦»æ•£çŠ¶æ€çš„åŠ¨æ€ç³»ç»Ÿï¼Œè®¡ç®—æ˜¯é€šè¿‡åœ¨çŠ¶æ€ç©ºé—´ä¸­è·Ÿéšä¸€æ¡è·¯å¾„æ¥å®Œæˆçš„ã€‚\nThe user of the machine has no control over the dynamics, which is determined by the state transition map. The userâ€™s program, data, and a standard initialization procedure prescribe the starting state of the machine. In a batch-mode computation, the answer is found when a stable point of the discrete dynamical system is reached, a state from which there are no transitions. A particular subset of the state bits (e.g., the contents of a particular machine register) will then describe the desired answer.\næœºå™¨çš„ç”¨æˆ·æ— æ³•æ§åˆ¶åŠ¨æ€ï¼ŒåŠ¨æ€ç”±çŠ¶æ€è½¬æ¢æ˜ å°„å†³å®šã€‚ç”¨æˆ·çš„ç¨‹åºã€æ•°æ®å’Œæ ‡å‡†åˆå§‹åŒ–è¿‡ç¨‹è§„å®šäº†æœºå™¨çš„èµ·å§‹çŠ¶æ€ã€‚åœ¨æ‰¹å¤„ç†è®¡ç®—ä¸­ï¼Œå½“è¾¾åˆ°ç¦»æ•£åŠ¨æ€ç³»ç»Ÿçš„ç¨³å®šç‚¹æ—¶ï¼Œå°±ä¼šæ‰¾åˆ°ç­”æ¡ˆï¼Œå³æ²¡æœ‰è½¬æ¢çš„çŠ¶æ€ã€‚ç„¶åï¼ŒçŠ¶æ€ä½çš„ç‰¹å®šå­é›†ï¼ˆä¾‹å¦‚ï¼Œç‰¹å®šæœºå™¨å¯„å­˜å™¨çš„å†…å®¹ï¼‰å°†æè¿°æ‰€éœ€çš„ç­”æ¡ˆã€‚\nBatch-mode analog computation can be similarly described by using continuous variables and continuous time. The idea of computation as a process carried out by a dynamical system in moving from an initial state to a final state is the same in both cases. In the analog case, the possible motions in state space describe a flow field as in Fig. 1, and computation done by moving with this flow from start to end. (Real â€œdigitalâ€ machines contain only analog components; the digital description is a representation in fewer variables which contains the essence of the continuous dynamics.)\næ‰¹å¤„ç†æ¨¡æ‹Ÿè®¡ç®—ä¹Ÿå¯ä»¥é€šè¿‡ä½¿ç”¨è¿ç»­å˜é‡å’Œè¿ç»­æ—¶é—´æ¥ç±»ä¼¼åœ°æè¿°ã€‚ä½œä¸ºåŠ¨æ€ç³»ç»Ÿé€šè¿‡ä»åˆå§‹çŠ¶æ€ç§»åŠ¨åˆ°æœ€ç»ˆçŠ¶æ€æ¥å®Œæˆçš„è¿‡ç¨‹çš„è®¡ç®—æ¦‚å¿µåœ¨ä¸¤ç§æƒ…å†µä¸‹éƒ½æ˜¯ç›¸åŒçš„ã€‚åœ¨æ¨¡æ‹Ÿæƒ…å†µä¸‹ï¼ŒçŠ¶æ€ç©ºé—´ä¸­çš„å¯èƒ½è¿åŠ¨æè¿°äº†å›¾ 1 ä¸­çš„æµåœºï¼Œè®¡ç®—æ˜¯é€šè¿‡éšè¯¥æµåŠ¨ä»å¼€å§‹åˆ°ç»“æŸæ¥å®Œæˆçš„ã€‚ï¼ˆçœŸæ­£çš„ â€œæ•°å­—â€ æœºå™¨åªåŒ…å«æ¨¡æ‹Ÿç»„ä»¶ï¼›æ•°å­—æè¿°æ˜¯åœ¨æ›´å°‘å˜é‡ä¸­çš„ä¸€ç§è¡¨ç¤ºï¼ŒåŒ…å«äº†è¿ç»­åŠ¨æ€çš„æœ¬è´¨ã€‚ï¼‰\nThe flow field of a simple analog computer. The stable points of the flow, marked by $x$â€™s, are possible answers. To initiate the computation, the initial location in state space must be given. A complex analog computer would have such a flow field in a very large number of dimensions.\nç®€å•æ¨¡æ‹Ÿè®¡ç®—æœºçš„æµåœºã€‚æµçš„ç¨³å®šç‚¹ç”± $x$ æ ‡è®°ï¼Œæ˜¯å¯èƒ½çš„ç­”æ¡ˆã€‚è¦å¯åŠ¨è®¡ç®—ï¼Œå¿…é¡»ç»™å‡ºçŠ¶æ€ç©ºé—´ä¸­çš„åˆå§‹ä½ç½®ã€‚å¤æ‚çš„æ¨¡æ‹Ÿè®¡ç®—æœºä¼šå…·å¤‡ä¸€ä¸ªè¿™æ ·çš„æé«˜ç»´æ•°çš„æµåœºã€‚\nDYNAMICAL MODEL OF NEURAL ACTIVITY The anatomy of a â€œtypicalâ€ neuron in a mammalian brain is sketched in Fig. 2 (Kandel, Schwartz, and Jessell, 1991). It has three major regions: dendrites, a cell body, and an axon. Each cell is connected by structures called synapses with approximately 1000 other cells. Inputs to a cell are made at synapses on its dendrites. The output of that cell is through synapses made by its axon onto the dendrites of other cells. The interior of the neuron is surrounded by a membrane of high resistivity and is filled with a conducting ionic solution. Ion-specific pumps transport ions across the membrane, maintaining an electrical potential difference between the inside and the outside of the cell. Ion-specific channels whose electrical conductivity is voltage dependent and dynamic play a key role in the evolution of the â€œstateâ€ of a neuron.\nå“ºä¹³åŠ¨ç‰©å¤§è„‘ä¸­ â€œå…¸å‹â€ ç¥ç»å…ƒçš„è§£å‰–ç»“æ„å¦‚å›¾ 2 æ‰€ç¤ºï¼ˆKandelã€Schwartz å’Œ Jessellï¼Œ1991ï¼‰ã€‚å®ƒæœ‰ä¸‰ä¸ªä¸»è¦åŒºåŸŸï¼šæ ‘çªã€ç»†èƒä½“å’Œè½´çªã€‚æ¯ä¸ªç»†èƒé€šè¿‡ç§°ä¸ºçªè§¦çš„ç»“æ„ä¸å¤§çº¦ 1000 ä¸ªå…¶ä»–ç»†èƒè¿æ¥ã€‚å¯¹ç»†èƒçš„è¾“å…¥æ˜¯åœ¨å…¶æ ‘çªä¸Šçš„çªè§¦å¤„è¿›è¡Œçš„ã€‚è¯¥ç»†èƒçš„è¾“å‡ºæ˜¯é€šè¿‡å…¶è½´çªåœ¨å…¶ä»–ç»†èƒçš„æ ‘çªä¸Šå½¢æˆçš„çªè§¦ã€‚ç¥ç»å…ƒçš„å†…éƒ¨è¢«é«˜ç”µé˜»ç‡çš„è†œåŒ…å›´ï¼Œå¹¶å……æ»¡äº†å¯¼ç”µç¦»å­æº¶æ¶²ã€‚ç¦»å­ç‰¹å¼‚æ€§æ³µå°†ç¦»å­è¿è¾“ç©¿è¿‡è†œï¼Œç»´æŒç»†èƒå†…å¤–ä¹‹é—´çš„ç”µä½å·®ã€‚ç”µå¯¼ç‡ä¾èµ–äºç”µå‹ä¸”åŠ¨æ€å˜åŒ–çš„ç¦»å­ç‰¹å¼‚æ€§é€šé“åœ¨ç¥ç»å…ƒ â€œçŠ¶æ€â€ çš„æ¼”å˜ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚\nA sketch of a neuron and its style of interconnections. Axons may be as long as many centimeters, though most are on the scale of a millimeter. Typical cell bodies are a few microns in diameter.\nç¥ç»å…ƒåŠå…¶äº’è¿æ–¹å¼çš„ç¤ºæ„å›¾ã€‚è½´çªå¯èƒ½é•¿è¾¾æ•°å˜ç±³ï¼Œå°½ç®¡å¤§å¤šæ•°è½´çªçš„é•¿åº¦åœ¨æ¯«ç±³çº§åˆ«ã€‚å…¸å‹çš„ç»†èƒä½“ç›´å¾„ä¸ºå‡ å¾®ç±³ã€‚\nA simple â€œintegrate and fireâ€ model captures much of the mathematics of what a compact nerve cell does (Junge, 1981). Figure 3 shows the time-dependent voltage difference between the inside and the outside of a simple functioning neuron. The electrical potential is generally slowly changing, but occasionally there is a stereotype voltage spike of about two milliseconds duration. Such a spike is produced every time the interior potential of this cell rises above a threshold, $u_{\\text{thresh}}$, of about 53 millivolts. The voltage then resets to a $u_{\\text{reset}}$ of about 270 millivolts. This â€œaction potentialâ€ spike is caused by the dynamics of voltage-dependent ionic conductivities in the cell membrane. If an electrical current is injected into the cell, then except for the action potentials, the interior potential approximately obeys\n$$ C\\frac{\\mathrm{d}u}{\\mathrm{d}t} = -\\frac{u-u_{\\text{rest}}}{R} + i(t) $$\nwhere $R$ is the resistance of the cell membrane, $C$ the capacitance of the cell membrane, and $u_{\\text{rest}}$ is the potential to which the cell tends to drift. If $i(t)$ is a constant $i_{c}$ , then the cell potential will change in an almost linear fashion between $u_{\\text{rest}}$ and $u_{\\text{thresh}}$ . An action potential will be generated each time $u_{\\text{thresh}}$ is reached, resetting $u$ to $u_{\\text{reset}}$ similar to what is seen in Fig. 3. The time $P$ between the equally spaced action potentials when $R$ is very large is\n$$ P = C\\frac{u_{\\text{thresh}} - u_{\\text{rest}}}{i_{c}},\\quad \\text{or firing rate}\\quad \\frac{1}{P}\\sim i_{c} $$\nIf $i_{c}$ is negative, no action potentials will be produced. The firing rate $1/P$ of a more realistic cell is not simply linear in $i_{c}$, but asymptotes to a maximum value of about 500 per second (due to the finite time duration of action potentials). It may also have a nonzero threshold current due to leakage currents (of either sign) in the membrane.\nä¸€ä¸ªç®€å•çš„ â€œç§¯åˆ†å’Œå‘å°„â€ æ¨¡å‹æ•æ‰äº†ç´§å‡‘ç¥ç»ç»†èƒæ‰€åšçš„è®¸å¤šæ•°å­¦å†…å®¹ï¼ˆJungeï¼Œ1981ï¼‰ã€‚å›¾ 3 æ˜¾ç¤ºäº†ç®€å•åŠŸèƒ½ç¥ç»å…ƒå†…å¤–ä¹‹é—´çš„æ—¶å˜ç”µå‹å·®ã€‚ç”µä½é€šå¸¸ç¼“æ…¢å˜åŒ–ï¼Œä½†å¶å°”ä¼šå‡ºç°å¤§çº¦ä¸¤æ¯«ç§’æŒç»­æ—¶é—´çš„å…¸å‹ç”µå‹å³°å€¼ã€‚æ¯å½“è¯¥ç»†èƒçš„å†…éƒ¨ç”µä½ä¸Šå‡åˆ°å¤§çº¦ 53 æ¯«ä¼çš„é˜ˆå€¼ $u_{\\text{thresh}}$ ä»¥ä¸Šæ—¶ï¼Œå°±ä¼šäº§ç”Ÿè¿™æ ·çš„å³°å€¼ã€‚ç„¶åï¼Œç”µå‹é‡ç½®ä¸ºå¤§çº¦ 270 æ¯«ä¼çš„ $u_{\\text{reset}}$ã€‚è¿™ç§ â€œåŠ¨ä½œç”µä½â€ å³°å€¼æ˜¯ç”±ç»†èƒè†œä¸­ç”µå‹ä¾èµ–æ€§ç¦»å­ç”µå¯¼çš„åŠ¨æ€å¼•èµ·çš„ã€‚å¦‚æœå‘ç»†èƒæ³¨å…¥ç”µæµï¼Œé‚£ä¹ˆé™¤äº†åŠ¨ä½œç”µä½å¤–ï¼Œç»†èƒå†…éƒ¨ç”µä½å¤§è‡´éµå¾ª\n$$ C\\frac{\\mathrm{d}u}{\\mathrm{d}t} = -\\frac{u-u_{\\text{rest}}}{R} + i(t) $$\nå…¶ä¸­ $R$ æ˜¯ç»†èƒè†œçš„ç”µé˜»ï¼Œ$C$ æ˜¯ç»†èƒè†œçš„ç”µå®¹ï¼Œ$u_{\\text{rest}}$ æ˜¯ç»†èƒå€¾å‘äºæ¼‚ç§»åˆ°çš„ç”µä½ã€‚å¦‚æœ $i(t)$ æ˜¯ä¸€ä¸ªå¸¸æ•° $i_{c}$ï¼Œé‚£ä¹ˆç»†èƒç”µä½å°†åœ¨ $u_{\\text{rest}}$ å’Œ $u_{\\text{thresh}}$ ä¹‹é—´ä»¥å‡ ä¹çº¿æ€§çš„æ–¹å¼å˜åŒ–ã€‚æ¯æ¬¡è¾¾åˆ° $u_{\\text{thresh}}$ æ—¶éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªåŠ¨ä½œç”µä½ï¼Œå°† $u$ é‡ç½®ä¸ºç±»ä¼¼äºå›¾ 3 ä¸­æ‰€ç¤ºçš„ $u_{\\text{reset}}$ã€‚å½“ $R$ éå¸¸å¤§æ—¶ï¼Œç­‰é—´éš”åŠ¨ä½œç”µä½ä¹‹é—´çš„æ—¶é—´ $P$ ä¸º\n$$ P = C\\frac{u_{\\text{thresh}} - u_{\\text{rest}}}{i_{c}},\\quad \\text{or firing rate}\\quad \\frac{1}{P}\\sim i_{c} $$\nå¦‚æœ $i_{c}$ ä¸ºè´Ÿï¼Œåˆ™ä¸ä¼šäº§ç”ŸåŠ¨ä½œç”µä½ã€‚æ›´ç°å®çš„ç»†èƒçš„å‘å°„ç‡ $1/P$ å¹¶ä¸æ˜¯ç®€å•åœ°ä¸ $i_{c}$ æˆçº¿æ€§å…³ç³»ï¼Œè€Œæ˜¯æ¸è¿‘äºæ¯ç§’çº¦ 500 çš„æœ€å¤§å€¼ï¼ˆç”±äºåŠ¨ä½œç”µä½çš„æœ‰é™æ—¶é—´æŒç»­ï¼‰ã€‚å®ƒä¹Ÿå¯èƒ½ç”±äºè†œä¸­çš„æ³„æ¼ç”µæµï¼ˆæ— è®ºç¬¦å·å¦‚ä½•ï¼‰è€Œå…·æœ‰éé›¶é˜ˆå€¼ç”µæµã€‚\nAction potentials will be taken to be $\\delta$ functions, lasting a negligible time. They propagate at constant velocity along axons. When an action potential arrives at a synaptic terminal, it releases a neurotransmitter which activates specific ionic conductivity channels in the postsynaptic dendrite to which it makes contact (Kandel, Schwartz, and Jessell, 1991). This short conductivity pulse can be modeled by\n$$ \\begin{aligned} \\sigma(t) \u0026= 0\\quad \u0026tt_{0} \\end{aligned} $$\nThe maximum conductivity of the postsynaptic membrane in response to the action potential is $s$. The ionspecific current which flows is equal to the chemical potential difference $V_{\\text{ion}}$ times $s(t)$. Thus at a synapse from cell $j$ to cell $k$, an action potential arriving on axon $j$ at time $t_{0}$ results in a current\n$$ \\begin{aligned} i_{kj}(t) \u0026= 0\\quad \u0026tt_{0} \\end{aligned} $$\nflows into the cell $k$. The parameter $S_{kj} = V_{\\text{ion}}s_{kj}$ can take either sign. Define the instantaneous firing rate of neuron $k$, which generates action potentials at times $t_{n}^{k}$, as\n$$ f_{k}(t) = \\sum_{n}\\delta(t-t_{n}^{k}) $$\nBy differentiation and substitution\n$$ \\frac{\\mathrm{d}i_{k}}{\\mathrm{d}t} = -\\frac{i_{k}}{\\tau} + \\sum_{j}S_{kj} \\cdot f_{j}(t) + \\text{another term if a sensory cell} $$\nThis equation, though exact, is awkward to deal with because the times at which the action potentials occur are only implicitly given.\nåŠ¨ä½œç”µä½å°†è¢«è§†ä¸ºæŒç»­æ—¶é—´å¯å¿½ç•¥çš„ $\\delta$ å‡½æ•°ã€‚å®ƒä»¬ä»¥æ’å®šé€Ÿåº¦æ²¿è½´çªä¼ æ’­ã€‚å½“åŠ¨ä½œç”µä½åˆ°è¾¾çªè§¦æœ«ç«¯æ—¶ï¼Œå®ƒä¼šé‡Šæ”¾ä¸€ç§ç¥ç»é€’è´¨ï¼Œæ¿€æ´»ä¸å…¶æ¥è§¦çš„çªè§¦åæ ‘çªä¸­çš„ç‰¹å®šç¦»å­ç”µå¯¼é€šé“ï¼ˆKandelã€Schwartz å’Œ Jessellï¼Œ1991ï¼‰ã€‚è¿™ç§çŸ­æš‚çš„ç”µå¯¼è„‰å†²å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å»ºæ¨¡\n$$ \\begin{aligned} \\sigma(t) \u0026= 0\\quad \u0026tt_{0} \\end{aligned} $$\nçªè§¦åè†œå¯¹åŠ¨ä½œç”µä½çš„æœ€å¤§ç”µå¯¼ä¸º $s$ã€‚æµåŠ¨çš„ç¦»å­ç‰¹å¼‚æ€§ç”µæµç­‰äºåŒ–å­¦åŠ¿å·® $V_{\\text{ion}}$ ä¹˜ä»¥ $s(t)$ã€‚å› æ­¤ï¼Œåœ¨ä»ç»†èƒ $j$ åˆ°ç»†èƒ $k$ çš„çªè§¦å¤„ï¼Œåœ¨æ—¶é—´ $t_{0}$ åœ¨è½´çª $j$ ä¸Šåˆ°è¾¾çš„åŠ¨ä½œç”µä½ä¼šå¯¼è‡´ä¸€ä¸ªç”µæµ\n$$ \\begin{aligned} i_{kj}(t) \u0026= 0\\quad \u0026tt_{0} \\end{aligned} $$\næµå…¥ç»†èƒ $k$ã€‚å‚æ•° $S_{kj} = V_{\\text{ion}}s_{kj}$ å¯ä»¥å–ä»»æ„ç¬¦å·ã€‚å®šä¹‰ç¥ç»å…ƒ $k$ çš„ç¬æ—¶å‘å°„ç‡ï¼Œè¯¥ç¥ç»å…ƒåœ¨æ—¶é—´ $t_{n}^{k}$ äº§ç”ŸåŠ¨ä½œç”µä½ï¼Œä¸º\n$$ f_{k}(t) = \\sum_{n}\\delta(t-t_{n}^{k}) $$\né€šè¿‡å¾®åˆ†å’Œæ›¿æ¢\n$$ \\frac{\\mathrm{d}i_{k}}{\\mathrm{d}t} = -\\frac{i_{k}}{\\tau} + \\sum_{j}S_{kj} \\cdot f_{j}(t) + \\text{another term if a sensory cell} $$\nè¿™ä¸ªæ–¹ç¨‹è™½ç„¶æ˜¯ç²¾ç¡®çš„ï¼Œä½†å¤„ç†èµ·æ¥å¾ˆå°´å°¬ï¼Œå› ä¸ºåŠ¨ä½œç”µä½å‘ç”Ÿçš„æ—¶é—´åªæ˜¯éšå¼ç»™å‡ºçš„ã€‚\nThe usual approximation relies on there being many contributions to the sum in Eq. (6) during a reasonable time interval due to the high connectivity. It should then be permissible to ignore the spiky nature of $f_{j}(t)$, replacing it by a convolution with a smoothing function. In addition, the functional form of $V(i_{c})$ is presumed to hold when $i_{c}$ is slowly varying in time. What results is like Eq. (6), but with fj(t) now a smooth function given by $f_{j}(t) = V[i_{j}(t)] = V[i_{j}(t)]$:\n$$ \\frac{\\mathrm{d}i_{k}}{\\mathrm{d}t} = -\\frac{i_{k}}{\\tau} + \\sum_{j}S_{kj} \\cdot V[i_{j}(t)] + I_{k}(\\text{last term only if a sensory cell}) $$\nThe main effect of the approximation, which assumes no strong correlations between spikes of different neurons, is to neglect shot noise. (Electrical circuits using continuous variables are based on a similar approximation.) While equations of this structure are in common use, they have somewhat evolved, and do not have a sharp original reference.\né€šå¸¸çš„è¿‘ä¼¼ä¾èµ–äºç”±äºé«˜è¿æ¥æ€§ï¼Œåœ¨åˆç†çš„æ—¶é—´é—´éš”å†…å¯¹æ–¹ç¨‹ï¼ˆ6ï¼‰ä¸­çš„æ€»å’Œæœ‰è®¸å¤šè´¡çŒ®ã€‚ç„¶ååº”è¯¥å…è®¸å¿½ç•¥ $f_{j}(t)$ çš„å°–å³°æ€§è´¨ï¼Œç”¨ä¸å¹³æ»‘å‡½æ•°çš„å·ç§¯æ¥æ›¿ä»£å®ƒã€‚æ­¤å¤–ï¼Œå½“ $i_{c}$ éšæ—¶é—´ç¼“æ…¢å˜åŒ–æ—¶ï¼Œå‡å®š $V(i_{c})$ çš„å‡½æ•°å½¢å¼æˆç«‹ã€‚ç»“æœç±»ä¼¼äºæ–¹ç¨‹ï¼ˆ6ï¼‰ï¼Œä½†ç°åœ¨ fj(t) æ˜¯ç”± $f_{j}(t) = V[i_{j}(t)] = V[i_{j}(t)]$ ç»™å‡ºçš„å¹³æ»‘å‡½æ•°ï¼š\n$$ \\frac{\\mathrm{d}i_{k}}{\\mathrm{d}t} = -\\frac{i_{k}}{\\tau} + \\sum_{j}S_{kj} \\cdot V[i_{j}(t)] + I_{k}(\\text{last term only if a sensory cell}) $$\nè¯¥è¿‘ä¼¼çš„ä¸»è¦æ•ˆæœæ˜¯å‡è®¾ä¸åŒç¥ç»å…ƒçš„å³°å€¼ä¹‹é—´æ²¡æœ‰å¼ºç›¸å…³æ€§ï¼Œä»è€Œå¿½ç•¥äº†æ•£å¼¹å™ªå£°ã€‚ï¼ˆä½¿ç”¨è¿ç»­å˜é‡çš„ç”µè·¯åŸºäºç±»ä¼¼çš„è¿‘ä¼¼ã€‚ï¼‰è™½ç„¶è¿™ç§ç»“æ„çš„æ–¹ç¨‹è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†å®ƒä»¬å·²ç»æœ‰æ‰€å‘å±•ï¼Œå¹¶ä¸”æ²¡æœ‰æ˜ç¡®çš„åŸå§‹å‚è€ƒæ–‡çŒ®ã€‚\nTHE DYNAMICS OF SYNAPSES The second dynamical equation for neuronal dynamics describes the changes in the synaptic connections. In neurobiology, a synapse can modify its strength or its temporary behavior in the following ways:\nAs a result of the activity of its presynaptic neuron, independent of the activity of the postsynaptic neuron. As a result of the activity of its postsynaptic neuron, independent of the activity of the presynaptic neuron. As a result of the coordinated activity of the preand postsynaptic neurons. As a result of the regional release of a neuromodulator. Neuromodulators also can modulate processes 1, 2, and 3. The most interesting of these is (3) in which the synapse strength $S_{kj}$ changes as a result of the roughly simultaneous activity of cells $k$ and $j$. This kind of change is needed to represent information about the association between two events. A synapse whose change algorithm involves only the simultaneous activity of the pre- and postsynaptic neurons and no other detailed information is now called a Hebbian synapse (Hebb, 1949). A simple version of such dynamics (using firing rates rather than detailed times of individual action potentials) might be written\n$$ \\frac{\\mathrm{d}S_{kj}}{\\mathrm{d}t} = \\alpha \\cdot i_{k} \\cdot f_{j}(t) - \\text{decay terms} $$\nDecay terms, perhaps involving $i_{k}$ and $f(i_{j})$, are essential to forget old information. A nonlinearity or control process is important to keep synapse strength from increasing without bound. The learning rate a might also be varied by neuromodulator molecules which control the overall learning process. The details of synaptic modification biophysics are not completely established, and Eq. (8) is only qualitative. A somewhat better approximation replaces a by a kernel over time and involves a more complicated form in $i$ and $f$. Long-term potentiation (LTP) is the most significant paradigm of neurobiological synapse modification (Kandel, Schwartz, and Jessell, 1991). Synapse change rules of a Hebbian type have been found to reproduce results of a variety of experiments on the development of the eye dominance and orientation selectivity of cells in the visual cortex of the cat (Bear, Cooper, and Ebner, 1987).\nç¥ç»åŠ¨åŠ›å­¦çš„ç¬¬äºŒä¸ªåŠ¨æ€æ–¹ç¨‹æè¿°äº†çªè§¦è¿æ¥çš„å˜åŒ–ã€‚åœ¨ç¥ç»ç”Ÿç‰©å­¦ä¸­ï¼Œçªè§¦å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿®æ”¹å…¶å¼ºåº¦æˆ–ä¸´æ—¶è¡Œä¸ºï¼š\nä½œä¸ºå…¶çªè§¦å‰ç¥ç»å…ƒæ´»åŠ¨çš„ç»“æœï¼Œä¸çªè§¦åç¥ç»å…ƒçš„æ´»åŠ¨æ— å…³ã€‚ ä½œä¸ºå…¶çªè§¦åç¥ç»å…ƒæ´»åŠ¨çš„ç»“æœï¼Œä¸çªè§¦å‰ç¥ç»å…ƒçš„æ´»åŠ¨æ— å…³ã€‚ ä½œä¸ºçªè§¦å‰å’Œçªè§¦åç¥ç»å…ƒåè°ƒæ´»åŠ¨çš„ç»“æœã€‚ ä½œä¸ºç¥ç»è°ƒèŠ‚å‰‚åŒºåŸŸé‡Šæ”¾çš„ç»“æœã€‚ç¥ç»è°ƒèŠ‚å‰‚è¿˜å¯ä»¥è°ƒèŠ‚è¿‡ç¨‹ 1ã€2 å’Œ 3ã€‚ å…¶ä¸­æœ€æœ‰è¶£çš„æ˜¯ï¼ˆ3ï¼‰ï¼Œå…¶ä¸­çªè§¦å¼ºåº¦ $S_{kj}$ ç”±äºç»†èƒ $k$ å’Œ $j$ çš„å¤§è‡´åŒæ—¶æ´»åŠ¨è€Œå‘ç”Ÿå˜åŒ–ã€‚è¿™ç§å˜åŒ–å¯¹äºè¡¨ç¤ºä¸¤ä¸ªäº‹ä»¶ä¹‹é—´å…³è”çš„ä¿¡æ¯æ˜¯å¿…è¦çš„ã€‚å…¶å˜åŒ–ç®—æ³•ä»…æ¶‰åŠçªè§¦å‰å’Œçªè§¦åç¥ç»å…ƒçš„åŒæ—¶æ´»åŠ¨ä¸”æ²¡æœ‰å…¶ä»–è¯¦ç»†ä¿¡æ¯çš„çªè§¦ç°åœ¨ç§°ä¸º Hebbian çªè§¦ï¼ˆHebbï¼Œ1949ï¼‰ã€‚è¿™ç§åŠ¨åŠ›å­¦çš„ä¸€ä¸ªç®€å•ç‰ˆæœ¬ï¼ˆä½¿ç”¨å‘å°„ç‡è€Œä¸æ˜¯å•ä¸ªåŠ¨ä½œç”µä½çš„è¯¦ç»†æ—¶é—´ï¼‰å¯ä»¥å†™æˆ\n$$ \\frac{\\mathrm{d}S_{kj}}{\\mathrm{d}t} = \\alpha \\cdot i_{k} \\cdot f_{j}(t) - \\text{decay terms} $$\nè¡°å‡é¡¹ï¼Œå¯èƒ½æ¶‰åŠ $i_{k}$ å’Œ $f(i_{j})$ï¼Œå¯¹äºå¿˜è®°æ—§ä¿¡æ¯æ˜¯å¿…ä¸å¯å°‘çš„ã€‚éçº¿æ€§æˆ–æ§åˆ¶è¿‡ç¨‹å¯¹äºé˜²æ­¢çªè§¦å¼ºåº¦æ— é™å¢åŠ éå¸¸é‡è¦ã€‚å­¦ä¹ ç‡ a ä¹Ÿå¯èƒ½é€šè¿‡æ§åˆ¶æ•´ä½“å­¦ä¹ è¿‡ç¨‹çš„ç¥ç»è°ƒèŠ‚åˆ†å­æ¥å˜åŒ–ã€‚çªè§¦ä¿®æ”¹ç”Ÿç‰©ç‰©ç†å­¦çš„ç»†èŠ‚å°šæœªå®Œå…¨ç¡®å®šï¼Œæ–¹ç¨‹ï¼ˆ8ï¼‰åªæ˜¯å®šæ€§çš„ã€‚ä¸€ä¸ªç¨å¾®å¥½ä¸€ç‚¹çš„è¿‘ä¼¼æ˜¯ç”¨æ—¶é—´å†…çš„æ ¸æ›¿æ¢ aï¼Œå¹¶æ¶‰åŠ $i$ å’Œ $f$ ä¸­æ›´å¤æ‚çš„å½¢å¼ã€‚é•¿æœŸå¢å¼ºï¼ˆLTPï¼‰æ˜¯ç¥ç»ç”Ÿç‰©å­¦çªè§¦ä¿®æ”¹çš„æœ€é‡è¦èŒƒä¾‹ï¼ˆKandelã€Schwartz å’Œ Jessellï¼Œ1991ï¼‰ã€‚å·²ç»å‘ç° Hebbian ç±»å‹çš„çªè§¦å˜åŒ–è§„åˆ™å¯ä»¥é‡ç°å¯¹çŒ«è§†è§‰çš®å±‚ä¸­ç»†èƒçš„çœ¼ç›ä¼˜åŠ¿å’Œæ–¹å‘é€‰æ‹©æ€§çš„å„ç§å®éªŒç»“æœï¼ˆBearã€Cooper å’Œ Ebnerï¼Œ1987ï¼‰ã€‚\nThe tacit assumption is often made that synapse change is involved in learning and development, and that the dynamics of neural activity is what performs a computation. However, the dynamics of synapse modification should not be ignored as a possible tool for doing computation.\né€šå¸¸å‡è®¾çªè§¦å˜åŒ–æ¶‰åŠå­¦ä¹ å’Œå‘å±•ï¼Œè€Œç¥ç»æ´»åŠ¨çš„åŠ¨æ€æ‰§è¡Œè®¡ç®—ã€‚ç„¶è€Œï¼Œä¸åº”å¿½è§†çªè§¦ä¿®æ”¹çš„åŠ¨æ€ä½œä¸ºè¿›è¡Œè®¡ç®—çš„å¯èƒ½å·¥å…·ã€‚\nPROGRAMMING LANGUAGES FOR ARTIFICIAL NEURAL NETWORKS (ANN) Let batch-mode computation, simple (point) attractor dynamics, and fixed connections be our initial â€œneural networkâ€ computing paradigm. The connections need to be chosen so that for any input (â€œdataâ€) the network activity will go to a stable state, and so that the state achieved from a given input is the desired â€œanswer.â€ Is there a programming language? The simplest approaches to this issue involve establishing an overall architecture or â€œanatomyâ€ for the network which guarantees going to a stable state. Within this given architecture, the connections can be arbitrarily chosen. â€œProgrammingâ€ involves the â€œinverse problemâ€ of finding the set of connections for which the dynamics will carry out the desired task.\nè®©æ‰¹å¤„ç†è®¡ç®—ã€ç®€å•ï¼ˆç‚¹ï¼‰å¸å¼•å­åŠ¨åŠ›å­¦å’Œå›ºå®šè¿æ¥æˆä¸ºæˆ‘ä»¬æœ€åˆçš„ â€œç¥ç»ç½‘ç»œâ€ è®¡ç®—èŒƒä¾‹ã€‚éœ€è¦é€‰æ‹©è¿æ¥ï¼Œä»¥ä¾¿å¯¹äºä»»ä½•è¾“å…¥ï¼ˆâ€œæ•°æ®â€ï¼‰ï¼Œç½‘ç»œæ´»åŠ¨éƒ½å°†è¿›å…¥ç¨³å®šçŠ¶æ€ï¼Œå¹¶ä¸”ä»ç»™å®šè¾“å…¥è¾¾åˆ°çš„çŠ¶æ€æ˜¯æ‰€éœ€çš„ â€œç­”æ¡ˆâ€ã€‚æ˜¯å¦å­˜åœ¨ä¸€ç§ç¼–ç¨‹è¯­è¨€ï¼Ÿå¯¹æ­¤é—®é¢˜çš„æœ€ç®€å•æ–¹æ³•æ¶‰åŠä¸ºç½‘ç»œå»ºç«‹ä¸€ä¸ªæ•´ä½“æ¶æ„æˆ– â€œè§£å‰–ç»“æ„â€ï¼Œä»¥ä¿è¯è¿›å…¥ç¨³å®šçŠ¶æ€ã€‚åœ¨è¿™ä¸ªç»™å®šçš„æ¶æ„å†…ï¼Œè¿æ¥å¯ä»¥ä»»æ„é€‰æ‹©ã€‚â€œç¼–ç¨‹â€ æ¶‰åŠå¯»æ‰¾ä¸€ç»„è¿æ¥çš„ â€œé€†é—®é¢˜â€ï¼Œå…¶åŠ¨åŠ›å­¦å°†æ‰§è¡Œæ‰€éœ€çš„ä»»åŠ¡ã€‚\nFeed-forward networks The simplest two styles of networks for computation are shown in Fig. 4. The feed-forward network is mathematically like a set of connected nonlinear amplifiers without feedback paths, and is trivially stable.\næœ€ç®€å•çš„ä¸¤ç§è®¡ç®—ç½‘ç»œæ ·å¼å¦‚å›¾ 4 æ‰€ç¤ºã€‚å‰é¦ˆç½‘ç»œåœ¨æ•°å­¦ä¸Šç±»ä¼¼äºä¸€ç»„è¿æ¥çš„éçº¿æ€§æ”¾å¤§å™¨ï¼Œæ²¡æœ‰åé¦ˆè·¯å¾„ï¼Œå¹¶ä¸”æ˜¯å¹³å‡¡ç¨³å®šçš„ã€‚\nTwo extreme forms of neural networks with good stability properties but very different complexities of dynamics and learning. The feedback network can be proved stable if it has symmetric connections. Scaling of variables generates a broad class of networks which are equivalent to symmetric networks, and surprisingly, the feed-forward network can be obtained from a symmetric network by scaling.\nå…·æœ‰è‰¯å¥½ç¨³å®šæ€§ä½†åŠ¨æ€å’Œå­¦ä¹ å¤æ‚æ€§éå¸¸ä¸åŒçš„ä¸¤ç§æç«¯å½¢å¼çš„ç¥ç»ç½‘ç»œã€‚åé¦ˆç½‘ç»œå¦‚æœå…·æœ‰å¯¹ç§°è¿æ¥åˆ™å¯ä»¥è¯æ˜æ˜¯ç¨³å®šçš„ã€‚å˜é‡çš„ç¼©æ”¾ç”Ÿæˆäº†ä¸å¯¹ç§°ç½‘ç»œç­‰æ•ˆçš„å¹¿æ³›ç±»åˆ«çš„ç½‘ç»œï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå‰é¦ˆç½‘ç»œå¯ä»¥é€šè¿‡ç¼©æ”¾ä»å¯¹ç§°ç½‘ç»œä¸­è·å¾—ã€‚\nThis fact allows us to evaluate how much computation must be done to find the stable point. It is:\n$$ (1\\text{ multiply}+1\\text{ add})(\\text{number of connections}) + (\\text{number of â€œneuronsâ€})(1\\text{ look-up}) $$\nè¿™ä¸ªäº‹å®ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯„ä¼°æ‰¾åˆ°ç¨³å®šç‚¹å¿…é¡»è¿›è¡Œå¤šå°‘è®¡ç®—ã€‚å®ƒæ˜¯ï¼š\n$$ (1\\text{ multiply}+1\\text{ add})(\\text{number of connections}) + (\\text{number of â€œneuronsâ€})(1\\text{ look-up}) $$\nThis evaluation requires no dynamics and involves a trivial amount of computation. How is it then that feedforward ANNâ€™s, which have almost no computing power, are very useful even when implemented inefficiently on digital machines? The answer is that their utility comes chiefly from the immense computational work necessary to find an appropriate set of connections for a problem which is implicitly described by a large data base. The resulting network is a compact representation of the data, which allows it to be used with much less computational effort than would otherwise be necessary. The output of the network is merely a function of its input. In this case the problem of finding the best set of connections reduces to finding the set of connections that minimizes output error.\nè¿™ç§è¯„ä¼°ä¸éœ€è¦åŠ¨æ€ï¼Œå¹¶ä¸”æ¶‰åŠå¾®ä¸è¶³é“çš„è®¡ç®—é‡ã€‚é‚£ä¹ˆï¼Œå‡ ä¹æ²¡æœ‰è®¡ç®—èƒ½åŠ›çš„å‰é¦ˆ ANN æ˜¯å¦‚ä½•éå¸¸æœ‰ç”¨çš„ï¼Œå³ä½¿åœ¨æ•°å­—æœºå™¨ä¸Šå®ç°æ•ˆç‡ä½ä¸‹ï¼Ÿç­”æ¡ˆæ˜¯ï¼Œå®ƒä»¬çš„æ•ˆç”¨ä¸»è¦æ¥è‡ªäºä¸ºä¸€ä¸ªç”±å¤§å‹æ•°æ®åº“éšå¼æè¿°çš„é—®é¢˜æ‰¾åˆ°é€‚å½“è¿æ¥é›†æ‰€éœ€çš„å¤§é‡è®¡ç®—å·¥ä½œã€‚ç”±æ­¤äº§ç”Ÿçš„ç½‘ç»œæ˜¯æ•°æ®çš„ç´§å‡‘è¡¨ç¤ºï¼Œè¿™ä½¿å¾—å®ƒå¯ä»¥ä»¥æ¯”å…¶ä»–æ–¹å¼æ‰€éœ€çš„è®¡ç®—å·¥ä½œé‡å°‘å¾—å¤šçš„æ–¹å¼ä½¿ç”¨ã€‚ç½‘ç»œçš„è¾“å‡ºä»…ä»…æ˜¯å…¶è¾“å…¥çš„å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰¾åˆ°æœ€ä½³è¿æ¥é›†çš„é—®é¢˜ç®€åŒ–ä¸ºæ‰¾åˆ°æœ€å°åŒ–è¾“å‡ºè¯¯å·®çš„è¿æ¥é›†ã€‚\nWhen the inputs of all amplifiers are connected by a network to the external inputs and the outputs of the same amplifiers are used as the desired outputs, a feedforward network is said to have â€œno hidden units.â€ If the amplifiers have a continuous input-output relation, a best set of connections can be found by starting with a random set of connections and doing gradient descent on the error function. For most problems, the terrain is relatively smooth, and there is little difficulty of being trapped in poor local minima by doing gradient descent. When the input-output relation is a step function, as it was chosen to be in the perceptron (Rosenblatt, 1962), the problem is somewhat more difficult, but satisfactory algorithms can still be found. An interesting â€œstatistical mechanicsâ€ of their capabilities in random problems has been described (Gardiner, 1988).\nå½“æ‰€æœ‰æ”¾å¤§å™¨çš„è¾“å…¥é€šè¿‡ç½‘ç»œè¿æ¥åˆ°å¤–éƒ¨è¾“å…¥ï¼Œå¹¶ä¸”ç›¸åŒæ”¾å¤§å™¨çš„è¾“å‡ºç”¨ä½œæ‰€éœ€è¾“å‡ºæ—¶ï¼Œå‰é¦ˆç½‘ç»œè¢«ç§°ä¸º â€œæ— éšè—å•å…ƒâ€ã€‚å¦‚æœæ”¾å¤§å™¨å…·æœ‰è¿ç»­çš„è¾“å…¥è¾“å‡ºå…³ç³»ï¼Œåˆ™å¯ä»¥é€šè¿‡ä»éšæœºè¿æ¥é›†å¼€å§‹å¹¶å¯¹è¯¯å·®å‡½æ•°è¿›è¡Œæ¢¯åº¦ä¸‹é™æ¥æ‰¾åˆ°æœ€ä½³è¿æ¥é›†ã€‚å¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼Œåœ°å½¢ç›¸å¯¹å¹³æ»‘ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™é™·å…¥è¾ƒå·®çš„å±€éƒ¨æå°å€¼å‡ ä¹æ²¡æœ‰å›°éš¾ã€‚å½“è¾“å…¥è¾“å‡ºå…³ç³»æ˜¯é˜¶è·ƒå‡½æ•°æ—¶ï¼Œå°±åƒåœ¨æ„ŸçŸ¥å™¨ä¸­é€‰æ‹©çš„é‚£æ ·ï¼ˆRosenblattï¼Œ1962ï¼‰ï¼Œé—®é¢˜å°±æœ‰äº›æ›´éš¾ï¼Œä½†ä»ç„¶å¯ä»¥æ‰¾åˆ°ä»¤äººæ»¡æ„çš„ç®—æ³•ã€‚å·²ç»æè¿°äº†å®ƒä»¬åœ¨éšæœºé—®é¢˜ä¸­çš„èƒ½åŠ›çš„æœ‰è¶£çš„ â€œç»Ÿè®¡åŠ›å­¦â€ï¼ˆGardinerï¼Œ1988ï¼‰ã€‚\nUnfortunately, networks with a single layer of weights are severely limited in the functions they can represent. The detailed description of that limitation by Minsky and Pappert (1969) and â€œour view that the extension [to multiple layers] is sterileâ€ had a great deal to do with destroying a budding perceptron enthusiasm in the 1960s. It was even then clear that networks with hidden units are much more powerful, but the â€œfailure to produce an interesting learning theorem for the multilayered machineâ€ was chilling.\nä¸å¹¸çš„æ˜¯ï¼Œå…·æœ‰å•å±‚æƒé‡çš„ç½‘ç»œåœ¨å®ƒä»¬å¯ä»¥è¡¨ç¤ºçš„å‡½æ•°æ–¹é¢å—åˆ°ä¸¥é‡é™åˆ¶ã€‚Minsky å’Œ Pappertï¼ˆ1969ï¼‰å¯¹è¯¥é™åˆ¶çš„è¯¦ç»†æè¿°ä»¥åŠ â€œæˆ‘ä»¬è®¤ä¸ºæ‰©å±•[åˆ°å¤šå±‚]æ˜¯æ— æ•ˆçš„â€ åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ‘§æ¯äº† 1960 å¹´ä»£èŒèŠ½çš„æ„ŸçŸ¥å™¨çƒ­æƒ…ã€‚å³ä½¿é‚£æ—¶ä¹Ÿå¾ˆæ¸…æ¥šï¼Œå…·æœ‰éšè—å•å…ƒçš„ç½‘ç»œæ›´åŠ å¼ºå¤§ï¼Œä½† â€œæœªèƒ½ä¸ºå¤šå±‚æœºå™¨äº§ç”Ÿæœ‰è¶£çš„å­¦ä¹ å®šç†â€ æ˜¯ä»¤äººå¯’å¿ƒçš„ã€‚\nFor the analog feed-forward ANN with hidden units, the problem of finding the best fit to a desired inputoutput relation is relatively simple since the output can be explicitly written in terms of the inputs and connections. Gradient descent on the error surface in â€œweight spaceâ€ can be carried out, beginning from small random initial connections, to find a locally optimal solution to the problem. This elegant simple point was noted by Werbos (1974), but had no impact at the time, and was independently rediscovered at least twice in the 1980s. A variety of more complex ways to find good sets of connections have since been explored.\nå¯¹äºå…·æœ‰éšè—å•å…ƒçš„æ¨¡æ‹Ÿå‰é¦ˆ ANNï¼Œæ‰¾åˆ°ä¸æ‰€éœ€è¾“å…¥è¾“å‡ºå…³ç³»çš„æœ€ä½³æ‹Ÿåˆçš„é—®é¢˜ç›¸å¯¹ç®€å•ï¼Œå› ä¸ºè¾“å‡ºå¯ä»¥ç”¨è¾“å…¥å’Œè¿æ¥æ˜ç¡®åœ°å†™å‡ºæ¥ã€‚å¯ä»¥åœ¨ â€œæƒé‡ç©ºé—´â€ ä¸­å¯¹è¯¯å·®è¡¨é¢è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œä»å°çš„éšæœºåˆå§‹è¿æ¥å¼€å§‹ï¼Œä»¥æ‰¾åˆ°é—®é¢˜çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚Werbosï¼ˆ1974ï¼‰æ³¨æ„åˆ°äº†è¿™ä¸€ä¼˜é›…è€Œç®€å•çš„è§‚ç‚¹ï¼Œä½†å½“æ—¶æ²¡æœ‰å½±å“ï¼Œå¹¶ä¸”åœ¨ 1980 å¹´ä»£è‡³å°‘è¢«ç‹¬ç«‹é‡æ–°å‘ç°äº†ä¸¤æ¬¡ã€‚æ­¤åï¼Œå·²ç»æ¢ç´¢äº†å„ç§æ›´å¤æ‚çš„æ–¹æ³•æ¥æ‰¾åˆ°è‰¯å¥½çš„è¿æ¥é›†ã€‚\nWhy was the Werbos suggestion not followed up and subsequently lost? Several factors were involved. First, the landscape of the function on which gradient descent is being done is very rugged; local minima abound, and whether a useful network can be found is a computational issue, not a question which can be demonstrated from mathematics. There was little understanding of such landscapes at the time. Worse, the demonstrations that such a simple procedure would actually work consumed an immense amount of computer cycles even in its time (1983-5) and would have been impossibly costly on the computers of 1973. Artificial intelligence was still in full bloom, and no one that was interested in pattern recognition would waste machine cycles on searches in spaces having hundreds of dimensions (parameters) when sheer logic and rules seemed all that was necessary.\nä¸ºä»€ä¹ˆæ²¡æœ‰è·Ÿè¿› Werbos çš„å»ºè®®å¹¶éšåå¤±å»å®ƒï¼Ÿæ¶‰åŠå‡ ä¸ªå› ç´ ã€‚é¦–å…ˆï¼Œæ­£åœ¨è¿›è¡Œæ¢¯åº¦ä¸‹é™çš„å‡½æ•°çš„æ™¯è§‚éå¸¸å´å²–ï¼›å±€éƒ¨æå°å€¼æ¯”æ¯”çš†æ˜¯ï¼Œæ˜¯å¦å¯ä»¥æ‰¾åˆ°æœ‰ç”¨çš„ç½‘ç»œæ˜¯ä¸€ä¸ªè®¡ç®—é—®é¢˜ï¼Œè€Œä¸æ˜¯å¯ä»¥é€šè¿‡æ•°å­¦è¯æ˜çš„é—®é¢˜ã€‚å½“æ—¶å¯¹è¿™ç§æ™¯è§‚å‡ ä¹æ²¡æœ‰äº†è§£ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œè¯æ˜è¿™æ ·ä¸€ä¸ªç®€å•çš„ç¨‹åºå®é™…ä¸Šä¼šèµ·ä½œç”¨ï¼Œå³ä½¿åœ¨å½“æ—¶ï¼ˆ1983-5ï¼‰ä¹Ÿæ¶ˆè€—äº†å¤§é‡çš„è®¡ç®—æœºå‘¨æœŸï¼Œåœ¨ 1973 å¹´çš„è®¡ç®—æœºä¸Šå°†æ˜¯ä¸å¯èƒ½æ‰¿å—çš„æˆæœ¬ã€‚äººå·¥æ™ºèƒ½ä»ç„¶å¤„äºå…¨é¢ç¹è£é˜¶æ®µï¼Œå¯¹æ¨¡å¼è¯†åˆ«æ„Ÿå…´è¶£çš„äººä¸ä¼šåœ¨å…·æœ‰æ•°ç™¾ä¸ªç»´åº¦ï¼ˆå‚æ•°ï¼‰çš„ç©ºé—´ä¸­æµªè´¹æœºå™¨å‘¨æœŸè¿›è¡Œæœç´¢ï¼Œè€Œçº¯ç²¹çš„é€»è¾‘å’Œè§„åˆ™ä¼¼ä¹æ˜¯å¿…è¦çš„ã€‚\nAnd finally, the procedure looks absurd. Consider as a physicist, being told to fit a 200-parameter, highly nonlinear model to 500 data points. (And sometimes, the authors would be fitting 200 parameters to 150 data points!) We were all brought up on the Wignerism â€œif you give me two free parameters, I can describe an elephant. If you give me three, I can make him viggle his tail.â€ We all knew that the parameters would be meaningless. And so they are. Two tries from initially different random starting sets of connections usually wind up with entirely different parameters. For most problems, the connection strengths seem to have little meaning. What is useful in this case, however, is not the connection strengths, but the quality of fit to the data. The situation is entirely different from the usual scientific â€œfitsâ€ to data, normally designed chiefly to derive meaningful parameters.\næœ€åï¼Œè¿™ä¸ªç¨‹åºçœ‹èµ·æ¥å¾ˆè’è°¬ã€‚ä½œä¸ºä¸€ä¸ªç‰©ç†å­¦å®¶ï¼Œè€ƒè™‘è¢«å‘ŠçŸ¥å°†ä¸€ä¸ªå…·æœ‰ 200 ä¸ªå‚æ•°çš„é«˜åº¦éçº¿æ€§æ¨¡å‹æ‹Ÿåˆåˆ° 500 ä¸ªæ•°æ®ç‚¹ã€‚ï¼ˆæœ‰æ—¶ï¼Œä½œè€…ä¼šå°† 200 ä¸ªå‚æ•°æ‹Ÿåˆåˆ° 150 ä¸ªæ•°æ®ç‚¹ï¼ï¼‰æˆ‘ä»¬éƒ½å—è¿‡ Wignerism çš„æ•™è‚² â€œå¦‚æœä½ ç»™æˆ‘ä¸¤ä¸ªè‡ªç”±å‚æ•°ï¼Œæˆ‘å¯ä»¥æè¿°ä¸€åªå¤§è±¡ã€‚å¦‚æœä½ ç»™æˆ‘ä¸‰ä¸ªï¼Œæˆ‘å¯ä»¥è®©å®ƒæ‘‡å°¾å·´ã€‚â€œæˆ‘ä»¬éƒ½çŸ¥é“è¿™äº›å‚æ•°å°†æ¯«æ— æ„ä¹‰ã€‚äº‹å®ç¡®å®å¦‚æ­¤ã€‚ä»æœ€åˆä¸åŒçš„éšæœºèµ·å§‹è¿æ¥é›†è¿›è¡Œçš„ä¸¤æ¬¡å°è¯•é€šå¸¸ä¼šå¾—åˆ°å®Œå…¨ä¸åŒçš„å‚æ•°ã€‚å¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼Œè¿æ¥å¼ºåº¦ä¼¼ä¹æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æœ‰ç”¨çš„ä¸æ˜¯è¿æ¥å¼ºåº¦ï¼Œè€Œæ˜¯ä¸æ•°æ®çš„æ‹Ÿåˆè´¨é‡ã€‚è¿™ç§æƒ…å†µä¸é€šå¸¸æ—¨åœ¨ä¸»è¦æ¨å¯¼æœ‰æ„ä¹‰å‚æ•°çš„æ•°æ® â€œæ‹Ÿåˆâ€ å®Œå…¨ä¸åŒã€‚\nFeed-forward networks with hidden units have been successfully applied to evaluating loan applications, pap smear classification, optical character recognition, protein folding prediction, adjusting telescope mirrors, and playing backgammon. The nature of the features must be carefully chosen. The choice of network size and structure is important to success (Bishop, 1995) particularly when generalization is the important aspect of the problem (i.e., responding appropriately to a new input pattern that is not one on which the network was trained).\nå…·æœ‰éšè—å•å…ƒçš„å‰é¦ˆç½‘ç»œå·²æˆåŠŸåº”ç”¨äºè¯„ä¼°è´·æ¬¾ç”³è¯·ã€å­å®«é¢ˆæŠ¹ç‰‡åˆ†ç±»ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ã€è›‹ç™½è´¨æŠ˜å é¢„æµ‹ã€è°ƒæ•´æœ›è¿œé•œé•œå­å’Œç©åŒé™†æ£‹ã€‚å¿…é¡»ä»”ç»†é€‰æ‹©ç‰¹å¾çš„æ€§è´¨ã€‚ç½‘ç»œå¤§å°å’Œç»“æ„çš„é€‰æ‹©å¯¹äºæˆåŠŸéå¸¸é‡è¦ï¼ˆBishopï¼Œ1995ï¼‰ï¼Œç‰¹åˆ«æ˜¯å½“æ³›åŒ–æ˜¯é—®é¢˜çš„é‡è¦æ–¹é¢æ—¶ï¼ˆå³ï¼Œå¯¹ç½‘ç»œæœªç»è¿‡è®­ç»ƒçš„æ–°è¾“å…¥æ¨¡å¼åšå‡ºé€‚å½“å“åº”ï¼‰ã€‚\nFeedback networks There is immense feedback in brain connectivity. For example, axons carry signals from the retina to the LGN (lateral geniculate nucleus). Axons originating in the LGN carry signals to cortical processing area V1. But there are more axons carrying signals from V1 back to LGN than in the â€œforwardâ€ direction. The axons from LGN make synapses on cells in layer IV of area V1. However, most of the synaptic inputs within layer IV come from other cells within V1. Such facts lead to strong interest in neural circuits with feedback.\nå¤§è„‘è¿æ¥ä¸­å­˜åœ¨å¤§é‡åé¦ˆã€‚ä¾‹å¦‚ï¼Œè½´çªå°†ä¿¡å·ä»è§†ç½‘è†œä¼ é€’åˆ°å¤–ä¾§è†çŠ¶ä½“ï¼ˆLGNï¼‰ã€‚èµ·æºäº LGN çš„è½´çªå°†ä¿¡å·ä¼ é€’åˆ°çš®å±‚å¤„ç†åŒºåŸŸ V1ã€‚ä½†æ˜¯ï¼Œä» V1 åé¦ˆåˆ° LGN çš„ä¿¡å·è½´çªæ¯” â€œå‰å‘â€ æ–¹å‘çš„æ›´å¤šã€‚æ¥è‡ª LGN çš„è½´çªåœ¨ V1 åŒºåŸŸçš„ IV å±‚ä¸Šçš„ç»†èƒä¸Šå½¢æˆçªè§¦ã€‚ç„¶è€Œï¼ŒIV å±‚å†…çš„å¤§éƒ¨åˆ†çªè§¦è¾“å…¥æ¥è‡ª V1 å†…çš„å…¶ä»–ç»†èƒã€‚è¿™äº›äº‹å®å¼•èµ·äº†äººä»¬å¯¹å…·æœ‰åé¦ˆçš„ç¥ç»ç”µè·¯çš„æµ“åšå…´è¶£ã€‚\nThe style of feedback circuit whose mathematics is most simply understood has symmetric connection, i.e., $S_{kj} = S_{jk}$ . In this case, there is a Lyapunov or â€œenergyâ€ function for Eq. (8), and the quantity $f_{i}$\n$$ E = -\\frac{1}{2}\\sum S_{ij}V_{i}V_{j} - \\sum I_{i}V_{i} + \\frac{1}{\\tau}\\sum\\int V^{-1}(f^{\\prime})\\mathrm{d}f^{\\prime} $$\nalways decreases in time (Hopfield, 1982, 1994). The dynamics then is described by a flow to an attractor where the motion ceases.\nå…·æœ‰å¯¹ç§°è¿æ¥çš„åé¦ˆç”µè·¯æ ·å¼ï¼Œå…¶æ•°å­¦æœ€å®¹æ˜“ç†è§£ï¼Œå³ $S_{kj} = S_{jk}$ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ–¹ç¨‹ï¼ˆ8ï¼‰å­˜åœ¨ä¸€ä¸ª Lyapunov æˆ– â€œèƒ½é‡â€ å‡½æ•°ï¼Œé‡ $f_{i}$\n$$ E = -\\frac{1}{2}\\sum S_{ij}V_{i}V_{j} - \\sum I_{i}V_{i} + \\frac{1}{\\tau}\\sum\\int V^{-1}(f^{\\prime})\\mathrm{d}f^{\\prime} $$\næ€»æ˜¯éšç€æ—¶é—´çš„æ¨ç§»è€Œå‡å°‘ï¼ˆHopfieldï¼Œ1982ï¼Œ1994ï¼‰ã€‚ç„¶åï¼ŒåŠ¨åŠ›å­¦ç”±æµå‘å¸å¼•å­æè¿°ï¼Œåœ¨é‚£é‡Œè¿åŠ¨åœæ­¢ã€‚\nIn the high-gain limit, where the input-output relationship is a step between two asymptotic values, the system has a direct relationship to physics. It can be stated most simply when the asymptotic values are scaled to $\\pm 1$. The stable points of the dynamic system then have each $V_{i} = \\pm 1$, and the stable states of the dynamical system are the stable points of an Ising magnet with exchange parameters $J_{ij} = S_{ij}$.\nåœ¨é«˜å¢ç›Šæé™ä¸‹ï¼Œè¾“å…¥è¾“å‡ºå…³ç³»åœ¨ä¸¤ä¸ªæ¸è¿‘å€¼ä¹‹é—´æ˜¯ä¸€ä¸ªæ­¥éª¤ï¼Œç³»ç»Ÿä¸ç‰©ç†å­¦æœ‰ç›´æ¥å…³ç³»ã€‚å½“æ¸è¿‘å€¼ç¼©æ”¾åˆ° $\\pm 1$ æ—¶ï¼Œå¯ä»¥æœ€ç®€å•åœ°è¯´æ˜ã€‚ç„¶åï¼ŒåŠ¨æ€ç³»ç»Ÿçš„ç¨³å®šç‚¹æ¯ä¸ª $V_{i} = \\pm 1$ï¼ŒåŠ¨æ€ç³»ç»Ÿçš„ç¨³å®šçŠ¶æ€æ˜¯å…·æœ‰äº¤æ¢å‚æ•° $J_{ij} = S_{ij}$ çš„ Ising ç£ä½“çš„ç¨³å®šç‚¹ã€‚\nThe existence of this energy function provides a programming tool (Hopfield and Tank, 1985; Takefuji, 1991). Many difficult computational problems can be posed as optimization problems. If the quantity to be optimized can be mapped onto the form Eq. (8), it defines the connections and the â€œprogramâ€ to solve the optimization problem.\nèƒ½é‡å‡½æ•°çš„å­˜åœ¨æä¾›äº†ä¸€ç§ç¼–ç¨‹å·¥å…·ï¼ˆHopfield å’Œ Tankï¼Œ1985ï¼›Takefujiï¼Œ1991ï¼‰ã€‚è®¸å¤šå›°éš¾çš„è®¡ç®—é—®é¢˜å¯ä»¥è¢«æå‡ºä½œä¸ºä¼˜åŒ–é—®é¢˜ã€‚å¦‚æœè¦ä¼˜åŒ–çš„é‡å¯ä»¥æ˜ å°„åˆ°æ–¹ç¨‹ï¼ˆ8ï¼‰çš„å½¢å¼ï¼Œå®ƒå°±å®šä¹‰äº†è¿æ¥å’Œè§£å†³ä¼˜åŒ–é—®é¢˜çš„ â€œç¨‹åºâ€ã€‚\nThe trivial generalization of the Ising system to finite temperature generates a statistical mechanics. However, a â€œlearning ruleâ€ can then be found for this system, even in the presence of hidden units. This was the first successful learning rule used for networks with hidden units (Hinton and Sejnowski, 1983). Because it is computationally intensive, practical applications have chiefly used analog â€œneuronsâ€ and the faster â€œbackpropagationâ€ learning rule when applicable. The relationship with statistical mechanics and entropic information measures, however, give the Boltzmann machine continuing interest.\nIsing ç³»ç»Ÿåˆ°æœ‰é™æ¸©åº¦çš„å¹³å‡¡æ¨å¹¿äº§ç”Ÿäº†ç»Ÿè®¡åŠ›å­¦ã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨å­˜åœ¨éšè—å•å…ƒçš„æƒ…å†µä¸‹ï¼Œä¹Ÿå¯ä»¥ä¸ºè¯¥ç³»ç»Ÿæ‰¾åˆ° â€œå­¦ä¹ è§„åˆ™â€ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªæˆåŠŸç”¨äºå…·æœ‰éšè—å•å…ƒçš„ç½‘ç»œçš„å­¦ä¹ è§„åˆ™ï¼ˆHinton å’Œ Sejnowskiï¼Œ1983ï¼‰ã€‚ç”±äºå®ƒåœ¨è®¡ç®—ä¸Šå¾ˆå¯†é›†ï¼Œå®é™…åº”ç”¨ä¸»è¦ä½¿ç”¨æ¨¡æ‹Ÿ â€œç¥ç»å…ƒâ€ å’Œæ›´å¿«çš„ â€œåå‘ä¼ æ’­â€ å­¦ä¹ è§„åˆ™ï¼ˆå¦‚æœé€‚ç”¨ï¼‰ã€‚ç„¶è€Œï¼Œä¸ç»Ÿè®¡åŠ›å­¦å’Œç†µä¿¡æ¯åº¦é‡çš„å…³ç³»ä½¿ Boltzmann æœºæŒç»­å—åˆ°å…³æ³¨ã€‚\nAssociative memories are thought of as a set of linked features $f_{1}$, $f_{2}$, etc. The activity of a particular neuron signifies the presence of the feature represented by that neuron. A memory is a state in which the cells representing the features of that memory are simultaneously active. The relationship between features is symmetric in that each implies the other and is expressed in a symmetric network. An elegant analysis of the capacity of such memories for random patterns is related to the spin glass (Amit, 1989).\nè”æƒ³è®°å¿†è¢«è®¤ä¸ºæ˜¯ä¸€ç»„é“¾æ¥çš„ç‰¹å¾ $f_{1}$ã€$f_{2}$ ç­‰ã€‚ç‰¹å®šç¥ç»å…ƒçš„æ´»åŠ¨è¡¨ç¤ºè¯¥ç¥ç»å…ƒæ‰€ä»£è¡¨çš„ç‰¹å¾çš„å­˜åœ¨ã€‚è®°å¿†æ˜¯ä¸€ç§çŠ¶æ€ï¼Œå…¶ä¸­ä»£è¡¨è¯¥è®°å¿†ç‰¹å¾çš„ç»†èƒåŒæ—¶å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚ç‰¹å¾ä¹‹é—´çš„å…³ç³»æ˜¯å¯¹ç§°çš„ï¼Œå› ä¸ºæ¯ä¸ªç‰¹å¾éƒ½æš—ç¤ºå¦ä¸€ä¸ªç‰¹å¾ï¼Œå¹¶åœ¨å¯¹ç§°ç½‘ç»œä¸­è¡¨è¾¾ã€‚å¯¹éšæœºæ¨¡å¼çš„è¿™ç§è®°å¿†å®¹é‡çš„ä¼˜é›…åˆ†æä¸è‡ªæ—‹ç»ç’ƒæœ‰å…³ï¼ˆAmitï¼Œ1989ï¼‰ã€‚\nMany nonsymmetric networks can be mapped onto networks with related Lyaupanov functions. Thus, while symmetric networks are exceptional in biology, the study of networks with Lyapunov functions is a useful approach to understanding biological networks. Line attractors have been used in connection with keeping the eye gaze stable at any set position (Seung, 1996).\nè®¸å¤šéå¯¹ç§°ç½‘ç»œå¯ä»¥æ˜ å°„åˆ°å…·æœ‰ç›¸å…³ Lyaupanov å‡½æ•°çš„ç½‘ç»œä¸Šã€‚å› æ­¤ï¼Œè™½ç„¶å¯¹ç§°ç½‘ç»œåœ¨ç”Ÿç‰©å­¦ä¸­æ˜¯ä¾‹å¤–ï¼Œä½†ç ”ç©¶å…·æœ‰ Lyapunov å‡½æ•°çš„ç½‘ç»œæ˜¯ç†è§£ç”Ÿç‰©ç½‘ç»œçš„æœ‰ç”¨æ–¹æ³•ã€‚çº¿æ€§å¸å¼•å­å·²è¢«ç”¨äºä¸ä¿æŒçœ¼ç›å‡è§†åœ¨ä»»ä½•è®¾å®šä½ç½®ç¨³å®šæœ‰å…³çš„ç ”ç©¶ï¼ˆSeungï¼Œ1996ï¼‰ã€‚\nNetworks which have feedback may oscillate. The olfactory bulb is an example of a circuit with a strong excitatory-inhibitory feedback loop. In mammals, the olfactory bulb bursts into 30â€“50 Hz oscillations with every sniff (Freeman and Skarda, 1985).\nå…·æœ‰åé¦ˆçš„ç½‘ç»œå¯èƒ½ä¼šæŒ¯è¡ã€‚å—…çƒæ˜¯å…·æœ‰å¼ºå…´å¥‹-æŠ‘åˆ¶åé¦ˆå›è·¯çš„ç”µè·¯çš„ä¸€ä¸ªä¾‹å­ã€‚åœ¨å“ºä¹³åŠ¨ç‰©ä¸­ï¼Œå—…çƒéšç€æ¯æ¬¡å¸æ°”è€Œçˆ†å‘å‡º 30-50 Hz çš„æŒ¯è¡ï¼ˆFreeman å’Œ Skardaï¼Œ1985ï¼‰ã€‚\nDEVELOPMENT AND SYNAPSE PLASTICITY For simple animals such as the C. elegans (a round worm) the nervous system is essentially determined. Each genetically identical C. elegans has the same number of nerve cells, each cell identifiable in morphology and position. The synaptic connections between such â€œidenticalâ€ animals are 90% identical. Mammals, at the other end of the spectrum, have identifiable cell types, identifiable brain structures and regions, but no cells in 1:1 correspondence between different individuals. The â€œwiringâ€ between cells clearly has rules, and also a strong random element arising from development. How, then, can we have the system of fine-tuned connections between neurons which produces visual acuity sharper than the size of a retinal photoreceptor, or coordinates the two eyes so that we have stereoscopic vision? The answer to this puzzle lies in the synapses change due to coordinated activity during development. Coordinated activity of neurons arises from the correlated nature of the visual world and is carried through to higher level neurons. The importance of neuronal activity patterns and external input is dramatically illustrated in depth perception. If a â€œwandering eyeâ€ through muscular miscoordination, is corrected in the first six months, a child develops normal binocular stereopsis. Corrected after two years, the two eyes are used in a coordinate fashion and seem completely normal, but the child will never develop stereoscopic vision.\nå¯¹äºåƒçº¿è™«ï¼ˆåœ†å½¢è •è™«ï¼‰è¿™æ ·ç®€å•çš„åŠ¨ç‰©ï¼Œç¥ç»ç³»ç»ŸåŸºæœ¬ä¸Šæ˜¯ç¡®å®šçš„ã€‚æ¯ä¸ªåŸºå› ç›¸åŒçš„çº¿è™«éƒ½æœ‰ç›¸åŒæ•°é‡çš„ç¥ç»ç»†èƒï¼Œæ¯ä¸ªç»†èƒåœ¨å½¢æ€å’Œä½ç½®ä¸Šéƒ½æ˜¯å¯è¯†åˆ«çš„ã€‚è¿™äº› â€œç›¸åŒâ€ åŠ¨ç‰©ä¹‹é—´çš„çªè§¦è¿æ¥æœ‰ 90% æ˜¯ç›¸åŒçš„ã€‚å¤„äºå…‰è°±å¦ä¸€ç«¯çš„å“ºä¹³åŠ¨ç‰©å…·æœ‰å¯è¯†åˆ«çš„ç»†èƒç±»å‹ã€å¯è¯†åˆ«çš„å¤§è„‘ç»“æ„å’ŒåŒºåŸŸï¼Œä½†ä¸åŒä¸ªä½“ä¹‹é—´æ²¡æœ‰ 1:1 å¯¹åº”çš„ç»†èƒã€‚ç»†èƒä¹‹é—´çš„ â€œå¸ƒçº¿â€ æ˜¾ç„¶æœ‰è§„åˆ™ï¼Œå¹¶ä¸”è¿˜å…·æœ‰æ¥è‡ªå‘è‚²çš„å¼ºéšæœºå…ƒç´ ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•æ‹¥æœ‰ç¥ç»å…ƒä¹‹é—´å¾®è°ƒè¿æ¥çš„ç³»ç»Ÿï¼Œä»è€Œäº§ç”Ÿæ¯”è§†ç½‘è†œå…‰æ„Ÿå—å™¨æ›´æ¸…æ™°çš„è§†è§‰æ•é”åº¦ï¼Œæˆ–åè°ƒåŒçœ¼ä»¥å®ç°ç«‹ä½“è§†è§‰ï¼Ÿè¿™ä¸ªè°œé¢˜çš„ç­”æ¡ˆåœ¨äºå‘è‚²è¿‡ç¨‹ä¸­ç”±äºåè°ƒæ´»åŠ¨è€Œå‘ç”Ÿå˜åŒ–çš„çªè§¦ã€‚ç¥ç»å…ƒçš„åè°ƒæ´»åŠ¨æºè‡ªè§†è§‰ä¸–ç•Œçš„ç›¸å…³æ€§è´¨ï¼Œå¹¶ä¼ é€’åˆ°æ›´é«˜çº§åˆ«çš„ç¥ç»å…ƒã€‚ç¥ç»å…ƒæ´»åŠ¨æ¨¡å¼å’Œå¤–éƒ¨è¾“å…¥çš„é‡è¦æ€§åœ¨æ·±åº¦æ„ŸçŸ¥ä¸­å¾—åˆ°äº†æˆå‰§æ€§çš„è¯´æ˜ã€‚å¦‚æœé€šè¿‡è‚Œè‚‰å¤±è°ƒå¼•èµ·çš„ â€œæ¸¸è¡çœ¼â€ åœ¨å¤´å…­ä¸ªæœˆå†…å¾—åˆ°çº æ­£ï¼Œå­©å­ä¼šå‘å±•å‡ºæ­£å¸¸çš„åŒçœ¼ç«‹ä½“è§†è§‰ã€‚åœ¨ä¸¤å¹´åçº æ­£åï¼Œä¸¤åªçœ¼ç›ä»¥åè°ƒçš„æ–¹å¼ä½¿ç”¨ï¼Œçœ‹èµ·æ¥å®Œå…¨æ­£å¸¸ï¼Œä½†å­©å­æ°¸è¿œä¸ä¼šå‘å±•å‡ºç«‹ä½“è§†è§‰ã€‚\nWhen multiple input patterns are present, the dynamics generates a cellular competition for the representation of these patterns. The idealized mathematics is that of a symmetry breaking. Once symmetry is broken, the competition continues to refine the connections (Linsker 1986). This mathematics was originally used to describe the development of connections between the retina and the optic tectum of the frog. It describes well the generation of orientation-selective cells in the cat visual cortex. A hierarchy of such symmetry breakings has been used to describe the selectivity of cells in the mammalian visual pathway. This analysis is simple only in cases where the details of the biology have been maximally suppressed, but such models are slowly being given more detailed connections to biology (Miller, 1994).\nå½“å­˜åœ¨å¤šä¸ªè¾“å…¥æ¨¡å¼æ—¶ï¼ŒåŠ¨åŠ›å­¦ä¼šäº§ç”Ÿç»†èƒç«äº‰ä»¥è¡¨ç¤ºè¿™äº›æ¨¡å¼ã€‚ç†æƒ³åŒ–çš„æ•°å­¦æ˜¯å¯¹ç§°æ€§ç ´ç¼ºã€‚ä¸€æ—¦å¯¹ç§°æ€§è¢«æ‰“ç ´ï¼Œç«äº‰å°±ä¼šç»§ç»­å®Œå–„è¿æ¥ï¼ˆLinsker 1986ï¼‰ã€‚è¿™ç§æ•°å­¦æœ€åˆç”¨äºæè¿°é’è›™è§†ç½‘è†œå’Œè§†é¡¶ç›–ä¹‹é—´è¿æ¥çš„å‘å±•ã€‚å®ƒå¾ˆå¥½åœ°æè¿°äº†çŒ«è§†è§‰çš®å±‚ä¸­æ–¹å‘é€‰æ‹©æ€§ç»†èƒçš„äº§ç”Ÿã€‚è¿™æ ·çš„å¯¹ç§°æ€§ç ´ç¼ºå±‚æ¬¡ç»“æ„å·²è¢«ç”¨æ¥æè¿°å“ºä¹³åŠ¨ç‰©è§†è§‰é€šè·¯ä¸­ç»†èƒçš„é€‰æ‹©æ€§ã€‚åªæœ‰åœ¨æœ€å¤§ç¨‹åº¦ä¸ŠæŠ‘åˆ¶ç”Ÿç‰©å­¦ç»†èŠ‚çš„æƒ…å†µä¸‹ï¼Œè¿™ç§åˆ†ææ‰æ˜¯ç®€å•çš„ï¼Œä½†è¿™æ ·çš„æ¨¡å‹æ­£åœ¨æ…¢æ…¢åœ°ä¸ç”Ÿç‰©å­¦å»ºç«‹æ›´è¯¦ç»†çš„è”ç³»ï¼ˆMillerï¼Œ1994ï¼‰ã€‚\nThere is an ongoing debate in such areas about â€œinstructionismâ€ versus â€œselectionism,â€ and on the role of genetics versus environmental influences. â€œNatureâ€ versus â€œnurtureâ€ has been an issue in psychology and brain science for decades and is seen at its most elementary level in trying to understand how the functional wiring of an adult brain is generated.\nåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œå…³äº â€œæŒ‡å¯¼ä¸»ä¹‰â€ ä¸ â€œé€‰æ‹©ä¸»ä¹‰â€ ä»¥åŠé—ä¼ å­¦ä¸ç¯å¢ƒå½±å“çš„ä½œç”¨å­˜åœ¨æŒç»­çš„äº‰è®ºã€‚â€œå¤©æ€§â€ ä¸ â€œæ•™å…»â€ å¤šå¹´æ¥ä¸€ç›´æ˜¯å¿ƒç†å­¦å’Œå¤§è„‘ç§‘å­¦ä¸­çš„ä¸€ä¸ªé—®é¢˜ï¼Œå¹¶ä¸”åœ¨è¯•å›¾ç†è§£æˆäººå¤§è„‘çš„åŠŸèƒ½è¿æ¥æ˜¯å¦‚ä½•äº§ç”Ÿæ—¶ï¼Œåœ¨å…¶æœ€åŸºæœ¬çš„å±‚é¢ä¸Šå¾—åˆ°äº†ä½“ç°ã€‚\nACTION POTENTIAL TIMING The detailed timing in a train of action potentials carries information beyond that described by the shortterm firing rate. When several presynaptic neurons fire action potentials simultaneously, the event can have a saliency for driving a cell that would not occur if the events were more spread out in time. These facts suggest that for some neural computations, Eq. (7) may lose the essence of Eq. (6). Theoretically, information can be encoded in action potential timing and computed efficiently and rapidly (Hopfield, 1995).\nè¯¦ç»†çš„åŠ¨ä½œç”µä½åˆ—è½¦æ—¶åºæºå¸¦çš„ä¿¡æ¯è¶…å‡ºäº†çŸ­æœŸå‘å°„ç‡æ‰€æè¿°çš„ä¿¡æ¯ã€‚å½“å‡ ä¸ªçªè§¦å‰ç¥ç»å…ƒåŒæ—¶å‘å‡ºåŠ¨ä½œç”µä½æ—¶ï¼Œè¯¥äº‹ä»¶å¯ä»¥å¯¹é©±åŠ¨ç»†èƒäº§ç”Ÿæ˜¾è‘—æ€§ï¼Œå¦‚æœäº‹ä»¶åœ¨æ—¶é—´ä¸Šæ›´åˆ†æ•£åˆ™ä¸ä¼šå‘ç”Ÿè¿™äº›äº‹ä»¶ã€‚è¿™äº›äº‹å®è¡¨æ˜ï¼Œå¯¹äºæŸäº›ç¥ç»è®¡ç®—ï¼Œæ–¹ç¨‹ï¼ˆ7ï¼‰å¯èƒ½ä¼šå¤±å»æ–¹ç¨‹ï¼ˆ6ï¼‰çš„æœ¬è´¨ã€‚ä»ç†è®ºä¸Šè®²ï¼Œä¿¡æ¯å¯ä»¥ç¼–ç åœ¨åŠ¨ä½œç”µä½æ—¶åºä¸­ï¼Œå¹¶ä¸”å¯ä»¥é«˜æ•ˆä¸”å¿«é€Ÿåœ°è®¡ç®—ï¼ˆHopfieldï¼Œ1995ï¼‰ã€‚\nDirect observations also suggest the importance of action potential timing. Experiments in cats indicate that the synchrony of action potentials between different cells might represent the â€œobjectnessâ€ of an extended visual object (Gray and Singer, 1989). Synchronization effects are seen in insect olfaction (Stopfer et al., 1997). Azimuthal sound localization by birds effectively involves coincidences between action potentials arriving via right- and left-ear pathways. A neuron in rat hippocampus which is firing at a low rate carries information about the spatial location of the rat in its phase of firing with respect to the theta rhythm (Burgess, Oâ€™Keefe, and Recce, 1993). Action potentials in low-firing-rate frontal cortex seem to have unusual temporal correlation. Action potentials propagate back into some dendrites of pyramidal cells, and their synapses have implicit information both from when the presynaptic cell fired and when the postsynaptic cell fired, potentially important in a synapse-change process.\nç›´æ¥è§‚å¯Ÿä¹Ÿè¡¨æ˜åŠ¨ä½œç”µä½æ—¶åºçš„é‡è¦æ€§ã€‚çŒ«çš„å®éªŒè¡¨æ˜ï¼Œä¸åŒç»†èƒä¹‹é—´åŠ¨ä½œç”µä½çš„åŒæ­¥å¯èƒ½ä»£è¡¨æ‰©å±•è§†è§‰å¯¹è±¡çš„ â€œç‰©ä½“æ€§â€ï¼ˆGray å’Œ Singerï¼Œ1989ï¼‰ã€‚åœ¨æ˜†è™«å—…è§‰ä¸­è§‚å¯Ÿåˆ°äº†åŒæ­¥æ•ˆåº”ï¼ˆStopfer ç­‰äººï¼Œ1997ï¼‰ã€‚é¸Ÿç±»çš„æ–¹ä½å£°å®šä½æœ‰æ•ˆåœ°æ¶‰åŠé€šè¿‡å³è€³å’Œå·¦è€³é€šè·¯åˆ°è¾¾çš„åŠ¨ä½œç”µä½ä¹‹é—´çš„å·§åˆã€‚å¤§é¼ æµ·é©¬ä½“ä¸­çš„ä¸€ä¸ªä»¥ä½é€Ÿç‡å‘å°„çš„ç¥ç»å…ƒåœ¨å…¶ç›¸å¯¹äº theta èŠ‚å¾‹çš„å‘å°„ç›¸ä½ä¸­æºå¸¦æœ‰å…³å¤§é¼ ç©ºé—´ä½ç½®çš„ä¿¡æ¯ï¼ˆBurgessã€Oâ€™Keefe å’Œ Recceï¼Œ1993ï¼‰ã€‚ä½å‘å°„ç‡é¢å¶çš®å±‚ä¸­çš„åŠ¨ä½œç”µä½ä¼¼ä¹å…·æœ‰ä¸å¯»å¸¸çš„æ—¶é—´ç›¸å…³æ€§ã€‚åŠ¨ä½œç”µä½ä¼ æ’­å›é”¥ä½“ç»†èƒçš„ä¸€äº›æ ‘çªä¸­ï¼Œå®ƒä»¬çš„çªè§¦éšå«äº†æ¥è‡ªçªè§¦å‰ç»†èƒå‘å°„æ—¶é—´å’Œçªè§¦åç»†èƒå‘å°„æ—¶é—´çš„ä¿¡æ¯ï¼Œè¿™åœ¨çªè§¦å˜åŒ–è¿‡ç¨‹ä¸­å¯èƒ½å¾ˆé‡è¦ã€‚\nTHE FUTURE The field now known as â€œcomputational neurobiologyâ€ has been based on an explosion in our knowledge of the electrical signals of cells during significant processing events and on its relationship to theory including understanding simple neural circuits, the attractor model of neural computation, the role of activity in development, and the information-theoretic view of neural coding. The short-term future will exploit the new ways to visualize neural activity, involving multi-electrode recording, optical signals from cells (voltage-dependent dyes, ion-binding fluorophores, and intrinsic signals) functional magnetic resonance imaging, magnetoencephalography, patch clamp techniques, confocal microscopy, and microelectrode arrays. Molecular biology tools have now also begun to be significant for computational neurobiology. On the modeling side it will involve understanding more of the computational power of biological systems by using additional biological features.\nç°åœ¨è¢«ç§°ä¸º â€œè®¡ç®—ç¥ç»ç”Ÿç‰©å­¦â€ çš„é¢†åŸŸï¼ŒåŸºäºæˆ‘ä»¬å¯¹ç»†èƒåœ¨é‡è¦å¤„ç†äº‹ä»¶æœŸé—´çš„ç”µä¿¡å·åŠå…¶ä¸ç†è®ºçš„å…³ç³»çš„çŸ¥è¯†çˆ†ç‚¸ï¼ŒåŒ…æ‹¬ç†è§£ç®€å•çš„ç¥ç»ç”µè·¯ã€ç¥ç»è®¡ç®—çš„å¸å¼•å­æ¨¡å‹ã€æ´»åŠ¨åœ¨å‘è‚²ä¸­çš„ä½œç”¨ä»¥åŠç¥ç»ç¼–ç çš„ä¿¡æ¯ç†è®ºè§‚ç‚¹ã€‚çŸ­æœŸå†…çš„æœªæ¥å°†åˆ©ç”¨å¯è§†åŒ–ç¥ç»æ´»åŠ¨çš„æ–°æ–¹æ³•ï¼Œæ¶‰åŠå¤šç”µæè®°å½•ã€æ¥è‡ªç»†èƒçš„å…‰å­¦ä¿¡å·ï¼ˆç”µå‹ä¾èµ–æ€§æŸ“æ–™ã€ç¦»å­ç»“åˆè§å…‰ç´ å’Œå†…åœ¨ä¿¡å·ï¼‰ã€åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒã€è„‘ç£å›¾ã€è†œç‰‡é’³æŠ€æœ¯ã€å…±èšç„¦æ˜¾å¾®é•œå’Œå¾®ç”µæé˜µåˆ—ã€‚åˆ†å­ç”Ÿç‰©å­¦å·¥å…·ç°åœ¨ä¹Ÿå¼€å§‹å¯¹è®¡ç®—ç¥ç»ç”Ÿç‰©å­¦å…·æœ‰é‡è¦æ„ä¹‰ã€‚åœ¨å»ºæ¨¡æ–¹é¢ï¼Œå®ƒå°†æ¶‰åŠé€šè¿‡ä½¿ç”¨é¢å¤–çš„ç”Ÿç‰©ç‰¹å¾æ¥ç†è§£ç”Ÿç‰©ç³»ç»Ÿçš„æ›´å¤šè®¡ç®—èƒ½åŠ›ã€‚\nThe study of silicon very large scale integrated circuits (VLSIâ€™s) for analog â€˜â€˜neuralâ€™â€™ circuits (Mead, 1989) has yielded one relevant general principle. When the physics of a device can be used in an algorithm, the device is highly effective in computation compared to its effectiveness in general purpose use. Evolution will have exploited the biophysical molecular and circuit devices available. For any particular behavior, some facts of neurobiology will be very significant because they are used in the algorithm, and others will be able to be subsumed in a model which is far simpler than the actual biophysics of the system. It is important to make such separations, for neurobiology is so filled with details that we will never understand the neurobiological basis of perception, cognition, and psychology merely by accumulating facts and doing ever more detailed simulations. Linear systems are simple to characterize completely. Computational systems are highly nonlinear, and a complete characterization by brute force requires a number of experiments which grows exponentially with the size of the system. When only a limited number of experiments is performed, the behavior of the system is not fully characterized, and to a considerable extent the experimental design builds in the answers that will be found. For working at higher computational levels, experiments on anaesthetized animals, or in highly simplified, overlearned artificial situations, are not going to be enough. Nor will the characterization of the behavior of a very small number of cells during a behavior be adequate to understand how or why the behavior is being generated. Thus it will be necessary to build a better bridge between lower animals, which can be more completely studied, and higher animals, whose rich mental behavior is the ultimate goal of computational neurobiology.\nç ”ç©¶ç”¨äºæ¨¡æ‹Ÿ â€œç¥ç»â€ ç”µè·¯çš„ç¡…è¶…å¤§è§„æ¨¡é›†æˆç”µè·¯ï¼ˆVLSIï¼‰ï¼ˆMeadï¼Œ1989ï¼‰å·²ç»äº§ç”Ÿäº†ä¸€ä¸ªç›¸å…³çš„ä¸€èˆ¬åŸåˆ™ã€‚å½“è®¾å¤‡çš„ç‰©ç†å­¦å¯ä»¥åœ¨ç®—æ³•ä¸­ä½¿ç”¨æ—¶ï¼Œè¯¥è®¾å¤‡åœ¨è®¡ç®—ä¸­çš„æ•ˆæœä¸å…¶åœ¨é€šç”¨ç”¨é€”ä¸­çš„æ•ˆæœç›¸æ¯”æ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚è¿›åŒ–å°†åˆ©ç”¨å¯ç”¨çš„ç”Ÿç‰©ç‰©ç†åˆ†å­å’Œç”µè·¯è®¾å¤‡ã€‚å¯¹äºä»»ä½•ç‰¹å®šçš„è¡Œä¸ºï¼Œç¥ç»ç”Ÿç‰©å­¦çš„ä¸€äº›äº‹å®å°†éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä»¬è¢«ç”¨äºç®—æ³•ï¼Œè€Œå…¶ä»–äº‹å®å°†èƒ½å¤Ÿè¢«çº³å…¥ä¸€ä¸ªè¿œæ¯”ç³»ç»Ÿçš„å®é™…ç”Ÿç‰©ç‰©ç†å­¦æ›´ç®€å•çš„æ¨¡å‹ä¸­ã€‚è¿›è¡Œè¿™ç§åˆ†ç¦»å¾ˆé‡è¦ï¼Œå› ä¸ºç¥ç»ç”Ÿç‰©å­¦å……æ»¡äº†ç»†èŠ‚ï¼Œæˆ‘ä»¬æ°¸è¿œæ— æ³•ä»…é€šè¿‡ç§¯ç´¯äº‹å®å’Œè¿›è¡Œè¶Šæ¥è¶Šè¯¦ç»†çš„æ¨¡æ‹Ÿæ¥ç†è§£æ„ŸçŸ¥ã€è®¤çŸ¥å’Œå¿ƒç†å­¦çš„ç¥ç»ç”Ÿç‰©å­¦åŸºç¡€ã€‚çº¿æ€§ç³»ç»Ÿå¾ˆå®¹æ˜“å®Œå…¨è¡¨å¾ã€‚è®¡ç®—ç³»ç»Ÿæ˜¯é«˜åº¦éçº¿æ€§çš„ï¼Œé€šè¿‡è›®åŠ›è¿›è¡Œå®Œæ•´è¡¨å¾æ‰€éœ€çš„å®éªŒæ•°é‡éšç€ç³»ç»Ÿå¤§å°å‘ˆæŒ‡æ•°å¢é•¿ã€‚å½“åªæ‰§è¡Œæœ‰é™æ•°é‡çš„å®éªŒæ—¶ï¼Œç³»ç»Ÿçš„è¡Œä¸ºæ²¡æœ‰å¾—åˆ°å……åˆ†è¡¨å¾ï¼Œåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šï¼Œå®éªŒè®¾è®¡æ„å»ºäº†å°†è¦æ‰¾åˆ°çš„ç­”æ¡ˆã€‚ä¸ºäº†åœ¨æ›´é«˜çš„è®¡ç®—çº§åˆ«ä¸Šå·¥ä½œï¼Œå¯¹éº»é†‰åŠ¨ç‰©æˆ–åœ¨é«˜åº¦ç®€åŒ–ã€è¿‡åº¦å­¦ä¹ çš„äººå·¥æƒ…å†µä¸‹è¿›è¡Œçš„å®éªŒå°†æ˜¯ä¸å¤Ÿçš„ã€‚åœ¨è¡Œä¸ºè¿‡ç¨‹ä¸­å¯¹æå°‘æ•°ç»†èƒè¡Œä¸ºçš„è¡¨å¾ä¹Ÿä¸è¶³ä»¥ç†è§£è¡Œä¸ºæ˜¯å¦‚ä½•æˆ–ä¸ºä»€ä¹ˆè¢«ç”Ÿæˆçš„ã€‚å› æ­¤ï¼Œæœ‰å¿…è¦åœ¨å¯ä»¥æ›´å…¨é¢ç ”ç©¶çš„ä½ç­‰åŠ¨ç‰©å’Œå…¶ä¸°å¯Œçš„å¿ƒç†è¡Œä¸ºæ˜¯è®¡ç®—ç¥ç»ç”Ÿç‰©å­¦æœ€ç»ˆç›®æ ‡çš„é«˜ç­‰åŠ¨ç‰©ä¹‹é—´å»ºç«‹æ›´å¥½çš„æ¡¥æ¢ã€‚\n",
  "wordCount" : "14002",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/11/19/T2aHrkA4UjSN3Yu.png","datePublished": "2025-11-19T00:18:23+08:00",
  "dateModified": "2025-11-19T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/reference/brain-neural-networks-and-computation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "æ— å¤„æƒ¹å°˜åŸƒ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿— (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿—</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/">ğŸ“• é˜…è¯»</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/reference/">ğŸ“• æ–‡çŒ®</a></div>
            <h1 class="post-title">
                Brain, neural networks, and computation
            </h1>
            <div class="post-description">
                Computation through dynamics
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-11-19
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>14002å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>28åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/11/19/T2aHrkA4UjSN3Yu.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#brain-as-a-computer" aria-label="BRAIN AS A COMPUTER">BRAIN AS A COMPUTER</a></li>
                <li>
                    <a href="#computers-as-dynamical-systems" aria-label="COMPUTERS AS DYNAMICAL SYSTEMS">COMPUTERS AS DYNAMICAL SYSTEMS</a></li>
                <li>
                    <a href="#dynamical-model-of-neural-activity" aria-label="DYNAMICAL MODEL OF NEURAL ACTIVITY">DYNAMICAL MODEL OF NEURAL ACTIVITY</a></li>
                <li>
                    <a href="#the-dynamics-of-synapses" aria-label="THE DYNAMICS OF SYNAPSES">THE DYNAMICS OF SYNAPSES</a></li>
                <li>
                    <a href="#programming-languages-for-artificial-neural-networks-ann" aria-label="PROGRAMMING LANGUAGES FOR ARTIFICIAL NEURAL NETWORKS (ANN)">PROGRAMMING LANGUAGES FOR ARTIFICIAL NEURAL NETWORKS (ANN)</a><ul>
                        
                <li>
                    <a href="#feed-forward-networks" aria-label="Feed-forward networks">Feed-forward networks</a></li>
                <li>
                    <a href="#feedback-networks" aria-label="Feedback networks">Feedback networks</a></li></ul>
                </li>
                <li>
                    <a href="#development-and-synapse-plasticity" aria-label="DEVELOPMENT AND SYNAPSE PLASTICITY">DEVELOPMENT AND SYNAPSE PLASTICITY</a></li>
                <li>
                    <a href="#action-potential-timing" aria-label="ACTION POTENTIAL TIMING">ACTION POTENTIAL TIMING</a></li>
                <li>
                    <a href="#the-future" aria-label="THE FUTURE">THE FUTURE</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<blockquote>
<p>The method by which brain produces mind has for centuries been discussed in terms of the most complex engineering and science metaphors of the day. Descartes described mind in terms of interacting vortices. Psychologists have metaphorized memory in terms of paths or traces worn in a landscape, a geological record of our experiences. To McCulloch and Pitts (1943) and von Neumann (1958), the appropriate metaphor was the digital computer, then in its infancy. The field of &ldquo;neural networks&rdquo; is the study of the computational properties and behavior of networks of &ldquo;neuronlike&rdquo; elements. It lies somewhere between a model of neurobiology and a metaphor for how the brain computes. It is inspired by two goals: to understand how neurobiology works, and to understand how to solve problems which neurobiology solves rapidly and effortlessly and which are very hard on present digital machines.</p>
</blockquote>
<p>å‡ ä¸ªä¸–çºªä»¥æ¥ï¼Œå¤§è„‘äº§ç”Ÿå¿ƒæ™ºçš„æ–¹æ³•ä¸€ç›´ä»¥å½“æ—¶æœ€å¤æ‚çš„å·¥ç¨‹å’Œç§‘å­¦éšå–»æ¥è®¨è®ºã€‚ç¬›å¡å°”å°†å¿ƒæ™ºæè¿°ä¸ºç›¸äº’ä½œç”¨çš„æ¼©æ¶¡ã€‚å¿ƒç†å­¦å®¶å°†è®°å¿†éšå–»ä¸ºæ™¯è§‚ä¸­ç£¨æŸçš„è·¯å¾„æˆ–ç—•è¿¹ï¼Œæ˜¯æˆ‘ä»¬ç»å†çš„åœ°è´¨è®°å½•ã€‚å¯¹äº McCulloch å’Œ Pittsï¼ˆ1943ï¼‰ä»¥åŠå†¯Â·è¯ºä¾æ›¼ï¼ˆ1958ï¼‰æ¥è¯´ï¼Œé€‚å½“çš„éšå–»æ˜¯å½“æ—¶è¿˜å¤„äºèµ·æ­¥é˜¶æ®µçš„æ•°å­—è®¡ç®—æœºã€‚&ldquo;ç¥ç»ç½‘ç»œ&rdquo; é¢†åŸŸæ˜¯å¯¹ &ldquo;ç±»ç¥ç»å…ƒ&rdquo; å…ƒç´ ç½‘ç»œçš„è®¡ç®—å±æ€§å’Œè¡Œä¸ºçš„ç ”ç©¶ã€‚å®ƒä»‹äºç¥ç»ç”Ÿç‰©å­¦æ¨¡å‹å’Œå¤§è„‘å¦‚ä½•è®¡ç®—çš„éšå–»ä¹‹é—´ã€‚å®ƒå—ä¸¤ä¸ªç›®æ ‡çš„å¯å‘ï¼šç†è§£ç¥ç»ç”Ÿç‰©å­¦çš„å·¥ä½œåŸç†ï¼Œä»¥åŠç†è§£å¦‚ä½•è§£å†³ç¥ç»ç”Ÿç‰©å­¦èƒ½å¤Ÿå¿«é€Ÿè½»æ¾è§£å†³è€Œå½“å‰æ•°å­—æœºå™¨å´å¾ˆéš¾è§£å†³çš„é—®é¢˜ã€‚</p>
<blockquote>
<p>Most physicists will find it obvious that understanding biology might help in engineering. The obverse engineering-toward-biological link can be made by testing a circuit of &ldquo;model neurons&rdquo; on a difficult real-world problem such as oral word recognition. If the &ldquo;neural circuit&rdquo; with some particular biological feature is capable of solving a real problem which circuits without that feature solve poorly, the plausibility that the biological feature selected is computationally useful in biology is bolstered. If not, then it is more plausible that the feature can be dispensed with in modeling biology. These are not strong arguments, but they do provide an approach to finding out what, of the myriad of details in neurobiology, is truly important and what is merely true. The study of a 1950 digital computer, in the spirit of neurobiology, would have a strong commitment to studying BaO, then the material of vacuum tube cathodes. The study of the digital computer in 1998 would have a strong commitment to SiO2, the essential insulating material below each gate. Yet the computing structure of the two machines could be identical, hidden amongst the lowest levels of detail. The study of &ldquo;artificial neural networks&rdquo; in the spirit of biology will relate to aspects of how neurobiology computes in the same sense that understanding the computer of 1998 relates to understanding the computer of 1950.</p>
</blockquote>
<p>å¤§å¤šæ•°ç‰©ç†å­¦å®¶ä¼šå‘ç°ï¼Œç†è§£ç”Ÿç‰©å­¦å¯èƒ½æœ‰åŠ©äºå·¥ç¨‹å­¦æ˜¯æ˜¾è€Œæ˜“è§çš„ã€‚å¯ä»¥é€šè¿‡åœ¨å£è¯­è¯è¯†åˆ«ç­‰å›°éš¾çš„ç°å®é—®é¢˜ä¸Šæµ‹è¯• &ldquo;æ¨¡å‹ç¥ç»å…ƒ&rdquo; ç”µè·¯æ¥å»ºç«‹åå‘çš„å·¥ç¨‹æœå‘ç”Ÿç‰©å­¦çš„è”ç³»ã€‚å¦‚æœå…·æœ‰æŸäº›ç‰¹å®šç”Ÿç‰©ç‰¹å¾çš„ &ldquo;ç¥ç»ç”µè·¯&rdquo; èƒ½å¤Ÿè§£å†³æ²¡æœ‰è¯¥ç‰¹å¾çš„ç”µè·¯è§£å†³å¾—å¾ˆå·®çš„å®é™…é—®é¢˜ï¼Œé‚£ä¹ˆæ‰€é€‰æ‹©çš„ç”Ÿç‰©ç‰¹å¾åœ¨ç”Ÿç‰©å­¦ä¸­å…·æœ‰è®¡ç®—ç”¨é€”çš„å¯ä¿¡åº¦å°±ä¼šå¢å¼ºã€‚åä¹‹ï¼Œåˆ™æ›´æœ‰å¯èƒ½åœ¨æ¨¡æ‹Ÿç”Ÿç‰©å­¦æ—¶å¯ä»¥çœç•¥è¯¥ç‰¹å¾ã€‚è¿™äº›ä¸æ˜¯å¼ºæœ‰åŠ›çš„è®ºæ®ï¼Œä½†å®ƒä»¬ç¡®å®æä¾›äº†ä¸€ç§æ–¹æ³•æ¥æ‰¾å‡ºç¥ç»ç”Ÿç‰©å­¦ä¸­æ— æ•°ç»†èŠ‚ä¸­å“ªäº›æ˜¯çœŸæ­£é‡è¦çš„ï¼Œå“ªäº›åªæ˜¯æ­£ç¡®çš„ã€‚ä»¥ç¥ç»ç”Ÿç‰©å­¦çš„ç²¾ç¥ç ”ç©¶ 1950 å¹´çš„æ•°å­—è®¡ç®—æœºï¼Œå°†å¼ºçƒˆè‡´åŠ›äºç ”ç©¶ BaO(å½“æ—¶æ˜¯çœŸç©ºç®¡é˜´æçš„ææ–™)ã€‚1998å¹´å¯¹æ•°å­—è®¡ç®—æœºçš„ç ”ç©¶å°†å¼ºçƒˆè‡´åŠ›äº SiO2(æ¯ä¸ªé—¨ä¸‹æ–¹çš„åŸºæœ¬ç»ç¼˜ææ–™)ã€‚ç„¶è€Œï¼Œè¿™ä¸¤å°æœºå™¨çš„è®¡ç®—ç»“æ„å¯èƒ½æ˜¯ç›¸åŒçš„ï¼Œéšè—åœ¨æœ€ä½çº§åˆ«çš„ç»†èŠ‚ä¸­ã€‚ä»¥ç”Ÿç‰©å­¦ç²¾ç¥ç ”ç©¶ &ldquo;äººå·¥ç¥ç»ç½‘ç»œ&rdquo; å°†ä¸ç†è§£ 1998 å¹´è®¡ç®—æœºä¸ç†è§£ 1950 å¹´è®¡ç®—æœºç›¸å…³è”ï¼Œå°±åƒç†è§£ 1998 å¹´è®¡ç®—æœºä¸ç†è§£ 1950 å¹´è®¡ç®—æœºä¸€æ ·ã€‚</p>
<h1 id="brain-as-a-computer">BRAIN AS A COMPUTER<a hidden class="anchor" aria-hidden="true" href="#brain-as-a-computer">#</a></h1>
<blockquote>
<p>A digital machine can be programmed to compare a present image with a three-dimensional representation of a person, and thus the problem of recognizing a friend can be solved by a computation. Similarly, how to drive the actuators of a robot for a desired motion is a problem in classical mechanics that can be solved on a computer. While we may not know how to write efficient algorithms for these tasks, such examples do illustrate that what the nervous system does might be described as computation.</p>
</blockquote>
<p>æ•°å­—æœºå™¨å¯ä»¥è¢«ç¼–ç¨‹ä¸ºå°†å½“å‰å›¾åƒä¸ä¸€ä¸ªäººçš„ä¸‰ç»´è¡¨ç¤ºè¿›è¡Œæ¯”è¾ƒï¼Œå› æ­¤è¯†åˆ«æœ‹å‹çš„é—®é¢˜å¯ä»¥é€šè¿‡è®¡ç®—æ¥è§£å†³ã€‚ç±»ä¼¼åœ°ï¼Œå¦‚ä½•é©±åŠ¨æœºå™¨äººçš„æ‰§è¡Œå™¨ä»¥å®ç°æ‰€éœ€çš„è¿åŠ¨æ˜¯ä¸€ä¸ªç»å…¸åŠ›å­¦é—®é¢˜ï¼Œå¯ä»¥åœ¨è®¡ç®—æœºä¸Šè§£å†³ã€‚è™½ç„¶æˆ‘ä»¬å¯èƒ½ä¸çŸ¥é“å¦‚ä½•ä¸ºè¿™äº›ä»»åŠ¡ç¼–å†™é«˜æ•ˆçš„ç®—æ³•ï¼Œä½†è¿™äº›ä¾‹å­ç¡®å®è¯´æ˜äº†ç¥ç»ç³»ç»Ÿæ‰€åšçš„äº‹æƒ…å¯ä»¥æè¿°ä¸ºè®¡ç®—ã€‚</p>
<blockquote>
<p>For present purposes, a computer can be viewed as an input-output device, with input and output signals that are in the same format (Hopfield, 1994). Thus in a very simple digital computer, the input is a string of bits (in time), and the output is another string of bits. A million axons carry electrochemical pulses from the eye to the brain. Similar signaling pulses are used to drive the muscles of the vocal tract. When we look at a person and say, &ldquo;Hello, Jessica,&rdquo; our brain is producing a complicated transformation from one (parallel) input pulse sequence coming from the eye to another (parallel) output pulse sequence which results in sound waves being generated. The idea of composition is important in this definition. The output of one computer can be used as the input for another computer of the same general type, for they are compatible signals. Within this definition, a digital chip is a computer, and large computers are built as composites of smaller ones. Each neuron is a simple computer according to this definition, and the brain is a large composite computer.</p>
</blockquote>
<p>å¯¹äºç›®å‰çš„ç›®çš„ï¼Œè®¡ç®—æœºå¯ä»¥è¢«è§†ä¸ºä¸€ç§è¾“å…¥è¾“å‡ºè®¾å¤‡ï¼Œå…¶è¾“å…¥å’Œè¾“å‡ºä¿¡å·å…·æœ‰ç›¸åŒçš„æ ¼å¼ï¼ˆHopfieldï¼Œ1994ï¼‰ã€‚å› æ­¤ï¼Œåœ¨ä¸€ä¸ªéå¸¸ç®€å•çš„æ•°å­—è®¡ç®—æœºä¸­ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªæ¯”ç‰¹ä¸²ï¼ˆéšæ—¶é—´å˜åŒ–ï¼‰ï¼Œè¾“å‡ºæ˜¯å¦ä¸€ä¸ªæ¯”ç‰¹ä¸²ã€‚ä¸€ç™¾ä¸‡ä¸ªè½´çªå°†æ¥è‡ªçœ¼ç›çš„ç”µåŒ–å­¦è„‰å†²ä¼ é€åˆ°å¤§è„‘ã€‚ç±»ä¼¼çš„ä¿¡å·è„‰å†²ç”¨äºé©±åŠ¨å‘å£°é“çš„è‚Œè‚‰ã€‚å½“æˆ‘ä»¬çœ‹ç€ä¸€ä¸ªäººå¹¶è¯´ &ldquo;ä½ å¥½ï¼Œæ°è¥¿å¡&rdquo; æ—¶ï¼Œæˆ‘ä»¬çš„å¤§è„‘æ­£åœ¨å°†æ¥è‡ªçœ¼ç›çš„ä¸€ä¸ªï¼ˆå¹¶è¡Œï¼‰è¾“å…¥è„‰å†²åºåˆ—è½¬æ¢ä¸ºå¦ä¸€ä¸ªï¼ˆå¹¶è¡Œï¼‰è¾“å‡ºè„‰å†²åºåˆ—ï¼Œä»è€Œäº§ç”Ÿå£°æ³¢ã€‚ç»„åˆçš„æ¦‚å¿µåœ¨è¿™ä¸ªå®šä¹‰ä¸­å¾ˆé‡è¦ã€‚ä¸€ä¸ªè®¡ç®—æœºçš„è¾“å‡ºå¯ä»¥ç”¨ä½œå¦ä¸€ä¸ªç›¸åŒç±»å‹è®¡ç®—æœºçš„è¾“å…¥ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å…¼å®¹çš„ä¿¡å·ã€‚åœ¨è¿™ä¸ªå®šä¹‰ä¸­ï¼Œæ•°å­—èŠ¯ç‰‡æ˜¯ä¸€ä¸ªè®¡ç®—æœºï¼Œè€Œå¤§å‹è®¡ç®—æœºæ˜¯ç”±è¾ƒå°çš„è®¡ç®—æœºç»„æˆçš„å¤åˆä½“ã€‚æ ¹æ®è¿™ä¸ªå®šä¹‰ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½æ˜¯ä¸€ä¸ªç®€å•çš„è®¡ç®—æœºï¼Œè€Œå¤§è„‘åˆ™æ˜¯ä¸€ä¸ªå¤§å‹å¤åˆè®¡ç®—æœºã€‚</p>
<h1 id="computers-as-dynamical-systems">COMPUTERS AS DYNAMICAL SYSTEMS<a hidden class="anchor" aria-hidden="true" href="#computers-as-dynamical-systems">#</a></h1>
<blockquote>
<p>The operation of a digital machine is most simply illustrated for batch-mode computation. The computer has $N$ storage registers, each storing a single binary bit. The logical state of the machine at a particular time is specified by a vector $10010110000\cdots$ of $N$ bits. The state changes each clock cycle. The transition map, describing which state follows which, is implicitly built into the machine by its design. The computer can thus be described as a dynamical system that changes its discrete state in discrete time, and the computation is carried out by following a path in state space.</p>
</blockquote>
<p>æ•°å­—æœºå™¨çš„æ“ä½œæœ€ç®€å•åœ°é€šè¿‡æ‰¹å¤„ç†è®¡ç®—æ¥è¯´æ˜ã€‚è®¡ç®—æœºæœ‰ $N$ ä¸ªå­˜å‚¨å¯„å­˜å™¨ï¼Œæ¯ä¸ªå¯„å­˜å™¨å­˜å‚¨ä¸€ä¸ªäºŒè¿›åˆ¶ä½ã€‚æœºå™¨åœ¨ç‰¹å®šæ—¶é—´çš„é€»è¾‘çŠ¶æ€ç”±ä¸€ä¸ª $N$ ä½çš„å‘é‡ $10010110000\cdots$ æŒ‡å®šã€‚çŠ¶æ€åœ¨æ¯ä¸ªæ—¶é’Ÿå‘¨æœŸéƒ½ä¼šæ”¹å˜ã€‚æè¿°å“ªä¸ªçŠ¶æ€è·Ÿéšå“ªä¸ªçŠ¶æ€çš„è½¬æ¢æ˜ å°„æ˜¯é€šè¿‡å…¶è®¾è®¡éšå¼æ„å»ºåˆ°æœºå™¨ä¸­çš„ã€‚å› æ­¤ï¼Œè®¡ç®—æœºå¯ä»¥è¢«æè¿°ä¸ºä¸€ä¸ªåœ¨ç¦»æ•£æ—¶é—´å†…æ”¹å˜å…¶ç¦»æ•£çŠ¶æ€çš„åŠ¨æ€ç³»ç»Ÿï¼Œè®¡ç®—æ˜¯é€šè¿‡åœ¨çŠ¶æ€ç©ºé—´ä¸­è·Ÿéšä¸€æ¡è·¯å¾„æ¥å®Œæˆçš„ã€‚</p>
<blockquote>
<p>The user of the machine has no control over the dynamics, which is determined by the state transition map. The userâ€™s program, data, and a standard initialization procedure prescribe the starting state of the machine. In a batch-mode computation, the answer is found when a stable point of the discrete dynamical system is reached, a state from which there are no transitions. A particular subset of the state bits (e.g., the contents of a particular machine register) will then describe the desired answer.</p>
</blockquote>
<p>æœºå™¨çš„ç”¨æˆ·æ— æ³•æ§åˆ¶åŠ¨æ€ï¼ŒåŠ¨æ€ç”±çŠ¶æ€è½¬æ¢æ˜ å°„å†³å®šã€‚ç”¨æˆ·çš„ç¨‹åºã€æ•°æ®å’Œæ ‡å‡†åˆå§‹åŒ–è¿‡ç¨‹è§„å®šäº†æœºå™¨çš„èµ·å§‹çŠ¶æ€ã€‚åœ¨æ‰¹å¤„ç†è®¡ç®—ä¸­ï¼Œå½“è¾¾åˆ°ç¦»æ•£åŠ¨æ€ç³»ç»Ÿçš„ç¨³å®šç‚¹æ—¶ï¼Œå°±ä¼šæ‰¾åˆ°ç­”æ¡ˆï¼Œå³æ²¡æœ‰è½¬æ¢çš„çŠ¶æ€ã€‚ç„¶åï¼ŒçŠ¶æ€ä½çš„ç‰¹å®šå­é›†ï¼ˆä¾‹å¦‚ï¼Œç‰¹å®šæœºå™¨å¯„å­˜å™¨çš„å†…å®¹ï¼‰å°†æè¿°æ‰€éœ€çš„ç­”æ¡ˆã€‚</p>
<blockquote>
<p>Batch-mode analog computation can be similarly described by using continuous variables and continuous time. The idea of computation as a process carried out by a dynamical system in moving from an initial state to a final state is the same in both cases. In the analog case, the possible motions in state space describe a flow field as in Fig. 1, and computation done by moving with this flow from start to end. (Real &ldquo;digital&rdquo; machines contain only analog components; the digital description is a representation in fewer variables which contains the essence of the continuous dynamics.)</p>
</blockquote>
<p>æ‰¹å¤„ç†æ¨¡æ‹Ÿè®¡ç®—ä¹Ÿå¯ä»¥é€šè¿‡ä½¿ç”¨è¿ç»­å˜é‡å’Œè¿ç»­æ—¶é—´æ¥ç±»ä¼¼åœ°æè¿°ã€‚ä½œä¸ºåŠ¨æ€ç³»ç»Ÿé€šè¿‡ä»åˆå§‹çŠ¶æ€ç§»åŠ¨åˆ°æœ€ç»ˆçŠ¶æ€æ¥å®Œæˆçš„è¿‡ç¨‹çš„è®¡ç®—æ¦‚å¿µåœ¨ä¸¤ç§æƒ…å†µä¸‹éƒ½æ˜¯ç›¸åŒçš„ã€‚åœ¨æ¨¡æ‹Ÿæƒ…å†µä¸‹ï¼ŒçŠ¶æ€ç©ºé—´ä¸­çš„å¯èƒ½è¿åŠ¨æè¿°äº†å›¾ 1 ä¸­çš„æµåœºï¼Œè®¡ç®—æ˜¯é€šè¿‡éšè¯¥æµåŠ¨ä»å¼€å§‹åˆ°ç»“æŸæ¥å®Œæˆçš„ã€‚ï¼ˆçœŸæ­£çš„ &ldquo;æ•°å­—&rdquo; æœºå™¨åªåŒ…å«æ¨¡æ‹Ÿç»„ä»¶ï¼›æ•°å­—æè¿°æ˜¯åœ¨æ›´å°‘å˜é‡ä¸­çš„ä¸€ç§è¡¨ç¤ºï¼ŒåŒ…å«äº†è¿ç»­åŠ¨æ€çš„æœ¬è´¨ã€‚ï¼‰</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/7Lu1U9TtB658gCQ.png" alt=""  /></p>
</blockquote>
<blockquote>
<p>The flow field of a simple analog computer. The stable points of the flow, marked by $x$&rsquo;s, are possible answers. To initiate the computation, the initial location in state space must be given. A complex analog computer would have such a flow field in a very large number of dimensions.</p>
</blockquote>
<p>ç®€å•æ¨¡æ‹Ÿè®¡ç®—æœºçš„æµåœºã€‚æµçš„ç¨³å®šç‚¹ç”± $x$ æ ‡è®°ï¼Œæ˜¯å¯èƒ½çš„ç­”æ¡ˆã€‚è¦å¯åŠ¨è®¡ç®—ï¼Œå¿…é¡»ç»™å‡ºçŠ¶æ€ç©ºé—´ä¸­çš„åˆå§‹ä½ç½®ã€‚å¤æ‚çš„æ¨¡æ‹Ÿè®¡ç®—æœºä¼šå…·å¤‡ä¸€ä¸ªè¿™æ ·çš„æé«˜ç»´æ•°çš„æµåœºã€‚</p>
</blockquote>
<h1 id="dynamical-model-of-neural-activity">DYNAMICAL MODEL OF NEURAL ACTIVITY<a hidden class="anchor" aria-hidden="true" href="#dynamical-model-of-neural-activity">#</a></h1>
<blockquote>
<p>The anatomy of a &ldquo;typical&rdquo; neuron in a mammalian brain is sketched in Fig. 2 (Kandel, Schwartz, and Jessell, 1991). It has three major regions: dendrites, a cell body, and an axon. Each cell is connected by structures called synapses with approximately 1000 other cells. Inputs to a cell are made at synapses on its dendrites. The output of that cell is through synapses made by its axon onto the dendrites of other cells. The interior of the neuron is surrounded by a membrane of high resistivity and is filled with a conducting ionic solution. Ion-specific pumps transport ions across the membrane, maintaining an electrical potential difference between the inside and the outside of the cell. Ion-specific channels whose electrical conductivity is voltage dependent and dynamic play a key role in the evolution of the &ldquo;state&rdquo; of a neuron.</p>
</blockquote>
<p>å“ºä¹³åŠ¨ç‰©å¤§è„‘ä¸­ &ldquo;å…¸å‹&rdquo; ç¥ç»å…ƒçš„è§£å‰–ç»“æ„å¦‚å›¾ 2 æ‰€ç¤ºï¼ˆKandelã€Schwartz å’Œ Jessellï¼Œ1991ï¼‰ã€‚å®ƒæœ‰ä¸‰ä¸ªä¸»è¦åŒºåŸŸï¼šæ ‘çªã€ç»†èƒä½“å’Œè½´çªã€‚æ¯ä¸ªç»†èƒé€šè¿‡ç§°ä¸ºçªè§¦çš„ç»“æ„ä¸å¤§çº¦ 1000 ä¸ªå…¶ä»–ç»†èƒè¿æ¥ã€‚å¯¹ç»†èƒçš„è¾“å…¥æ˜¯åœ¨å…¶æ ‘çªä¸Šçš„çªè§¦å¤„è¿›è¡Œçš„ã€‚è¯¥ç»†èƒçš„è¾“å‡ºæ˜¯é€šè¿‡å…¶è½´çªåœ¨å…¶ä»–ç»†èƒçš„æ ‘çªä¸Šå½¢æˆçš„çªè§¦ã€‚ç¥ç»å…ƒçš„å†…éƒ¨è¢«é«˜ç”µé˜»ç‡çš„è†œåŒ…å›´ï¼Œå¹¶å……æ»¡äº†å¯¼ç”µç¦»å­æº¶æ¶²ã€‚ç¦»å­ç‰¹å¼‚æ€§æ³µå°†ç¦»å­è¿è¾“ç©¿è¿‡è†œï¼Œç»´æŒç»†èƒå†…å¤–ä¹‹é—´çš„ç”µä½å·®ã€‚ç”µå¯¼ç‡ä¾èµ–äºç”µå‹ä¸”åŠ¨æ€å˜åŒ–çš„ç¦»å­ç‰¹å¼‚æ€§é€šé“åœ¨ç¥ç»å…ƒ &ldquo;çŠ¶æ€&rdquo; çš„æ¼”å˜ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/19/NXSIP1hsbKaHG3E.png" alt=""  /></p>
<p>A sketch of a neuron and its style of interconnections. Axons may be as long as many centimeters, though most are on the scale of a millimeter. Typical cell bodies are a few microns in diameter.</p>
</blockquote>
<p>ç¥ç»å…ƒåŠå…¶äº’è¿æ–¹å¼çš„ç¤ºæ„å›¾ã€‚è½´çªå¯èƒ½é•¿è¾¾æ•°å˜ç±³ï¼Œå°½ç®¡å¤§å¤šæ•°è½´çªçš„é•¿åº¦åœ¨æ¯«ç±³çº§åˆ«ã€‚å…¸å‹çš„ç»†èƒä½“ç›´å¾„ä¸ºå‡ å¾®ç±³ã€‚</p>
</blockquote>
<blockquote>
<p>A simple &ldquo;integrate and fire&rdquo; model captures much of the mathematics of what a compact nerve cell does (Junge, 1981). Figure 3 shows the time-dependent voltage difference between the inside and the outside of a simple functioning neuron. The electrical potential is generally slowly changing, but occasionally there is a stereotype voltage spike of about two milliseconds duration. Such a spike is produced every time the interior potential of this cell rises above a threshold, $u_{\text{thresh}}$, of about 53 millivolts. The voltage then resets to a $u_{\text{reset}}$ of about 270 millivolts. This &ldquo;action potential&rdquo; spike is caused by the dynamics of voltage-dependent ionic conductivities in the cell membrane. If an electrical current is injected into the cell, then except for the action potentials, the interior potential approximately obeys</p>
<p>$$
C\frac{\mathrm{d}u}{\mathrm{d}t} = -\frac{u-u_{\text{rest}}}{R} + i(t)
$$</p>
<p>where $R$ is the resistance of the cell membrane, $C$ the capacitance of the cell membrane, and $u_{\text{rest}}$ is the potential to which the cell tends to drift. If $i(t)$ is a constant $i_{c}$ , then the cell potential will change in an almost linear fashion between $u_{\text{rest}}$ and $u_{\text{thresh}}$ . An action potential will be generated each time $u_{\text{thresh}}$ is reached, resetting $u$ to $u_{\text{reset}}$ similar to what is seen in Fig. 3. The time $P$ between the equally spaced action potentials when $R$ is very large is</p>
<p>$$
P = C\frac{u_{\text{thresh}} - u_{\text{rest}}}{i_{c}},\quad \text{or firing rate}\quad \frac{1}{P}\sim i_{c}
$$</p>
<p>If $i_{c}$ is negative, no action potentials will be produced. The firing rate $1/P$ of a more realistic cell is not simply linear in $i_{c}$, but asymptotes to a maximum value of about 500 per second (due to the finite time duration of action potentials). It may also have a nonzero threshold current due to leakage currents (of either sign) in the membrane.</p>
</blockquote>
<p>ä¸€ä¸ªç®€å•çš„ &ldquo;ç§¯åˆ†å’Œå‘å°„&rdquo; æ¨¡å‹æ•æ‰äº†ç´§å‡‘ç¥ç»ç»†èƒæ‰€åšçš„è®¸å¤šæ•°å­¦å†…å®¹ï¼ˆJungeï¼Œ1981ï¼‰ã€‚å›¾ 3 æ˜¾ç¤ºäº†ç®€å•åŠŸèƒ½ç¥ç»å…ƒå†…å¤–ä¹‹é—´çš„æ—¶å˜ç”µå‹å·®ã€‚ç”µä½é€šå¸¸ç¼“æ…¢å˜åŒ–ï¼Œä½†å¶å°”ä¼šå‡ºç°å¤§çº¦ä¸¤æ¯«ç§’æŒç»­æ—¶é—´çš„å…¸å‹ç”µå‹å³°å€¼ã€‚æ¯å½“è¯¥ç»†èƒçš„å†…éƒ¨ç”µä½ä¸Šå‡åˆ°å¤§çº¦ 53 æ¯«ä¼çš„é˜ˆå€¼ $u_{\text{thresh}}$ ä»¥ä¸Šæ—¶ï¼Œå°±ä¼šäº§ç”Ÿè¿™æ ·çš„å³°å€¼ã€‚ç„¶åï¼Œç”µå‹é‡ç½®ä¸ºå¤§çº¦ 270 æ¯«ä¼çš„ $u_{\text{reset}}$ã€‚è¿™ç§ &ldquo;åŠ¨ä½œç”µä½&rdquo; å³°å€¼æ˜¯ç”±ç»†èƒè†œä¸­ç”µå‹ä¾èµ–æ€§ç¦»å­ç”µå¯¼çš„åŠ¨æ€å¼•èµ·çš„ã€‚å¦‚æœå‘ç»†èƒæ³¨å…¥ç”µæµï¼Œé‚£ä¹ˆé™¤äº†åŠ¨ä½œç”µä½å¤–ï¼Œç»†èƒå†…éƒ¨ç”µä½å¤§è‡´éµå¾ª</p>
<p>$$
C\frac{\mathrm{d}u}{\mathrm{d}t} = -\frac{u-u_{\text{rest}}}{R} + i(t)
$$</p>
<p>å…¶ä¸­ $R$ æ˜¯ç»†èƒè†œçš„ç”µé˜»ï¼Œ$C$ æ˜¯ç»†èƒè†œçš„ç”µå®¹ï¼Œ$u_{\text{rest}}$ æ˜¯ç»†èƒå€¾å‘äºæ¼‚ç§»åˆ°çš„ç”µä½ã€‚å¦‚æœ $i(t)$ æ˜¯ä¸€ä¸ªå¸¸æ•° $i_{c}$ï¼Œé‚£ä¹ˆç»†èƒç”µä½å°†åœ¨ $u_{\text{rest}}$ å’Œ $u_{\text{thresh}}$ ä¹‹é—´ä»¥å‡ ä¹çº¿æ€§çš„æ–¹å¼å˜åŒ–ã€‚æ¯æ¬¡è¾¾åˆ° $u_{\text{thresh}}$ æ—¶éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªåŠ¨ä½œç”µä½ï¼Œå°† $u$ é‡ç½®ä¸ºç±»ä¼¼äºå›¾ 3 ä¸­æ‰€ç¤ºçš„ $u_{\text{reset}}$ã€‚å½“ $R$ éå¸¸å¤§æ—¶ï¼Œç­‰é—´éš”åŠ¨ä½œç”µä½ä¹‹é—´çš„æ—¶é—´ $P$ ä¸º</p>
<p>$$
P = C\frac{u_{\text{thresh}} - u_{\text{rest}}}{i_{c}},\quad \text{or firing rate}\quad \frac{1}{P}\sim i_{c}
$$</p>
<p>å¦‚æœ $i_{c}$ ä¸ºè´Ÿï¼Œåˆ™ä¸ä¼šäº§ç”ŸåŠ¨ä½œç”µä½ã€‚æ›´ç°å®çš„ç»†èƒçš„å‘å°„ç‡ $1/P$ å¹¶ä¸æ˜¯ç®€å•åœ°ä¸ $i_{c}$ æˆçº¿æ€§å…³ç³»ï¼Œè€Œæ˜¯æ¸è¿‘äºæ¯ç§’çº¦ 500 çš„æœ€å¤§å€¼ï¼ˆç”±äºåŠ¨ä½œç”µä½çš„æœ‰é™æ—¶é—´æŒç»­ï¼‰ã€‚å®ƒä¹Ÿå¯èƒ½ç”±äºè†œä¸­çš„æ³„æ¼ç”µæµï¼ˆæ— è®ºç¬¦å·å¦‚ä½•ï¼‰è€Œå…·æœ‰éé›¶é˜ˆå€¼ç”µæµã€‚</p>
<blockquote>
<p>Action potentials will be taken to be $\delta$ functions, lasting a negligible time. They propagate at constant velocity along axons. When an action potential arrives at a synaptic terminal, it releases a neurotransmitter which activates specific ionic conductivity channels in the postsynaptic dendrite to which it makes contact (Kandel, Schwartz, and Jessell, 1991). This short conductivity pulse can be modeled by</p>
<p>$$
\begin{aligned}
\sigma(t) &amp;= 0\quad &amp;t&lt;t_{0}\\
&amp;= s_{kj}\exp{[-(t-t_{0})/\tau]}\quad &amp;t&gt;t_{0}
\end{aligned}
$$</p>
<p>The maximum conductivity of the postsynaptic membrane in response to the action potential is $s$. The ionspecific current which flows is equal to the chemical potential difference $V_{\text{ion}}$ times $s(t)$. Thus at a synapse from cell $j$ to cell $k$, an action potential arriving on axon $j$ at time $t_{0}$ results in a current</p>
<p>$$
\begin{aligned}
i_{kj}(t) &amp;= 0\quad &amp;t&lt;t_{0}\\
&amp;= S_{kj}\exp{[-(t-t_{0})/\tau]}\quad &amp;t&gt;t_{0}
\end{aligned}
$$</p>
<p>flows into the cell $k$. The parameter $S_{kj} = V_{\text{ion}}s_{kj}$ can take either sign. Define the instantaneous firing rate of neuron $k$, which generates action potentials at times $t_{n}^{k}$,  as</p>
<p>$$
f_{k}(t) = \sum_{n}\delta(t-t_{n}^{k})
$$</p>
<p>By differentiation and substitution</p>
<p>$$
\frac{\mathrm{d}i_{k}}{\mathrm{d}t} = -\frac{i_{k}}{\tau} + \sum_{j}S_{kj} \cdot f_{j}(t) + \text{another term if a sensory cell}
$$</p>
<p>This equation, though exact, is awkward to deal with because the times at which the action potentials occur are only implicitly given.</p>
</blockquote>
<p>åŠ¨ä½œç”µä½å°†è¢«è§†ä¸ºæŒç»­æ—¶é—´å¯å¿½ç•¥çš„ $\delta$ å‡½æ•°ã€‚å®ƒä»¬ä»¥æ’å®šé€Ÿåº¦æ²¿è½´çªä¼ æ’­ã€‚å½“åŠ¨ä½œç”µä½åˆ°è¾¾çªè§¦æœ«ç«¯æ—¶ï¼Œå®ƒä¼šé‡Šæ”¾ä¸€ç§ç¥ç»é€’è´¨ï¼Œæ¿€æ´»ä¸å…¶æ¥è§¦çš„çªè§¦åæ ‘çªä¸­çš„ç‰¹å®šç¦»å­ç”µå¯¼é€šé“ï¼ˆKandelã€Schwartz å’Œ Jessellï¼Œ1991ï¼‰ã€‚è¿™ç§çŸ­æš‚çš„ç”µå¯¼è„‰å†²å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å»ºæ¨¡</p>
<p>$$
\begin{aligned}
\sigma(t) &amp;= 0\quad &amp;t&lt;t_{0}\\
&amp;= s_{kj}\exp{[-(t-t_{0})/\tau]}\quad &amp;t&gt;t_{0}
\end{aligned}
$$</p>
<p>çªè§¦åè†œå¯¹åŠ¨ä½œç”µä½çš„æœ€å¤§ç”µå¯¼ä¸º $s$ã€‚æµåŠ¨çš„ç¦»å­ç‰¹å¼‚æ€§ç”µæµç­‰äºåŒ–å­¦åŠ¿å·® $V_{\text{ion}}$ ä¹˜ä»¥ $s(t)$ã€‚å› æ­¤ï¼Œåœ¨ä»ç»†èƒ $j$ åˆ°ç»†èƒ $k$ çš„çªè§¦å¤„ï¼Œåœ¨æ—¶é—´ $t_{0}$ åœ¨è½´çª $j$ ä¸Šåˆ°è¾¾çš„åŠ¨ä½œç”µä½ä¼šå¯¼è‡´ä¸€ä¸ªç”µæµ</p>
<p>$$
\begin{aligned}
i_{kj}(t) &amp;= 0\quad &amp;t&lt;t_{0}\\
&amp;= S_{kj}\exp{[-(t-t_{0})/\tau]}\quad &amp;t&gt;t_{0}
\end{aligned}
$$</p>
<p>æµå…¥ç»†èƒ $k$ã€‚å‚æ•° $S_{kj} = V_{\text{ion}}s_{kj}$ å¯ä»¥å–ä»»æ„ç¬¦å·ã€‚å®šä¹‰ç¥ç»å…ƒ $k$ çš„ç¬æ—¶å‘å°„ç‡ï¼Œè¯¥ç¥ç»å…ƒåœ¨æ—¶é—´ $t_{n}^{k}$ äº§ç”ŸåŠ¨ä½œç”µä½ï¼Œä¸º</p>
<p>$$
f_{k}(t) = \sum_{n}\delta(t-t_{n}^{k})
$$</p>
<p>é€šè¿‡å¾®åˆ†å’Œæ›¿æ¢</p>
<p>$$
\frac{\mathrm{d}i_{k}}{\mathrm{d}t} = -\frac{i_{k}}{\tau} + \sum_{j}S_{kj} \cdot f_{j}(t) + \text{another term if a sensory cell}
$$</p>
<p>è¿™ä¸ªæ–¹ç¨‹è™½ç„¶æ˜¯ç²¾ç¡®çš„ï¼Œä½†å¤„ç†èµ·æ¥å¾ˆå°´å°¬ï¼Œå› ä¸ºåŠ¨ä½œç”µä½å‘ç”Ÿçš„æ—¶é—´åªæ˜¯éšå¼ç»™å‡ºçš„ã€‚</p>
<blockquote>
<p>The usual approximation relies on there being many contributions to the sum in Eq. (6) during a reasonable time interval due to the high connectivity. It should then be permissible to ignore the spiky nature of $f_{j}(t)$, replacing it by a convolution with a smoothing function. In addition, the functional form of $V(i_{c})$ is presumed to hold when $i_{c}$ is slowly varying in time. What results is like Eq. (6), but with fj(t) now a smooth function given by $f_{j}(t) = V[i_{j}(t)] = V[i_{j}(t)]$:</p>
<p>$$
\frac{\mathrm{d}i_{k}}{\mathrm{d}t} = -\frac{i_{k}}{\tau} + \sum_{j}S_{kj} \cdot V[i_{j}(t)] + I_{k}(\text{last term only if a sensory cell})
$$</p>
<p>The main effect of the approximation, which assumes no strong correlations between spikes of different neurons, is to neglect shot noise. (Electrical circuits using continuous variables are based on a similar approximation.) While equations of this structure are in common use, they have somewhat evolved, and do not have a sharp original reference.</p>
</blockquote>
<p>é€šå¸¸çš„è¿‘ä¼¼ä¾èµ–äºç”±äºé«˜è¿æ¥æ€§ï¼Œåœ¨åˆç†çš„æ—¶é—´é—´éš”å†…å¯¹æ–¹ç¨‹ï¼ˆ6ï¼‰ä¸­çš„æ€»å’Œæœ‰è®¸å¤šè´¡çŒ®ã€‚ç„¶ååº”è¯¥å…è®¸å¿½ç•¥ $f_{j}(t)$ çš„å°–å³°æ€§è´¨ï¼Œç”¨ä¸å¹³æ»‘å‡½æ•°çš„å·ç§¯æ¥æ›¿ä»£å®ƒã€‚æ­¤å¤–ï¼Œå½“ $i_{c}$ éšæ—¶é—´ç¼“æ…¢å˜åŒ–æ—¶ï¼Œå‡å®š $V(i_{c})$ çš„å‡½æ•°å½¢å¼æˆç«‹ã€‚ç»“æœç±»ä¼¼äºæ–¹ç¨‹ï¼ˆ6ï¼‰ï¼Œä½†ç°åœ¨ fj(t) æ˜¯ç”± $f_{j}(t) = V[i_{j}(t)] = V[i_{j}(t)]$ ç»™å‡ºçš„å¹³æ»‘å‡½æ•°ï¼š</p>
<p>$$
\frac{\mathrm{d}i_{k}}{\mathrm{d}t} = -\frac{i_{k}}{\tau} + \sum_{j}S_{kj} \cdot V[i_{j}(t)] + I_{k}(\text{last term only if a sensory cell})
$$</p>
<p>è¯¥è¿‘ä¼¼çš„ä¸»è¦æ•ˆæœæ˜¯å‡è®¾ä¸åŒç¥ç»å…ƒçš„å³°å€¼ä¹‹é—´æ²¡æœ‰å¼ºç›¸å…³æ€§ï¼Œä»è€Œå¿½ç•¥äº†æ•£å¼¹å™ªå£°ã€‚ï¼ˆä½¿ç”¨è¿ç»­å˜é‡çš„ç”µè·¯åŸºäºç±»ä¼¼çš„è¿‘ä¼¼ã€‚ï¼‰è™½ç„¶è¿™ç§ç»“æ„çš„æ–¹ç¨‹è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†å®ƒä»¬å·²ç»æœ‰æ‰€å‘å±•ï¼Œå¹¶ä¸”æ²¡æœ‰æ˜ç¡®çš„åŸå§‹å‚è€ƒæ–‡çŒ®ã€‚</p>
<h1 id="the-dynamics-of-synapses">THE DYNAMICS OF SYNAPSES<a hidden class="anchor" aria-hidden="true" href="#the-dynamics-of-synapses">#</a></h1>
<blockquote>
<p>The second dynamical equation for neuronal dynamics describes the changes in the synaptic connections. In neurobiology, a synapse can modify its strength or its temporary behavior in the following ways:</p>
<ol>
<li>As a result of the activity of its presynaptic neuron, independent of the activity of the postsynaptic neuron.</li>
<li>As a result of the activity of its postsynaptic neuron, independent of the activity of the presynaptic neuron.</li>
<li>As a result of the coordinated activity of the preand postsynaptic neurons.</li>
<li>As a result of the regional release of a neuromodulator. Neuromodulators also can modulate processes 1, 2, and 3.</li>
</ol>
<p>The most interesting of these is (3) in which the synapse strength $S_{kj}$ changes as a result of the roughly simultaneous activity of cells $k$ and $j$. This kind of change is needed to represent information about the association between two events. A synapse whose change algorithm involves only the simultaneous activity of the pre- and postsynaptic neurons and no other detailed information is now called a Hebbian synapse (Hebb, 1949). A simple version of such dynamics (using firing rates rather than detailed times of individual action potentials) might be written</p>
<p>$$
\frac{\mathrm{d}S_{kj}}{\mathrm{d}t} = \alpha \cdot i_{k} \cdot f_{j}(t) - \text{decay terms}
$$</p>
<p>Decay terms, perhaps involving $i_{k}$ and $f(i_{j})$, are essential to forget old information. A nonlinearity or control process is important to keep synapse strength from increasing without bound. The learning rate a might also be varied by neuromodulator molecules which control the overall learning process. The details of synaptic modification biophysics are not completely established, and Eq. (8) is only qualitative. A somewhat better approximation replaces a by a kernel over time and involves a more complicated form in $i$ and $f$. Long-term potentiation (LTP) is the most significant paradigm of neurobiological synapse modification (Kandel, Schwartz, and Jessell, 1991). Synapse change rules of a Hebbian type have been found to reproduce results of a variety of experiments on the development of the eye dominance and orientation selectivity of cells in the visual cortex of the cat (Bear, Cooper, and Ebner, 1987).</p>
</blockquote>
<p>ç¥ç»åŠ¨åŠ›å­¦çš„ç¬¬äºŒä¸ªåŠ¨æ€æ–¹ç¨‹æè¿°äº†çªè§¦è¿æ¥çš„å˜åŒ–ã€‚åœ¨ç¥ç»ç”Ÿç‰©å­¦ä¸­ï¼Œçªè§¦å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿®æ”¹å…¶å¼ºåº¦æˆ–ä¸´æ—¶è¡Œä¸ºï¼š</p>
<ol>
<li>ä½œä¸ºå…¶çªè§¦å‰ç¥ç»å…ƒæ´»åŠ¨çš„ç»“æœï¼Œä¸çªè§¦åç¥ç»å…ƒçš„æ´»åŠ¨æ— å…³ã€‚</li>
<li>ä½œä¸ºå…¶çªè§¦åç¥ç»å…ƒæ´»åŠ¨çš„ç»“æœï¼Œä¸çªè§¦å‰ç¥ç»å…ƒçš„æ´»åŠ¨æ— å…³ã€‚</li>
<li>ä½œä¸ºçªè§¦å‰å’Œçªè§¦åç¥ç»å…ƒåè°ƒæ´»åŠ¨çš„ç»“æœã€‚</li>
<li>ä½œä¸ºç¥ç»è°ƒèŠ‚å‰‚åŒºåŸŸé‡Šæ”¾çš„ç»“æœã€‚ç¥ç»è°ƒèŠ‚å‰‚è¿˜å¯ä»¥è°ƒèŠ‚è¿‡ç¨‹ 1ã€2 å’Œ 3ã€‚</li>
</ol>
<p>å…¶ä¸­æœ€æœ‰è¶£çš„æ˜¯ï¼ˆ3ï¼‰ï¼Œå…¶ä¸­çªè§¦å¼ºåº¦ $S_{kj}$ ç”±äºç»†èƒ $k$ å’Œ $j$ çš„å¤§è‡´åŒæ—¶æ´»åŠ¨è€Œå‘ç”Ÿå˜åŒ–ã€‚è¿™ç§å˜åŒ–å¯¹äºè¡¨ç¤ºä¸¤ä¸ªäº‹ä»¶ä¹‹é—´å…³è”çš„ä¿¡æ¯æ˜¯å¿…è¦çš„ã€‚å…¶å˜åŒ–ç®—æ³•ä»…æ¶‰åŠçªè§¦å‰å’Œçªè§¦åç¥ç»å…ƒçš„åŒæ—¶æ´»åŠ¨ä¸”æ²¡æœ‰å…¶ä»–è¯¦ç»†ä¿¡æ¯çš„çªè§¦ç°åœ¨ç§°ä¸º Hebbian çªè§¦ï¼ˆHebbï¼Œ1949ï¼‰ã€‚è¿™ç§åŠ¨åŠ›å­¦çš„ä¸€ä¸ªç®€å•ç‰ˆæœ¬ï¼ˆä½¿ç”¨å‘å°„ç‡è€Œä¸æ˜¯å•ä¸ªåŠ¨ä½œç”µä½çš„è¯¦ç»†æ—¶é—´ï¼‰å¯ä»¥å†™æˆ</p>
<p>$$
\frac{\mathrm{d}S_{kj}}{\mathrm{d}t} = \alpha \cdot i_{k} \cdot f_{j}(t) - \text{decay terms}
$$</p>
<p>è¡°å‡é¡¹ï¼Œå¯èƒ½æ¶‰åŠ $i_{k}$ å’Œ $f(i_{j})$ï¼Œå¯¹äºå¿˜è®°æ—§ä¿¡æ¯æ˜¯å¿…ä¸å¯å°‘çš„ã€‚éçº¿æ€§æˆ–æ§åˆ¶è¿‡ç¨‹å¯¹äºé˜²æ­¢çªè§¦å¼ºåº¦æ— é™å¢åŠ éå¸¸é‡è¦ã€‚å­¦ä¹ ç‡ a ä¹Ÿå¯èƒ½é€šè¿‡æ§åˆ¶æ•´ä½“å­¦ä¹ è¿‡ç¨‹çš„ç¥ç»è°ƒèŠ‚åˆ†å­æ¥å˜åŒ–ã€‚çªè§¦ä¿®æ”¹ç”Ÿç‰©ç‰©ç†å­¦çš„ç»†èŠ‚å°šæœªå®Œå…¨ç¡®å®šï¼Œæ–¹ç¨‹ï¼ˆ8ï¼‰åªæ˜¯å®šæ€§çš„ã€‚ä¸€ä¸ªç¨å¾®å¥½ä¸€ç‚¹çš„è¿‘ä¼¼æ˜¯ç”¨æ—¶é—´å†…çš„æ ¸æ›¿æ¢ aï¼Œå¹¶æ¶‰åŠ $i$ å’Œ $f$ ä¸­æ›´å¤æ‚çš„å½¢å¼ã€‚é•¿æœŸå¢å¼ºï¼ˆLTPï¼‰æ˜¯ç¥ç»ç”Ÿç‰©å­¦çªè§¦ä¿®æ”¹çš„æœ€é‡è¦èŒƒä¾‹ï¼ˆKandelã€Schwartz å’Œ Jessellï¼Œ1991ï¼‰ã€‚å·²ç»å‘ç° Hebbian ç±»å‹çš„çªè§¦å˜åŒ–è§„åˆ™å¯ä»¥é‡ç°å¯¹çŒ«è§†è§‰çš®å±‚ä¸­ç»†èƒçš„çœ¼ç›ä¼˜åŠ¿å’Œæ–¹å‘é€‰æ‹©æ€§çš„å„ç§å®éªŒç»“æœï¼ˆBearã€Cooper å’Œ Ebnerï¼Œ1987ï¼‰ã€‚</p>
<blockquote>
<p>The tacit assumption is often made that synapse change is involved in learning and development, and that the dynamics of neural activity is what performs a computation. However, the dynamics of synapse modification should not be ignored as a possible tool for doing computation.</p>
</blockquote>
<p>é€šå¸¸å‡è®¾çªè§¦å˜åŒ–æ¶‰åŠå­¦ä¹ å’Œå‘å±•ï¼Œè€Œç¥ç»æ´»åŠ¨çš„åŠ¨æ€æ‰§è¡Œè®¡ç®—ã€‚ç„¶è€Œï¼Œä¸åº”å¿½è§†çªè§¦ä¿®æ”¹çš„åŠ¨æ€ä½œä¸ºè¿›è¡Œè®¡ç®—çš„å¯èƒ½å·¥å…·ã€‚</p>
<h1 id="programming-languages-for-artificial-neural-networks-ann">PROGRAMMING LANGUAGES FOR ARTIFICIAL NEURAL NETWORKS (ANN)<a hidden class="anchor" aria-hidden="true" href="#programming-languages-for-artificial-neural-networks-ann">#</a></h1>
<blockquote>
<p>Let batch-mode computation, simple (point) attractor dynamics, and fixed connections be our initial &ldquo;neural network&rdquo; computing paradigm. The connections need to be chosen so that for any input (&ldquo;data&rdquo;) the network activity will go to a stable state, and so that the state achieved from a given input is the desired &ldquo;answer.&rdquo; Is there a programming language? The simplest approaches to this issue involve establishing an overall architecture or &ldquo;anatomy&rdquo; for the network which guarantees going to a stable state. Within this given architecture, the connections can be arbitrarily chosen. &ldquo;Programming&rdquo; involves the &ldquo;inverse problem&rdquo; of finding the set of connections for which the dynamics will carry out the desired task.</p>
</blockquote>
<p>è®©æ‰¹å¤„ç†è®¡ç®—ã€ç®€å•ï¼ˆç‚¹ï¼‰å¸å¼•å­åŠ¨åŠ›å­¦å’Œå›ºå®šè¿æ¥æˆä¸ºæˆ‘ä»¬æœ€åˆçš„ &ldquo;ç¥ç»ç½‘ç»œ&rdquo; è®¡ç®—èŒƒä¾‹ã€‚éœ€è¦é€‰æ‹©è¿æ¥ï¼Œä»¥ä¾¿å¯¹äºä»»ä½•è¾“å…¥ï¼ˆ&ldquo;æ•°æ®&rdquo;ï¼‰ï¼Œç½‘ç»œæ´»åŠ¨éƒ½å°†è¿›å…¥ç¨³å®šçŠ¶æ€ï¼Œå¹¶ä¸”ä»ç»™å®šè¾“å…¥è¾¾åˆ°çš„çŠ¶æ€æ˜¯æ‰€éœ€çš„ &ldquo;ç­”æ¡ˆ&rdquo;ã€‚æ˜¯å¦å­˜åœ¨ä¸€ç§ç¼–ç¨‹è¯­è¨€ï¼Ÿå¯¹æ­¤é—®é¢˜çš„æœ€ç®€å•æ–¹æ³•æ¶‰åŠä¸ºç½‘ç»œå»ºç«‹ä¸€ä¸ªæ•´ä½“æ¶æ„æˆ– &ldquo;è§£å‰–ç»“æ„&rdquo;ï¼Œä»¥ä¿è¯è¿›å…¥ç¨³å®šçŠ¶æ€ã€‚åœ¨è¿™ä¸ªç»™å®šçš„æ¶æ„å†…ï¼Œè¿æ¥å¯ä»¥ä»»æ„é€‰æ‹©ã€‚&ldquo;ç¼–ç¨‹&rdquo; æ¶‰åŠå¯»æ‰¾ä¸€ç»„è¿æ¥çš„ &ldquo;é€†é—®é¢˜&rdquo;ï¼Œå…¶åŠ¨åŠ›å­¦å°†æ‰§è¡Œæ‰€éœ€çš„ä»»åŠ¡ã€‚</p>
<h2 id="feed-forward-networks">Feed-forward networks<a hidden class="anchor" aria-hidden="true" href="#feed-forward-networks">#</a></h2>
<blockquote>
<p>The simplest two styles of networks for computation are shown in Fig. 4. The feed-forward network is mathematically like a set of connected nonlinear amplifiers without feedback paths, and is trivially stable.</p>
</blockquote>
<p>æœ€ç®€å•çš„ä¸¤ç§è®¡ç®—ç½‘ç»œæ ·å¼å¦‚å›¾ 4 æ‰€ç¤ºã€‚å‰é¦ˆç½‘ç»œåœ¨æ•°å­¦ä¸Šç±»ä¼¼äºä¸€ç»„è¿æ¥çš„éçº¿æ€§æ”¾å¤§å™¨ï¼Œæ²¡æœ‰åé¦ˆè·¯å¾„ï¼Œå¹¶ä¸”æ˜¯å¹³å‡¡ç¨³å®šçš„ã€‚</p>
<blockquote>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/11/20/BvpOzyU15JWEekG.png" alt=""  /></p>
<p>Two extreme forms of neural networks with good stability properties but very different complexities of dynamics and learning. The feedback network can be proved stable if it has symmetric connections. Scaling of variables generates a broad class of networks which are equivalent to symmetric networks, and surprisingly, the feed-forward network can be obtained from a symmetric network by scaling.</p>
</blockquote>
<p>å…·æœ‰è‰¯å¥½ç¨³å®šæ€§ä½†åŠ¨æ€å’Œå­¦ä¹ å¤æ‚æ€§éå¸¸ä¸åŒçš„ä¸¤ç§æç«¯å½¢å¼çš„ç¥ç»ç½‘ç»œã€‚åé¦ˆç½‘ç»œå¦‚æœå…·æœ‰å¯¹ç§°è¿æ¥åˆ™å¯ä»¥è¯æ˜æ˜¯ç¨³å®šçš„ã€‚å˜é‡çš„ç¼©æ”¾ç”Ÿæˆäº†ä¸å¯¹ç§°ç½‘ç»œç­‰æ•ˆçš„å¹¿æ³›ç±»åˆ«çš„ç½‘ç»œï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå‰é¦ˆç½‘ç»œå¯ä»¥é€šè¿‡ç¼©æ”¾ä»å¯¹ç§°ç½‘ç»œä¸­è·å¾—ã€‚</p>
</blockquote>
<blockquote>
<p>This fact allows us to evaluate how much computation must be done to find the stable point. It is:</p>
<p>$$
(1\text{ multiply}+1\text{ add})(\text{number of connections}) + (\text{number of &ldquo;neurons&rdquo;})(1\text{ look-up})
$$</p>
</blockquote>
<p>è¿™ä¸ªäº‹å®ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯„ä¼°æ‰¾åˆ°ç¨³å®šç‚¹å¿…é¡»è¿›è¡Œå¤šå°‘è®¡ç®—ã€‚å®ƒæ˜¯ï¼š</p>
<p>$$
(1\text{ multiply}+1\text{ add})(\text{number of connections}) + (\text{number of &ldquo;neurons&rdquo;})(1\text{ look-up})
$$</p>
<blockquote>
<p>This evaluation requires no dynamics and involves a trivial amount of computation. How is it then that feedforward ANNâ€™s, which have almost no computing power, are very useful even when implemented inefficiently on digital machines? The answer is that their utility comes chiefly from the immense computational work necessary to find an appropriate set of connections for a problem which is implicitly described by a large data base. The resulting network is a compact representation of the data, which allows it to be used with much less computational effort than would otherwise be necessary. The output of the network is merely a function of its input. In this case the problem of finding the best set of connections reduces to finding the set of connections that minimizes output error.</p>
</blockquote>
<p>è¿™ç§è¯„ä¼°ä¸éœ€è¦åŠ¨æ€ï¼Œå¹¶ä¸”æ¶‰åŠå¾®ä¸è¶³é“çš„è®¡ç®—é‡ã€‚é‚£ä¹ˆï¼Œå‡ ä¹æ²¡æœ‰è®¡ç®—èƒ½åŠ›çš„å‰é¦ˆ ANN æ˜¯å¦‚ä½•éå¸¸æœ‰ç”¨çš„ï¼Œå³ä½¿åœ¨æ•°å­—æœºå™¨ä¸Šå®ç°æ•ˆç‡ä½ä¸‹ï¼Ÿç­”æ¡ˆæ˜¯ï¼Œå®ƒä»¬çš„æ•ˆç”¨ä¸»è¦æ¥è‡ªäºä¸ºä¸€ä¸ªç”±å¤§å‹æ•°æ®åº“éšå¼æè¿°çš„é—®é¢˜æ‰¾åˆ°é€‚å½“è¿æ¥é›†æ‰€éœ€çš„å¤§é‡è®¡ç®—å·¥ä½œã€‚ç”±æ­¤äº§ç”Ÿçš„ç½‘ç»œæ˜¯æ•°æ®çš„ç´§å‡‘è¡¨ç¤ºï¼Œè¿™ä½¿å¾—å®ƒå¯ä»¥ä»¥æ¯”å…¶ä»–æ–¹å¼æ‰€éœ€çš„è®¡ç®—å·¥ä½œé‡å°‘å¾—å¤šçš„æ–¹å¼ä½¿ç”¨ã€‚ç½‘ç»œçš„è¾“å‡ºä»…ä»…æ˜¯å…¶è¾“å…¥çš„å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰¾åˆ°æœ€ä½³è¿æ¥é›†çš„é—®é¢˜ç®€åŒ–ä¸ºæ‰¾åˆ°æœ€å°åŒ–è¾“å‡ºè¯¯å·®çš„è¿æ¥é›†ã€‚</p>
<blockquote>
<p>When the inputs of all amplifiers are connected by a network to the external inputs and the outputs of the same amplifiers are used as the desired outputs, a feedforward network is said to have &ldquo;no hidden units.&rdquo; If the amplifiers have a continuous input-output relation, a best set of connections can be found by starting with a random set of connections and doing gradient descent on the error function. For most problems, the terrain is relatively smooth, and there is little difficulty of being trapped in poor local minima by doing gradient descent. When the input-output relation is a step function, as it was chosen to be in the perceptron (Rosenblatt, 1962), the problem is somewhat more difficult, but satisfactory algorithms can still be found. An interesting &ldquo;statistical mechanics&rdquo; of their capabilities in random problems has been described (Gardiner, 1988).</p>
</blockquote>
<p>å½“æ‰€æœ‰æ”¾å¤§å™¨çš„è¾“å…¥é€šè¿‡ç½‘ç»œè¿æ¥åˆ°å¤–éƒ¨è¾“å…¥ï¼Œå¹¶ä¸”ç›¸åŒæ”¾å¤§å™¨çš„è¾“å‡ºç”¨ä½œæ‰€éœ€è¾“å‡ºæ—¶ï¼Œå‰é¦ˆç½‘ç»œè¢«ç§°ä¸º &ldquo;æ— éšè—å•å…ƒ&rdquo;ã€‚å¦‚æœæ”¾å¤§å™¨å…·æœ‰è¿ç»­çš„è¾“å…¥è¾“å‡ºå…³ç³»ï¼Œåˆ™å¯ä»¥é€šè¿‡ä»éšæœºè¿æ¥é›†å¼€å§‹å¹¶å¯¹è¯¯å·®å‡½æ•°è¿›è¡Œæ¢¯åº¦ä¸‹é™æ¥æ‰¾åˆ°æœ€ä½³è¿æ¥é›†ã€‚å¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼Œåœ°å½¢ç›¸å¯¹å¹³æ»‘ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™é™·å…¥è¾ƒå·®çš„å±€éƒ¨æå°å€¼å‡ ä¹æ²¡æœ‰å›°éš¾ã€‚å½“è¾“å…¥è¾“å‡ºå…³ç³»æ˜¯é˜¶è·ƒå‡½æ•°æ—¶ï¼Œå°±åƒåœ¨æ„ŸçŸ¥å™¨ä¸­é€‰æ‹©çš„é‚£æ ·ï¼ˆRosenblattï¼Œ1962ï¼‰ï¼Œé—®é¢˜å°±æœ‰äº›æ›´éš¾ï¼Œä½†ä»ç„¶å¯ä»¥æ‰¾åˆ°ä»¤äººæ»¡æ„çš„ç®—æ³•ã€‚å·²ç»æè¿°äº†å®ƒä»¬åœ¨éšæœºé—®é¢˜ä¸­çš„èƒ½åŠ›çš„æœ‰è¶£çš„ &ldquo;ç»Ÿè®¡åŠ›å­¦&rdquo;ï¼ˆGardinerï¼Œ1988ï¼‰ã€‚</p>
<blockquote>
<p>Unfortunately, networks with a single layer of weights are severely limited in the functions they can represent. The detailed description of that limitation by Minsky and Pappert (1969) and &ldquo;our view that the extension [to multiple layers] is sterile&rdquo; had a great deal to do with destroying a budding perceptron enthusiasm in the 1960s. It was even then clear that networks with hidden units are much more powerful, but the &ldquo;failure to produce an interesting learning theorem for the multilayered machine&rdquo; was chilling.</p>
</blockquote>
<p>ä¸å¹¸çš„æ˜¯ï¼Œå…·æœ‰å•å±‚æƒé‡çš„ç½‘ç»œåœ¨å®ƒä»¬å¯ä»¥è¡¨ç¤ºçš„å‡½æ•°æ–¹é¢å—åˆ°ä¸¥é‡é™åˆ¶ã€‚Minsky å’Œ Pappertï¼ˆ1969ï¼‰å¯¹è¯¥é™åˆ¶çš„è¯¦ç»†æè¿°ä»¥åŠ &ldquo;æˆ‘ä»¬è®¤ä¸ºæ‰©å±•[åˆ°å¤šå±‚]æ˜¯æ— æ•ˆçš„&rdquo; åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ‘§æ¯äº† 1960 å¹´ä»£èŒèŠ½çš„æ„ŸçŸ¥å™¨çƒ­æƒ…ã€‚å³ä½¿é‚£æ—¶ä¹Ÿå¾ˆæ¸…æ¥šï¼Œå…·æœ‰éšè—å•å…ƒçš„ç½‘ç»œæ›´åŠ å¼ºå¤§ï¼Œä½† &ldquo;æœªèƒ½ä¸ºå¤šå±‚æœºå™¨äº§ç”Ÿæœ‰è¶£çš„å­¦ä¹ å®šç†&rdquo; æ˜¯ä»¤äººå¯’å¿ƒçš„ã€‚</p>
<blockquote>
<p>For the analog feed-forward ANN with hidden units, the problem of finding the best fit to a desired inputoutput relation is relatively simple since the output can be explicitly written in terms of the inputs and connections. Gradient descent on the error surface in &ldquo;weight space&rdquo; can be carried out, beginning from small random initial connections, to find a locally optimal solution to the problem. This elegant simple point was noted by Werbos (1974), but had no impact at the time, and was independently rediscovered at least twice in the 1980s. A variety of more complex ways to find good sets of connections have since been explored.</p>
</blockquote>
<p>å¯¹äºå…·æœ‰éšè—å•å…ƒçš„æ¨¡æ‹Ÿå‰é¦ˆ ANNï¼Œæ‰¾åˆ°ä¸æ‰€éœ€è¾“å…¥è¾“å‡ºå…³ç³»çš„æœ€ä½³æ‹Ÿåˆçš„é—®é¢˜ç›¸å¯¹ç®€å•ï¼Œå› ä¸ºè¾“å‡ºå¯ä»¥ç”¨è¾“å…¥å’Œè¿æ¥æ˜ç¡®åœ°å†™å‡ºæ¥ã€‚å¯ä»¥åœ¨ &ldquo;æƒé‡ç©ºé—´&rdquo; ä¸­å¯¹è¯¯å·®è¡¨é¢è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œä»å°çš„éšæœºåˆå§‹è¿æ¥å¼€å§‹ï¼Œä»¥æ‰¾åˆ°é—®é¢˜çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚Werbosï¼ˆ1974ï¼‰æ³¨æ„åˆ°äº†è¿™ä¸€ä¼˜é›…è€Œç®€å•çš„è§‚ç‚¹ï¼Œä½†å½“æ—¶æ²¡æœ‰å½±å“ï¼Œå¹¶ä¸”åœ¨ 1980 å¹´ä»£è‡³å°‘è¢«ç‹¬ç«‹é‡æ–°å‘ç°äº†ä¸¤æ¬¡ã€‚æ­¤åï¼Œå·²ç»æ¢ç´¢äº†å„ç§æ›´å¤æ‚çš„æ–¹æ³•æ¥æ‰¾åˆ°è‰¯å¥½çš„è¿æ¥é›†ã€‚</p>
<blockquote>
<p>Why was the Werbos suggestion not followed up and subsequently lost? Several factors were involved. First, the landscape of the function on which gradient descent is being done is very rugged; local minima abound, and whether a useful network can be found is a computational issue, not a question which can be demonstrated from mathematics. There was little understanding of such landscapes at the time. Worse, the demonstrations that such a simple procedure would actually work consumed an immense amount of computer cycles even in its time (1983-5) and would have been impossibly costly on the computers of 1973. Artificial intelligence was still in full bloom, and no one that was interested in pattern recognition would waste machine cycles on searches in spaces having hundreds of dimensions (parameters) when sheer logic and rules seemed all that was necessary.</p>
</blockquote>
<p>ä¸ºä»€ä¹ˆæ²¡æœ‰è·Ÿè¿› Werbos çš„å»ºè®®å¹¶éšåå¤±å»å®ƒï¼Ÿæ¶‰åŠå‡ ä¸ªå› ç´ ã€‚é¦–å…ˆï¼Œæ­£åœ¨è¿›è¡Œæ¢¯åº¦ä¸‹é™çš„å‡½æ•°çš„æ™¯è§‚éå¸¸å´å²–ï¼›å±€éƒ¨æå°å€¼æ¯”æ¯”çš†æ˜¯ï¼Œæ˜¯å¦å¯ä»¥æ‰¾åˆ°æœ‰ç”¨çš„ç½‘ç»œæ˜¯ä¸€ä¸ªè®¡ç®—é—®é¢˜ï¼Œè€Œä¸æ˜¯å¯ä»¥é€šè¿‡æ•°å­¦è¯æ˜çš„é—®é¢˜ã€‚å½“æ—¶å¯¹è¿™ç§æ™¯è§‚å‡ ä¹æ²¡æœ‰äº†è§£ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œè¯æ˜è¿™æ ·ä¸€ä¸ªç®€å•çš„ç¨‹åºå®é™…ä¸Šä¼šèµ·ä½œç”¨ï¼Œå³ä½¿åœ¨å½“æ—¶ï¼ˆ1983-5ï¼‰ä¹Ÿæ¶ˆè€—äº†å¤§é‡çš„è®¡ç®—æœºå‘¨æœŸï¼Œåœ¨ 1973 å¹´çš„è®¡ç®—æœºä¸Šå°†æ˜¯ä¸å¯èƒ½æ‰¿å—çš„æˆæœ¬ã€‚äººå·¥æ™ºèƒ½ä»ç„¶å¤„äºå…¨é¢ç¹è£é˜¶æ®µï¼Œå¯¹æ¨¡å¼è¯†åˆ«æ„Ÿå…´è¶£çš„äººä¸ä¼šåœ¨å…·æœ‰æ•°ç™¾ä¸ªç»´åº¦ï¼ˆå‚æ•°ï¼‰çš„ç©ºé—´ä¸­æµªè´¹æœºå™¨å‘¨æœŸè¿›è¡Œæœç´¢ï¼Œè€Œçº¯ç²¹çš„é€»è¾‘å’Œè§„åˆ™ä¼¼ä¹æ˜¯å¿…è¦çš„ã€‚</p>
<blockquote>
<p>And finally, the procedure looks absurd. Consider as a physicist, being told to fit a 200-parameter, highly nonlinear model to 500 data points. (And sometimes, the authors would be fitting 200 parameters to 150 data points!) We were all brought up on the Wignerism &ldquo;if you give me two free parameters, I can describe an elephant. If you give me three, I can make him viggle his tail.&rdquo; We all knew that the parameters would be meaningless. And so they are. Two tries from initially different random starting sets of connections usually wind up with entirely different parameters. For most problems, the connection strengths seem to have little meaning. What is useful in this case, however, is not the connection strengths, but the quality of fit to the data. The situation is entirely different from the usual scientific &ldquo;fits&rdquo; to data, normally designed chiefly to derive meaningful parameters.</p>
</blockquote>
<p>æœ€åï¼Œè¿™ä¸ªç¨‹åºçœ‹èµ·æ¥å¾ˆè’è°¬ã€‚ä½œä¸ºä¸€ä¸ªç‰©ç†å­¦å®¶ï¼Œè€ƒè™‘è¢«å‘ŠçŸ¥å°†ä¸€ä¸ªå…·æœ‰ 200 ä¸ªå‚æ•°çš„é«˜åº¦éçº¿æ€§æ¨¡å‹æ‹Ÿåˆåˆ° 500 ä¸ªæ•°æ®ç‚¹ã€‚ï¼ˆæœ‰æ—¶ï¼Œä½œè€…ä¼šå°† 200 ä¸ªå‚æ•°æ‹Ÿåˆåˆ° 150 ä¸ªæ•°æ®ç‚¹ï¼ï¼‰æˆ‘ä»¬éƒ½å—è¿‡ Wignerism çš„æ•™è‚² &ldquo;å¦‚æœä½ ç»™æˆ‘ä¸¤ä¸ªè‡ªç”±å‚æ•°ï¼Œæˆ‘å¯ä»¥æè¿°ä¸€åªå¤§è±¡ã€‚å¦‚æœä½ ç»™æˆ‘ä¸‰ä¸ªï¼Œæˆ‘å¯ä»¥è®©å®ƒæ‘‡å°¾å·´ã€‚&ldquo;æˆ‘ä»¬éƒ½çŸ¥é“è¿™äº›å‚æ•°å°†æ¯«æ— æ„ä¹‰ã€‚äº‹å®ç¡®å®å¦‚æ­¤ã€‚ä»æœ€åˆä¸åŒçš„éšæœºèµ·å§‹è¿æ¥é›†è¿›è¡Œçš„ä¸¤æ¬¡å°è¯•é€šå¸¸ä¼šå¾—åˆ°å®Œå…¨ä¸åŒçš„å‚æ•°ã€‚å¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼Œè¿æ¥å¼ºåº¦ä¼¼ä¹æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æœ‰ç”¨çš„ä¸æ˜¯è¿æ¥å¼ºåº¦ï¼Œè€Œæ˜¯ä¸æ•°æ®çš„æ‹Ÿåˆè´¨é‡ã€‚è¿™ç§æƒ…å†µä¸é€šå¸¸æ—¨åœ¨ä¸»è¦æ¨å¯¼æœ‰æ„ä¹‰å‚æ•°çš„æ•°æ® &ldquo;æ‹Ÿåˆ&rdquo; å®Œå…¨ä¸åŒã€‚</p>
<blockquote>
<p>Feed-forward networks with hidden units have been successfully applied to evaluating loan applications, pap smear classification, optical character recognition, protein folding prediction, adjusting telescope mirrors, and playing backgammon. The nature of the features must be carefully chosen. The choice of network size and structure is important to success (Bishop, 1995) particularly when generalization is the important aspect of the problem (i.e., responding appropriately to a new input pattern that is not one on which the network was trained).</p>
</blockquote>
<p>å…·æœ‰éšè—å•å…ƒçš„å‰é¦ˆç½‘ç»œå·²æˆåŠŸåº”ç”¨äºè¯„ä¼°è´·æ¬¾ç”³è¯·ã€å­å®«é¢ˆæŠ¹ç‰‡åˆ†ç±»ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ã€è›‹ç™½è´¨æŠ˜å é¢„æµ‹ã€è°ƒæ•´æœ›è¿œé•œé•œå­å’Œç©åŒé™†æ£‹ã€‚å¿…é¡»ä»”ç»†é€‰æ‹©ç‰¹å¾çš„æ€§è´¨ã€‚ç½‘ç»œå¤§å°å’Œç»“æ„çš„é€‰æ‹©å¯¹äºæˆåŠŸéå¸¸é‡è¦ï¼ˆBishopï¼Œ1995ï¼‰ï¼Œç‰¹åˆ«æ˜¯å½“æ³›åŒ–æ˜¯é—®é¢˜çš„é‡è¦æ–¹é¢æ—¶ï¼ˆå³ï¼Œå¯¹ç½‘ç»œæœªç»è¿‡è®­ç»ƒçš„æ–°è¾“å…¥æ¨¡å¼åšå‡ºé€‚å½“å“åº”ï¼‰ã€‚</p>
<h2 id="feedback-networks">Feedback networks<a hidden class="anchor" aria-hidden="true" href="#feedback-networks">#</a></h2>
<blockquote>
<p>There is immense feedback in brain connectivity. For example, axons carry signals from the retina to the LGN (lateral geniculate nucleus). Axons originating in the LGN carry signals to cortical processing area V1. But there are more axons carrying signals from V1 back to LGN than in the &ldquo;forward&rdquo; direction. The axons from LGN make synapses on cells in layer IV of area V1. However, most of the synaptic inputs within layer IV come from other cells within V1. Such facts lead to strong interest in neural circuits with feedback.</p>
</blockquote>
<p>å¤§è„‘è¿æ¥ä¸­å­˜åœ¨å¤§é‡åé¦ˆã€‚ä¾‹å¦‚ï¼Œè½´çªå°†ä¿¡å·ä»è§†ç½‘è†œä¼ é€’åˆ°å¤–ä¾§è†çŠ¶ä½“ï¼ˆLGNï¼‰ã€‚èµ·æºäº LGN çš„è½´çªå°†ä¿¡å·ä¼ é€’åˆ°çš®å±‚å¤„ç†åŒºåŸŸ V1ã€‚ä½†æ˜¯ï¼Œä» V1 åé¦ˆåˆ° LGN çš„ä¿¡å·è½´çªæ¯” &ldquo;å‰å‘&rdquo; æ–¹å‘çš„æ›´å¤šã€‚æ¥è‡ª LGN çš„è½´çªåœ¨ V1 åŒºåŸŸçš„ IV å±‚ä¸Šçš„ç»†èƒä¸Šå½¢æˆçªè§¦ã€‚ç„¶è€Œï¼ŒIV å±‚å†…çš„å¤§éƒ¨åˆ†çªè§¦è¾“å…¥æ¥è‡ª V1 å†…çš„å…¶ä»–ç»†èƒã€‚è¿™äº›äº‹å®å¼•èµ·äº†äººä»¬å¯¹å…·æœ‰åé¦ˆçš„ç¥ç»ç”µè·¯çš„æµ“åšå…´è¶£ã€‚</p>
<blockquote>
<p>The style of feedback circuit whose mathematics is most simply understood has symmetric connection, i.e., $S_{kj} = S_{jk}$ . In this case, there is a Lyapunov or &ldquo;energy&rdquo; function for Eq. (8), and the quantity $f_{i}$</p>
<p>$$
E = -\frac{1}{2}\sum S_{ij}V_{i}V_{j} - \sum I_{i}V_{i} + \frac{1}{\tau}\sum\int V^{-1}(f^{\prime})\mathrm{d}f^{\prime}
$$</p>
<p>always decreases in time (Hopfield, 1982, 1994). The dynamics then is described by a flow to an attractor where the motion ceases.</p>
</blockquote>
<p>å…·æœ‰å¯¹ç§°è¿æ¥çš„åé¦ˆç”µè·¯æ ·å¼ï¼Œå…¶æ•°å­¦æœ€å®¹æ˜“ç†è§£ï¼Œå³ $S_{kj} = S_{jk}$ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ–¹ç¨‹ï¼ˆ8ï¼‰å­˜åœ¨ä¸€ä¸ª Lyapunov æˆ– &ldquo;èƒ½é‡&rdquo; å‡½æ•°ï¼Œé‡ $f_{i}$</p>
<p>$$
E = -\frac{1}{2}\sum S_{ij}V_{i}V_{j} - \sum I_{i}V_{i} + \frac{1}{\tau}\sum\int V^{-1}(f^{\prime})\mathrm{d}f^{\prime}
$$</p>
<p>æ€»æ˜¯éšç€æ—¶é—´çš„æ¨ç§»è€Œå‡å°‘ï¼ˆHopfieldï¼Œ1982ï¼Œ1994ï¼‰ã€‚ç„¶åï¼ŒåŠ¨åŠ›å­¦ç”±æµå‘å¸å¼•å­æè¿°ï¼Œåœ¨é‚£é‡Œè¿åŠ¨åœæ­¢ã€‚</p>
<blockquote>
<p>In the high-gain limit, where the input-output relationship is a step between two asymptotic values, the system has a direct relationship to physics. It can be stated most simply when the asymptotic values are scaled to $\pm 1$. The stable points of the dynamic system then have each $V_{i} = \pm 1$, and the stable states of the dynamical system are the stable points of an Ising magnet with exchange parameters $J_{ij} = S_{ij}$.</p>
</blockquote>
<p>åœ¨é«˜å¢ç›Šæé™ä¸‹ï¼Œè¾“å…¥è¾“å‡ºå…³ç³»åœ¨ä¸¤ä¸ªæ¸è¿‘å€¼ä¹‹é—´æ˜¯ä¸€ä¸ªæ­¥éª¤ï¼Œç³»ç»Ÿä¸ç‰©ç†å­¦æœ‰ç›´æ¥å…³ç³»ã€‚å½“æ¸è¿‘å€¼ç¼©æ”¾åˆ° $\pm 1$ æ—¶ï¼Œå¯ä»¥æœ€ç®€å•åœ°è¯´æ˜ã€‚ç„¶åï¼ŒåŠ¨æ€ç³»ç»Ÿçš„ç¨³å®šç‚¹æ¯ä¸ª $V_{i} = \pm 1$ï¼ŒåŠ¨æ€ç³»ç»Ÿçš„ç¨³å®šçŠ¶æ€æ˜¯å…·æœ‰äº¤æ¢å‚æ•° $J_{ij} = S_{ij}$ çš„ Ising ç£ä½“çš„ç¨³å®šç‚¹ã€‚</p>
<blockquote>
<p>The existence of this energy function provides a programming tool (Hopfield and Tank, 1985; Takefuji, 1991). Many difficult computational problems can be posed as optimization problems. If the quantity to be optimized can be mapped onto the form Eq. (8), it defines the connections and the &ldquo;program&rdquo; to solve the optimization problem.</p>
</blockquote>
<p>èƒ½é‡å‡½æ•°çš„å­˜åœ¨æä¾›äº†ä¸€ç§ç¼–ç¨‹å·¥å…·ï¼ˆHopfield å’Œ Tankï¼Œ1985ï¼›Takefujiï¼Œ1991ï¼‰ã€‚è®¸å¤šå›°éš¾çš„è®¡ç®—é—®é¢˜å¯ä»¥è¢«æå‡ºä½œä¸ºä¼˜åŒ–é—®é¢˜ã€‚å¦‚æœè¦ä¼˜åŒ–çš„é‡å¯ä»¥æ˜ å°„åˆ°æ–¹ç¨‹ï¼ˆ8ï¼‰çš„å½¢å¼ï¼Œå®ƒå°±å®šä¹‰äº†è¿æ¥å’Œè§£å†³ä¼˜åŒ–é—®é¢˜çš„ &ldquo;ç¨‹åº&rdquo;ã€‚</p>
<blockquote>
<p>The trivial generalization of the Ising system to finite temperature generates a statistical mechanics. However, a &ldquo;learning rule&rdquo; can then be found for this system, even in the presence of hidden units. This was the first successful learning rule used for networks with hidden units (Hinton and Sejnowski, 1983). Because it is computationally intensive, practical applications have chiefly used analog &ldquo;neurons&rdquo; and the faster &ldquo;backpropagation&rdquo; learning rule when applicable. The relationship with statistical mechanics and entropic information measures, however, give the Boltzmann machine continuing interest.</p>
</blockquote>
<p>Ising ç³»ç»Ÿåˆ°æœ‰é™æ¸©åº¦çš„å¹³å‡¡æ¨å¹¿äº§ç”Ÿäº†ç»Ÿè®¡åŠ›å­¦ã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨å­˜åœ¨éšè—å•å…ƒçš„æƒ…å†µä¸‹ï¼Œä¹Ÿå¯ä»¥ä¸ºè¯¥ç³»ç»Ÿæ‰¾åˆ° &ldquo;å­¦ä¹ è§„åˆ™&rdquo;ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªæˆåŠŸç”¨äºå…·æœ‰éšè—å•å…ƒçš„ç½‘ç»œçš„å­¦ä¹ è§„åˆ™ï¼ˆHinton å’Œ Sejnowskiï¼Œ1983ï¼‰ã€‚ç”±äºå®ƒåœ¨è®¡ç®—ä¸Šå¾ˆå¯†é›†ï¼Œå®é™…åº”ç”¨ä¸»è¦ä½¿ç”¨æ¨¡æ‹Ÿ &ldquo;ç¥ç»å…ƒ&rdquo; å’Œæ›´å¿«çš„ &ldquo;åå‘ä¼ æ’­&rdquo; å­¦ä¹ è§„åˆ™ï¼ˆå¦‚æœé€‚ç”¨ï¼‰ã€‚ç„¶è€Œï¼Œä¸ç»Ÿè®¡åŠ›å­¦å’Œç†µä¿¡æ¯åº¦é‡çš„å…³ç³»ä½¿ Boltzmann æœºæŒç»­å—åˆ°å…³æ³¨ã€‚</p>
<blockquote>
<p>Associative memories are thought of as a set of linked features $f_{1}$, $f_{2}$, etc. The activity of a particular neuron signifies the presence of the feature represented by that neuron. A memory is a state in which the cells representing the features of that memory are simultaneously active. The relationship between features is symmetric in that each implies the other and is expressed in a symmetric network. An elegant analysis of the capacity of such memories for random patterns is related to the spin glass (Amit, 1989).</p>
</blockquote>
<p>è”æƒ³è®°å¿†è¢«è®¤ä¸ºæ˜¯ä¸€ç»„é“¾æ¥çš„ç‰¹å¾ $f_{1}$ã€$f_{2}$ ç­‰ã€‚ç‰¹å®šç¥ç»å…ƒçš„æ´»åŠ¨è¡¨ç¤ºè¯¥ç¥ç»å…ƒæ‰€ä»£è¡¨çš„ç‰¹å¾çš„å­˜åœ¨ã€‚è®°å¿†æ˜¯ä¸€ç§çŠ¶æ€ï¼Œå…¶ä¸­ä»£è¡¨è¯¥è®°å¿†ç‰¹å¾çš„ç»†èƒåŒæ—¶å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚ç‰¹å¾ä¹‹é—´çš„å…³ç³»æ˜¯å¯¹ç§°çš„ï¼Œå› ä¸ºæ¯ä¸ªç‰¹å¾éƒ½æš—ç¤ºå¦ä¸€ä¸ªç‰¹å¾ï¼Œå¹¶åœ¨å¯¹ç§°ç½‘ç»œä¸­è¡¨è¾¾ã€‚å¯¹éšæœºæ¨¡å¼çš„è¿™ç§è®°å¿†å®¹é‡çš„ä¼˜é›…åˆ†æä¸è‡ªæ—‹ç»ç’ƒæœ‰å…³ï¼ˆAmitï¼Œ1989ï¼‰ã€‚</p>
<blockquote>
<p>Many nonsymmetric networks can be mapped onto networks with related Lyaupanov functions. Thus, while symmetric networks are exceptional in biology, the study of networks with Lyapunov functions is a useful approach to understanding biological networks. Line attractors have been used in connection with keeping the eye gaze stable at any set position (Seung, 1996).</p>
</blockquote>
<p>è®¸å¤šéå¯¹ç§°ç½‘ç»œå¯ä»¥æ˜ å°„åˆ°å…·æœ‰ç›¸å…³ Lyaupanov å‡½æ•°çš„ç½‘ç»œä¸Šã€‚å› æ­¤ï¼Œè™½ç„¶å¯¹ç§°ç½‘ç»œåœ¨ç”Ÿç‰©å­¦ä¸­æ˜¯ä¾‹å¤–ï¼Œä½†ç ”ç©¶å…·æœ‰ Lyapunov å‡½æ•°çš„ç½‘ç»œæ˜¯ç†è§£ç”Ÿç‰©ç½‘ç»œçš„æœ‰ç”¨æ–¹æ³•ã€‚çº¿æ€§å¸å¼•å­å·²è¢«ç”¨äºä¸ä¿æŒçœ¼ç›å‡è§†åœ¨ä»»ä½•è®¾å®šä½ç½®ç¨³å®šæœ‰å…³çš„ç ”ç©¶ï¼ˆSeungï¼Œ1996ï¼‰ã€‚</p>
<blockquote>
<p>Networks which have feedback may oscillate. The olfactory bulb is an example of a circuit with a strong excitatory-inhibitory feedback loop. In mammals, the olfactory bulb bursts into 30â€“50 Hz oscillations with every sniff (Freeman and Skarda, 1985).</p>
</blockquote>
<p>å…·æœ‰åé¦ˆçš„ç½‘ç»œå¯èƒ½ä¼šæŒ¯è¡ã€‚å—…çƒæ˜¯å…·æœ‰å¼ºå…´å¥‹-æŠ‘åˆ¶åé¦ˆå›è·¯çš„ç”µè·¯çš„ä¸€ä¸ªä¾‹å­ã€‚åœ¨å“ºä¹³åŠ¨ç‰©ä¸­ï¼Œå—…çƒéšç€æ¯æ¬¡å¸æ°”è€Œçˆ†å‘å‡º 30-50 Hz çš„æŒ¯è¡ï¼ˆFreeman å’Œ Skardaï¼Œ1985ï¼‰ã€‚</p>
<h1 id="development-and-synapse-plasticity">DEVELOPMENT AND SYNAPSE PLASTICITY<a hidden class="anchor" aria-hidden="true" href="#development-and-synapse-plasticity">#</a></h1>
<blockquote>
<p>For simple animals such as the C. elegans (a round worm) the nervous system is essentially determined. Each genetically identical C. elegans has the same number of nerve cells, each cell identifiable in morphology and position. The synaptic connections between such &ldquo;identical&rdquo; animals are 90% identical. Mammals, at the other end of the spectrum, have identifiable cell types, identifiable brain structures and regions, but no cells in 1:1 correspondence between different individuals. The &ldquo;wiring&rdquo; between cells clearly has rules, and also a strong random element arising from development. How, then, can we have the system of fine-tuned connections between neurons which produces visual acuity sharper than the size of a retinal photoreceptor, or coordinates the two eyes so that we have stereoscopic vision? The answer to this puzzle lies in the synapses change due to coordinated activity during development. Coordinated activity of neurons arises from the correlated nature of the visual world and is carried through to higher level neurons. The importance of neuronal activity patterns and external input is dramatically illustrated in depth perception. If a &ldquo;wandering eye&rdquo; through muscular miscoordination, is corrected in the first six months, a child develops normal binocular stereopsis. Corrected after two years, the two eyes are used in a coordinate fashion and seem completely normal, but the child will never develop stereoscopic vision.</p>
</blockquote>
<p>å¯¹äºåƒçº¿è™«ï¼ˆåœ†å½¢è •è™«ï¼‰è¿™æ ·ç®€å•çš„åŠ¨ç‰©ï¼Œç¥ç»ç³»ç»ŸåŸºæœ¬ä¸Šæ˜¯ç¡®å®šçš„ã€‚æ¯ä¸ªåŸºå› ç›¸åŒçš„çº¿è™«éƒ½æœ‰ç›¸åŒæ•°é‡çš„ç¥ç»ç»†èƒï¼Œæ¯ä¸ªç»†èƒåœ¨å½¢æ€å’Œä½ç½®ä¸Šéƒ½æ˜¯å¯è¯†åˆ«çš„ã€‚è¿™äº› &ldquo;ç›¸åŒ&rdquo; åŠ¨ç‰©ä¹‹é—´çš„çªè§¦è¿æ¥æœ‰ 90% æ˜¯ç›¸åŒçš„ã€‚å¤„äºå…‰è°±å¦ä¸€ç«¯çš„å“ºä¹³åŠ¨ç‰©å…·æœ‰å¯è¯†åˆ«çš„ç»†èƒç±»å‹ã€å¯è¯†åˆ«çš„å¤§è„‘ç»“æ„å’ŒåŒºåŸŸï¼Œä½†ä¸åŒä¸ªä½“ä¹‹é—´æ²¡æœ‰ 1:1 å¯¹åº”çš„ç»†èƒã€‚ç»†èƒä¹‹é—´çš„ &ldquo;å¸ƒçº¿&rdquo; æ˜¾ç„¶æœ‰è§„åˆ™ï¼Œå¹¶ä¸”è¿˜å…·æœ‰æ¥è‡ªå‘è‚²çš„å¼ºéšæœºå…ƒç´ ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•æ‹¥æœ‰ç¥ç»å…ƒä¹‹é—´å¾®è°ƒè¿æ¥çš„ç³»ç»Ÿï¼Œä»è€Œäº§ç”Ÿæ¯”è§†ç½‘è†œå…‰æ„Ÿå—å™¨æ›´æ¸…æ™°çš„è§†è§‰æ•é”åº¦ï¼Œæˆ–åè°ƒåŒçœ¼ä»¥å®ç°ç«‹ä½“è§†è§‰ï¼Ÿè¿™ä¸ªè°œé¢˜çš„ç­”æ¡ˆåœ¨äºå‘è‚²è¿‡ç¨‹ä¸­ç”±äºåè°ƒæ´»åŠ¨è€Œå‘ç”Ÿå˜åŒ–çš„çªè§¦ã€‚ç¥ç»å…ƒçš„åè°ƒæ´»åŠ¨æºè‡ªè§†è§‰ä¸–ç•Œçš„ç›¸å…³æ€§è´¨ï¼Œå¹¶ä¼ é€’åˆ°æ›´é«˜çº§åˆ«çš„ç¥ç»å…ƒã€‚ç¥ç»å…ƒæ´»åŠ¨æ¨¡å¼å’Œå¤–éƒ¨è¾“å…¥çš„é‡è¦æ€§åœ¨æ·±åº¦æ„ŸçŸ¥ä¸­å¾—åˆ°äº†æˆå‰§æ€§çš„è¯´æ˜ã€‚å¦‚æœé€šè¿‡è‚Œè‚‰å¤±è°ƒå¼•èµ·çš„ &ldquo;æ¸¸è¡çœ¼&rdquo; åœ¨å¤´å…­ä¸ªæœˆå†…å¾—åˆ°çº æ­£ï¼Œå­©å­ä¼šå‘å±•å‡ºæ­£å¸¸çš„åŒçœ¼ç«‹ä½“è§†è§‰ã€‚åœ¨ä¸¤å¹´åçº æ­£åï¼Œä¸¤åªçœ¼ç›ä»¥åè°ƒçš„æ–¹å¼ä½¿ç”¨ï¼Œçœ‹èµ·æ¥å®Œå…¨æ­£å¸¸ï¼Œä½†å­©å­æ°¸è¿œä¸ä¼šå‘å±•å‡ºç«‹ä½“è§†è§‰ã€‚</p>
<blockquote>
<p>When multiple input patterns are present, the dynamics generates a cellular competition for the representation of these patterns. The idealized mathematics is that of a symmetry breaking. Once symmetry is broken, the competition continues to refine the connections (Linsker 1986). This mathematics was originally used to describe the development of connections between the retina and the optic tectum of the frog. It describes well the generation of orientation-selective cells in the cat visual cortex. A hierarchy of such symmetry breakings has been used to describe the selectivity of cells in the mammalian visual pathway. This analysis is simple only in cases where the details of the biology have been maximally suppressed, but such models are slowly being given more detailed connections to biology (Miller, 1994).</p>
</blockquote>
<p>å½“å­˜åœ¨å¤šä¸ªè¾“å…¥æ¨¡å¼æ—¶ï¼ŒåŠ¨åŠ›å­¦ä¼šäº§ç”Ÿç»†èƒç«äº‰ä»¥è¡¨ç¤ºè¿™äº›æ¨¡å¼ã€‚ç†æƒ³åŒ–çš„æ•°å­¦æ˜¯å¯¹ç§°æ€§ç ´ç¼ºã€‚ä¸€æ—¦å¯¹ç§°æ€§è¢«æ‰“ç ´ï¼Œç«äº‰å°±ä¼šç»§ç»­å®Œå–„è¿æ¥ï¼ˆLinsker 1986ï¼‰ã€‚è¿™ç§æ•°å­¦æœ€åˆç”¨äºæè¿°é’è›™è§†ç½‘è†œå’Œè§†é¡¶ç›–ä¹‹é—´è¿æ¥çš„å‘å±•ã€‚å®ƒå¾ˆå¥½åœ°æè¿°äº†çŒ«è§†è§‰çš®å±‚ä¸­æ–¹å‘é€‰æ‹©æ€§ç»†èƒçš„äº§ç”Ÿã€‚è¿™æ ·çš„å¯¹ç§°æ€§ç ´ç¼ºå±‚æ¬¡ç»“æ„å·²è¢«ç”¨æ¥æè¿°å“ºä¹³åŠ¨ç‰©è§†è§‰é€šè·¯ä¸­ç»†èƒçš„é€‰æ‹©æ€§ã€‚åªæœ‰åœ¨æœ€å¤§ç¨‹åº¦ä¸ŠæŠ‘åˆ¶ç”Ÿç‰©å­¦ç»†èŠ‚çš„æƒ…å†µä¸‹ï¼Œè¿™ç§åˆ†ææ‰æ˜¯ç®€å•çš„ï¼Œä½†è¿™æ ·çš„æ¨¡å‹æ­£åœ¨æ…¢æ…¢åœ°ä¸ç”Ÿç‰©å­¦å»ºç«‹æ›´è¯¦ç»†çš„è”ç³»ï¼ˆMillerï¼Œ1994ï¼‰ã€‚</p>
<blockquote>
<p>There is an ongoing debate in such areas about &ldquo;instructionism&rdquo; versus &ldquo;selectionism,&rdquo; and on the role of genetics versus environmental influences. &ldquo;Nature&rdquo; versus &ldquo;nurture&rdquo; has been an issue in psychology and brain science for decades and is seen at its most elementary level in trying to understand how the functional wiring of an adult brain is generated.</p>
</blockquote>
<p>åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œå…³äº &ldquo;æŒ‡å¯¼ä¸»ä¹‰&rdquo; ä¸ &ldquo;é€‰æ‹©ä¸»ä¹‰&rdquo; ä»¥åŠé—ä¼ å­¦ä¸ç¯å¢ƒå½±å“çš„ä½œç”¨å­˜åœ¨æŒç»­çš„äº‰è®ºã€‚&ldquo;å¤©æ€§&rdquo; ä¸ &ldquo;æ•™å…»&rdquo; å¤šå¹´æ¥ä¸€ç›´æ˜¯å¿ƒç†å­¦å’Œå¤§è„‘ç§‘å­¦ä¸­çš„ä¸€ä¸ªé—®é¢˜ï¼Œå¹¶ä¸”åœ¨è¯•å›¾ç†è§£æˆäººå¤§è„‘çš„åŠŸèƒ½è¿æ¥æ˜¯å¦‚ä½•äº§ç”Ÿæ—¶ï¼Œåœ¨å…¶æœ€åŸºæœ¬çš„å±‚é¢ä¸Šå¾—åˆ°äº†ä½“ç°ã€‚</p>
<h1 id="action-potential-timing">ACTION POTENTIAL TIMING<a hidden class="anchor" aria-hidden="true" href="#action-potential-timing">#</a></h1>
<blockquote>
<p>The detailed timing in a train of action potentials carries information beyond that described by the shortterm firing rate. When several presynaptic neurons fire action potentials simultaneously, the event can have a saliency for driving a cell that would not occur if the events were more spread out in time. These facts suggest that for some neural computations, Eq. (7) may lose the essence of Eq. (6). Theoretically, information can be encoded in action potential timing and computed efficiently and rapidly (Hopfield, 1995).</p>
</blockquote>
<p>è¯¦ç»†çš„åŠ¨ä½œç”µä½åˆ—è½¦æ—¶åºæºå¸¦çš„ä¿¡æ¯è¶…å‡ºäº†çŸ­æœŸå‘å°„ç‡æ‰€æè¿°çš„ä¿¡æ¯ã€‚å½“å‡ ä¸ªçªè§¦å‰ç¥ç»å…ƒåŒæ—¶å‘å‡ºåŠ¨ä½œç”µä½æ—¶ï¼Œè¯¥äº‹ä»¶å¯ä»¥å¯¹é©±åŠ¨ç»†èƒäº§ç”Ÿæ˜¾è‘—æ€§ï¼Œå¦‚æœäº‹ä»¶åœ¨æ—¶é—´ä¸Šæ›´åˆ†æ•£åˆ™ä¸ä¼šå‘ç”Ÿè¿™äº›äº‹ä»¶ã€‚è¿™äº›äº‹å®è¡¨æ˜ï¼Œå¯¹äºæŸäº›ç¥ç»è®¡ç®—ï¼Œæ–¹ç¨‹ï¼ˆ7ï¼‰å¯èƒ½ä¼šå¤±å»æ–¹ç¨‹ï¼ˆ6ï¼‰çš„æœ¬è´¨ã€‚ä»ç†è®ºä¸Šè®²ï¼Œä¿¡æ¯å¯ä»¥ç¼–ç åœ¨åŠ¨ä½œç”µä½æ—¶åºä¸­ï¼Œå¹¶ä¸”å¯ä»¥é«˜æ•ˆä¸”å¿«é€Ÿåœ°è®¡ç®—ï¼ˆHopfieldï¼Œ1995ï¼‰ã€‚</p>
<blockquote>
<p>Direct observations also suggest the importance of action potential timing. Experiments in cats indicate that the synchrony of action potentials between different cells might represent the &ldquo;objectness&rdquo; of an extended visual object (Gray and Singer, 1989). Synchronization effects are seen in insect olfaction (Stopfer et al., 1997). Azimuthal sound localization by birds effectively involves coincidences between action potentials arriving via right- and left-ear pathways. A neuron in rat hippocampus which is firing at a low rate carries information about the spatial location of the rat in its phase of firing with respect to the theta rhythm (Burgess, Oâ€™Keefe, and Recce, 1993). Action potentials in low-firing-rate frontal cortex seem to have unusual temporal correlation. Action potentials propagate back into some dendrites of pyramidal cells, and their synapses have implicit information both from when the presynaptic cell fired and when the postsynaptic cell fired, potentially important in a synapse-change process.</p>
</blockquote>
<p>ç›´æ¥è§‚å¯Ÿä¹Ÿè¡¨æ˜åŠ¨ä½œç”µä½æ—¶åºçš„é‡è¦æ€§ã€‚çŒ«çš„å®éªŒè¡¨æ˜ï¼Œä¸åŒç»†èƒä¹‹é—´åŠ¨ä½œç”µä½çš„åŒæ­¥å¯èƒ½ä»£è¡¨æ‰©å±•è§†è§‰å¯¹è±¡çš„ &ldquo;ç‰©ä½“æ€§&rdquo;ï¼ˆGray å’Œ Singerï¼Œ1989ï¼‰ã€‚åœ¨æ˜†è™«å—…è§‰ä¸­è§‚å¯Ÿåˆ°äº†åŒæ­¥æ•ˆåº”ï¼ˆStopfer ç­‰äººï¼Œ1997ï¼‰ã€‚é¸Ÿç±»çš„æ–¹ä½å£°å®šä½æœ‰æ•ˆåœ°æ¶‰åŠé€šè¿‡å³è€³å’Œå·¦è€³é€šè·¯åˆ°è¾¾çš„åŠ¨ä½œç”µä½ä¹‹é—´çš„å·§åˆã€‚å¤§é¼ æµ·é©¬ä½“ä¸­çš„ä¸€ä¸ªä»¥ä½é€Ÿç‡å‘å°„çš„ç¥ç»å…ƒåœ¨å…¶ç›¸å¯¹äº theta èŠ‚å¾‹çš„å‘å°„ç›¸ä½ä¸­æºå¸¦æœ‰å…³å¤§é¼ ç©ºé—´ä½ç½®çš„ä¿¡æ¯ï¼ˆBurgessã€O&rsquo;Keefe å’Œ Recceï¼Œ1993ï¼‰ã€‚ä½å‘å°„ç‡é¢å¶çš®å±‚ä¸­çš„åŠ¨ä½œç”µä½ä¼¼ä¹å…·æœ‰ä¸å¯»å¸¸çš„æ—¶é—´ç›¸å…³æ€§ã€‚åŠ¨ä½œç”µä½ä¼ æ’­å›é”¥ä½“ç»†èƒçš„ä¸€äº›æ ‘çªä¸­ï¼Œå®ƒä»¬çš„çªè§¦éšå«äº†æ¥è‡ªçªè§¦å‰ç»†èƒå‘å°„æ—¶é—´å’Œçªè§¦åç»†èƒå‘å°„æ—¶é—´çš„ä¿¡æ¯ï¼Œè¿™åœ¨çªè§¦å˜åŒ–è¿‡ç¨‹ä¸­å¯èƒ½å¾ˆé‡è¦ã€‚</p>
<h1 id="the-future">THE FUTURE<a hidden class="anchor" aria-hidden="true" href="#the-future">#</a></h1>
<blockquote>
<p>The field now known as &ldquo;computational neurobiology&rdquo; has been based on an explosion in our knowledge of the electrical signals of cells during significant processing events and on its relationship to theory including understanding simple neural circuits, the attractor model of neural computation, the role of activity in development, and the information-theoretic view of neural coding. The short-term future will exploit the new ways to visualize neural activity, involving multi-electrode recording, optical signals from cells (voltage-dependent dyes, ion-binding fluorophores, and intrinsic signals) functional magnetic resonance imaging, magnetoencephalography, patch clamp techniques, confocal microscopy, and microelectrode arrays. Molecular biology tools have now also begun to be significant for computational neurobiology. On the modeling side it will involve understanding more of the computational power of biological systems by using additional biological features.</p>
</blockquote>
<p>ç°åœ¨è¢«ç§°ä¸º &ldquo;è®¡ç®—ç¥ç»ç”Ÿç‰©å­¦&rdquo; çš„é¢†åŸŸï¼ŒåŸºäºæˆ‘ä»¬å¯¹ç»†èƒåœ¨é‡è¦å¤„ç†äº‹ä»¶æœŸé—´çš„ç”µä¿¡å·åŠå…¶ä¸ç†è®ºçš„å…³ç³»çš„çŸ¥è¯†çˆ†ç‚¸ï¼ŒåŒ…æ‹¬ç†è§£ç®€å•çš„ç¥ç»ç”µè·¯ã€ç¥ç»è®¡ç®—çš„å¸å¼•å­æ¨¡å‹ã€æ´»åŠ¨åœ¨å‘è‚²ä¸­çš„ä½œç”¨ä»¥åŠç¥ç»ç¼–ç çš„ä¿¡æ¯ç†è®ºè§‚ç‚¹ã€‚çŸ­æœŸå†…çš„æœªæ¥å°†åˆ©ç”¨å¯è§†åŒ–ç¥ç»æ´»åŠ¨çš„æ–°æ–¹æ³•ï¼Œæ¶‰åŠå¤šç”µæè®°å½•ã€æ¥è‡ªç»†èƒçš„å…‰å­¦ä¿¡å·ï¼ˆç”µå‹ä¾èµ–æ€§æŸ“æ–™ã€ç¦»å­ç»“åˆè§å…‰ç´ å’Œå†…åœ¨ä¿¡å·ï¼‰ã€åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒã€è„‘ç£å›¾ã€è†œç‰‡é’³æŠ€æœ¯ã€å…±èšç„¦æ˜¾å¾®é•œå’Œå¾®ç”µæé˜µåˆ—ã€‚åˆ†å­ç”Ÿç‰©å­¦å·¥å…·ç°åœ¨ä¹Ÿå¼€å§‹å¯¹è®¡ç®—ç¥ç»ç”Ÿç‰©å­¦å…·æœ‰é‡è¦æ„ä¹‰ã€‚åœ¨å»ºæ¨¡æ–¹é¢ï¼Œå®ƒå°†æ¶‰åŠé€šè¿‡ä½¿ç”¨é¢å¤–çš„ç”Ÿç‰©ç‰¹å¾æ¥ç†è§£ç”Ÿç‰©ç³»ç»Ÿçš„æ›´å¤šè®¡ç®—èƒ½åŠ›ã€‚</p>
<blockquote>
<p>The study of silicon very large scale integrated circuits (VLSIâ€™s) for analog â€˜â€˜neuralâ€™â€™ circuits (Mead, 1989) has yielded one relevant general principle. When the physics of a device can be used in an algorithm, the device is highly effective in computation compared to its effectiveness in general purpose use. Evolution will have exploited the biophysical molecular and circuit devices available. For any particular behavior, some facts of neurobiology will be very significant because they are used in the algorithm, and others will be able to be subsumed in a model which is far simpler than the actual biophysics of the system. It is important to make such separations, for neurobiology is so filled with details that we will never understand the neurobiological basis of perception, cognition, and psychology merely by accumulating facts and doing ever more detailed simulations. Linear systems are simple to characterize completely. Computational systems are highly nonlinear, and a complete characterization by brute force requires a number of experiments which grows exponentially with the size of the system. When only a limited number of experiments is performed, the behavior of the system is not fully characterized, and to a considerable extent the experimental design builds in the answers that will be found. For working at higher computational levels, experiments on anaesthetized animals, or in highly simplified, overlearned artificial situations, are not going to be enough. Nor will the characterization of the behavior of a very small number of cells during a behavior be adequate to understand how or why the behavior is being generated. Thus it will be necessary to build a better bridge between lower animals, which can be more completely studied, and higher animals, whose rich mental behavior is the ultimate goal of computational neurobiology.</p>
</blockquote>
<p>ç ”ç©¶ç”¨äºæ¨¡æ‹Ÿ &ldquo;ç¥ç»&rdquo; ç”µè·¯çš„ç¡…è¶…å¤§è§„æ¨¡é›†æˆç”µè·¯ï¼ˆVLSIï¼‰ï¼ˆMeadï¼Œ1989ï¼‰å·²ç»äº§ç”Ÿäº†ä¸€ä¸ªç›¸å…³çš„ä¸€èˆ¬åŸåˆ™ã€‚å½“è®¾å¤‡çš„ç‰©ç†å­¦å¯ä»¥åœ¨ç®—æ³•ä¸­ä½¿ç”¨æ—¶ï¼Œè¯¥è®¾å¤‡åœ¨è®¡ç®—ä¸­çš„æ•ˆæœä¸å…¶åœ¨é€šç”¨ç”¨é€”ä¸­çš„æ•ˆæœç›¸æ¯”æ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚è¿›åŒ–å°†åˆ©ç”¨å¯ç”¨çš„ç”Ÿç‰©ç‰©ç†åˆ†å­å’Œç”µè·¯è®¾å¤‡ã€‚å¯¹äºä»»ä½•ç‰¹å®šçš„è¡Œä¸ºï¼Œç¥ç»ç”Ÿç‰©å­¦çš„ä¸€äº›äº‹å®å°†éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä»¬è¢«ç”¨äºç®—æ³•ï¼Œè€Œå…¶ä»–äº‹å®å°†èƒ½å¤Ÿè¢«çº³å…¥ä¸€ä¸ªè¿œæ¯”ç³»ç»Ÿçš„å®é™…ç”Ÿç‰©ç‰©ç†å­¦æ›´ç®€å•çš„æ¨¡å‹ä¸­ã€‚è¿›è¡Œè¿™ç§åˆ†ç¦»å¾ˆé‡è¦ï¼Œå› ä¸ºç¥ç»ç”Ÿç‰©å­¦å……æ»¡äº†ç»†èŠ‚ï¼Œæˆ‘ä»¬æ°¸è¿œæ— æ³•ä»…é€šè¿‡ç§¯ç´¯äº‹å®å’Œè¿›è¡Œè¶Šæ¥è¶Šè¯¦ç»†çš„æ¨¡æ‹Ÿæ¥ç†è§£æ„ŸçŸ¥ã€è®¤çŸ¥å’Œå¿ƒç†å­¦çš„ç¥ç»ç”Ÿç‰©å­¦åŸºç¡€ã€‚çº¿æ€§ç³»ç»Ÿå¾ˆå®¹æ˜“å®Œå…¨è¡¨å¾ã€‚è®¡ç®—ç³»ç»Ÿæ˜¯é«˜åº¦éçº¿æ€§çš„ï¼Œé€šè¿‡è›®åŠ›è¿›è¡Œå®Œæ•´è¡¨å¾æ‰€éœ€çš„å®éªŒæ•°é‡éšç€ç³»ç»Ÿå¤§å°å‘ˆæŒ‡æ•°å¢é•¿ã€‚å½“åªæ‰§è¡Œæœ‰é™æ•°é‡çš„å®éªŒæ—¶ï¼Œç³»ç»Ÿçš„è¡Œä¸ºæ²¡æœ‰å¾—åˆ°å……åˆ†è¡¨å¾ï¼Œåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šï¼Œå®éªŒè®¾è®¡æ„å»ºäº†å°†è¦æ‰¾åˆ°çš„ç­”æ¡ˆã€‚ä¸ºäº†åœ¨æ›´é«˜çš„è®¡ç®—çº§åˆ«ä¸Šå·¥ä½œï¼Œå¯¹éº»é†‰åŠ¨ç‰©æˆ–åœ¨é«˜åº¦ç®€åŒ–ã€è¿‡åº¦å­¦ä¹ çš„äººå·¥æƒ…å†µä¸‹è¿›è¡Œçš„å®éªŒå°†æ˜¯ä¸å¤Ÿçš„ã€‚åœ¨è¡Œä¸ºè¿‡ç¨‹ä¸­å¯¹æå°‘æ•°ç»†èƒè¡Œä¸ºçš„è¡¨å¾ä¹Ÿä¸è¶³ä»¥ç†è§£è¡Œä¸ºæ˜¯å¦‚ä½•æˆ–ä¸ºä»€ä¹ˆè¢«ç”Ÿæˆçš„ã€‚å› æ­¤ï¼Œæœ‰å¿…è¦åœ¨å¯ä»¥æ›´å…¨é¢ç ”ç©¶çš„ä½ç­‰åŠ¨ç‰©å’Œå…¶ä¸°å¯Œçš„å¿ƒç†è¡Œä¸ºæ˜¯è®¡ç®—ç¥ç»ç”Ÿç‰©å­¦æœ€ç»ˆç›®æ ‡çš„é«˜ç­‰åŠ¨ç‰©ä¹‹é—´å»ºç«‹æ›´å¥½çš„æ¡¥æ¢ã€‚</p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>Renormalization group for neurons</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/read/reference/benchmarking-of-hardware-efficient-real-time-neural-decoding-in-brain-computer-interfaces/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>Benchmarking of hardware-efficient real-time neural decoding in brainâ€“computer interfaces</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"æ— å¤„æƒ¹å°˜åŸƒ"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  
</body>







<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
