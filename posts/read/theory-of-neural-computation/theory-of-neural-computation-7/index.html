<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Recurrent Networks | æ— å¤„æƒ¹å°˜åŸƒ</title>
<meta name="keywords" content="">
<meta name="description" content="æ¯å‘¨é˜…è¯»ç¬”è®°">
<meta name="author" content="Muartz">
<link rel="canonical" href="https://Muatyz.github.io/posts/read/theory-of-neural-computation/theory-of-neural-computation-7/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://Muatyz.github.io/img/Head16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Muatyz.github.io/img/Head32.png">
<link rel="apple-touch-icon" href="https://Muatyz.github.io/img/Head32.png">
<link rel="mask-icon" href="https://Muatyz.github.io/img/Head32.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Muatyz.github.io/posts/read/theory-of-neural-computation/theory-of-neural-computation-7/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>







<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
        {left: "\\[", right: "\\]", display: true}
      ]
    });
  });
</script><meta property="og:title" content="Recurrent Networks" />
<meta property="og:description" content="æ¯å‘¨é˜…è¯»ç¬”è®°" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Muatyz.github.io/posts/read/theory-of-neural-computation/theory-of-neural-computation-7/" />
<meta property="og:image" content="https://s2.loli.net/2025/10/12/xl7ojGEO5neCiRM.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-20T00:18:23+08:00" />
<meta property="article:modified_time" content="2025-11-20T00:18:23+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://s2.loli.net/2025/10/12/xl7ojGEO5neCiRM.png" />
<meta name="twitter:title" content="Recurrent Networks"/>
<meta name="twitter:description" content="æ¯å‘¨é˜…è¯»ç¬”è®°"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Muatyz.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ“• é˜…è¯»",
          "item": "https://Muatyz.github.io/posts/read/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "ğŸ“• è®¡ç®—ç¥ç»ç§‘å­¦",
          "item": "https://Muatyz.github.io/posts/read/theory-of-neural-computation/"
        }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Recurrent Networks",
      "item": "https://Muatyz.github.io/posts/read/theory-of-neural-computation/theory-of-neural-computation-7/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Recurrent Networks",
  "name": "Recurrent Networks",
  "description": "æ¯å‘¨é˜…è¯»ç¬”è®°",
  "keywords": [
    ""
  ],
  "articleBody": "Recurrent Networks: With connections allowed both ways between units, and even from a unit to itself.\n7.1 Boltzmann Machines(ç»å°”å…¹æ›¼æœº) Stochastic networks with symmetric connections($w_{ij}=w_{ji}$).\nOriginal form: slow for averaging over stochastic variables. Mean field version: speeded up and more applications. Stochastic Units(éšæœºå•å…ƒ) Visible \u0026 hidden units.\nHidden units: No connection to outside world.\n$$ \\begin{aligned} P(S_{i} = +1) \u0026= g(h_{i}) = \\frac{1}{1+\\exp{(-2\\beta h_{i})}}\\\\ P(S_{i} = -1) \u0026= 1 - g(h_{i}) \\end{aligned} $$\n$\\begin{aligned}h_{i} = \\sum_{j}w_{ij}S_{j},\\quad\\beta = 1/T\\end{aligned}$.\nEnergy function:\n$$ H(\\{S_{i}\\}) = -\\frac{1}{2}\\sum_{ij}w_{ij}S_{i}S_{j},\\quad S_{i} = \\text{sgn}{(h_{i})} $$\nBoltzmann-Gibbs distribution:\n$$ P(\\{S_{i}\\}) = \\frac{e^{-\\beta H(\\{S_{i}\\})}}{Z} $$\nPartition function: $$Z = \\sum_{\\{S_{i}\\}} e^{-\\beta H(\\{S_{i}\\})}$$\naverage $\\langle X\\rangle$:\n$$ \\langle X\\rangle = \\sum_{\\{S_{i}\\}}P(\\{S_{i}\\}) X(\\{S_{i}\\}) $$\nPattern completion: At low $T$, only a few possible states have high and equal prob.\n$w_{ij}\\propto \\xi_{i}\\xi_{j}$: no information about links with hidden units.\n$\\alpha$: state of visible units;\n$\\beta$: state of hidden units.\n$N$ visible units, $K$ hidden units $\\Rightarrow 2^{N+K}$ possibilities.\nProbability $P_{\\alpha}$ of chosen state $\\alpha$:\n$$ \\begin{aligned} P_{\\alpha} \u0026= \\sum_{\\beta}P_{\\alpha\\beta} = \\sum_{\\beta}\\frac{1}{Z}e^{-\\beta H_{\\alpha\\beta}}\\\\ Z \u0026= \\sum_{\\alpha\\beta}e^{-\\beta H_{\\alpha\\beta}}\\\\ H_{\\alpha\\beta} \u0026= -\\frac{1}{2}\\sum_{ij}w_{ij}S_{i}^{\\alpha\\beta}S_{j}^{\\alpha\\beta} \\end{aligned} $$\nDesired probability: $R_{\\alpha}$\nCost function: relative entropy(to measure the difference):\n$$ E = \\sum_{\\alpha}R_{\\alpha}\\log{\\frac{R_{\\alpha}}{P_{\\alpha}}} = E_{0} {\\color{green}{- \\sum_{\\alpha}R_{\\alpha}\\log{P_{\\alpha}}}}\\geq 0 $$\n$E = 0$ only if $P_{\\alpha} = R_{\\alpha}$, $\\forall \\alpha$.\nGradient descent:\n$$ \\begin{aligned} {\\color{blue}{\\Delta w_{ij}}} \u0026= -\\eta \\frac{\\partial E}{\\partial w_{ij}} = \\eta\\sum_{\\alpha}\\frac{R_{\\alpha}}{P_{\\alpha}}{\\color{red}{\\frac{\\partial P_{\\alpha}}{\\partial w_{ij}}}}\\\\ {\\color{red}{\\frac{\\partial P_{\\alpha}}{\\partial w_{ij}}}} \u0026= \\sum_{\\beta}\\frac{1}{Z}e^{-\\beta H_{\\alpha\\beta}}S_{i}^{\\alpha\\beta}S_{j}^{\\alpha\\beta} - \\frac{1}{Z^{2}}\\left(\\sum_{\\beta}e^{-\\beta H_{\\alpha\\beta}}\\right)\\sum_{\\lambda\\mu}e^{-\\beta H_{\\lambda\\mu}}S_{i}^{\\lambda\\mu}S_{j}^{\\lambda\\mu}\\\\ \u0026= \\beta\\left[ \\sum_{\\beta}S_{i}^{\\alpha\\beta}S_{j}^{\\alpha\\beta}P_{\\alpha\\beta} - P_{\\alpha}\\langle S_{i}S_{j}\\rangle \\right]\\\\ {\\color{blue}{\\Delta w_{ij}}} \u0026= \\eta\\beta\\left[ \\sum_{\\alpha}\\frac{R_{\\alpha}}{P_{\\alpha}}\\sum_{\\beta}S_{i}^{\\alpha\\beta}S_{j}^{\\alpha\\beta}P_{\\alpha\\beta} - \\sum_{\\alpha}R_{\\alpha}\\langle S_{i}S_{j}\\rangle \\right]\\\\ \u0026= \\eta\\beta\\left[ \\sum_{\\alpha\\beta}R_{\\alpha}P_{\\beta|\\alpha}S_{i}^{\\alpha\\beta}S_{j}^{\\alpha\\beta} - \\langle S_{i}S_{j}\\rangle \\right]\\\\ \u0026= \\eta\\beta\\left[ \\overset{\\text{Hebb term}}{\\overline{\\langle S_{i}S_{j}\\rangle}_{\\text{clamped}}} - \\overset{\\text{unlearning term}}{\\langle S_{i}S_{j}\\rangle_{\\text{free}}} \\right],\\quad\\text{Boltzmann learning rule} \\end{aligned} $$\nconditional probability: $P_{\\beta|\\alpha} = P_{\\alpha\\beta}/P_{\\alpha}$.\n$\\overline{\\langle S_{i}S_{j}\\rangle}_{\\text{clamped}}$: åœ¨ç»™å®š $\\alpha$ æ¦‚ç‡ä¸º $R_{\\alpha}$ çš„æ¡ä»¶ä¸‹, $S_{i}S_{j}$ çš„å¹³å‡å€¼\nBring the system to equilibrium before taking an average.\nMonte Carlo simulation:\nTwo kinds of methods:\nSelect a unit at random, update its state according to $P(S_{i} = \\pm 1)$. Flip a unit with prob $$P(S_{i}\\rightarrow -S_{i}) = \\frac{1}{1+\\exp{(\\beta\\Delta H_{i})}}.$$ Slow at low $T$ (tends to get trapped in local minima) Faster solution: simulated annealing procedure(æ¸è¿›æ¨¡æ‹Ÿ)\nAdjust weights for convergence; Calculate $\\langle S_{i}S_{j}\\rangle$ in clamped and freestates; Annealing schedule $T(t)$; Sample and update units. Fast simulated annealing(Cauchy machine): multiple flips.\nSlow but effective.\nVariations on Boltzmann Machines(ç»å°”å…¹æ›¼æœºçš„å˜ç§) Weight decay terms $w_{ij}^{\\text{new}} = (1-\\varepsilon)w_{ij}^{\\text{old}}$: automatically become symmetric; Incremental rule(å¢é‡è§„åˆ™) If $S_{i}$ \u0026 $S_{j}$ on together, step size $\\varepsilon$, clamped: $w_{ij}\\uparrow$; free: $w_{ij}\\downarrow$\nAvoid oscillations in valley when relying on local gradient $\\exist \\alpha$, $R_{\\alpha}=0$. increase it to $\\epsilon\u003e0$ to avoid infinite weights. Input($\\gamma$), output($\\alpha$), hidden($\\beta$). Learn $\\gamma\\rightarrow \\alpha$ (å³ç›®æ ‡æ˜¯ ä»¤ $P_{\\alpha|\\gamma}=R_{\\alpha|\\gamma}$)\nError measure(entropic cost function):\n$$ E = \\sum_{\\gamma}p_{\\gamma}\\sum_{\\alpha}R_{\\alpha|\\gamma}\\log{\\frac{R_{\\alpha|\\gamma}}{P_{\\alpha|\\gamma}}} $$\nLearning rule:\n$$ \\Delta w_{ij} = \\eta\\beta\\left[ \\overline{\\langle S_{i}S_{j}\\rangle}_{\\text{I,O clamped}} - \\overline{\\langle S_{i}S_{j}\\rangle}_{\\text{I clamped}} \\right] $$\nHarmonium: two-layer Boltzmann machine. Connections between layers only.\nHopfield: without hidden units. Average $\\xi_{i}^{\\mu}\\xi_{j}^{\\mu}$ over patterns $\\mu$. Start from random cfg and relax at $T=0$. Faster than incremental one.\nStatistical Mechanics Reformulation(ç»Ÿè®¡åŠ›å­¦é‡æ„) $P_{\\alpha}$: Prob of visible units in state $\\alpha$\n$\\begin{aligned}Z_{\\text{clamped}}^{\\alpha} = \\sum_{\\beta}e^{-\\beta H_{\\alpha\\beta}}\\end{aligned}$: Partition function (visible units clamped in state $\\alpha$)\n$$ Z = e^{-\\beta F} $$\n$$ P_{\\alpha} = \\frac{Z_{\\text{clamped}}^{\\alpha}}{Z} = \\frac{e^{-\\beta F_{\\text{clamped}}^{\\alpha}}}{e^{-\\beta F}} = e^{-\\beta (F_{\\text{clamped}}^{\\alpha} - F)} $$\nCost function:\n$$ \\begin{aligned} E = E_{0} - \\sum_{\\alpha}R_{\\alpha}\\log{P_{\\alpha}} \u0026= E_{0} + \\beta\\sum_{\\alpha}R_{\\alpha}(F_{\\text{clamped}}^{\\alpha} - F)\\\\ \u0026= E_{0} + \\beta\\left[ \\overline{F_{\\text{clamped}}^{\\alpha}} - F \\right] \\end{aligned} $$\n$$ \\begin{aligned} \\langle S_{i}S_{j}\\rangle \u0026= -\\frac{\\partial F}{\\partial w_{ij}} \\\\ \u0026= T\\frac{\\partial \\log{Z}}{\\partial w_{ij}} = \\frac{T}{Z}\\frac{\\partial Z}{\\partial w_{ij}} = \\frac{1}{Z}\\sum_{\\alpha\\beta} S_{i}^{\\alpha\\beta}S_{j}^{\\alpha\\beta}e^{-\\beta H_{\\alpha\\beta}} \\end{aligned} $$\nDeterministic Boltzmann Machines(ç¡®å®šæ€§ç»å°”å…¹æ›¼æœº) Mean field annealing: $\\begin{aligned}m_{i}\\equiv\\langle S_{i}\\rangle = \\tanh{\\left(\\beta\\sum_{j}w_{ij}m_{j}\\right)},\\quad \\langle S_{i}S_{j}\\rangle\\approx m_{i}m_{j}\\end{aligned}$\nIteration\n$$ m_{i}^{\\text{new}} = \\tanh{\\left(\\beta\\sum_{j}w_{ij}m_{j}^{\\text{old}}\\right)} $$\nEntropy:\n$$ S = - \\sum_{i}\\left( p_{i}^{+}\\log{p_{i}^{+}} + p_{i}^{-}\\log{p_{i}^{-}} \\right), \\quad p_{i}^{\\pm} = P(S_{i} = \\pm 1) $$\n$p_{i}^{+} + p_{i}^{-} = 1$, $m_{i} = p_{i}^{+} - p_{i}^{-}$\n$$ p_{i}^{\\pm} = \\frac{1\\pm m_{i}}{2} $$\nMean field free energy:\n$$ \\begin{aligned} F_{\\text{MF}} \u0026= H - TS \\\\ \u0026= -\\frac{1}{2}\\sum_{ij}w_{ij}m_{i}m_{j} + T\\sum_{i}\\left( \\frac{1+m_{i}}{2}\\log{\\frac{1+m_{i}}{2}} + \\frac{1-m_{i}}{2}\\log{\\frac{1-m_{i}}{2}} \\right) \\end{aligned} $$\n$$ E = E_{0} + \\beta\\left[ \\overline{F_{\\text{clamped}}^{\\alpha}} - F \\right]\\overset{\\text{MF}}{\\longrightarrow}E_{\\text{MF}} = E_{0} + \\beta\\left[ \\overline{F_{\\text{MF}}^{\\alpha}} - F_{\\text{MF}} \\right] $$\nGradient descent for $w_{ij}$:\n$$ \\Delta w_{ij} = -\\eta\\frac{\\partial E_{\\text{MF}}}{\\partial w_{ij}} = \\eta\\beta \\bigg[\\overline{m_{i}^{\\alpha}m_{i}^{\\alpha}} - m_{i}m_{j}\\bigg] $$\nåŸå‹: $$ \\Delta w_{ij} = \\eta\\beta \\bigg[\\overline{\\langle S_{i}S_{j}\\rangle}_{\\text{clamped}} - \\langle S_{i}S_{j}\\rangle_{\\text{free}}\\bigg] $$\n7.2 Recurrent Back-Propagation(é€’å½’åå‘ä¼ æ’­) $V_{i}$: $N$ continuous-valued units\n$w_{ij}$: connections\n$g(h)$: activation function\n$\\zeta_{i}^{\\mu}$: desired training values.\nDynamic evolution rules:\n$$ \\tau\\frac{\\mathrm{d}V_{i}}{\\mathrm{d}t} = -V_{i} + g\\left( \\sum_{j}w_{ij}V_{j} + \\xi_{i} \\right) $$\nfixed point equation $\\begin{aligned}\\left(\\frac{\\mathrm{d}V_{i}}{\\mathrm{d}t} = 0\\right)\\end{aligned}$:\n$$ V_{i} = g(h_{i}) = g\\left( \\sum_{j}w_{ij}V_{j} + \\xi_{i} \\right) $$\nAssumption: At least 1 fixed point exists and is a stable attractor.\nError measure(Cost function):\n$$ E = \\frac{1}{2}\\sum_{k}E_{k}^{2},\\quad E_{k} = \\begin{cases} \\zeta_{k} - V_{k} \u0026 \\text{if }k\\text{ is output unit}\\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$\nGradient descent:\n$$ \\Delta w_{pq} = -\\eta\\frac{\\partial E}{\\partial w_{pq}} = \\eta\\sum_{k}E_{k}\\frac{\\partial V_{k}}{\\partial w_{pq}} $$\nDifferentiating fixed point equation:\n$$ \\begin{aligned} \\frac{\\partial V_{i}}{\\partial w_{pq}} \u0026= g^{\\prime}(h_{i}){\\color{red}{\\frac{\\partial h_{i}}{\\partial w_{pq}}}}\\\\ {\\color{red}{\\frac{\\partial h_{i}}{\\partial w_{pq}}}} \u0026= \\frac{\\partial }{\\partial w_{pq}}\\left(\\sum_{j}{\\color{blue}{w_{ij}V_{j}}} + \\xi_{i}\\right)\\\\ \\frac{\\partial ({\\color{blue}{w_{ij}V_{j}}})}{\\partial w_{pq}} \u0026= {\\color{green}{\\frac{\\partial w_{ij}}{\\partial w_{pq}}}}V_{j} + w_{ij}\\frac{\\partial V_{j}}{\\partial w_{pq}} = {\\color{green}{\\delta_{ip}\\delta_{jq}}}V_{j} + w_{ij}\\frac{\\partial V_{j}}{\\partial w_{pq}}\\\\ \\Rightarrow {\\color{red}{\\frac{\\partial h_{i}}{\\partial w_{pq}}}} \u0026= \\sum_{{\\color{green}{j}}}\\left( \\delta_{ip}\\delta_{{\\color{green}{j}}q}V_{{\\color{green}{j}}} + w_{ij}\\frac{\\partial V_{j}}{\\partial w_{pq}} \\right) = \\delta_{ip}V_{q} + \\sum_{j}w_{ij}\\frac{\\partial V_{j}}{\\partial w_{pq}}\\\\ \\Rightarrow \\frac{\\partial V_{i}}{\\partial w_{pq}} \u0026= g^{\\prime}(h_{i})\\left( \\delta_{ip}V_{q} + \\sum_{j}w_{ij}\\frac{\\partial V_{j}}{\\partial w_{pq}} \\right)\\\\ \u0026= \\delta_{ip}g^{\\prime}(h_{i})V_{q} + \\sum_{j}g^{\\prime}(h_{i})w_{ij}\\frac{\\partial V_{j}}{\\partial w_{pq}} \\end{aligned} $$\nå°†å•é¡¹å†™ä½œ delta-æ±‚å’Œ å½¢å¼ $\\begin{aligned}\\frac{\\partial V_{i}}{\\partial w_{pq}} = \\sum_{j}\\delta_{ij}\\frac{\\partial V_{j}}{\\partial w_{pq}}\\end{aligned}$, å†å°†ç­‰å¼å³è¾¹ç¬¬äºŒé¡¹ç§»åˆ°å·¦è¾¹\n$$ \\sum_{j}{\\color{red}{\\bigg[\\delta_{ij}-g^{\\prime}(h_{i})w_{ij}\\bigg]}}\\frac{\\partial V_{j}}{\\partial w_{pq}} = \\delta_{ip}g^{\\prime}(h_{i})V_{q}\\\\ \\sum_{j}{\\color{red}{\\mathbf{L}_{ij}}}\\frac{\\partial V_{j}}{\\partial w_{pq}} = \\delta_{ip}g^{\\prime}(h_{i})V_{q} $$\nå®é™…å¯ä»¥ç†è§£ä¸º $\\mathbf{L}_{i\\times j}\\times \\frac{\\partial \\mathbf{V}}{\\partial w_{pq}}_{j\\times 1}$ çš„çŸ©é˜µä¹˜æ³•. å·¦å³åŒæ—¶å·¦ä¹˜ $\\mathbf{L}^{-1}$:\n$$ \\frac{\\partial V_{k}}{\\partial w_{pq}} = (\\mathbf{L}^{-1})_{kp}g^{\\prime}(h_{p})V_{q} $$\ndelta-rule:\n$$ \\begin{aligned} \\Delta w_{pq} \u0026= \\eta{\\color{red}{\\sum_{k} E_{k}(\\mathbf{L}^{-1})_{kp}g^{\\prime}(h_{p})}}V_{q}\\\\ \u0026= \\eta{\\color{red}{\\delta_{p}}}V_{q} \\end{aligned} $$\nRecall: delta rule æ˜¯æºè‡ªæ¢¯åº¦ä¸‹é™çš„æ¦‚å¿µ, ä½†æ˜¯å¯ä»¥è¢«æ¨å¹¿ä¸ºæ›´æ™®é€‚çš„å½¢å¼.\nCost function:\n$$ E[\\vec{w}] = \\frac{1}{2}\\sum_{i\\mu}(\\zeta_{i}^{\\mu} - O_{i}^{\\mu})^{2} = \\frac{1}{2}\\sum_{i\\mu}\\left(\\zeta_{i}^{\\mu}-\\sum_{k}w_{ik}\\xi_{k}^{\\mu}\\right)^{2} $$\n$$ \\Delta w_{ik} = -\\eta\\frac{\\partial E}{\\partial w_{ik}} = \\eta\\sum_{\\mu}(\\zeta_{i}^{\\mu} - O_{i}^{\\mu})\\xi_{k}^{\\mu} $$\nå¯¹äºæŸå›ºå®š $\\mu$:\n$$ \\Delta w_{ik} = \\eta(\\zeta_{i}^{\\mu}-O_{i}^{\\mu})\\xi_{k}^{\\mu} = {\\color{red}{\\eta\\delta_{i}\\xi_{k}^{\\mu}}} $$\néœ€è¦æ±‚é€†è€Œè€—è´¹æ—¶é—´.\næ”¹è¿›:\n$$ \\delta_{p} = {\\color{red}{\\sum_{k} E_{k}(\\mathbf{L}^{-1})_{kp}}}g^{\\prime}(h_{p}) = g^{\\prime}(h_{p}){\\color{red}{Y_{p}}} $$\n$$ \\begin{aligned} \\sum_{p}\\mathbf{L}_{pi}Y_{p} \u0026= \\sum_{p}\\mathbf{L}_{pi}\\left[ \\sum_{k}E_{k}(\\mathbf{L}^{-1})_{kp} \\right] \\\\ \u0026= \\sum_{k}E_{k}\\left[ \\sum_{p}\\mathbf{L}_{pi}(\\mathbf{L}^{-1})_{kp} \\right]\\\\ \u0026= \\sum_{k}E_{k}\\delta_{ik} = E_{i} \\end{aligned} $$\nRecall: $$ \\mathbf{L}_{ij} = \\delta_{ij} - g^{\\prime}(h_{i})w_{ij} $$\né‚£ä¹ˆ\n$$ \\begin{aligned} \\sum_{p}\\mathbf{L}_{pi}Y_{p} \u0026= \\sum_{p}\\bigg[ \\delta_{pi}-g^{\\prime}(h_{p})w_{pi} \\bigg]Y_{p} \\\\ \u0026= \\sum_{p}\\delta_{pi}Y_{p} - \\sum_{p}g^{\\prime}(h_{p})w_{pi}Y_{p} \\\\ \u0026= \\boxed{Y_{i} - \\sum_{p}g^{\\prime}(h_{p})w_{pi}Y_{p} = E_{i}} \\end{aligned} $$\nError-propagation network:\n$$ \\tau\\frac{\\mathrm{d}Y_{i}}{\\mathrm{d}t} = - Y_{i} + \\sum_{p}g^{\\prime}(h_{p})w_{pi}Y_{p} + E_{i} $$\n$w_{ij}\\rightarrow g^{\\prime}(h_{i})w_{ij}$\n$g(x)\\rightarrow x$\nnetwork transposition(ç½‘ç»œè½¬ç½®): é€šè¿‡è½¬ç½®é¿å…æ˜¾å¼æ±‚é€†\nProcedure:\n$\\begin{aligned}\\tau\\frac{\\mathrm{d}V_{i}}{\\mathrm{d}t} = -V_{i} + g\\left(\\sum_{j}w_{ij}V_{j} + \\xi_{i}\\right)\\end{aligned}$ ä»¥æ‰¾åˆ° $V_{i}$;\n$\\begin{aligned}E_{k} = \\begin{cases}\\zeta_{k}-V_{k} \u0026 k\\text{ is output}\\\\0\u0026\\text{ others}\\end{cases}\\end{aligned}$ ä»¥ç¡®å®š $E_{i}$;\n$\\begin{aligned}\\tau\\frac{\\mathrm{d}Y_{i}}{\\mathrm{d}t} = - Y_{i} + \\sum_{p}g^{\\prime}(h_{p})w_{pi}Y_{p} + E_{i}\\end{aligned}$ ä»¥æ‰¾åˆ° $Y_{i}$;\n$\\Delta w_{pq}=\\eta\\delta_{p}V_{q}$, $\\delta_{p} = g^{\\prime}(h_{p})Y_{p}$ æ›´æ–°æƒé‡.\nwithout requiring any non-local operations(matrix inversion)\noriginal network: supply error signals $E_{i}$;\nerror-propagation network: adjust weights in the original network.\n$\\begin{aligned}Y_{i} - \\sum_{p}g^{\\prime}(h_{p})w_{pi}Y_{p} = E_{i}\\end{aligned}$ is a stable attractor if $\\begin{aligned}V_{i} = g\\left(\\sum_{j}w_{ij}V_{j} + \\xi_{i}\\right)\\end{aligned}$ is a stable attractor.\nè®¾ $V_{i}^{*}$ å’Œ $Y_{i}^{*}$ å„ä¸ºä¸¤ä¸ªåŠ¨åŠ›å­¦æ–¹ç¨‹çš„ä¸åŠ¨ç‚¹. ç ”ç©¶å…¶é‚»åŸŸ $V_{i} = V_{i}^{*} + \\epsilon_{i}$ å’Œ $Y_{i} = Y_{i}^{*} + \\eta_{i}$. ä»£å…¥å¾®åˆ†æ–¹ç¨‹ä¸”ä¿ç•™ä¸€é˜¶é¡¹:\n$$ \\begin{aligned} \\tau\\frac{\\mathrm{d}\\epsilon_{i}}{\\mathrm{d}t} \u0026= -\\epsilon_{i} + g^{\\prime}(h_{i})\\sum_{j}w_{ij}\\epsilon_{j} = -\\sum_{j}\\mathbf{L}_{ij}\\epsilon_{j}\\\\ \\tau\\frac{\\mathrm{d}\\eta_{i}}{\\mathrm{d}t} \u0026= -\\eta_{i} + \\sum_{p}g^{\\prime}(h_{p})w_{pi}\\eta_{p} = -\\sum_{p}\\mathbf{L}_{ip}^{T}\\eta_{p} \\end{aligned} $$\n$\\mathbf{L}$ å’Œ $\\mathbf{L}^{T}$ ç‰¹å¾å€¼ç›¸åŒ, æ‰€ä»¥å±€éƒ¨ç¨³å®šæ€§ä¸€è‡´.\nå…¨è¿æ¥ç½‘ç»œ($\\mathbf{L}_{N\\times N}$)æ±‚é€†æ—¶é—´å¤æ‚åº¦ ä¸º $\\mathcal{O}(N^3)$. è€Œç½‘ç»œè½¬ç½®æ³•çš„æ—¶é—´å¤æ‚åº¦ä¸º $\\mathcal{O}(N^2)$.\n7.3 Learning Time Sequences(æ—¶é—´åºåˆ—å­¦ä¹ ) Sequence Recognition(åºåˆ—è¯†åˆ«): specific input sequence $\\to$ particular output pattern\nSequence Reproduction(åºåˆ—å†ç°): generate the rest of a sequence given part of it.\nTemporal Association(æ—¶é—´å…³è”): particular output sequence in response to a specific input sequence.\nTapped Delay Lines(æŠ½å¤´å»¶è¿Ÿçº¿) time-delay neural networks:\nä»ä¿¡å· $x[t]$ ä¸­é‡‡é›†çš„ $x[\\tau], x[\\tau-\\Delta], \\cdots, x[\\tau-(m-1)\\Delta]$ åŒæ—¶ä½œä¸ºç½‘ç»œè¾“å…¥.\nnot for arbitrary-length sequences, ä¸çµæ´»; large training examples, slow computation, è€—ç®—åŠ›; precise clock, åŒæ­¥è¦æ±‚é«˜. Tank \u0026 Hopfield:\nè®¾ raw signal $\\vec{x}(t)$.\nUsual delay line: $\\vec{x}(t), \\vec{x}(t-\\tau_{1}),\\vec{x}(t-\\tau_{2}),\\cdots, \\vec{x}(t-\\tau_{m})$;\nTank \u0026 Hopfield: $\\begin{aligned} y(t;\\tau_{i}) = \\int_{-\\infty}^{t}G(t-t^{\\prime};\\tau_{i})\\vec{x}(t^{\\prime})\\mathrm{d}t^{\\prime}\\end{aligned}$\n$$ G(t;\\tau_{i}) = \\left(\\frac{t}{\\tau_{i}}\\right)^{\\alpha}e^{\\alpha(1-t/\\tau_{i})} $$\n$t=\\tau_{i}$ å–æå€¼ 1. $\\alpha$ è¶Šå¤§ $G$ è¶Šå°–é”.\nno need for precise synchronization by a central clock.\nç±»ä¼¼äºå·ç§¯+æ»‘åŠ¨çª—å£(window) çš„æ€è·¯.\nContext Units(ä¸Šä¸‹æ–‡/è®°å¿†å•å…ƒ) Partially recurrent networks(sequential networks):\nmainly feed-forward; chosen set of feed-back connections context units $C_{i}$: receive clocked feedback signals.\n$t$ æ—¶åˆ», context unit æ¥æ”¶ $t-1$ çš„ç½‘ç»œä¿¡å·\nå•ç®­å¤´: ç¬¬ $i$ å•å…ƒåªè¿æ¥ä¸‹ä¸€å±‚çš„ç¬¬ $i$ å•å…ƒ\né˜´å½±ç®­å¤´: æ¯ä¸€ä¸ªç¥ç»å…ƒéƒ½å’Œä¸Šä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒè¿æ¥\n(a) context units hold a copy of the activations of the hidden units from the previous time step.\n(b) updating rule:\n$$ C_{i}(t+1) = \\alpha C_{i}(t) + O_{i}(t) $$\n$O_{i}$: output units;\n$\\alpha \u003c 1$: strength of the self-connections.\nIf $O_{i}$ fixed: $C_{i}\\to O_{i}/(1-\\alpha)$\nå› æ­¤, ä¹Ÿè¢«ç§°ä½œ decay/integrating/capacitive units.\nIterating:\n$$ \\begin{aligned} C_{i}(t+1) \u0026= O_{i}(t) + \\alpha O_{i}(t-1) + \\alpha^{2}O_{i}(t-2)+\\cdots\\\\ \u0026= \\sum_{t^{\\prime}=0}^{t}\\alpha^{t-t^{\\prime}}O_{i}(t^{\\prime})\\\\ \u0026\\Rightarrow \\int_{0}^{t}e^{-\\gamma (t-t^{\\prime})}O_{i}(t^{\\prime})\\mathrm{d}t^{\\prime},\\quad \\gamma = |\\log{\\alpha}| \\end{aligned} $$\nè¿™è¡¨æ˜ $\\alpha$ åº”å½“å’Œè¾“å…¥åºåˆ—çš„æ—¶é—´å°ºåº¦ç›¸åŒ¹é…. å¯ä»¥ä½¿ç”¨å¤šä¸ª context å•å…ƒç»„, æ¯ç»„æœ‰ä¸åŒçš„ $\\alpha$ å€¼.\n(c) Feedback: context units themselves only.\n(d)\nInput å’Œ context ä¹‹é—´çš„è¿æ¥ä¸ºå…¨è¿æ¥è€Œéé€ä¸€è¿æ¥. Self-connections can be trained. Back-propagation: No modifiable recurrent connections.\nMozer:\n$$ C_{i}(t+1) = \\alpha_{i}C_{i}(t) + g\\left( \\sum_{j}w_{ij}\\xi_{j} \\right) $$\n$\\xi_{j}$: input pattern\nWilliams \u0026 Zipser:\n$$ C_{i}(t+1) = g\\left[ \\alpha_{i}C_{i}(t) + \\sum_{j}w_{ij}\\xi_{j} \\right] $$\nBack-Propagation Through Time(æ—¶é—´åå‘ä¼ æ’­) fully recurrent networks: Any unit $V_{i}$ may be connected to any other.\nUpdate rule:\n$$ V_{i}(t+1) = g[h_{i}(t)] = g\\left[ \\sum_{j}w_{ij}V_{j}(t) + \\xi_{i}(t) \\right] $$\n$\\xi_{i}(t)$: input at node $i$, at time $t$. ä¸å­˜åœ¨æ—¶ä¸º $0$.\nProduce $\\zeta_{i}(t)$ in response to input sequence $\\xi_{i}(t)$\nTrick: ä»»æ„ recurrent network å¯å±•å¼€ä¸º ç­‰æ•ˆ feed-forward network.\n2-unit, $T=4$. $w_{ij}$ independent of $t$.\nNo clock needed;\nNeed large computer resources:\nStorage; Computer time for simulation; Number of training examples. Once trained (in unfolded form), it works for temporal association task.\nReal-Time Recurrent Learning(å®æ—¶é€’å½’å­¦ä¹ ) Learning rule without duplicating the units.\nDeal with sequence of arbitrary length.\nDynamics:\n$$ V_{i}(t) = g[h_{i}(t-1)] = g\\left[ \\sum_{j}w_{ij}V_{j}(t-1) + \\xi_{i}(t-1) \\right] $$\n$\\zeta_{k}(t)$: target output\nError measure:\n$$ E_{k}(t) = \\begin{cases} \\zeta_{k}(t) - V_{k}(t) \u0026 \\text{if }\\zeta_{k}(t)\\text{ is defined at time }t\\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$\nCost function:\n$$ E = \\sum_{t=0}^{T} E(t) $$\n$$ E(t) = \\frac{1}{2}\\sum_{k}[E_{k}(t)]^{2},\\quad t = 0,1,2,\\cdots,T $$\nGradient descent:\n$$ \\Delta w_{pq}(t) = -\\eta\\frac{\\partial E(t)}{\\partial w_{pq}} = \\eta\\sum_{k}E_{k}(t)\\frac{\\partial V_{k}(t)}{\\partial w_{pq}} $$\ndifferentiating dynamics:\n$$ \\frac{\\partial V_{i}(t)}{\\partial w_{pq}} = g^{\\prime}[h_{i}(t-1)]\\left[ \\delta_{ip}V_{q}(t-1) + \\sum_{j}w_{ij}\\frac{\\partial V_{j}(t-1)}{\\partial w_{pq}} \\right] $$\nRecall: $$ \\frac{\\partial V_{i}}{\\partial w_{pq}} = g^{\\prime}(h_{i})\\left( \\delta_{ip}V_{q} + \\sum_{j}w_{ij}\\frac{\\partial V_{j}}{\\partial w_{pq}} \\right) $$\nIf only a stable attractor interested, let $\\partial_{w_{pq}}V_{i}(t)=\\partial_{w_{pq}}V_{i}(t-1)$, or $\\partial_{w_{pq}}V_{i}(0)=0$\n$N$ units, $N^{3}$ derivatives($N$ neurons $\\times N^{2}$ weights), $\\mathcal{O}(N^{4})$ time complexity($N^{3}$ derivatives $\\times N$ nodes).\nReal-time recurrent learning: Weight update in real time\nAvoid array storage.\nteacher forcing: always replace $V_{k}(t)$ by $\\zeta_{k}(t)$ after $E_{k}(t)$ \u0026 derivatives computed.\nåŠ é€Ÿè®­ç»ƒæ”¶æ•›, é¿å…æ—©æœŸé¢„æµ‹é”™è¯¯çš„ç§¯ç´¯.\nAttractor under teacher forcing may be removed when the network runs freely, even turn into a repellor(æ’æ–¥å­)\nflip-flop(è§¦å‘å™¨): if â€œA $\\to$ (arbitrary interval) $\\to$ Bâ€: output a signal.\nTapped delay line could never do this.\nTime-Dependent Recurrent Back-Propagation(å«æ—¶é€’å½’åå‘ä¼ æ’­) dynamics:\n$$ \\tau_{i}\\frac{\\mathrm{d}V_{i}}{\\mathrm{d}t} = -V_{i} + g\\left( \\sum_{j}w_{ij}V_{j} \\right) + \\xi_{i}(t)\\tag{*} $$\nRecall: Recurrent Back-Propagation $$ \\tau\\frac{\\mathrm{d}V_{i}}{\\mathrm{d}t} = -V_{i} + g\\left(\\sum_{j}w_{ij}V_{j} + \\xi_{i}\\right) $$\nError function:\n$$ E = \\frac{1}{2}\\int_{0}^{T}\\sum_{k\\in O}[V_{k}(t)-\\zeta_{k}(t)]^{2}\\mathrm{d}t $$\n$O$: output units\n$\\zeta_{k}(t)$: target output\nGradient descent:\nfunctional derivative(æ³›å‡½å¯¼æ•°): $$ E_{k}(t) = \\frac{\\delta E}{\\delta V_{k}(t)} = [V_{k}(t) - \\zeta_{k}(t)] $$\ngradient: $$ \\frac{\\partial E}{\\partial w_{ij}} = \\frac{1}{\\tau_{i}}\\int_{0}^{T}Y_{i}g^{\\prime}(h_{i})V_{j}\\mathrm{d}t $$\nå¯ä»¥é€šè¿‡ $\\begin{aligned}\\frac{\\partial E}{\\partial \\tau_{i}}\\end{aligned}$ ä»è€Œé€šè¿‡è°ƒæ•´ $\\tau_{i}$ è¿›è¡Œä¼˜åŒ–.\n$\\begin{aligned}h_{i}(t) = \\sum_{j}w_{ij}V_{j}(t)\\end{aligned}$\n$Y_{i}(t)$: solution of\n$$ \\frac{\\mathrm{d}Y_{i}}{\\mathrm{d}t} = \\frac{1}{\\tau_{i}}Y_{i} - \\sum_{j}\\frac{1}{\\tau_{j}}w_{ji}g^{\\prime}(h_{j})Y_{j} - E_{i}(t),\\quad Y_{i}(T) = 0, \\forall i\\tag{**} $$\nrecall:\n$$ \\mathbf{L}_{ij} = \\delta_{ij} - g^{\\prime}(h_{i})w_{ij}\\\\ Y_{p} = \\sum_{k}E_{k}(\\mathbf{L}^{-1})_{kp} $$ error propagation network dynamics: $$ \\tau\\frac{\\mathrm{d}Y_{i}}{\\mathrm{d}t} = -Y_{i} + \\sum_{p}g^{\\prime}(h_{p})w_{pi}Y_{p} + E_{i} $$\n$\\int_{0}^{T}(*)\\Rightarrow V_{i}(t)$;\n$\\int_{T}^{0}(**)\\Rightarrow Y_{i}(t)$.\nso that we get $\\begin{aligned}\\Delta w_{ij} = -\\eta\\frac{\\partial E}{\\partial w_{ij}}\\end{aligned}$\nlarge time \u0026 storage requirements: $N$ fully recurrent units \u0026 $K$ time steps ($0\\to T$)\ntime per forward-backward pass $\\sim N^{2}K$ (Real-Time Recurrent Learning: $\\sim N^{4}K$) memory $\\sim aNK+bN^{2}$ (RTRL: $\\sim N^{3}$) 7.4 Reinforcement Learning(å¼ºåŒ–å­¦ä¹ ) No detailed target values. Only â€œrightâ€ or â€œwrongâ€. Learn with a critic, not a teacher.\nIntroduce randomness to explore a correct value(by using stochastic units).\nReinforcement learning problems:\nI. Same signal for a given i-o pair. (IO mapping) No reference to previous outputs.\nII. Stochastic environment. Given same io-pair, fixed probability distribution of reinforcement. Independent of history.\nTwo-armed bandit problem(åŒè‡‚èµŒåš): ä¸¤ä¸ªæœªçŸ¥æ¦‚ç‡çš„è€è™æœº. æ‰¾åˆ°å¹³å‡æ”¶ç›Šæœ€é«˜çš„ä¸€ä¸ª. Method: stochastic learning automata(éšæœºå­¦ä¹ è‡ªåŠ¨æœº)\nIII. Reinforcement \u0026 input patterns depend on the output history arbitrarily. Game theory(åšå¼ˆè®º): â€œenvironmentâ€ = other players. [Example] Chess. Win/lose after a long sequence of moves. Credit assignment problem: how to judge each moveâ€™s contribution to victory/defeat?\nAssociative Reward-Penalty(è”æƒ³å¥–æƒ©) $A_{\\text{RP}}$ $S_{i}=\\pm 1$: output units\nDynamical rule:\n$$ P(S_{i} = \\pm 1) = g(\\pm h_{i}) = f_{\\beta}(\\pm h_{i}) = \\frac{1}{1+\\exp{(\\mp 2\\beta h_{i})}} $$\n$$ h_{i} = \\sum_{j}w_{ij}V_{j} $$\n$V_{j}$: activations of hidden/input units.\nReinforcement signal $r=\\pm 1$ (reward/penalty).\nTarget patterns:\n$$ \\zeta_{i}^{\\mu} = \\begin{cases} +S_{i}^{\\mu} \u0026 \\text{if }r^{\\mu} = +1\\\\ -S_{i}^{\\mu} \u0026 \\text{if }r^{\\mu} = -1 \\end{cases} $$\nRecall: Stochastic Units\n$$ \\begin{aligned} P(S_{i}=\\pm 1) \u0026= f_{\\beta}(\\pm h_{i}) = \\frac{1}{1+\\exp{(\\mp 2\\beta h_{i})}},\\quad h_{i}^{\\mu} = \\sum_{k}w_{ik}\\xi_{k}^{\\mu}\\\\ \\langle S_{i}^{\\mu}\\rangle \u0026= (+1)g(h_{i}^{\\mu}) + (-1)[1-g(h_{i}^{\\mu})] = \\tanh{\\left(\\beta h_{i}^{\\mu}\\right)}\\\\ \\Delta w_{ik} \u0026= \\eta\\delta_{i}^{\\mu}\\xi_{k}^{\\mu}, \\quad \\delta_{i}^{\\mu} = \\zeta_{i}^{\\mu} - \\langle S_{i}^{\\mu}\\rangle \\end{aligned} $$\nWeight update rule\n$$ \\Delta w_{ij} = \\eta(r^{\\mu})\\delta_{i}^{\\mu}V_{j}^{\\mu} $$\n$\\times g^{\\prime}(h_{i}^{\\mu})$ to minimize $\\begin{aligned}\\sum_{i}(\\delta_{i}^{\\mu})^{2}\\end{aligned}$ (not for entropic cost function).\n$\\eta(+1)$ is 10-100 times larger than $\\eta(-1)$\nSo the learning rule\n$$ \\Delta w_{ij} = \\begin{cases} \\eta^{+}[+S_{i}^{\\mu}-\\langle S_{i}^{\\mu}\\rangle]V_{j}^{\\mu} \u0026 \\text{if }r^{\\mu} = +1\\\\ \\eta^{-}[-S_{i}^{\\mu}-\\langle S_{i}^{\\mu}\\rangle]V_{j}^{\\mu} \u0026 \\text{if }r^{\\mu} = -1 \\end{cases} $$\n$\\eta^{\\pm 1} = \\eta(\\pm 1)$.\nSlow compared to supervised learning. Speed depends on the size of the output space and correct fraction of it.\n[Example] Only 1 correct answer for each input, the search can be very long.\nVariations on $A_{\\text{RP}}$:\n0/1 units. $-S_{i}^{\\mu}\\to 1-S_{i}^{\\mu}$, $\\langle S_{i}^{\\mu}\\rangle\\to p_{i}^{\\mu}$. continuous-valued reward $r\\in [0,1]$ for $[\\text{terrible}, \\text{excellent}]$. simplify the formula: $$ \\Delta w_{ij} = \\eta(r^{\\mu})\\{ r^{\\mu}[S_{i}^{\\mu} - \\langle S_{i}^{\\mu}\\rangle] + (1-r^{\\mu})[-S_{i}^{\\mu} - \\langle S_{i}^{\\mu}\\rangle] \\}V_{j}^{\\mu} $$\nfor penalty case, use $\\langle S_{i}^{\\mu}\\rangle - S_{i}^{\\mu}$ instead of $-S_{i}^{\\mu} - \\langle S_{i}^{\\mu}\\rangle$: $$ \\Delta w_{ij} = \\eta r^{\\mu}[S_{i}^{\\mu}-\\langle S_{i}^{\\mu}\\rangle]V_{j}^{\\mu} $$\nignore dependence of $\\eta$ on $r$.\n$r=-1$. æ ‡å‡†å½¢å¼: å’Œç›®å‰æ‰€åšç›¸å; æ­¤å½¢å¼: ä¸è¦åšç›®å‰æ‰€åš.\nstandard form works better for larger weight changes.\nthis variation is more amenable to theoretical analysis.\nspeed $\\uparrow$: present $\\mu$ several times before moving to the next pattern. batch mode: accumulating all $\\Delta w$ Theory of Associative Reward-Penalty(è”æƒ³å¥–æƒ©ç†è®º) Convergence: proved only in special cases.\nUsual approach: Cost function $\\to \\epsilon$\nNot so satisfactory for the stochastic process.\nCost function: $-\\langle r\\rangle$\n$E\\downarrow \\Rightarrow \\langle r\\rangle\\uparrow$\n$V_{j}\\to \\xi_{j}$. Deterministic environment(class I). So $r = r(\\mathbf{S},\\mathbf{\\xi})$\n$\\mathbf{S}$: output pattern; $\\mathbf{\\xi}$: input pattern.\n$$ \\langle r^{\\mu}\\rangle = \\sum_{\\mathbf{S}}P(\\mathbf{S}|\\mathbf{w},\\xi^{\\mu})r(\\mathbf{S},\\xi^{\\mu}) $$\n$P(\\mathbf{S}|\\mathbf{w},\\mathbf{\\xi}^{\\mu})$: prob of $\\mathbf{S}$ if $\\mathbf{w}$ and $\\mathbf{\\xi}^{\\mu}$.\n$$ P(\\mathbf{S}|\\mathbf{w},\\mathbf{\\xi}^{\\mu}) = \\prod_{k}\\begin{cases} g(h_{k}^{\\mu}) \u0026 \\text{if }S_{k} = +1\\\\ 1-g(h_{k}^{\\mu}) \u0026 \\text{if }S_{k} = -1 \\end{cases},\\quad h_{k}^{\\mu} = \\sum_{j}w_{kj}\\xi_{j}^{\\mu} $$\nGradient:\ndifferentiating $P$: $$ \\frac{\\partial P(\\mathbf{S}|\\mathbf{w},\\mathbf{\\xi}^{\\mu})}{\\partial w_{ij}} = \\left(\\prod_{k\\neq i}\\begin{cases} g(h_{k}^{\\mu}) \u0026 \\text{if }S_{k} = +1\\\\ 1-g(h_{k}^{\\mu}) \u0026 \\text{if }S_{k} = -1 \\end{cases}\\right)\\begin{cases} g^{\\prime}(h_{i}^{\\mu})V_{j}^{\\mu} \u0026 \\text{if }S_{i} = +1\\\\ -g^{\\prime}(h_{i}^{\\mu})V_{j}^{\\mu} \u0026 \\text{if }S_{i} = -1 \\end{cases}\\\\ = P(\\mathbf{S}|\\mathbf{w},\\mathbf{\\xi}^{\\mu})\\left(\\begin{cases} g^{\\prime}(h_{i}^{\\mu})/g(h_{i}^{\\mu}) \u0026 \\text{if }S_{i} = +1\\\\ -g^{\\prime}(h_{i}^{\\mu})/[1-g(h_{i}^{\\mu})] \u0026 \\text{if }S_{i} = -1 \\end{cases}\\right)V_{j}^{\\mu} $$\n$g(h) = \\frac{1}{1+e^{-2\\beta h}}$, $g^{\\prime}(h) = 2\\beta g(h)[1-g(h)]$\n$\\langle S\\rangle = \\tanh{(\\beta h)} = \\frac{e^{\\beta h} - e^{-\\beta h}}{e^{\\beta h} + e^{-\\beta h}}$\n$\\Rightarrow g(h) = \\frac{1}{2}(\\langle S\\rangle + 1)$\nSimplify:\n$$ \\begin{cases} g^{\\prime}(h_{i}^{\\mu})/g(h_{i}^{\\mu}) \u0026 \\text{if }S_{i} = +1\\\\ -g^{\\prime}(h_{i}^{\\mu})/[1-g(h_{i}^{\\mu})] \u0026 \\text{if }S_{i} = -1 \\end{cases} = \\beta[S_{i}^{\\mu} - \\langle S_{i}^{\\mu}\\rangle] $$\ndifferentiating $\\langle r^{\\mu}\\rangle$: $$ \\begin{aligned} \\frac{\\partial\\langle r\\rangle^{\\mu}}{\\partial w_{ij}} \u0026= \\beta\\sum_{\\mathbf{S}}P(\\mathbf{S}|\\mathbf{w},\\mathbf{\\xi}^{\\mu})r(\\mathbf{S},\\mathbf{\\xi}^{\\mu})[S_{i}^{\\mu}-\\langle S_{i}^{\\mu}\\rangle]V_{j}^{\\mu}\\\\ \u0026= \\beta\\bigg\\langle r(\\mathbf{S},\\mathbf{\\xi}^{\\mu})[S_{i}^{\\mu}-\\langle S_{i}^{\\mu}\\rangle]\\bigg\\rangle V_{j}^{\\mu} \\end{aligned} $$\n$\\bigg\\langle\\cdots\\bigg\\rangle$: over all output patterns $\\mathbf{S}$.\nwhen $\\partial_{\\mathbf{S}}r(\\mathbf{S},\\mathbf{\\xi}^{\\mu})=0$, $\\partial_{w_{ij}}\\langle r\\rangle^{\\mu} = 0$, which reaches a correct solution and almost always get $r^{\\mu} = +1$.\nUpdate rule:\n$$ \\begin{aligned} \\langle \\Delta w_{ij}\\rangle^{\\mu} = \\frac{\\eta}{\\beta}\\frac{\\partial \\langle r\\rangle^{\\mu}}{\\partial w_{ij}}\\\\ \\overset{\\text{average over all }\\mu}{\\Rightarrow} \\langle \\Delta w_{ij}\\rangle = \\frac{\\eta}{\\beta}\\frac{\\partial \\langle r\\rangle}{\\partial w_{ij}} \\end{aligned} $$\nä»… reinforcement signal æœ€å¤§å€¼æ—¶æƒé‡åœæ­¢æ›´æ–°.\n$A_{\\text{RP}}\\overset{\\eta^{-}\\to 0}{\\rightarrow} A_{\\text{RI}}$ (associative reward-inaction rule)\nModels \u0026 Critics Environment: an auxiliary network(è¾…åŠ©ç½‘ç»œ). Provide a target for each output (instead of global $r$).\nmodel. evaluation unit(è¯„ä¼°å•å…ƒ): continuous-valued output unit $R$. $R\\approx r$: good model.\nå…ˆè®­ç»ƒ model, å†è®­ç»ƒä¸»ç½‘ç»œ, ä½¿å¾— $R$ æœ€å¤§.\né€šè¿‡ back-propagation å®ç°: é€šè¿‡ $\\delta = -R$ ä¼ å›ä¸»ç½‘ç»œå„å•å…ƒç‹¬ç«‹çš„è¯¯å·®ä¿¡å·. æ­¤æ—¶å¯ä½¿ç”¨å¸¸è§„çš„ç›‘ç£å­¦ä¹ .\næœ€å°åŒ– $(R-r)^{2}$ çš„åŒæ—¶æœ€å¤§åŒ– $r$. æœ€ä½³ç­–ç•¥æ˜¯ä¼˜å…ˆè®­ç»ƒ model network.\nII:\naverage fluctuations from environment. chase meaningless $r$ fluctuation easy to maximize $R$. III:\nshould exstimate a weighted average of the future reinforcement, for cumulative, not instantaneous value. (sacrifice shot-term reward for long-term gain) critic: raw $r\\overset{\\text{critic}}{\\to}$ processed signal $\\rho$\nreinforcement comparison: critic generate a prediction $R$ of $r$, could use $\\rho = r-R$. ($\\rho\u003e0$: reward; $\\rho\u003c0$: penalty)\n",
  "wordCount" : "3574",
  "inLanguage": "zh",
  "image":"https://s2.loli.net/2025/10/12/xl7ojGEO5neCiRM.png","datePublished": "2025-11-20T00:18:23+08:00",
  "dateModified": "2025-11-20T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "Muartz"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Muatyz.github.io/posts/read/theory-of-neural-computation/theory-of-neural-computation-7/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "æ— å¤„æƒ¹å°˜åŸƒ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Muatyz.github.io/img/Head32.png"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  CommonHTML: {
  scale: 100
  },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<style>
  code.has-jax {
      font: "LXGW WenKai Screen", sans-serif, Arial;
      scale: 1;
      background: "LXGW WenKai Screen", sans-serif, Arial;
      border: "LXGW WenKai Screen", sans-serif, Arial;
      color: #515151;
  }
</style>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Muatyz.github.io/" accesskey="h" title="è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿— (Alt + H)">
            <img src="https://Muatyz.github.io/img/Head64.png" alt="logo" aria-label="logo"
                 height="35">è®¡ç®—ç‰©ç†å­¦ä¹ æ—¥å¿—</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Muatyz.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Muatyz.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Muatyz.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/">ğŸ“• é˜…è¯»</a>&nbsp;Â»&nbsp;<a href="https://Muatyz.github.io/posts/read/theory-of-neural-computation/">ğŸ“• è®¡ç®—ç¥ç»ç§‘å­¦</a></div>
            <h1 class="post-title">
                Recurrent Networks
            </h1>
            <div class="post-description">
                æ¯å‘¨é˜…è¯»ç¬”è®°
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-11-20
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>3574å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>8åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Muartz
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Muatyz.github.io/tags/physics/" style="color: var(--secondary)!important;">Physics</a>
                &nbsp;<a href="https://Muatyz.github.io/tags/numerical-calculation/" style="color: var(--secondary)!important;">Numerical Calculation</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Muatyz.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "Admin", 
                                region: "ap-shanghai", 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://s2.loli.net/2025/10/12/xl7ojGEO5neCiRM.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#71-boltzmann-machines%e7%8e%bb%e5%b0%94%e5%85%b9%e6%9b%bc%e6%9c%ba" aria-label="7.1 Boltzmann Machines(ç»å°”å…¹æ›¼æœº)">7.1 Boltzmann Machines(ç»å°”å…¹æ›¼æœº)</a><ul>
                        
                <li>
                    <a href="#stochastic-units%e9%9a%8f%e6%9c%ba%e5%8d%95%e5%85%83" aria-label="Stochastic Units(éšæœºå•å…ƒ)">Stochastic Units(éšæœºå•å…ƒ)</a></li>
                <li>
                    <a href="#variations-on-boltzmann-machines%e7%8e%bb%e5%b0%94%e5%85%b9%e6%9b%bc%e6%9c%ba%e7%9a%84%e5%8f%98%e7%a7%8d" aria-label="Variations on Boltzmann Machines(ç»å°”å…¹æ›¼æœºçš„å˜ç§)">Variations on Boltzmann Machines(ç»å°”å…¹æ›¼æœºçš„å˜ç§)</a></li>
                <li>
                    <a href="#statistical-mechanics-reformulation%e7%bb%9f%e8%ae%a1%e5%8a%9b%e5%ad%a6%e9%87%8d%e6%9e%84" aria-label="Statistical Mechanics Reformulation(ç»Ÿè®¡åŠ›å­¦é‡æ„)">Statistical Mechanics Reformulation(ç»Ÿè®¡åŠ›å­¦é‡æ„)</a></li>
                <li>
                    <a href="#deterministic-boltzmann-machines%e7%a1%ae%e5%ae%9a%e6%80%a7%e7%8e%bb%e5%b0%94%e5%85%b9%e6%9b%bc%e6%9c%ba" aria-label="Deterministic Boltzmann Machines(ç¡®å®šæ€§ç»å°”å…¹æ›¼æœº)">Deterministic Boltzmann Machines(ç¡®å®šæ€§ç»å°”å…¹æ›¼æœº)</a></li></ul>
                </li>
                <li>
                    <a href="#72-recurrent-back-propagation%e9%80%92%e5%bd%92%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad" aria-label="7.2 Recurrent Back-Propagation(é€’å½’åå‘ä¼ æ’­)">7.2 Recurrent Back-Propagation(é€’å½’åå‘ä¼ æ’­)</a></li>
                <li>
                    <a href="#73-learning-time-sequences%e6%97%b6%e9%97%b4%e5%ba%8f%e5%88%97%e5%ad%a6%e4%b9%a0" aria-label="7.3 Learning Time Sequences(æ—¶é—´åºåˆ—å­¦ä¹ )">7.3 Learning Time Sequences(æ—¶é—´åºåˆ—å­¦ä¹ )</a><ul>
                        
                <li>
                    <a href="#tapped-delay-lines%e6%8a%bd%e5%a4%b4%e5%bb%b6%e8%bf%9f%e7%ba%bf" aria-label="Tapped Delay Lines(æŠ½å¤´å»¶è¿Ÿçº¿)">Tapped Delay Lines(æŠ½å¤´å»¶è¿Ÿçº¿)</a></li>
                <li>
                    <a href="#context-units%e4%b8%8a%e4%b8%8b%e6%96%87%e8%ae%b0%e5%bf%86%e5%8d%95%e5%85%83" aria-label="Context Units(ä¸Šä¸‹æ–‡/è®°å¿†å•å…ƒ)">Context Units(ä¸Šä¸‹æ–‡/è®°å¿†å•å…ƒ)</a></li>
                <li>
                    <a href="#back-propagation-through-time%e6%97%b6%e9%97%b4%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad" aria-label="Back-Propagation Through Time(æ—¶é—´åå‘ä¼ æ’­)">Back-Propagation Through Time(æ—¶é—´åå‘ä¼ æ’­)</a></li>
                <li>
                    <a href="#real-time-recurrent-learning%e5%ae%9e%e6%97%b6%e9%80%92%e5%bd%92%e5%ad%a6%e4%b9%a0" aria-label="Real-Time Recurrent Learning(å®æ—¶é€’å½’å­¦ä¹ )">Real-Time Recurrent Learning(å®æ—¶é€’å½’å­¦ä¹ )</a></li>
                <li>
                    <a href="#time-dependent-recurrent-back-propagation%e5%90%ab%e6%97%b6%e9%80%92%e5%bd%92%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad" aria-label="Time-Dependent Recurrent Back-Propagation(å«æ—¶é€’å½’åå‘ä¼ æ’­)">Time-Dependent Recurrent Back-Propagation(å«æ—¶é€’å½’åå‘ä¼ æ’­)</a></li></ul>
                </li>
                <li>
                    <a href="#74-reinforcement-learning%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0" aria-label="7.4 Reinforcement Learning(å¼ºåŒ–å­¦ä¹ )">7.4 Reinforcement Learning(å¼ºåŒ–å­¦ä¹ )</a><ul>
                        
                <li>
                    <a href="#associative-reward-penalty%e8%81%94%e6%83%b3%e5%a5%96%e6%83%a9-a_textrp" aria-label="Associative Reward-Penalty(è”æƒ³å¥–æƒ©) $A_{\text{RP}}$">Associative Reward-Penalty(è”æƒ³å¥–æƒ©) $A_{\text{RP}}$</a></li>
                <li>
                    <a href="#theory-of-associative-reward-penalty%e8%81%94%e6%83%b3%e5%a5%96%e6%83%a9%e7%90%86%e8%ae%ba" aria-label="Theory of Associative Reward-Penalty(è”æƒ³å¥–æƒ©ç†è®º)">Theory of Associative Reward-Penalty(è”æƒ³å¥–æƒ©ç†è®º)</a></li>
                <li>
                    <a href="#models--critics" aria-label="Models &amp; Critics">Models &amp; Critics</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><p><strong>Recurrent Networks</strong>: With connections allowed both ways between units, and even from a unit to itself.</p>
<h2 id="71-boltzmann-machinesç»å°”å…¹æ›¼æœº">7.1 Boltzmann Machines(ç»å°”å…¹æ›¼æœº)<a hidden class="anchor" aria-hidden="true" href="#71-boltzmann-machinesç»å°”å…¹æ›¼æœº">#</a></h2>
<p>Stochastic networks with <strong>symmetric</strong> connections($w_{ij}=w_{ji}$).</p>
<ul>
<li>Original form: slow for averaging over stochastic variables.</li>
<li>Mean field version: speeded up and more applications.</li>
</ul>
<h3 id="stochastic-unitséšæœºå•å…ƒ">Stochastic Units(éšæœºå•å…ƒ)<a hidden class="anchor" aria-hidden="true" href="#stochastic-unitséšæœºå•å…ƒ">#</a></h3>
<p>Visible &amp; hidden units.</p>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/03/F146LzfHP3Urakl.png" alt=""  /></p>
</blockquote>
<blockquote>
<p><strong>Hidden units</strong>: No connection to outside world.</p>
</blockquote>
<p>$$
\begin{aligned}
P(S_{i} = +1) &amp;= g(h_{i}) = \frac{1}{1+\exp{(-2\beta h_{i})}}\\
P(S_{i} = -1) &amp;= 1 - g(h_{i})
\end{aligned}
$$</p>
<blockquote>
<p>$\begin{aligned}h_{i} = \sum_{j}w_{ij}S_{j},\quad\beta = 1/T\end{aligned}$.</p>
</blockquote>
<p>Energy function:</p>
<p>$$
H(\{S_{i}\}) = -\frac{1}{2}\sum_{ij}w_{ij}S_{i}S_{j},\quad S_{i} = \text{sgn}{(h_{i})}
$$</p>
<p>Boltzmann-Gibbs distribution:</p>
<p>$$
P(\{S_{i}\}) = \frac{e^{-\beta H(\{S_{i}\})}}{Z}
$$</p>
<blockquote>
<p>Partition function: $$Z = \sum_{\{S_{i}\}} e^{-\beta H(\{S_{i}\})}$$</p>
</blockquote>
<p>average $\langle X\rangle$:</p>
<p>$$
\langle X\rangle = \sum_{\{S_{i}\}}P(\{S_{i}\}) X(\{S_{i}\})
$$</p>
<p><strong>Pattern completion</strong>: At low $T$, only a few possible states have high and equal prob.</p>
<p>$w_{ij}\propto \xi_{i}\xi_{j}$: no information about links with hidden units.</p>
<hr>
<blockquote>
<p>$\alpha$: state of <em>visible</em> units;</p>
<p>$\beta$: state of <em>hidden</em> units.</p>
<p>$N$ visible units, $K$ hidden units $\Rightarrow 2^{N+K}$ possibilities.</p>
</blockquote>
<p>Probability $P_{\alpha}$ of chosen state $\alpha$:</p>
<p>$$
\begin{aligned}
P_{\alpha} &amp;= \sum_{\beta}P_{\alpha\beta} = \sum_{\beta}\frac{1}{Z}e^{-\beta H_{\alpha\beta}}\\
Z &amp;= \sum_{\alpha\beta}e^{-\beta H_{\alpha\beta}}\\
H_{\alpha\beta} &amp;= -\frac{1}{2}\sum_{ij}w_{ij}S_{i}^{\alpha\beta}S_{j}^{\alpha\beta}
\end{aligned}
$$</p>
<p><strong>Desired probability</strong>: $R_{\alpha}$</p>
<p><strong>Cost function</strong>: relative entropy(to measure the difference):</p>
<p>$$
E = \sum_{\alpha}R_{\alpha}\log{\frac{R_{\alpha}}{P_{\alpha}}} = E_{0} {\color{green}{- \sum_{\alpha}R_{\alpha}\log{P_{\alpha}}}}\geq 0
$$</p>
<blockquote>
<p>$E = 0$ only if $P_{\alpha} = R_{\alpha}$, $\forall \alpha$.</p>
</blockquote>
<p><strong>Gradient descent</strong>:</p>
<p>$$
\begin{aligned}
{\color{blue}{\Delta w_{ij}}} &amp;= -\eta \frac{\partial E}{\partial w_{ij}} = \eta\sum_{\alpha}\frac{R_{\alpha}}{P_{\alpha}}{\color{red}{\frac{\partial P_{\alpha}}{\partial w_{ij}}}}\\
{\color{red}{\frac{\partial P_{\alpha}}{\partial w_{ij}}}} &amp;= \sum_{\beta}\frac{1}{Z}e^{-\beta H_{\alpha\beta}}S_{i}^{\alpha\beta}S_{j}^{\alpha\beta} - \frac{1}{Z^{2}}\left(\sum_{\beta}e^{-\beta H_{\alpha\beta}}\right)\sum_{\lambda\mu}e^{-\beta H_{\lambda\mu}}S_{i}^{\lambda\mu}S_{j}^{\lambda\mu}\\
&amp;= \beta\left[
\sum_{\beta}S_{i}^{\alpha\beta}S_{j}^{\alpha\beta}P_{\alpha\beta} - P_{\alpha}\langle S_{i}S_{j}\rangle
\right]\\
{\color{blue}{\Delta w_{ij}}} &amp;= \eta\beta\left[
\sum_{\alpha}\frac{R_{\alpha}}{P_{\alpha}}\sum_{\beta}S_{i}^{\alpha\beta}S_{j}^{\alpha\beta}P_{\alpha\beta} - \sum_{\alpha}R_{\alpha}\langle S_{i}S_{j}\rangle
\right]\\
&amp;= \eta\beta\left[
\sum_{\alpha\beta}R_{\alpha}P_{\beta|\alpha}S_{i}^{\alpha\beta}S_{j}^{\alpha\beta} - \langle S_{i}S_{j}\rangle
\right]\\
&amp;= \eta\beta\left[
\overset{\text{Hebb term}}{\overline{\langle S_{i}S_{j}\rangle}_{\text{clamped}}} - \overset{\text{unlearning term}}{\langle S_{i}S_{j}\rangle_{\text{free}}}
\right],\quad\text{Boltzmann learning rule}
\end{aligned}
$$</p>
<blockquote>
<p>conditional probability: $P_{\beta|\alpha} = P_{\alpha\beta}/P_{\alpha}$.</p>
<p>$\overline{\langle S_{i}S_{j}\rangle}_{\text{clamped}}$: åœ¨ç»™å®š $\alpha$ æ¦‚ç‡ä¸º $R_{\alpha}$ çš„æ¡ä»¶ä¸‹, $S_{i}S_{j}$ çš„å¹³å‡å€¼</p>
</blockquote>
<p>Bring the system to equilibrium before taking an average.</p>
<p><strong>Monte Carlo simulation</strong>:</p>
<p>Two kinds of methods:</p>
<ul>
<li>Select a unit at random, update its state according to $P(S_{i} = \pm 1)$.</li>
<li>Flip a unit with prob $$P(S_{i}\rightarrow -S_{i}) = \frac{1}{1+\exp{(\beta\Delta H_{i})}}.$$ Slow at low $T$ (tends to get trapped in local minima)</li>
</ul>
<blockquote>
<p>Faster solution: <strong>simulated annealing procedure(æ¸è¿›æ¨¡æ‹Ÿ)</strong></p>
</blockquote>
<ol>
<li>Adjust weights for convergence;</li>
<li>Calculate $\langle S_{i}S_{j}\rangle$ in clamped and freestates;</li>
<li>Annealing schedule $T(t)$;</li>
<li>Sample and update units.</li>
</ol>
<blockquote>
<p>Fast simulated annealing(Cauchy machine): multiple flips.</p>
</blockquote>
<p>Slow but effective.</p>
<h3 id="variations-on-boltzmann-machinesç»å°”å…¹æ›¼æœºçš„å˜ç§">Variations on Boltzmann Machines(ç»å°”å…¹æ›¼æœºçš„å˜ç§)<a hidden class="anchor" aria-hidden="true" href="#variations-on-boltzmann-machinesç»å°”å…¹æ›¼æœºçš„å˜ç§">#</a></h3>
<ol>
<li>Weight decay terms $w_{ij}^{\text{new}} = (1-\varepsilon)w_{ij}^{\text{old}}$: automatically become symmetric;</li>
<li>Incremental rule(å¢é‡è§„åˆ™)</li>
</ol>
<blockquote>
<p>If $S_{i}$ &amp; $S_{j}$ on together, step size $\varepsilon$, clamped: $w_{ij}\uparrow$; free: $w_{ij}\downarrow$</p>
</blockquote>
<blockquote>
<p>Avoid oscillations in valley when relying on local gradient
<img loading="lazy" src="https://s2.loli.net/2025/12/03/JGumwB1MrvRSNAf.png" alt=""  /></p>
</blockquote>
<ul>
<li>$\exist \alpha$, $R_{\alpha}=0$. increase it to $\epsilon&gt;0$ to avoid infinite weights.</li>
</ul>
<hr>
<p>Input($\gamma$), output($\alpha$), hidden($\beta$). Learn $\gamma\rightarrow \alpha$ (å³ç›®æ ‡æ˜¯ ä»¤ $P_{\alpha|\gamma}=R_{\alpha|\gamma}$)</p>
<p><strong>Error measure</strong>(entropic cost function):</p>
<p>$$
E = \sum_{\gamma}p_{\gamma}\sum_{\alpha}R_{\alpha|\gamma}\log{\frac{R_{\alpha|\gamma}}{P_{\alpha|\gamma}}}
$$</p>
<p><strong>Learning rule</strong>:</p>
<p>$$
\Delta w_{ij} = \eta\beta\left[
\overline{\langle S_{i}S_{j}\rangle}_{\text{I,O clamped}} - \overline{\langle S_{i}S_{j}\rangle}_{\text{I clamped}}
\right]
$$</p>
<ol start="3">
<li>
<p><strong>Harmonium</strong>: two-layer Boltzmann machine. Connections between layers only.</p>
</li>
<li>
<p><strong>Hopfield</strong>: without hidden units. Average $\xi_{i}^{\mu}\xi_{j}^{\mu}$ over patterns $\mu$. Start from random cfg and relax at $T=0$. Faster than incremental one.</p>
</li>
</ol>
<h3 id="statistical-mechanics-reformulationç»Ÿè®¡åŠ›å­¦é‡æ„">Statistical Mechanics Reformulation(ç»Ÿè®¡åŠ›å­¦é‡æ„)<a hidden class="anchor" aria-hidden="true" href="#statistical-mechanics-reformulationç»Ÿè®¡åŠ›å­¦é‡æ„">#</a></h3>
<p>$P_{\alpha}$: Prob of visible units in state $\alpha$</p>
<p>$\begin{aligned}Z_{\text{clamped}}^{\alpha} = \sum_{\beta}e^{-\beta H_{\alpha\beta}}\end{aligned}$: Partition function (visible units clamped in state $\alpha$)</p>
<p>$$
Z = e^{-\beta F}
$$</p>
<p>$$
P_{\alpha} = \frac{Z_{\text{clamped}}^{\alpha}}{Z} = \frac{e^{-\beta F_{\text{clamped}}^{\alpha}}}{e^{-\beta F}} = e^{-\beta (F_{\text{clamped}}^{\alpha} - F)}
$$</p>
<p>Cost function:</p>
<p>$$
\begin{aligned}
E = E_{0} - \sum_{\alpha}R_{\alpha}\log{P_{\alpha}} &amp;= E_{0} + \beta\sum_{\alpha}R_{\alpha}(F_{\text{clamped}}^{\alpha} - F)\\
&amp;= E_{0} + \beta\left[
\overline{F_{\text{clamped}}^{\alpha}} - F
\right]
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\langle S_{i}S_{j}\rangle &amp;= -\frac{\partial F}{\partial w_{ij}} \\
&amp;= T\frac{\partial \log{Z}}{\partial w_{ij}} = \frac{T}{Z}\frac{\partial Z}{\partial w_{ij}} = \frac{1}{Z}\sum_{\alpha\beta} S_{i}^{\alpha\beta}S_{j}^{\alpha\beta}e^{-\beta H_{\alpha\beta}}
\end{aligned}
$$</p>
<h3 id="deterministic-boltzmann-machinesç¡®å®šæ€§ç»å°”å…¹æ›¼æœº">Deterministic Boltzmann Machines(ç¡®å®šæ€§ç»å°”å…¹æ›¼æœº)<a hidden class="anchor" aria-hidden="true" href="#deterministic-boltzmann-machinesç¡®å®šæ€§ç»å°”å…¹æ›¼æœº">#</a></h3>
<p><strong>Mean field annealing</strong>: $\begin{aligned}m_{i}\equiv\langle S_{i}\rangle = \tanh{\left(\beta\sum_{j}w_{ij}m_{j}\right)},\quad \langle S_{i}S_{j}\rangle\approx m_{i}m_{j}\end{aligned}$</p>
<p>Iteration</p>
<p>$$
m_{i}^{\text{new}} = \tanh{\left(\beta\sum_{j}w_{ij}m_{j}^{\text{old}}\right)}
$$</p>
<p>Entropy:</p>
<p>$$
S = - \sum_{i}\left(
p_{i}^{+}\log{p_{i}^{+}} + p_{i}^{-}\log{p_{i}^{-}}
\right), \quad p_{i}^{\pm} = P(S_{i} = \pm 1)
$$</p>
<blockquote>
<p>$p_{i}^{+} + p_{i}^{-} = 1$, $m_{i} = p_{i}^{+} - p_{i}^{-}$</p>
<p>$$
p_{i}^{\pm} = \frac{1\pm m_{i}}{2}
$$</p>
</blockquote>
<p>Mean field free energy:</p>
<p>$$
\begin{aligned}
F_{\text{MF}} &amp;= H - TS \\
&amp;= -\frac{1}{2}\sum_{ij}w_{ij}m_{i}m_{j} + T\sum_{i}\left(
\frac{1+m_{i}}{2}\log{\frac{1+m_{i}}{2}} + \frac{1-m_{i}}{2}\log{\frac{1-m_{i}}{2}}
\right)
\end{aligned}
$$</p>
<p>$$
E = E_{0} + \beta\left[
\overline{F_{\text{clamped}}^{\alpha}} - F
\right]\overset{\text{MF}}{\longrightarrow}E_{\text{MF}} = E_{0} + \beta\left[
\overline{F_{\text{MF}}^{\alpha}} - F_{\text{MF}}
\right]
$$</p>
<p>Gradient descent for $w_{ij}$:</p>
<p>$$
\Delta w_{ij} = -\eta\frac{\partial E_{\text{MF}}}{\partial w_{ij}} = \eta\beta \bigg[\overline{m_{i}^{\alpha}m_{i}^{\alpha}} - m_{i}m_{j}\bigg]
$$</p>
<blockquote>
<p>åŸå‹:
$$
\Delta w_{ij} = \eta\beta \bigg[\overline{\langle S_{i}S_{j}\rangle}_{\text{clamped}} - \langle S_{i}S_{j}\rangle_{\text{free}}\bigg]
$$</p>
</blockquote>
<h2 id="72-recurrent-back-propagationé€’å½’åå‘ä¼ æ’­">7.2 Recurrent Back-Propagation(é€’å½’åå‘ä¼ æ’­)<a hidden class="anchor" aria-hidden="true" href="#72-recurrent-back-propagationé€’å½’åå‘ä¼ æ’­">#</a></h2>
<blockquote>
<p>$V_{i}$: $N$ continuous-valued units</p>
<p>$w_{ij}$: connections</p>
<p>$g(h)$: activation function</p>
<p>$\zeta_{i}^{\mu}$: desired training values.</p>
</blockquote>
<p>Dynamic evolution rules:</p>
<p>$$
\tau\frac{\mathrm{d}V_{i}}{\mathrm{d}t} = -V_{i} + g\left(
\sum_{j}w_{ij}V_{j} + \xi_{i}
\right)
$$</p>
<p>fixed point equation $\begin{aligned}\left(\frac{\mathrm{d}V_{i}}{\mathrm{d}t} = 0\right)\end{aligned}$:</p>
<p>$$
V_{i} = g(h_{i}) = g\left(
\sum_{j}w_{ij}V_{j} + \xi_{i}
\right)
$$</p>
<blockquote>
<p>Assumption: At least 1 fixed point exists and is a stable attractor.</p>
</blockquote>
<p>Error measure(Cost function):</p>
<p>$$
E = \frac{1}{2}\sum_{k}E_{k}^{2},\quad E_{k} = \begin{cases}
\zeta_{k} - V_{k} &amp; \text{if }k\text{ is output unit}\\
0 &amp; \text{otherwise}
\end{cases}
$$</p>
<p>Gradient descent:</p>
<p>$$
\Delta w_{pq} = -\eta\frac{\partial E}{\partial w_{pq}} = \eta\sum_{k}E_{k}\frac{\partial V_{k}}{\partial w_{pq}}
$$</p>
<p>Differentiating <em>fixed point equation</em>:</p>
<p>$$
\begin{aligned}
\frac{\partial V_{i}}{\partial w_{pq}} &amp;= g^{\prime}(h_{i}){\color{red}{\frac{\partial h_{i}}{\partial w_{pq}}}}\\
{\color{red}{\frac{\partial h_{i}}{\partial w_{pq}}}} &amp;= \frac{\partial }{\partial w_{pq}}\left(\sum_{j}{\color{blue}{w_{ij}V_{j}}} + \xi_{i}\right)\\
\frac{\partial ({\color{blue}{w_{ij}V_{j}}})}{\partial w_{pq}} &amp;= {\color{green}{\frac{\partial w_{ij}}{\partial w_{pq}}}}V_{j} + w_{ij}\frac{\partial V_{j}}{\partial w_{pq}} = {\color{green}{\delta_{ip}\delta_{jq}}}V_{j} + w_{ij}\frac{\partial V_{j}}{\partial w_{pq}}\\
\Rightarrow {\color{red}{\frac{\partial h_{i}}{\partial w_{pq}}}} &amp;= \sum_{{\color{green}{j}}}\left(
\delta_{ip}\delta_{{\color{green}{j}}q}V_{{\color{green}{j}}} + w_{ij}\frac{\partial V_{j}}{\partial w_{pq}}
\right) = \delta_{ip}V_{q} + \sum_{j}w_{ij}\frac{\partial V_{j}}{\partial w_{pq}}\\
\Rightarrow \frac{\partial V_{i}}{\partial w_{pq}} &amp;= g^{\prime}(h_{i})\left(
\delta_{ip}V_{q} + \sum_{j}w_{ij}\frac{\partial V_{j}}{\partial w_{pq}}
\right)\\
&amp;= \delta_{ip}g^{\prime}(h_{i})V_{q} + \sum_{j}g^{\prime}(h_{i})w_{ij}\frac{\partial V_{j}}{\partial w_{pq}}
\end{aligned}
$$</p>
<p>å°†å•é¡¹å†™ä½œ delta-æ±‚å’Œ å½¢å¼ $\begin{aligned}\frac{\partial V_{i}}{\partial w_{pq}} = \sum_{j}\delta_{ij}\frac{\partial V_{j}}{\partial w_{pq}}\end{aligned}$, å†å°†ç­‰å¼å³è¾¹ç¬¬äºŒé¡¹ç§»åˆ°å·¦è¾¹</p>
<p>$$
\sum_{j}{\color{red}{\bigg[\delta_{ij}-g^{\prime}(h_{i})w_{ij}\bigg]}}\frac{\partial V_{j}}{\partial w_{pq}} = \delta_{ip}g^{\prime}(h_{i})V_{q}\\
\sum_{j}{\color{red}{\mathbf{L}_{ij}}}\frac{\partial V_{j}}{\partial w_{pq}} = \delta_{ip}g^{\prime}(h_{i})V_{q}
$$</p>
<p>å®é™…å¯ä»¥ç†è§£ä¸º $\mathbf{L}_{i\times j}\times \frac{\partial \mathbf{V}}{\partial w_{pq}}_{j\times 1}$ çš„çŸ©é˜µä¹˜æ³•. å·¦å³åŒæ—¶å·¦ä¹˜ $\mathbf{L}^{-1}$:</p>
<p>$$
\frac{\partial V_{k}}{\partial w_{pq}} = (\mathbf{L}^{-1})_{kp}g^{\prime}(h_{p})V_{q}
$$</p>
<p>delta-rule:</p>
<p>$$
\begin{aligned}
\Delta w_{pq} &amp;= \eta{\color{red}{\sum_{k} E_{k}(\mathbf{L}^{-1})_{kp}g^{\prime}(h_{p})}}V_{q}\\
&amp;= \eta{\color{red}{\delta_{p}}}V_{q}
\end{aligned}
$$</p>
<blockquote>
<p>Recall: <strong>delta rule</strong> æ˜¯æºè‡ªæ¢¯åº¦ä¸‹é™çš„æ¦‚å¿µ, ä½†æ˜¯å¯ä»¥è¢«æ¨å¹¿ä¸ºæ›´æ™®é€‚çš„å½¢å¼.</p>
<p>Cost function:</p>
<p>$$
E[\vec{w}] = \frac{1}{2}\sum_{i\mu}(\zeta_{i}^{\mu} - O_{i}^{\mu})^{2} = \frac{1}{2}\sum_{i\mu}\left(\zeta_{i}^{\mu}-\sum_{k}w_{ik}\xi_{k}^{\mu}\right)^{2}
$$</p>
<p>$$
\Delta w_{ik} = -\eta\frac{\partial E}{\partial w_{ik}} = \eta\sum_{\mu}(\zeta_{i}^{\mu} - O_{i}^{\mu})\xi_{k}^{\mu}
$$</p>
<p>å¯¹äºæŸå›ºå®š $\mu$:</p>
<p>$$
\Delta w_{ik} = \eta(\zeta_{i}^{\mu}-O_{i}^{\mu})\xi_{k}^{\mu} = {\color{red}{\eta\delta_{i}\xi_{k}^{\mu}}}
$$</p>
</blockquote>
<p>éœ€è¦æ±‚é€†è€Œè€—è´¹æ—¶é—´.</p>
<hr>
<p>æ”¹è¿›:</p>
<p>$$
\delta_{p} = {\color{red}{\sum_{k} E_{k}(\mathbf{L}^{-1})_{kp}}}g^{\prime}(h_{p}) = g^{\prime}(h_{p}){\color{red}{Y_{p}}}
$$</p>
<p>$$
\begin{aligned}
\sum_{p}\mathbf{L}_{pi}Y_{p} &amp;= \sum_{p}\mathbf{L}_{pi}\left[
\sum_{k}E_{k}(\mathbf{L}^{-1})_{kp}
\right] \\
&amp;= \sum_{k}E_{k}\left[
\sum_{p}\mathbf{L}_{pi}(\mathbf{L}^{-1})_{kp}
\right]\\
&amp;= \sum_{k}E_{k}\delta_{ik} = E_{i}
\end{aligned}
$$</p>
<blockquote>
<p>Recall: $$
\mathbf{L}_{ij} = \delta_{ij} - g^{\prime}(h_{i})w_{ij}
$$</p>
</blockquote>
<p>é‚£ä¹ˆ</p>
<p>$$
\begin{aligned}
\sum_{p}\mathbf{L}_{pi}Y_{p} &amp;= \sum_{p}\bigg[
\delta_{pi}-g^{\prime}(h_{p})w_{pi}
\bigg]Y_{p} \\
&amp;= \sum_{p}\delta_{pi}Y_{p} - \sum_{p}g^{\prime}(h_{p})w_{pi}Y_{p} \\
&amp;= \boxed{Y_{i} - \sum_{p}g^{\prime}(h_{p})w_{pi}Y_{p} = E_{i}}
\end{aligned}
$$</p>
<p>Error-propagation network:</p>
<p>$$
\tau\frac{\mathrm{d}Y_{i}}{\mathrm{d}t} = - Y_{i} + \sum_{p}g^{\prime}(h_{p})w_{pi}Y_{p} + E_{i}
$$</p>
<blockquote>
<p>$w_{ij}\rightarrow g^{\prime}(h_{i})w_{ij}$</p>
<p>$g(x)\rightarrow x$</p>
</blockquote>
<blockquote>
<p>network transposition(ç½‘ç»œè½¬ç½®): é€šè¿‡è½¬ç½®é¿å…æ˜¾å¼æ±‚é€†</p>
</blockquote>
<p>Procedure:</p>
<ol>
<li>
<p>$\begin{aligned}\tau\frac{\mathrm{d}V_{i}}{\mathrm{d}t} = -V_{i} + g\left(\sum_{j}w_{ij}V_{j} + \xi_{i}\right)\end{aligned}$ ä»¥æ‰¾åˆ° $V_{i}$;</p>
</li>
<li>
<p>$\begin{aligned}E_{k} = \begin{cases}\zeta_{k}-V_{k} &amp; k\text{ is output}\\0&amp;\text{ others}\end{cases}\end{aligned}$ ä»¥ç¡®å®š $E_{i}$;</p>
</li>
<li>
<p>$\begin{aligned}\tau\frac{\mathrm{d}Y_{i}}{\mathrm{d}t} = - Y_{i} + \sum_{p}g^{\prime}(h_{p})w_{pi}Y_{p} + E_{i}\end{aligned}$ ä»¥æ‰¾åˆ° $Y_{i}$;</p>
</li>
<li>
<p>$\Delta w_{pq}=\eta\delta_{p}V_{q}$, $\delta_{p} = g^{\prime}(h_{p})Y_{p}$ æ›´æ–°æƒé‡.</p>
</li>
</ol>
<blockquote>
<p>without requiring any non-local operations(matrix inversion)</p>
</blockquote>
<p>original network: supply error signals $E_{i}$;</p>
<p>error-propagation network: adjust weights in the original network.</p>
<hr>
<p>$\begin{aligned}Y_{i} - \sum_{p}g^{\prime}(h_{p})w_{pi}Y_{p} = E_{i}\end{aligned}$ is a stable attractor if $\begin{aligned}V_{i} = g\left(\sum_{j}w_{ij}V_{j} + \xi_{i}\right)\end{aligned}$ is a stable attractor.</p>
<p>è®¾ $V_{i}^{*}$ å’Œ $Y_{i}^{*}$ å„ä¸ºä¸¤ä¸ªåŠ¨åŠ›å­¦æ–¹ç¨‹çš„ä¸åŠ¨ç‚¹. ç ”ç©¶å…¶é‚»åŸŸ $V_{i} = V_{i}^{*} + \epsilon_{i}$ å’Œ $Y_{i} = Y_{i}^{*} + \eta_{i}$. ä»£å…¥å¾®åˆ†æ–¹ç¨‹ä¸”ä¿ç•™ä¸€é˜¶é¡¹:</p>
<p>$$
\begin{aligned}
\tau\frac{\mathrm{d}\epsilon_{i}}{\mathrm{d}t} &amp;= -\epsilon_{i} + g^{\prime}(h_{i})\sum_{j}w_{ij}\epsilon_{j} = -\sum_{j}\mathbf{L}_{ij}\epsilon_{j}\\
\tau\frac{\mathrm{d}\eta_{i}}{\mathrm{d}t} &amp;= -\eta_{i} + \sum_{p}g^{\prime}(h_{p})w_{pi}\eta_{p} = -\sum_{p}\mathbf{L}_{ip}^{T}\eta_{p}
\end{aligned}
$$</p>
<blockquote>
<p>$\mathbf{L}$ å’Œ $\mathbf{L}^{T}$ ç‰¹å¾å€¼ç›¸åŒ, æ‰€ä»¥å±€éƒ¨ç¨³å®šæ€§ä¸€è‡´.</p>
</blockquote>
<p>å…¨è¿æ¥ç½‘ç»œ($\mathbf{L}_{N\times N}$)æ±‚é€†æ—¶é—´å¤æ‚åº¦ ä¸º $\mathcal{O}(N^3)$. è€Œç½‘ç»œè½¬ç½®æ³•çš„æ—¶é—´å¤æ‚åº¦ä¸º $\mathcal{O}(N^2)$.</p>
<h2 id="73-learning-time-sequencesæ—¶é—´åºåˆ—å­¦ä¹ ">7.3 Learning Time Sequences(æ—¶é—´åºåˆ—å­¦ä¹ )<a hidden class="anchor" aria-hidden="true" href="#73-learning-time-sequencesæ—¶é—´åºåˆ—å­¦ä¹ ">#</a></h2>
<ul>
<li>
<p>Sequence Recognition(åºåˆ—è¯†åˆ«): specific input sequence $\to$ particular output pattern</p>
</li>
<li>
<p>Sequence Reproduction(åºåˆ—å†ç°): generate the rest of a sequence given part of it.</p>
</li>
<li>
<p>Temporal Association(æ—¶é—´å…³è”): particular output sequence in response to a specific input sequence.</p>
</li>
</ul>
<h3 id="tapped-delay-linesæŠ½å¤´å»¶è¿Ÿçº¿">Tapped Delay Lines(æŠ½å¤´å»¶è¿Ÿçº¿)<a hidden class="anchor" aria-hidden="true" href="#tapped-delay-linesæŠ½å¤´å»¶è¿Ÿçº¿">#</a></h3>
<p>time-delay neural networks:</p>
<p>ä»ä¿¡å· $x[t]$ ä¸­é‡‡é›†çš„ $x[\tau], x[\tau-\Delta], \cdots, x[\tau-(m-1)\Delta]$ åŒæ—¶ä½œä¸ºç½‘ç»œè¾“å…¥.</p>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/04/fnbyBtlZjhLiSFd.png" alt=""  /></p>
</blockquote>
<ul>
<li>not for arbitrary-length sequences, ä¸çµæ´»;</li>
<li>large training examples, slow computation, è€—ç®—åŠ›;</li>
<li>precise clock, åŒæ­¥è¦æ±‚é«˜.</li>
</ul>
<hr>
<p>Tank &amp; Hopfield:</p>
<p>è®¾ raw signal $\vec{x}(t)$.</p>
<ul>
<li>
<p>Usual delay line: $\vec{x}(t), \vec{x}(t-\tau_{1}),\vec{x}(t-\tau_{2}),\cdots, \vec{x}(t-\tau_{m})$;</p>
</li>
<li>
<p>Tank &amp; Hopfield: $\begin{aligned} y(t;\tau_{i}) = \int_{-\infty}^{t}G(t-t^{\prime};\tau_{i})\vec{x}(t^{\prime})\mathrm{d}t^{\prime}\end{aligned}$</p>
</li>
</ul>
<blockquote>
<p>$$
G(t;\tau_{i}) = \left(\frac{t}{\tau_{i}}\right)^{\alpha}e^{\alpha(1-t/\tau_{i})}
$$</p>
<p>$t=\tau_{i}$ å–æå€¼ 1. $\alpha$ è¶Šå¤§ $G$ è¶Šå°–é”.</p>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/04/G6jBogLXN2pTqxD.png" alt=""  /></p>
</blockquote>
<p>no need for precise <strong>synchronization</strong> by a central clock.</p>
<p>ç±»ä¼¼äºå·ç§¯+æ»‘åŠ¨çª—å£(window) çš„æ€è·¯.</p>
<h3 id="context-unitsä¸Šä¸‹æ–‡è®°å¿†å•å…ƒ">Context Units(ä¸Šä¸‹æ–‡/è®°å¿†å•å…ƒ)<a hidden class="anchor" aria-hidden="true" href="#context-unitsä¸Šä¸‹æ–‡è®°å¿†å•å…ƒ">#</a></h3>
<p>Partially recurrent networks(sequential networks):</p>
<ul>
<li>mainly feed-forward;</li>
<li>chosen set of feed-back connections</li>
</ul>
<p><strong>context units</strong> $C_{i}$: receive clocked feedback signals.</p>
<blockquote>
<p>$t$ æ—¶åˆ», context unit æ¥æ”¶ $t-1$ çš„ç½‘ç»œä¿¡å·</p>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/04/vSpqjNH7ZXPaJEU.png" alt=""  /></p>
<blockquote>
<p>å•ç®­å¤´: ç¬¬ $i$ å•å…ƒåªè¿æ¥ä¸‹ä¸€å±‚çš„ç¬¬ $i$ å•å…ƒ</p>
<p>é˜´å½±ç®­å¤´: æ¯ä¸€ä¸ªç¥ç»å…ƒéƒ½å’Œä¸Šä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒè¿æ¥</p>
</blockquote>
</blockquote>
<p><strong>(a)</strong> context units hold a copy of the activations of the hidden units from the previous time step.</p>
<p><strong>(b)</strong> updating rule:</p>
<p>$$
C_{i}(t+1) = \alpha C_{i}(t) + O_{i}(t)
$$</p>
<blockquote>
<p>$O_{i}$: output units;</p>
<p>$\alpha &lt; 1$: strength of the self-connections.</p>
</blockquote>
<p>If $O_{i}$ fixed: $C_{i}\to O_{i}/(1-\alpha)$</p>
<p>å› æ­¤, ä¹Ÿè¢«ç§°ä½œ decay/integrating/capacitive units.</p>
<p>Iterating:</p>
<p>$$
\begin{aligned}
C_{i}(t+1) &amp;= O_{i}(t) + \alpha O_{i}(t-1) + \alpha^{2}O_{i}(t-2)+\cdots\\
&amp;= \sum_{t^{\prime}=0}^{t}\alpha^{t-t^{\prime}}O_{i}(t^{\prime})\\
&amp;\Rightarrow \int_{0}^{t}e^{-\gamma (t-t^{\prime})}O_{i}(t^{\prime})\mathrm{d}t^{\prime},\quad \gamma = |\log{\alpha}|
\end{aligned}
$$</p>
<blockquote>
<p>è¿™è¡¨æ˜ $\alpha$ åº”å½“å’Œè¾“å…¥åºåˆ—çš„æ—¶é—´å°ºåº¦ç›¸åŒ¹é…. å¯ä»¥ä½¿ç”¨å¤šä¸ª context å•å…ƒç»„, æ¯ç»„æœ‰ä¸åŒçš„ $\alpha$ å€¼.</p>
</blockquote>
<p><strong>(c)</strong> Feedback: context units themselves only.</p>
<p><strong>(d)</strong></p>
<ul>
<li>Input å’Œ context ä¹‹é—´çš„è¿æ¥ä¸ºå…¨è¿æ¥è€Œéé€ä¸€è¿æ¥.</li>
<li>Self-connections can be trained.</li>
</ul>
<hr>
<p>Back-propagation: No modifiable recurrent connections.</p>
<p>Mozer:</p>
<p>$$
C_{i}(t+1) = \alpha_{i}C_{i}(t) + g\left(
\sum_{j}w_{ij}\xi_{j}
\right)
$$</p>
<blockquote>
<p>$\xi_{j}$: input pattern</p>
</blockquote>
<p>Williams &amp; Zipser:</p>
<p>$$
C_{i}(t+1) = g\left[
\alpha_{i}C_{i}(t) + \sum_{j}w_{ij}\xi_{j}
\right]
$$</p>
<h3 id="back-propagation-through-timeæ—¶é—´åå‘ä¼ æ’­">Back-Propagation Through Time(æ—¶é—´åå‘ä¼ æ’­)<a hidden class="anchor" aria-hidden="true" href="#back-propagation-through-timeæ—¶é—´åå‘ä¼ æ’­">#</a></h3>
<p><strong>fully recurrent networks</strong>: Any unit $V_{i}$ may be connected to any other.</p>
<p>Update rule:</p>
<p>$$
V_{i}(t+1) = g[h_{i}(t)] = g\left[
\sum_{j}w_{ij}V_{j}(t) + \xi_{i}(t)
\right]
$$</p>
<blockquote>
<p>$\xi_{i}(t)$: input at node $i$, at time $t$. ä¸å­˜åœ¨æ—¶ä¸º $0$.</p>
</blockquote>
<p>Produce $\zeta_{i}(t)$ in response to input sequence $\xi_{i}(t)$</p>
<p>Trick: ä»»æ„ recurrent network å¯å±•å¼€ä¸º ç­‰æ•ˆ feed-forward network.</p>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/04/NREu8YCDPXiTwsF.png" alt=""  /></p>
<p>2-unit, $T=4$. $w_{ij}$ independent of $t$.</p>
</blockquote>
<p>No clock needed;</p>
<p>Need large computer resources:</p>
<ul>
<li>Storage;</li>
<li>Computer time for simulation;</li>
<li>Number of training examples.</li>
</ul>
<p>Once trained (in unfolded form), it works for temporal association task.</p>
<h3 id="real-time-recurrent-learningå®æ—¶é€’å½’å­¦ä¹ ">Real-Time Recurrent Learning(å®æ—¶é€’å½’å­¦ä¹ )<a hidden class="anchor" aria-hidden="true" href="#real-time-recurrent-learningå®æ—¶é€’å½’å­¦ä¹ ">#</a></h3>
<p>Learning rule without duplicating the units.</p>
<p>Deal with sequence of arbitrary length.</p>
<hr>
<p><strong>Dynamics</strong>:</p>
<p>$$
V_{i}(t) = g[h_{i}(t-1)] = g\left[
\sum_{j}w_{ij}V_{j}(t-1) + \xi_{i}(t-1)
\right]
$$</p>
<blockquote>
<p>$\zeta_{k}(t)$: target output</p>
</blockquote>
<p><strong>Error measure</strong>:</p>
<p>$$
E_{k}(t) = \begin{cases}
\zeta_{k}(t) - V_{k}(t) &amp; \text{if }\zeta_{k}(t)\text{ is defined at time }t\\
0 &amp; \text{otherwise}
\end{cases}
$$</p>
<p><strong>Cost function</strong>:</p>
<p>$$
E = \sum_{t=0}^{T} E(t)
$$</p>
<blockquote>
<p>$$
E(t) = \frac{1}{2}\sum_{k}[E_{k}(t)]^{2},\quad t = 0,1,2,\cdots,T
$$</p>
</blockquote>
<p><strong>Gradient descent</strong>:</p>
<p>$$
\Delta w_{pq}(t) = -\eta\frac{\partial E(t)}{\partial w_{pq}} = \eta\sum_{k}E_{k}(t)\frac{\partial V_{k}(t)}{\partial w_{pq}}
$$</p>
<p>differentiating <em>dynamics</em>:</p>
<p>$$
\frac{\partial V_{i}(t)}{\partial w_{pq}} = g^{\prime}[h_{i}(t-1)]\left[
\delta_{ip}V_{q}(t-1) + \sum_{j}w_{ij}\frac{\partial V_{j}(t-1)}{\partial w_{pq}}
\right]
$$</p>
<blockquote>
<p>Recall:
$$
\frac{\partial V_{i}}{\partial w_{pq}} = g^{\prime}(h_{i})\left(
\delta_{ip}V_{q} + \sum_{j}w_{ij}\frac{\partial V_{j}}{\partial w_{pq}}
\right)
$$</p>
</blockquote>
<p>If only a stable attractor interested, let $\partial_{w_{pq}}V_{i}(t)=\partial_{w_{pq}}V_{i}(t-1)$, or $\partial_{w_{pq}}V_{i}(0)=0$</p>
<blockquote>
<p>$N$ units, $N^{3}$ derivatives($N$ neurons $\times N^{2}$ weights), $\mathcal{O}(N^{4})$ time complexity($N^{3}$ derivatives $\times N$ nodes).</p>
</blockquote>
<p>Real-time recurrent learning: Weight update in real time</p>
<blockquote>
<p>Avoid array storage.</p>
</blockquote>
<p><strong>teacher forcing</strong>: always replace $V_{k}(t)$ by $\zeta_{k}(t)$ after $E_{k}(t)$ &amp; derivatives computed.</p>
<blockquote>
<p>åŠ é€Ÿè®­ç»ƒæ”¶æ•›, é¿å…æ—©æœŸé¢„æµ‹é”™è¯¯çš„ç§¯ç´¯.</p>
</blockquote>
<p>Attractor under teacher forcing may be removed when the network runs freely, even turn into a repellor(æ’æ–¥å­)</p>
<p><strong>flip-flop(è§¦å‘å™¨)</strong>: if &ldquo;A $\to$ (arbitrary interval) $\to$ B&rdquo;: output a signal.</p>
<blockquote>
<p>Tapped delay line could never do this.</p>
</blockquote>
<h3 id="time-dependent-recurrent-back-propagationå«æ—¶é€’å½’åå‘ä¼ æ’­">Time-Dependent Recurrent Back-Propagation(å«æ—¶é€’å½’åå‘ä¼ æ’­)<a hidden class="anchor" aria-hidden="true" href="#time-dependent-recurrent-back-propagationå«æ—¶é€’å½’åå‘ä¼ æ’­">#</a></h3>
<p><strong>dynamics</strong>:</p>
<p>$$
\tau_{i}\frac{\mathrm{d}V_{i}}{\mathrm{d}t} = -V_{i} + g\left(
\sum_{j}w_{ij}V_{j}
\right) + \xi_{i}(t)\tag{*}
$$</p>
<blockquote>
<p>Recall: Recurrent Back-Propagation
$$
\tau\frac{\mathrm{d}V_{i}}{\mathrm{d}t} = -V_{i} + g\left(\sum_{j}w_{ij}V_{j} + \xi_{i}\right)
$$</p>
</blockquote>
<p><strong>Error function</strong>:</p>
<p>$$
E = \frac{1}{2}\int_{0}^{T}\sum_{k\in O}[V_{k}(t)-\zeta_{k}(t)]^{2}\mathrm{d}t
$$</p>
<blockquote>
<p>$O$: output units</p>
<p>$\zeta_{k}(t)$: target output</p>
</blockquote>
<p><strong>Gradient descent</strong>:</p>
<ol>
<li>functional derivative(æ³›å‡½å¯¼æ•°):</li>
</ol>
<p>$$
E_{k}(t) = \frac{\delta E}{\delta V_{k}(t)} = [V_{k}(t) - \zeta_{k}(t)]
$$</p>
<ol start="2">
<li>gradient:</li>
</ol>
<p>$$
\frac{\partial E}{\partial w_{ij}} = \frac{1}{\tau_{i}}\int_{0}^{T}Y_{i}g^{\prime}(h_{i})V_{j}\mathrm{d}t
$$</p>
<p>å¯ä»¥é€šè¿‡ $\begin{aligned}\frac{\partial E}{\partial \tau_{i}}\end{aligned}$ ä»è€Œé€šè¿‡è°ƒæ•´ $\tau_{i}$ è¿›è¡Œä¼˜åŒ–.</p>
<blockquote>
<p>$\begin{aligned}h_{i}(t) = \sum_{j}w_{ij}V_{j}(t)\end{aligned}$</p>
<p>$Y_{i}(t)$: solution of</p>
<p>$$
\frac{\mathrm{d}Y_{i}}{\mathrm{d}t} = \frac{1}{\tau_{i}}Y_{i} - \sum_{j}\frac{1}{\tau_{j}}w_{ji}g^{\prime}(h_{j})Y_{j} - E_{i}(t),\quad Y_{i}(T) = 0, \forall i\tag{**}
$$</p>
<blockquote>
<p>recall:</p>
<p>$$
\mathbf{L}_{ij} = \delta_{ij} - g^{\prime}(h_{i})w_{ij}\\
Y_{p} = \sum_{k}E_{k}(\mathbf{L}^{-1})_{kp}
$$
error propagation network dynamics:
$$
\tau\frac{\mathrm{d}Y_{i}}{\mathrm{d}t} = -Y_{i} + \sum_{p}g^{\prime}(h_{p})w_{pi}Y_{p} + E_{i}
$$</p>
</blockquote>
</blockquote>
<p>$\int_{0}^{T}(*)\Rightarrow V_{i}(t)$;</p>
<p>$\int_{T}^{0}(**)\Rightarrow Y_{i}(t)$.</p>
<p>so that we get $\begin{aligned}\Delta w_{ij} = -\eta\frac{\partial E}{\partial w_{ij}}\end{aligned}$</p>
<hr>
<p>large time &amp; storage requirements: $N$ fully recurrent units &amp; $K$ time steps ($0\to T$)</p>
<ul>
<li>time per forward-backward pass $\sim N^{2}K$ (Real-Time Recurrent Learning: $\sim N^{4}K$)</li>
<li>memory $\sim aNK+bN^{2}$ (RTRL: $\sim N^{3}$)</li>
</ul>
<h2 id="74-reinforcement-learningå¼ºåŒ–å­¦ä¹ ">7.4 Reinforcement Learning(å¼ºåŒ–å­¦ä¹ )<a hidden class="anchor" aria-hidden="true" href="#74-reinforcement-learningå¼ºåŒ–å­¦ä¹ ">#</a></h2>
<p>No detailed target values. Only &ldquo;right&rdquo; or &ldquo;wrong&rdquo;. <strong>Learn with a critic</strong>, not a teacher.</p>
<p>Introduce <strong>randomness</strong> to explore a correct value(by using stochastic units).</p>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/05/RyBEfAzWkI1j85N.png" alt=""  /></p>
</blockquote>
<p>Reinforcement learning problems:</p>
<ul>
<li>
<p><strong>I</strong>. <strong>Same signal</strong> for a given i-o pair. (IO mapping) No reference to previous outputs.</p>
</li>
<li>
<p><strong>II</strong>. Stochastic environment. Given same io-pair, <strong>fixed probability</strong> distribution of reinforcement. Independent of history.</p>
</li>
</ul>
<p><strong>Two-armed bandit problem(åŒè‡‚èµŒåš)</strong>: ä¸¤ä¸ªæœªçŸ¥æ¦‚ç‡çš„è€è™æœº. æ‰¾åˆ°å¹³å‡æ”¶ç›Šæœ€é«˜çš„ä¸€ä¸ª. Method: stochastic learning automata(éšæœºå­¦ä¹ è‡ªåŠ¨æœº)</p>
<ul>
<li><strong>III</strong>. Reinforcement &amp; input patterns depend on the output history <strong>arbitrarily</strong>. Game theory(åšå¼ˆè®º): <strong>&ldquo;environment&rdquo; = other players.</strong></li>
</ul>
<p>[Example] Chess. Win/lose after a long sequence of moves. <strong>Credit assignment problem</strong>: how to judge each move&rsquo;s contribution to victory/defeat?</p>
<h3 id="associative-reward-penaltyè”æƒ³å¥–æƒ©-a_textrp">Associative Reward-Penalty(è”æƒ³å¥–æƒ©) $A_{\text{RP}}$<a hidden class="anchor" aria-hidden="true" href="#associative-reward-penaltyè”æƒ³å¥–æƒ©-a_textrp">#</a></h3>
<p>$S_{i}=\pm 1$: output units</p>
<p><strong>Dynamical rule</strong>:</p>
<p>$$
P(S_{i} = \pm 1) = g(\pm h_{i}) = f_{\beta}(\pm h_{i}) = \frac{1}{1+\exp{(\mp 2\beta h_{i})}}
$$</p>
<blockquote>
<p>$$
h_{i} = \sum_{j}w_{ij}V_{j}
$$</p>
<p>$V_{j}$: activations of hidden/input units.</p>
</blockquote>
<p>Reinforcement signal $r=\pm 1$ (reward/penalty).</p>
<p>Target patterns:</p>
<p>$$
\zeta_{i}^{\mu} = \begin{cases}
+S_{i}^{\mu} &amp; \text{if }r^{\mu} = +1\\
-S_{i}^{\mu} &amp; \text{if }r^{\mu} = -1
\end{cases}
$$</p>
<blockquote>
<p>Recall: Stochastic Units</p>
<p>$$
\begin{aligned}
P(S_{i}=\pm 1) &amp;= f_{\beta}(\pm h_{i}) = \frac{1}{1+\exp{(\mp 2\beta h_{i})}},\quad h_{i}^{\mu} = \sum_{k}w_{ik}\xi_{k}^{\mu}\\
\langle S_{i}^{\mu}\rangle &amp;= (+1)g(h_{i}^{\mu}) + (-1)[1-g(h_{i}^{\mu})] = \tanh{\left(\beta h_{i}^{\mu}\right)}\\
\Delta w_{ik} &amp;= \eta\delta_{i}^{\mu}\xi_{k}^{\mu}, \quad \delta_{i}^{\mu} = \zeta_{i}^{\mu} - \langle S_{i}^{\mu}\rangle
\end{aligned}
$$</p>
</blockquote>
<p><strong>Weight update rule</strong></p>
<p>$$
\Delta w_{ij} = \eta(r^{\mu})\delta_{i}^{\mu}V_{j}^{\mu}
$$</p>
<p>$\times g^{\prime}(h_{i}^{\mu})$ to minimize $\begin{aligned}\sum_{i}(\delta_{i}^{\mu})^{2}\end{aligned}$ (not for entropic cost function).</p>
<blockquote>
<p>$\eta(+1)$ is 10-100 times larger than $\eta(-1)$</p>
</blockquote>
<p>So the <strong>learning rule</strong></p>
<p>$$
\Delta w_{ij} = \begin{cases}
\eta^{+}[+S_{i}^{\mu}-\langle S_{i}^{\mu}\rangle]V_{j}^{\mu} &amp; \text{if }r^{\mu} = +1\\
\eta^{-}[-S_{i}^{\mu}-\langle S_{i}^{\mu}\rangle]V_{j}^{\mu} &amp; \text{if }r^{\mu} = -1
\end{cases}
$$</p>
<blockquote>
<p>$\eta^{\pm 1} = \eta(\pm 1)$.</p>
</blockquote>
<p>Slow compared to supervised learning. Speed depends on the size of the output space and correct fraction of it.</p>
<p>[Example] Only 1 correct answer for each input, the search can be very long.</p>
<p>Variations on $A_{\text{RP}}$:</p>
<ul>
<li>0/1 units. $-S_{i}^{\mu}\to 1-S_{i}^{\mu}$, $\langle S_{i}^{\mu}\rangle\to p_{i}^{\mu}$.</li>
<li>continuous-valued reward $r\in [0,1]$ for $[\text{terrible}, \text{excellent}]$. simplify the formula:</li>
</ul>
<p>$$
\Delta w_{ij} = \eta(r^{\mu})\{
r^{\mu}[S_{i}^{\mu} - \langle S_{i}^{\mu}\rangle] + (1-r^{\mu})[-S_{i}^{\mu} - \langle S_{i}^{\mu}\rangle]
\}V_{j}^{\mu}
$$</p>
<ul>
<li>for penalty case, use $\langle S_{i}^{\mu}\rangle - S_{i}^{\mu}$ instead of $-S_{i}^{\mu} - \langle S_{i}^{\mu}\rangle$:</li>
</ul>
<p>$$
\Delta w_{ij} = \eta r^{\mu}[S_{i}^{\mu}-\langle S_{i}^{\mu}\rangle]V_{j}^{\mu}
$$</p>
<blockquote>
<p>ignore dependence of $\eta$ on $r$.</p>
</blockquote>
<p>$r=-1$. æ ‡å‡†å½¢å¼: å’Œç›®å‰æ‰€åšç›¸å; æ­¤å½¢å¼: ä¸è¦åšç›®å‰æ‰€åš.</p>
<blockquote>
<p>standard form works better for larger weight changes.</p>
<p>this variation is more amenable to theoretical analysis.</p>
</blockquote>
<ul>
<li>speed $\uparrow$: present $\mu$ several times before moving to the next pattern. batch mode: accumulating all $\Delta w$</li>
</ul>
<h3 id="theory-of-associative-reward-penaltyè”æƒ³å¥–æƒ©ç†è®º">Theory of Associative Reward-Penalty(è”æƒ³å¥–æƒ©ç†è®º)<a hidden class="anchor" aria-hidden="true" href="#theory-of-associative-reward-penaltyè”æƒ³å¥–æƒ©ç†è®º">#</a></h3>
<p>Convergence: proved only in special cases.</p>
<p>Usual approach: Cost function $\to \epsilon$</p>
<blockquote>
<p>Not so satisfactory for the stochastic process.</p>
</blockquote>
<p>Cost function: $-\langle r\rangle$</p>
<blockquote>
<p>$E\downarrow \Rightarrow \langle r\rangle\uparrow$</p>
</blockquote>
<hr>
<p>$V_{j}\to \xi_{j}$. Deterministic environment(class I). So $r = r(\mathbf{S},\mathbf{\xi})$</p>
<blockquote>
<p>$\mathbf{S}$: output pattern; $\mathbf{\xi}$: input pattern.</p>
</blockquote>
<p>$$
\langle r^{\mu}\rangle = \sum_{\mathbf{S}}P(\mathbf{S}|\mathbf{w},\xi^{\mu})r(\mathbf{S},\xi^{\mu})
$$</p>
<blockquote>
<p>$P(\mathbf{S}|\mathbf{w},\mathbf{\xi}^{\mu})$: prob of $\mathbf{S}$ if $\mathbf{w}$ and $\mathbf{\xi}^{\mu}$.</p>
<p>$$
P(\mathbf{S}|\mathbf{w},\mathbf{\xi}^{\mu}) = \prod_{k}\begin{cases}
g(h_{k}^{\mu}) &amp; \text{if }S_{k} = +1\\
1-g(h_{k}^{\mu}) &amp; \text{if }S_{k} = -1
\end{cases},\quad h_{k}^{\mu} = \sum_{j}w_{kj}\xi_{j}^{\mu}
$$</p>
</blockquote>
<p>Gradient:</p>
<ul>
<li>differentiating $P$:</li>
</ul>
<p>$$
\frac{\partial P(\mathbf{S}|\mathbf{w},\mathbf{\xi}^{\mu})}{\partial w_{ij}} = \left(\prod_{k\neq i}\begin{cases}
g(h_{k}^{\mu}) &amp; \text{if }S_{k} = +1\\
1-g(h_{k}^{\mu}) &amp; \text{if }S_{k} = -1
\end{cases}\right)\begin{cases}
g^{\prime}(h_{i}^{\mu})V_{j}^{\mu} &amp; \text{if }S_{i} = +1\\
-g^{\prime}(h_{i}^{\mu})V_{j}^{\mu} &amp; \text{if }S_{i} = -1
\end{cases}\\
= P(\mathbf{S}|\mathbf{w},\mathbf{\xi}^{\mu})\left(\begin{cases}
g^{\prime}(h_{i}^{\mu})/g(h_{i}^{\mu}) &amp; \text{if }S_{i} = +1\\
-g^{\prime}(h_{i}^{\mu})/[1-g(h_{i}^{\mu})] &amp; \text{if }S_{i} = -1
\end{cases}\right)V_{j}^{\mu}
$$</p>
<blockquote>
<p>$g(h) = \frac{1}{1+e^{-2\beta h}}$, $g^{\prime}(h) = 2\beta g(h)[1-g(h)]$</p>
<p>$\langle S\rangle = \tanh{(\beta h)} = \frac{e^{\beta h} - e^{-\beta h}}{e^{\beta h} + e^{-\beta h}}$</p>
<p>$\Rightarrow g(h) = \frac{1}{2}(\langle S\rangle + 1)$</p>
</blockquote>
<p>Simplify:</p>
<p>$$
\begin{cases}
g^{\prime}(h_{i}^{\mu})/g(h_{i}^{\mu}) &amp; \text{if }S_{i} = +1\\
-g^{\prime}(h_{i}^{\mu})/[1-g(h_{i}^{\mu})] &amp; \text{if }S_{i} = -1
\end{cases} = \beta[S_{i}^{\mu} - \langle S_{i}^{\mu}\rangle]
$$</p>
<ul>
<li>differentiating $\langle r^{\mu}\rangle$:</li>
</ul>
<p>$$
\begin{aligned}
\frac{\partial\langle r\rangle^{\mu}}{\partial w_{ij}} &amp;= \beta\sum_{\mathbf{S}}P(\mathbf{S}|\mathbf{w},\mathbf{\xi}^{\mu})r(\mathbf{S},\mathbf{\xi}^{\mu})[S_{i}^{\mu}-\langle S_{i}^{\mu}\rangle]V_{j}^{\mu}\\
&amp;= \beta\bigg\langle r(\mathbf{S},\mathbf{\xi}^{\mu})[S_{i}^{\mu}-\langle S_{i}^{\mu}\rangle]\bigg\rangle V_{j}^{\mu}
\end{aligned}
$$</p>
<blockquote>
<p>$\bigg\langle\cdots\bigg\rangle$: over all output patterns $\mathbf{S}$.</p>
</blockquote>
<p>when $\partial_{\mathbf{S}}r(\mathbf{S},\mathbf{\xi}^{\mu})=0$, $\partial_{w_{ij}}\langle r\rangle^{\mu} = 0$, which reaches a correct solution and almost always get $r^{\mu} = +1$.</p>
<p>Update rule:</p>
<p>$$
\begin{aligned}
\langle \Delta w_{ij}\rangle^{\mu} = \frac{\eta}{\beta}\frac{\partial \langle r\rangle^{\mu}}{\partial w_{ij}}\\
\overset{\text{average over all }\mu}{\Rightarrow} \langle \Delta w_{ij}\rangle = \frac{\eta}{\beta}\frac{\partial \langle r\rangle}{\partial w_{ij}}
\end{aligned}
$$</p>
<blockquote>
<p>ä»… reinforcement signal æœ€å¤§å€¼æ—¶æƒé‡åœæ­¢æ›´æ–°.</p>
</blockquote>
<p>$A_{\text{RP}}\overset{\eta^{-}\to 0}{\rightarrow} A_{\text{RI}}$ (associative reward-<strong>inaction</strong> rule)</p>
<h3 id="models--critics">Models &amp; Critics<a hidden class="anchor" aria-hidden="true" href="#models--critics">#</a></h3>
<p>Environment: an auxiliary network(è¾…åŠ©ç½‘ç»œ). Provide a target for each output (instead of global $r$).</p>
<blockquote>
<p><img loading="lazy" src="https://s2.loli.net/2025/12/07/evLDoz6ZFV83HOW.png" alt=""  /></p>
<p>model. <strong>evaluation unit(è¯„ä¼°å•å…ƒ)</strong>: continuous-valued output unit $R$. $R\approx r$: good model.</p>
</blockquote>
<p>å…ˆè®­ç»ƒ model, å†è®­ç»ƒä¸»ç½‘ç»œ, ä½¿å¾— $R$ æœ€å¤§.</p>
<blockquote>
<p>é€šè¿‡ back-propagation å®ç°: é€šè¿‡ $\delta = -R$ ä¼ å›ä¸»ç½‘ç»œå„å•å…ƒç‹¬ç«‹çš„è¯¯å·®ä¿¡å·. æ­¤æ—¶å¯ä½¿ç”¨å¸¸è§„çš„ç›‘ç£å­¦ä¹ .</p>
</blockquote>
<p>æœ€å°åŒ– $(R-r)^{2}$ çš„åŒæ—¶æœ€å¤§åŒ– $r$. æœ€ä½³ç­–ç•¥æ˜¯ä¼˜å…ˆè®­ç»ƒ model network.</p>
<p>II:</p>
<ul>
<li>average fluctuations from environment.</li>
<li>chase meaningless $r$ fluctuation</li>
<li>easy to maximize $R$.</li>
</ul>
<p>III:</p>
<ul>
<li>should exstimate a weighted average of the future reinforcement, for cumulative, not instantaneous value. (sacrifice shot-term reward for long-term gain)</li>
</ul>
<p><strong>critic</strong>: raw $r\overset{\text{critic}}{\to}$ processed signal $\rho$</p>
<p><strong>reinforcement comparison</strong>: critic generate a prediction $R$ of $r$, could use $\rho = r-R$. ($\rho&gt;0$: reward; $\rho&lt;0$: penalty)</p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Muatyz.github.io/posts/read/theory-of-neural-computation/theory-of-neural-computation-6/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>Multi-Layer Networks</span>
  </a>
  <a class="next" href="https://Muatyz.github.io/posts/read/reference/statistical-mechanics-for-networks-of-real-neurons/statistical-mechanics-for-networks-of-real-neurons-7/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>Renormalization group for neurons</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>





<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>
    <div id="tcomment"></div>
    <script src="https://cdn.staticfile.org/twikoo/1.6.16/twikoo.all.min.js"></script>
    <script>
        twikoo.init({
            envId: "https://twikoo-api-one-xi.vercel.app",  
            el: "#tcomment",
            lang: 'zh-CN',
            region: 'ap-shanghai',  
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        });
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>Muartz</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script><script>

    let detail = document.getElementsByClassName('details')
   
    details = [].slice.call(detail);
   
    for (let index = 0; index < details.length; index++) {
   
    let element = details[index]
   
    const summary = element.getElementsByClassName('details-summary')[0];
   
    if (summary) {
   
    summary.addEventListener('click', () => {
   
    element.classList.toggle('open');
   
    }, false);
   
    }
   
    }
   
   </script>   

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>


<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        
        
        
        
        
        
        
        
        
        

        
        
        
        
        
        
        
        
        
        
        

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>


<script>
    
    document.addEventListener('keydown', function(event) {
      
      if (event.key === 'j') {
        
        var nextPageLink = document.querySelector('.pagination-item.pagination-next > a');
        if (nextPageLink) {
          nextPageLink.click();
        }
      } else if (event.key === 'k') {
        
        var prevPageLink = document.querySelector('.pagination-item.pagination-previous > a');
        if (prevPageLink) {
          prevPageLink.click();
        }
      }
    });
  </script>
  
</body>







<body>
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
</body>


</html>
